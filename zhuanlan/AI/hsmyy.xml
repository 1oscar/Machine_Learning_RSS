<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Wed, 17 Aug 2016 20:31:13 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>CNN的数值实验</title><link>https://zhuanlan.zhihu.com/p/22027076</link><description>前面我们聊过了Caffe的整体架构，相信各位对Caffe的结构已经比较熟悉了，下面我们就来看看CNN中的一些细节问题，也顺着前面的思路进一步进行。&lt;h2&gt;序&lt;/h2&gt;&lt;p&gt;在好久之前介绍ReLU的时候，我们曾经提到ReLU相比于Sigmoid的一些优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sigmoid整体的梯度偏小，在外向传导的过程中会不断让梯度的幅度变小；&lt;/li&gt;&lt;li&gt;Sigmoid存在一个梯度饱和问题：那就是如果非线性部分的输出靠近0或者1，那么反向传导到了这里，Sigmoid的梯度会接近于0，这样就会导致梯度到这里就传导不下去了。从前面我们的那张图中可以看得出来。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有了这两个原因，我们认为ReLU这样的非线性函数在前向后向的传递性方面效果更好，如果CNN的网络层数比较深，ReLU更能够保证梯度的无损传递。&lt;/p&gt;&lt;p&gt;当我们的前辈完成了这个重大发现之后，大家都觉得“这次终于稳了”，AlexNet也横空出世，让CNN大放异彩。那个时候AlexNet已经算是非常"Deep"的网络了……&lt;/p&gt;&lt;p&gt;可是后来，大家对CNN模型能力的诉求越来越高，大家通过不断地实验，慢慢发现，现在的模型还不够深，想要模型够牛逼，模型的深度还得继续增加啊！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1049017686e7ebefdae7367397fa052e.jpg" data-rawwidth="296" data-rawheight="220"&gt;可是随着后面模型的深度不断增加，我们发现模型的训练过程变得越来越不可控，有时候一个不留神模型就训练溢出了，有时候模型训练着训练着就不动了。训练模型也变得越来越像炼丹了……&lt;/p&gt;&lt;p&gt;炼丹不是一件让人开心的事情，能炼好固然让人开心，但是炼不好就会让宝宝有情绪了。于是各位前辈又重新出发，试图找出新的方法解决这些恼人的问题。&lt;/p&gt;&lt;p&gt;当年众位大神齐聚一堂，开始讨论究竟什么样招式能破解这个困局。有的大神选择从数值的方向入手，有的大神选择从模型结构的方向入手，有的大神选择从模型结构入手，还有的大神走了其他的方法。一场“拯救大兵CNN”的好戏也由此上演……&lt;/p&gt;&lt;h2&gt;一个数值的小实验&lt;/h2&gt;&lt;p&gt;好了，让我们回到MNIST这种比较简单的问题上。我们也曾经展示过Caffe中MNIST的例子，以下这个模型是我们曾经用过的模型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Conv 20*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Conv 50*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Ip  500 - ReLU&lt;/li&gt;&lt;li&gt;Ip 10&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这一次我们将debug_info打开，记录一下网络中每一层数据的数值信息。Caffe中的debug_info默认可以给出所有数据的L1 norm，也就是绝对值的和。这个数值可以在一定程度上反映数据的幅度。如果数据比较大，那么说明整体的数值比较大。&lt;/p&gt;&lt;p&gt;完成了10000轮模型的训练，模型最终在test集合得到了0.991的精度，可以说精度已经很高了。那么各个层的数据表现如何呢？这里我们主要关注以下的数值：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;两个conv层和两个ip层的top data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的diff信息&lt;/li&gt;&lt;li&gt;所有参数的data和diff的L1 norm和L2 norm&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先是top data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8397d75b85e7c1af80e554bf62ff6e6a.png" data-rawwidth="918" data-rawheight="433"&gt;&lt;p&gt;可以看出除了一开始的数值波动，后面的迭代中数值整体表现比较稳定，稳中有升。&lt;/p&gt;&lt;p&gt;其次是w的data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dc0702632a1d330e7919ce4ce414f281.png" data-rawwidth="917" data-rawheight="437"&gt;&lt;p&gt;可以看出w整体表现不算稳定，不过考虑整个优化的过程就是在改变它，所以它的大幅波动也是可以理解的。&lt;/p&gt;&lt;p&gt;接下来是w的diff&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/72b4e4e7458586f86af667ea2262e1d4.png" data-rawwidth="919" data-rawheight="435"&gt;&lt;p&gt;可以看出diff在数值上表现得也比较稳定，基本上处于一个小的区间之中。&lt;/p&gt;&lt;p&gt;最后是L1 norm和L2 norm&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7ebf6288c1845e5a000dfaae4c8e37f3.png" data-rawwidth="914" data-rawheight="428"&gt;&lt;p&gt;norm和diff的表现总体上和前面展现的一致。&lt;/p&gt;&lt;p&gt;总体来说，topdata和w的diff数值没有太大的波动，保持在一个稳定幅度上，所以整体的数值比较稳定，无论是前向的top data还是后向的diff data没有出现太大的数值波动或者数值幅度问题。&lt;/p&gt;&lt;p&gt;看完了这个成功案例，我们再看看失败案例，当然这个失败案例一般不会出现，只是做个效果而已，不过这个效果也足以说明一些问题。&lt;/p&gt;&lt;p&gt;这个失败案例直接来自于刚才的例子。我们在刚才的模型上只修改一个地方：四个核心层（conv1,conv2,ip1,ip2）的w参数的初始化方法。&lt;/p&gt;&lt;p&gt;原来的初始化方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "xavier"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们将它们改成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "gaussian"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样的改法是比较脑残的，这相当于对参数采取均值为0，方差为1的高斯分布对参数进行初始化……&lt;/p&gt;&lt;p&gt;不用说，这个模型会死的很难看，10000轮训练结束后，最终的精度为0.1135。要知道一共只有10个数字，猜也可以达到0.1的精度。这个模型采用了各种高科技，结果却只比猜稍微好一点，简直弱到家了。&lt;/p&gt;&lt;p&gt;那么它在数值上的表现是怎样呢？直接来看图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/955f787ae8dc46b137e3f7fb077ad6d5.png" data-rawwidth="916" data-rawheight="419"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e2e676bf99194f1c89d7a6263d30a7c9.png" data-rawwidth="920" data-rawheight="435"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/54a5a4b652a4d1d4d6630d87c93cf593.png" data-rawwidth="916" data-rawheight="428"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e5b81d4199d1658ef8f0e419ea1a7f5.png" data-rawwidth="914" data-rawheight="429"&gt;&lt;p&gt;从图中可以看出，各个数据在幅度上的差距都非常大。尤其是最后一张图，data和diff在数值上的差距拉得非常大，那么小量的diff对大量的data完全起不到改变的作用。这么看来一定是初始化的锅了。&lt;/p&gt;&lt;p&gt;那真的完全是初始化的锅么？众人问：初始化同学，你还有没有什么要补充的？&lt;/p&gt;&lt;p&gt;初始化同学：其实是因为我和ReLU同学不熟，你们换Sigmoid试试……&lt;/p&gt;&lt;p&gt;众人：当真？&lt;/p&gt;&lt;p&gt;于是乎我们开始了第三个实验……&lt;/p&gt;&lt;h2&gt;第三个实验&lt;/h2&gt;&lt;p&gt;为了洗刷初始化的清白，我们在保持初始化的基础上，将3个非线性的部分重新换成sigmoid，由sigmoid再度出马。sigmoid表示这种小场面它Hold住……&lt;/p&gt;&lt;p&gt;于是我们来看看sigmoid的最终结果：0.9538，相关的数值图在这里就不全贴了，只贴一张L1 L2 norm的图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6f231a2392460c8b6b0b89c64ce3b312.png" data-rawwidth="914" data-rawheight="425"&gt;&lt;p&gt;从这个图上来看，虽然data的幅度很大，但是diff的幅度也很大，基本上算旗鼓相当。&lt;/p&gt;&lt;p&gt;这个最终的精确率不能算特别高，但是起码说明模型还是学到了很多有用的知识。而且和0.11的精度相比，简直是一个天上一个地下，完全没得比啊！大家纷纷表示关键时候还得靠老将才能扛得住，初始化同学是被冤枉了。&lt;/p&gt;&lt;p&gt;这时候众人重新以怀疑的眼光看着ReLU，原来真正的问题出你这啊！你的表现有点浪啊！&lt;/p&gt;&lt;p&gt;ReLU委屈地表示：我还能再说一句么？&lt;/p&gt;&lt;p&gt;众人表示：这次的版面已经不够了，有话下次再说……&lt;/p&gt;&lt;h2&gt;继续广告……&lt;/h2&gt;&lt;p&gt;欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;</description><author>冯超</author><pubDate>Tue, 16 Aug 2016 19:17:23 GMT</pubDate></item><item><title>Caffe代码阅读——Solver</title><link>https://zhuanlan.zhihu.com/p/21800004</link><description>前面我们聊了Net组装的内容，接下来我们来看看Solver的内容。Solver主体有两部分：初始化和训练。初始化内容相对比较简单，这里就不说了；下面我们来说说训练中的几个关键函数。&lt;h2&gt;核心函数：Step&lt;/h2&gt;&lt;p&gt;真正的训练在Step函数内，这里有多卡训练的关键回调函数：on_start()和on_gradient_ready()，具体的调用方法我们后面再说，在这两个回调函数中间有两个重要的过程：ForwardBackward和UpdateSmoothedLoss。在on_gradient_ready之后有一个关键函数ApplyUpdate()，这里面的代码在Sgd_solver中。下面我们详细看一下。&lt;/p&gt;&lt;h2&gt;ForwardBackward&lt;/h2&gt;&lt;p&gt;这里主要调用了Net中的代码，主要完成了前向后向的计算，前向用于计算模型的最终输出和Loss，后向用于计算每一层网络和参数的梯度。对于前向后向的具体内容这里需要详细叙述了，唯一值得一提的是前向的Loss计算，这部分代码实际上实在Layer里面，具体涉及到loss_weight这个参数相关的初始化和loss()的判断，同时还有Loss_Layer在Setup函数中的初始化。&lt;/p&gt;&lt;h2&gt;UpdateSmoothedLoss&lt;/h2&gt;&lt;p&gt;这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题。代码中的平滑方法比较简单，大家一看便知。&lt;/p&gt;&lt;p&gt;下面就是ApplyUpdate函数，这个函数真正完成了参数更新的任务。Caffe的参数更新只利用了模型的梯度信息，没有利用二阶信息。下面就详细介绍下更新参数的几个过程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;GetLearningRate&lt;/li&gt;&lt;li&gt;ClipGradients&lt;/li&gt;&lt;li&gt;Normalize&lt;/li&gt;&lt;li&gt;Regularize&lt;/li&gt;&lt;li&gt;ComputeUpdateValue&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;GetLearningRate&lt;/h2&gt;&lt;p&gt;learning rate的故事我们前面已经聊过了，在CNN训练中这确实是个大问题。Caffe为了让learning rate的设计更灵活，提供了一系列的learning rate方案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;fixed&lt;/b&gt;：lr永远不变&lt;/li&gt;&lt;li&gt;&lt;b&gt;step&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter / stepsize}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;exp&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;inv&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1+gamma*iter)^{-power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;multistep&lt;/b&gt;：直接写iter在某个范围内时lr应该是多少&lt;/li&gt;&lt;li&gt;&lt;b&gt;poly&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1-\frac{iter}{maxiter})^{power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;sigmoid&lt;/b&gt;：&lt;equation&gt;lr=baselr*\frac{1}{1+e^{-gamma*(iter-stepsize)}}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;这些方案各有优劣，选择自己顺手的就好。&lt;h2&gt;ClipGradients&lt;/h2&gt;&lt;p&gt;这一步主要是对梯度值做一个限制，如果梯度值过大，那么这里就会对梯度做一个修剪，对所有的参数乘以一个缩放因子，使得所有参数的平方和不超过参数中设定的梯度总值。这个功能感觉上像是对全局函数设置了一个Trust Region，可以防止更新的量过大二导致梯度发散。我认为这一步的想法是很好的，但是实际操作中可能会有问题。实际中可能只有部分参数的梯度比较大，而其他参数的梯度本身比较小，那么对所有的参数乘以相同的因子会让一些本来比较小的参数变得更小，这样会带来一些不公平。&lt;/p&gt;&lt;h2&gt;Normalize&lt;/h2&gt;&lt;p&gt;这一步同样考虑了一些单一Batch不足以完成训练的问题，通过限制每个Batch的更新量来控制更新总量，代码比较简单。&lt;/p&gt;&lt;h2&gt;Regularize&lt;/h2&gt;&lt;p&gt;到这一步终于要计算正则项的梯度了。Caffe提供两种正则方法——L2和L1，其中L2采用了标准的梯度下降方法，L1采用了sub-gradient的计算方法。L2的优化计算比较简单，没有什么好说的，但是L1的计算还是有点值得玩味的地方的。这里采用的sub-gradient方法其实本身没有什么问题，不过Lasso的优化还可以有其他的方法，这个问题以后可以再细聊。&lt;/p&gt;&lt;h2&gt;ComputeUpdateValue&lt;/h2&gt;&lt;p&gt;到这里，我们终于来到了梯度计算的最后一站，这时候我们终于完成了对梯度的计算，下面该考虑lr和梯度结合起来如何计算最终的梯度优化值了。sgd方法主要采用momentum加梯度的优化方法。关于momentum的优势我们前面已经聊过了。除此之外，Caffe还提供了一系列的梯度计算方法，这些优化方法各有特点，以后我们可以慢慢来看。&lt;/p&gt;&lt;p&gt;当计算完这一步，我们就可以调用Blob中的Update把每个参数的data和diff进行相加，计算出最终的结果。这样，整个优化过程就完成了。至于剩下的一些内容都不是核心过程，就略去不看了。&lt;/p&gt;&lt;p&gt;如果我们采用单卡训练的策略，那么阅读代码到这里也差不多了。不过多卡训练对于大规模的训练任务来说是必不可少的，所以我们接下来趁热打铁地看看Caffe的多卡训练。&lt;/p&gt;&lt;h2&gt;多卡训练算法&lt;/h2&gt;&lt;p&gt;Caffe的多卡训练算法总体思路是数据并行，我们用不同的GPU处理不同的数据，然后将所有的梯度更新汇总。由于Solver在训练中给了两个回调函数，多卡训练也主要利用了这两个回调函数进行：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;on_start()：将参数拷贝到每一个GPU中。&lt;/li&gt;&lt;li&gt;ForwardBackward()：每个GPU各自计算自己的前向后向结果。&lt;/li&gt;&lt;li&gt;on_gradient_ready()：将反向梯度汇总到一起。&lt;/li&gt;&lt;li&gt;ApplyUpdate()：在汇总的线程上进行参数更新&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中第2步由每一个CPU线程和自己的GPU并行完成，第4步由汇总的CPU和自己的GPU完成，剩下的1，3两步主要是完成数据传输的任务，也是多卡计算中主要完成的部分。&lt;/p&gt;&lt;p&gt;Caffe采用树型结构进行参数传递，其中一个CPU线程和GPU作为树型结构的根，其他的则作为根下面的节点。为了更快地传输GPU数据，树型结构的构建要考虑GPU之间是否相近，比方说两个GPU之间是否可以进行P2P的直传。在前面的翻译博客中我们已经聊过GPU之间数据传输的问题了，这里的树形结构也主要以此做考虑。&lt;/p&gt;&lt;p&gt;我们假设4块GPU的拓扑结构如下：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3  
GPU0   X     PHB    SOC    SOC    
GPU1   PHB    X     SOC    SOC    
GPU2   SOC    SOC    X     PHB   
GPU3   SOC    SOC    PHB    X     &lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;那么我们构造出的树型结构如下所示，数据传输也是按照这样的结构传输：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2b57bad80fd32d75715706d642836647.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;这样1，3的数据传递就解决了，具体的过程请详细阅读代码，这里就不叙述了。&lt;/p&gt;&lt;p&gt;对Caffe代码的基本介绍就到这里了，我们对代码的整体结构有了比较清晰的认识，下面我们将分析模型中各个部分的特性。&lt;/p&gt;&lt;h2&gt;最后的私货&lt;/h2&gt;&lt;p&gt;欢迎大家加入“我爱机器学习”QQ群：564533376，（省略情怀文若干字……）一起学习机器学习，一起成长！&lt;/p&gt;</description><author>冯超</author><pubDate>Thu, 11 Aug 2016 20:27:25 GMT</pubDate></item><item><title>[翻译]Exploring the Complexities of PCIe Connectivity and Peer-to-Peer Communication</title><link>https://zhuanlan.zhihu.com/p/21908564</link><description>本来准备今天开始写Caffe代码阅读的第三部分的，结果找到了一片介绍GPU通信的文章，觉得这篇文章的内容讲得非常好，而这方面的背景知识也是非常缺乏的，另外Caffe多卡训练的一些细节和这里面的知识也有关系，所以今天就来翻译一下这篇文章。&lt;p&gt;当然了，翻译之前先扯一段。读大学的时候我曾经报名过一节英文翻译课，当时教过我的一些具体的翻译小技巧我已经忘光了，但是有一段话我还记得，那就是翻译的三个目标——信、达、雅。翻译首先要准确，然后要通顺，最后要能做到优美那就再好不过了。这里不免想吐槽一下一些文章翻译，在达的方面做得实在不够好，我都可以从中文翻译中读出原文定语从句的味道……我个人希望自己能做到达，最后不需要做到雅，做到俗也是极好的。&lt;/p&gt;&lt;p&gt;文章出处：&lt;a href="http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/" class=""&gt;http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原作者：Ross Walker（玫瑰行走者？）&lt;/p&gt;&lt;p&gt;这篇文章我们来深入地看看PCI-E总线上数据通信方面存在的一些瓶颈，以及我们Exxact公司最新的牛逼系统是如何搞定这些问题，使得像机器学习这样严重依赖GPU的工作可以轻松点。&lt;/p&gt;&lt;h2&gt;传统套路&lt;/h2&gt;&lt;p&gt;PCI-E是个啥本文是没有兴趣给你讲的，我们直接来谈谈以它为主线的架构，以及架构的优势和劣势（主要是劣势）。在此之前，我们首先要明白一件事，那就是为啥PCI-E总线的数据传输速度是十分重要的。在GPU计算革命到来之前，PCI总线主要是用于和磁盘做数据通信，或者计算机结点间通信的（HPC小红人infiniband）。一个经典的PCI-E总线架构图就长成这个样子：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/8e11038678ca5afe5dd2aa485bc3f8e2.jpg" data-rawwidth="1025" data-rawheight="363"&gt;以前这个架构用得很好没问题，但后来GPU计算时代来临，我们搞GPU计算的朋友开始有了新的诉求，我们希望主存数据和GPU显存的数据传输能够尽量快点，于是乎，传统设计就暴露出了三个问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;GPU被分在了两个区域中，就像上面的图片，它们分别被不同的MCH（Memory Controller Hub）控制，连接在不同的CPU数据接口上。然后，不同区域的CPU在通过一个叫QPI的东西连接起来，以保证不同区域的数据也能够通信。当然这样只能说解决了数据可以传输的问题，但问题是，对于处于不同区域的GPU，比方说GPU0和GPU2，如果它们两之间想传递数据（多卡训练的时候你会遇到的）,就必须翻山越岭穿越层层障碍，绕个大圈才能把数据传过来。所以……它就是慢嘛，数据吞吐量和延时的数据肯定就不好看了。&lt;/li&gt;&lt;li&gt;前面看到同一区域的CPU和GPU之间都是通过PCI-E的通道连接，但是CPU的PCI-E通道是有限的，这个问题相信不少自己装机搞深度学习环境的朋友都遇到过，如果想搞多GPU，CPU的PCI-E通道数一定不能少！那么对于现在这一代的Intel CPU架构Haswell,它是拥有两个数据接口系统，像上面的图一样，每个接口的PCI-E通道数是40个，那么它最多只能接受4个GPU（要多少是多啊……），因为我们现在的GPU每个都需要连16个数据接口，那么一边最多也就连2块GPU。万一你要是觉得4块不够多，想再多搞点，对不起，那也不行，请走分布式的路子，再部署一台机器去，然后让机器间想办法互联（infiniband）。不过很显然，这比单机要慢一些了。而且40个通道已经被GPU分走32个了，inifiband也需要PCI-E通道的，就给inifiniband分剩下的8个？太瞧不起infiniband了吧？万一跨机间的数据传输速度慢了，infiniband表示这锅它不背。&lt;/li&gt;&lt;li&gt;你以为同在一个区域的GPU就好过了？MCH照样会把它们之间的传输速度拉下去（具体原理我也不懂……）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;“这病听上去感觉还挺严重的，那大夫该怎么判断我有没有得这个病呢？”&lt;/p&gt;&lt;p&gt;“老司机早已为你准备好药方，一个命令+一个程序”&lt;/p&gt;&lt;p&gt;首先是&lt;b&gt;nvidia-smi&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3   CPU Affinity
GPU0   X     PHB    SOC    SOC    0-9,20-29
GPU1   PHB    X     SOC    SOC    0-9,20-29
GPU2   SOC    SOC    X     PHB    10-19,30-39
GPU3   SOC    SOC    PHB    X     10-19,30-39

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的结果可以看到，PHB表示了PCI-E Host Bridge，就是我们上面图中的连接方式，而SOC就是要通过GPU-CPU-QPI-CPU-GPU这样长途跋涉才能传输的连接方式。&lt;/p&gt;&lt;p&gt;其次是CUDA中自带的一个程序，在examples/1_Utilities/p2pBandwidthLatencyTest，编译一下就可以运行，亲测会输出很多内容，以下只显示一部分：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;p2pBandwidthLatencyTest
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   19.18  12.22  11.77
     1  19.17     X   17.07  11.81
     2  12.23  12.17     X   19.17
     3  11.73  11.88  19.18     X
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的数据可以一目了然的看出，在一个区域和在不同区域的带宽差是比较明显的。当然如果不是MCH背锅，这个差距还可以更亮眼一点，16通道的PCI-E理论传输上限是32GB/s。所以不论是同一区域，还是不同区域，这个数据都不够好。&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   3.39   8.18  16.86  16.26
     1   7.22   3.74  13.56  16.54
     2  16.27  16.06   3.52   5.81
     3  15.98  15.92   6.62   3.20
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;传输时延的结果也十分明显，就不说了。&lt;/p&gt;&lt;h2&gt;重新思考以GPU为核心的三个代表母版设计思想&lt;/h2&gt;&lt;p&gt;前面喷了那么多，下面该说说我们设计方案了，首先展示的是我们的关键道具，PCI-E switch chip(PLX)，这家伙的通道数有48，80，96条，这样4块GPU就不用分到2个区域了，而且它可以让机器上GPU的数据传输更快！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5d9371405090e491c0a1ba477119b20d.jpg" data-rawwidth="567" data-rawheight="178"&gt;&lt;h2&gt;产品1： Deep Learning Dev Box&lt;/h2&gt;&lt;p&gt;废话不多说了，直接上架构图……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b499f46fc46e4aa7221eacee961e8e5e.jpg" data-rawwidth="1007" data-rawheight="513"&gt;我们略去浮华的产品介绍，我们的产品加上深度学习软件包，心动价只卖8999刀！8999你买不了吃亏，8999你买不了上当！下面直接看下数据：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
      GPU0  GPU1  GPU2  GPU3  CPU Affinity
GPU0  X    PIX   PHB   PHB   0-11
GPU1  PIX   X    PHB   PHB   0-11
GPU2  PHB   PHB   X    PIX   0-11
GPU3  PHB   PHB   PIX   X    0-11
 

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="text"&gt;Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   26.13  20.31  20.32
     1  25.97     X   20.31  20.32
     2  20.32  20.32     X   26.12
     3  20.32  20.32  26.12     X&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;可以看到，这数据好得吓人，要知道我们距离理论极限32GB/s已经不远了。再看看时延：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   4.15   6.10   6.27   6.05
     1   6.10   4.13   6.12   6.00
     2   6.31   5.96   4.19   6.04
     3   6.07   5.97   6.15   4.09

&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;在数据面前一切语言都显得那么的苍白，不要犹豫，赶紧订购吧！&lt;/p&gt;&lt;h2&gt;产品2，3，4&lt;/h2&gt;&lt;p&gt;你以为我还会一个挨一个的翻译这些产品介绍？想看就去看原文吧[微笑][微笑]&lt;/p&gt;&lt;h2&gt;总结（非翻译）&lt;/h2&gt;&lt;p&gt;抛开广告的环节，这篇文章很好地向我们解释了多GPU间数据传输这个问题。这个问题总体上算是个硬件层次的问题，那么对于软件方面，我们能做些什么呢？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;让你的CPU和GPU在同一个区域内，减少跨区域的数据拷贝传输&lt;/li&gt;&lt;li&gt;如果涉及到跨区域的GPU数据传输，尽量减少传输的次数，不然真的有可能会让程序慢下来&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;带着这两个结论，我们就可以更好地去看Caffe多卡训练中的一些问题了。&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 10 Aug 2016 00:01:01 GMT</pubDate></item><item><title>Caffe源码阅读——Net组装</title><link>https://zhuanlan.zhihu.com/p/21875025</link><description>&lt;p&gt;最近忙着看TI没有及时写文章，今天赶紧补一篇……&lt;/p&gt;Net是Caffe代码中一个比较核心的类，往下看它封装了所有的Layer，构建起了整个神经网络；往上看它对外提供了前向后向计算，以及核心数据结构的访问结构，使得再上层的Solver可以利用Net比较轻松地实现Train和Test的策略。当然，正是因为它的重要性，组装Net是一个比较复杂的部分。这一回我们就来看看Net的内容。&lt;p&gt;当然，说在前面，看Net组装的代码有两个目的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;了解作为一个成熟的CNN模型框架需要考虑的一些问题；&lt;/li&gt;&lt;li&gt;如果想对网络结构做扩展，如写一个新的Layer，其中的一些数据是如何在Layer和Net之间流动的&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先，为了使问题不那么复杂，我们先从训练模型时输出的log看看Net组装的几个关键步骤，然后再把这个过程慢慢展开，了解组装的所有细节。&lt;/p&gt;&lt;h2&gt;Log眼中的Net组装&lt;/h2&gt;&lt;p&gt;为了更好地展示Net组装的一些细节，我们在这里选取了一个实际例子，就是Caffe的examples里面的siamese model。关于这个model的细节这里就不多说了，感兴趣的可以去看官方或者非官方的文档，这里只提一点：这个网络除了包含其他正常网络中的一些特性之外，还具有网络参数复用的特点，在后面的分析中我们会用到。&lt;/p&gt;&lt;p&gt;下面我们要看的就是Net组装的Log。这段Log一般都是大家在训练网络时一闪而过的大段Log，当然如果它没有一闪而过而是停下来了，有可能是你的网络定义有问题爆出了错误。这段Log内容比较多，总体来说就是Train阶段和Test阶段的两个网络组装起来。我们重点关注其中的几个片段，来大概了解Net组装的一些核心内容，也是那些比较值得打印出来的内容。&lt;/p&gt;&lt;p&gt;首先是一个正常的卷积层conv1，Log如下所示（以下代码的行号可能会有不同，但位置是相近的）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer conv1
net.cpp:92] Creating Layer conv1
net.cpp:428] conv1 &amp;lt;- data
net.cpp:402] conv1 -&amp;gt; conv1
net.cpp:144] Setting up conv1
net.cpp:151] Top shape: 64 20 24 24 (737280)
net.cpp:159] Memory required for data: 3752192
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这其中第一行是创建这个Layer实例的代码，具体的创建过程在layer_factory里面。为了方便创建Layer，Caffe采用了工厂方法的设计模式，只要提供Layer的名字（在配置文件中参数叫type），就可以根据名字和对应参数实例化一个Layer。这部分的细节只要认真看一下就会明白。&lt;/p&gt;&lt;p&gt;第3，4行显示了创建当前层的bottom和top数据的过程。这里涉及到net.cpp中的AppendBottom和AppendTop两个方法，因为每一个bottom blob和top blob都有名字，这里就将他们之间的关系输出在了这里。&lt;/p&gt;&lt;p&gt;第5行看上去没什么干货，但是它代表了Layer的Setup函数已经调用完成（或者Layer被share）。Layer的Setup函数是Layer初始化的关键函数，这里面涉及到以下几个具体的操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;CheckBlobCounts(bottom, top);
LayerSetUp(bottom, top);
Reshape(bottom, top);
SetLossWeights(top);&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;总结地说，这四句完成了：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对bottom blob, top blob数量的检查，父类实现。&lt;/li&gt;&lt;li&gt;对Layer内部相关变量的初始化，由具体的子类实现&lt;/li&gt;&lt;li&gt;传入时bottom blob的维度已经确定，Layer需要根据自己要做的计算确定top blob的纬度。比方说这一层是卷积层，维度是20*5*5，输入图像是1*28*28，也就是bottom blob的维度，那么输入就是20*24*24，这也是上面log里面算出的结果，只不过还加了一个batch size。这个函数由具体的子类实现。&lt;/li&gt;&lt;li&gt;对Layer是否输出loss以及输出loss要做的操作进行初始化。父类实现。必须说一句，Caffe中关于Loss Layer中Loss_weight，loss_，top.cpu_diff的数据设定还是有点绕且有点trick的。&lt;/li&gt;&lt;/ol&gt;好了回到上面的log。接下来的那一句告诉了我们top层应该输出的维度。这里输出了维度就是为了让不放心的朋友算一下，看看和你想的是否一样。当然，输出这句log的循环不是只做了这件事，它的主要工作就是设置top blob的loss_weight。 &lt;p&gt;最后一句计算了该层top blob所占用的内存。可以看出截至到这一层，内存消耗大约是3M多，还不算大。&lt;/p&gt;&lt;p&gt;好，这就是一个最典型的Layer的初始化，下面这个ReLU层就稍微有些不同了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer relu1
net.cpp:92] Creating Layer relu1
net.cpp:428] relu1 &amp;lt;- ip1
net.cpp:389] relu1 -&amp;gt; ip1 (in-place)
net.cpp:144] Setting up relu1
net.cpp:151] Top shape: 64 500 (32000)
net.cpp:159] Memory required for data: 5769472
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里面最不同的就是第4行结尾的（in-place），这说明relu的bottom blob和top blob是同一个数据，这和我们在网络中的定义是一样的。in-place的好处就是减少内存的操作，但是这里在统计内存消耗时并没有考虑in-place带来的节省。&lt;/p&gt;&lt;p&gt;接下来就是共享网络的conv1_p了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer conv1_p
net.cpp:92] Creating Layer conv1_p
net.cpp:428] conv1_p &amp;lt;- data_p
net.cpp:402] conv1_p -&amp;gt; conv1_p
net.cpp:144] Setting up conv1_p
net.cpp:151] Top shape: 64 20 24 24 (737280)
net.cpp:159] Memory required for data: 8721664
net.cpp:488] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
net.cpp:488] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这一段最有特点的是最后两句“Sharing”，因为siamese model中拥有参数完全相同的两个网络，所以在构建时候，第二个网络检测到参数名字已经存在，说明该层的参数和其他层共享，于是在这里打印出来告诉用户这一点。当然，这一句之前没有打印出来的内容告诉了我们，实际上Net类中还负责了参数相关的初始化。这部分的内容实际上还挺多，除了参数共享，还有对参数learning rate，weight decay的设定。&lt;/p&gt;&lt;p&gt;最后是最特别的一层：loss层&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;net.cpp:92] Creating Layer loss
net.cpp:428] loss &amp;lt;- feat
net.cpp:428] loss &amp;lt;- feat_p
net.cpp:428] loss &amp;lt;- sim
net.cpp:402] loss -&amp;gt; loss
net.cpp:144] Setting up loss
net.cpp:151] Top shape: (1)
net.cpp:154]     with loss weight 1
net.cpp:159] Memory required for data: 10742020
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这一层看上去没有什么特别，该有的和前面一样，但是唯一不同的就是它的倒数第二行，这说明这一层是有loss weight的。至于有loss weight有什么用，以后我们会详细说这个事情。这里简单说一下，有loss weight表示这个blob会被用于计算loss。&lt;/p&gt;&lt;p&gt;前面的log主要解决了网络的组装和前向的一些计算，从log中，我们可以看出Net完成了以下的事情：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;实例化Layer&lt;/li&gt;&lt;li&gt;创建bottom blob,top blob&lt;/li&gt;&lt;li&gt;Setup Layer（初始化Layer，确定top blob维度）&lt;/li&gt;&lt;li&gt;确定layer的loss_weight&lt;/li&gt;&lt;li&gt;确定layer的参数是否共享，不共享则创建新的&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;从上面的过程也可以看出，整个网络中所有的流动性变量（bottom blob,top blob）都保存在Net中，同时对于各层的参数，根据各层的共享关系做了标记。这样好处是集中管理了网络中的数据，方便对数据进行操作。&lt;/p&gt;&lt;p&gt;再往下面，我们可以截取一小段log来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;net.cpp:220] pool1 needs backward computation.
net.cpp:220] conv1 needs backward computation.
net.cpp:222] slice_pair does not need backward computation.
net.cpp:222] pair_data does not need backward computation.
net.cpp:264] This network produces output loss
net.cpp:277] Network initialization done.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来是统计一个层次是否需要进行反向传播的计算。一般来说我们的层是都需要计算的，但是也会有一些层不需要计算，比方说数据层，就像上面的log那样，还有就是一些希望固定的层，这个一般在finetune网络的时候用的上。因为反向计算一般比前向计算慢，如果有不需要计算的Layer，直接跳过计算是可以节省时间的。&lt;/p&gt;&lt;p&gt;最后是整个网络产生的输出，这个输出会在训练迭代中显示出来的。&lt;/p&gt;&lt;p&gt;了解了这些，我们就对Net装载有了大概的了解，再去看它的代码就会轻松些。&lt;/p&gt;&lt;p&gt;最后，关于Net类中所有的成员变量与它们之间的关系，我们可以用下面的一张图来理解就好：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/66911c674bd5aae86a9eb088949f3e53.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;把Net的初始化理解后，其实Net以下的架构方面的问题就不多了。下面我再看看Net以上的东西，Solver以及Caffe里“简单”的多卡训练。&lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 06 Aug 2016 22:44:29 GMT</pubDate></item><item><title>Caffe代码阅读——层次结构</title><link>https://zhuanlan.zhihu.com/p/21796890</link><description>Caffe是一款优秀的深度神经网络的开源软件，下面我们来聊聊它的源代码以及它的实现。Caffe的代码整体上可读性很好，架构比较清晰，阅读代码并不算是一件很困难的事情。不过在阅读代码之前还是要回答两个问题：&lt;ol&gt;&lt;li&gt;阅读代码是为了什么？&lt;/li&gt;&lt;li&gt;阅读到什么程度？（这个问题实际上和前面的问题相关）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;阅读代码大体上来说有下面几个目的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;搞清楚代码所实现的算法或者功能。对算法本身不是很了解，希望通过阅读代码了解算法。&lt;/li&gt;&lt;li&gt;搞清楚代码在实现算法过程中的细节。这种情况下，一般对算法已经有大概的了解，读代码是为了了解代码中对算法细节的考量。当然，如果想使用代码，了解代码细节是很有帮助的。&lt;/li&gt;&lt;li&gt;扩展代码。在开源代码的基础上，利用已有的框架，增加或者修改功能，来实现自己想要的功能。这个就需要对代码的架构细节有更加深入的了解。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们的目标是扩展代码。Caffe中主要的扩展点就是Layer和Solver，当然其他的部分也可以扩展，只不过要改动的代码会多一些。&lt;/p&gt;&lt;p&gt;当确定了上面第一个问题，下面就是第二个问题了。读代码要读到什么程度？一般来说，我觉得阅读代码这件事情可以用一个Logistic型的函数来表示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/65a395cb2d773de04047c35af8f320fd.png" data-rawwidth="348" data-rawheight="232"&gt;&lt;p&gt;这个图上，横轴是阅读代码花费的时间，纵轴是阅读代码带来的效果。对于代码量比较大的项目，一开始阅读肯定是蒙的，需要花一定的时间梳理清楚各个文件，各个模块之间的关系。随着结构关系逐渐清晰，读者开始领会代码中所表达的含义，阅读代码的效果直线上升。然而当我们把代码主线和重要支线弄懂后，再读一些小支线的收益就不会太大。所以根据阅读代码的性价比和Caffe代码自身的特点，我们只会将主线和一些重要支线阅读完，估计也就是整体代码量的一半。&lt;/p&gt;&lt;h2&gt;Caffe代码的主线结构抽象&lt;/h2&gt;&lt;p&gt;不同于其他的一些框架，Caffe没有采用符号计算的模式进行编写，整体上的架构以系统级的抽象为主。所谓的抽象，就是逐层地封装一些细节问题，让上层的代码变得更加清晰。那么就让我们来顺着Caffe的抽象层级看看Caffe的主线结构：&lt;/p&gt;&lt;p&gt;&lt;b&gt;SyncedMem：&lt;/b&gt;这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&amp;gt;CPU内存-&amp;gt;GPU内存-&amp;gt;CPU内存-&amp;gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Blob：&lt;/b&gt;这个类做了两个封装：一个是操作数据的封装。在这里使用Blob，我们可以操纵高维的数据，可以快速访问其中的数据，变换数据的维度等等；另一个是对原始数据和更新量的封装。每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff用于存储反。向传播的梯度更新值。Blob使用了SyncedMem，这样也得到了不同处理端访问的便利。这样Blob就基本实现了整个Caffe数据部分结构的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。&lt;/p&gt;&lt;p&gt;数据的抽象到这个就可以了，接下来是层级的抽象。前面我们也分析过，神经网络的前后向计算可以做到层与层之间完全独立，那么每个层只要依照一定的接口规则实现，就可以确保整个网络的正确性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Layer：&lt;/b&gt;Caffe实现了一个基础的层级类&lt;b&gt;Layer&lt;/b&gt;，对于一些特殊种类还会有自己的抽象类（比如base_conv_layer），这些类主要采用了模板的设计模式（Template）,也就是说一些必须的代码在基类写好，一些具体的内容在子类中实现。比方说在Layer的Setup中，函数中包括Setup的几个步骤，其中的一些步骤由基类完成，一些步骤由子类完成。还有十分重要的Forward和Backward，基类实现了其中需要的一些逻辑，但是真正的运算部分则交给了子类。这样当我们需要实现一个新的层时，我们不需要管理琐碎的事物，只要关系好层的初始化和前后向即可。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Net：&lt;/b&gt;Net将数据和层组合起来做进一步的封装，对外暴露了初始化和前后向的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样。同时值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，所以我们可以通过配置实现多个神经网络层之间共享参数的功能，这也增强了我们对网络结构的想象力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Solver&lt;/b&gt;：有了Net我们实际上就可以进行网络的前向后向计算了，但是关于网络的学习训练的功能还有些缺乏，于是在此之上，&lt;b&gt;Solver&lt;/b&gt;类进一步封装了训练和预测相关的一些功能。与此同时，它还开放了两类接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如大家喜闻乐见的Momentum，Nesterov，Adagrad等。这样使得不同的优化算法能够应用其中。另外一个是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多卡训练算法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;IO&lt;/b&gt;：有了上面的东西就够了？还不够，我们还需要输入数据和参数，正所谓巧妇难为无米之炊，没有数据都是白搭。&lt;b&gt;DataReader&lt;/b&gt;和&lt;b&gt;DataTransformer&lt;/b&gt;帮助准备输入数据，&lt;b&gt;Filler&lt;/b&gt;对参数进行初始化。一些&lt;b&gt;Snapshot&lt;/b&gt;方法帮助模型的持久化，这样模型和数据的IO问题也解决了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;多卡&lt;/b&gt;：对于单GPU训练来说，基本的层次关系到这里也就结束了，如果要进行多GPU训练，那么上层还会有&lt;b&gt;InternalThread&lt;/b&gt;和&lt;b&gt;P2PSync&lt;/b&gt;两个类，这两个类属于最上层的类了，而他们所调用的也只有Solver和一些参数类。&lt;/p&gt;&lt;p&gt;其实到这里，Caffe的主线也就基本走完了。我们可以画一张图把Caffe的整体层次关系展示出来：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c6f445ed948594f81b3b2e35ce4c70f0.jpg" data-rawwidth="4032" data-rawheight="3024"&gt;&lt;p&gt;如果对这张图和图中的一些细节比较清楚的话，那么你对Caffe的了解应该已经不错了。后面关于Caffe源码分析的文章就可以不看了。如果没有，那么我们还是可以继续关注一下。当然如果想真正理解这张图中所表达的含义，还是要真正地读一下代码，去理解一些细节。但是有些细节这里就不做详细的分析了，下一回我们会站在Layer的角度去看一个Layer在训练过程的全部经历。&lt;/p&gt;</description><author>冯超</author><pubDate>Sun, 31 Jul 2016 19:23:46 GMT</pubDate></item><item><title>聊点轻松的——划个水</title><link>https://zhuanlan.zhihu.com/p/21788777</link><description>今天打开专栏，发现专栏的关注人数已经破千，心中满是感慨。6月30日我开始在知乎上写专栏的第一篇文章，到今天已经有1个月了。一个月的时间里，我完成了8篇技术文章的写作，每篇文章的字数在3000字左右。承蒙大家的喜爱、各位大神的帮助和捧场，还有自己的坚持努力，让一千个对机器学习感兴趣的小伙伴向我投来了感兴趣的目光。&lt;p&gt;知乎上有很多高手在积极地为大家回答问题，帮助大家解决机器学习相关的困惑，最初作为一个旁观者的我在浏览回答的过程中也学到了不少东西。现在我想和他们一样，把自己了解的一些皮毛知识分享给大家。写作的过程中难免有一些错误，欢迎大家多多提出宝贵意见，我会尽力改正。&lt;/p&gt;&lt;p&gt;在写完文章之后，我也会在手机上阅读写完的文章，在手机上阅读3000多字的文章其实是一件比较痛苦的事情，如果不能把问题讲得既清晰又有趣，阅读者是很难坚持读完的。这里面就需要把文章组织好。说实话，前面的文章在结构组织方面还是有些问题，而且文字相对比较多，而举例的图表比较少。大段的代码在手机上实际也不是很方便阅读，所以我也在考虑后面尽量少贴代码多用语言表达，代码可以放在github上供感兴趣的读者研究。&lt;/p&gt;&lt;p&gt;后面我还会继续聊CNN相关的事情，而且这部分有意思的内容还有不少。如果不出意外，我主要的实验工具就是Caffe了。&lt;/p&gt;&lt;p&gt;说起Caffe，最近我也阅读了市面上推出的《21天实战Caffe》。由于我对Caffe和深度学习有一些了解，所以这本书很快就读完了。看到了知乎上有对这本书看法的问题，我就在这里简单聊聊了。这本书的内容总体来说还是比较干的，它基本上把关于使用Caffe的方方面面的内容都做了入门级的介绍，所以我认为这本书还是很适合初学者的。如果你对Caffe官方网站的介绍不感冒，那么这本书一定能帮你更快地入门。但是对于Caffe的高级玩家，尤其是那些已经和Caffe朝夕相伴超过21天的朋友们，想从这本书获得更多新的内容还是比较困难。&lt;/p&gt;&lt;p&gt;所以我想给这本书的名字做一个补充，它应该叫《从零开始——一个深度学习者的21天实战Caffe》会比较合适点。这本书的中间部分花了大量篇幅贴了Caffe的源代码，这一部分让我感觉有点不太地道啊。虽说Caffe是开源的，但是你把代码直接粘到书上（排版还有些不好），补了两句注释，就算作自己的商业作品，难免有点凑字数的感觉。古人也有给名著写评语的例子，但大多数只能算作是原作的某个评注版，比如毛宗岗评三国演义。所以……这书的稿费是不是该分给Caffe社区点啊……（开个玩笑）&lt;/p&gt;&lt;p&gt;另外，我觉得这本书的中篇——关于对Caffe代码的分析也不算特别深入，对于大概了解Caffe结构是足够的（但是了解Caffe结构就不需要花那么多页贴代码了），但是对于更深入的细节问题——如何学习代码的细节，如何扩展代码，帮助就会相对小些。所以……转了一个大弯，我也想写点关于Caffe代码阅读的事情，不过角度会偏向了解Caffe机制细节和扩展代码的方向。所以会相对深入些。&lt;/p&gt;&lt;p&gt;此外，今后除了一条CNN的主线，如果因为调研或者实验导致写作进度赶不上来，我也会穿插一些其他的内容进来作补充，总之尽可能做到每周更新一篇。还是那句话，希望我的文章能够让大家减少机器学习的痛苦，享受其中的快乐。&lt;/p&gt;</description><author>冯超</author><pubDate>Fri, 29 Jul 2016 23:20:05 GMT</pubDate></item><item><title>卷积层（3）</title><link>https://zhuanlan.zhihu.com/p/21737674</link><description>上回说完了卷积层的线性部分，这次来聊聊非线性部分。其实在此之前我们在聊全连接层的时候就已经说过两个非线性部分的函数：&lt;ul&gt;&lt;li&gt;Sigmoid&lt;/li&gt;&lt;li&gt;Tanh&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;而且进入了深度网络学习的过程中，前辈们又发现了另一个好用的非线性函数，那就是ReLU，全称Rectify Linear Unit。它的函数形式是这样的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def relu(x):
    return x if x &amp;gt; 0 else 0&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;函数的形状大家也可以想象。在负数部分是一条和x轴平行的水平线，在正数部分是一条斜率为1的直线。这个函数一眼看去就觉得不是很复杂，很前面的两位比起来似乎只需要小学数学就可以明白，可是它却是现在最火的非线性函数，而且以它为基础，前辈们还演化出了一些变形函数。那些变形函数在这里就不提了。&lt;/p&gt;&lt;p&gt;话说前面两个函数已经称霸江湖许多年了，为什么一个新来的简单到不行的函数会抢尽风头？这也就是我们下面要聊的话题。&lt;/p&gt;&lt;h2&gt;梯度消失&lt;/h2&gt;&lt;p&gt;梯度消失是最让大家痛心的事情，这其中要数Sigmoid函数做得差了。早期的神经网络主要是浅层神经网络，在反向传导的过程中，因为中间经过的结构不算多，所以下层的网络得到的残差基本来还算是“新鲜”的。而随着深度学习的发展，网络层数的不断加深，反向传导逐渐变成了一个“漫长”的事情。从最上层的Loss开始向下传导，一路上会有各种各样的数据改变残差的数量，等到了下层的网络手上，这些被加工后的残差有时候会变得面目全非。&lt;/p&gt;&lt;p&gt;那么是怎样的面目全非法呢？这主要体现在数值的范围上。我们回顾一下两种经典的非线性函数的求导公式：&lt;/p&gt;&lt;p&gt;sigmoid：&lt;equation&gt;\frac{\partial f(x)}{x}=f(x) * (1 - f(x))&lt;/equation&gt;    其中函数值的范围是(0,1)&lt;/p&gt;&lt;p&gt;tanh：&lt;equation&gt;\frac{\partial f(x)}{x}=1-f(x)^2&lt;/equation&gt;     其中函数值的范围是(-1,1)&lt;/p&gt;&lt;p&gt;我们把两个函数画出来，就是这个样子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/76b022f61200039341865964783dc5bb.png" data-rawwidth="811" data-rawheight="392"&gt;从图上可以看出，tanh的导数的数值总体上还算不错，有大有小，但是终究是不大于1的。而sigmoid就比较可怜了，最大也就只有0.25。前面我们聊过关于多层神经网络求导的问题。当导数通过非线性部分到达下面的线性部分时，非线性的函数同样会改变导数的数值。上面的图就是他们改变的数量。&lt;p&gt;这时候sigmoid就显得十分渺小了，因为它最好情况下也会把传递的导数数值除以4。对于不深的网络，这点损失还算不上什么，但是对于深层的神经网络，这将会产生巨大的灾难，上层的网络拿到的梯度是比较大的，下层网络拿到的梯度却明显小很多。这种别人吃肉我喝汤的事情自然会让底层网络发育不良，很容易让深层网络无法发挥出应有的效果。&lt;/p&gt;&lt;p&gt;为了验证这个事情，这次拿出一个不太深的网络先举个例子。为了节省时间这里没有自己写代码，而是采用了知名的开源项目caffe。在其给出的例子——MNist的求解模型lenet的基础上稍作改动。关于MNist的内容前面介绍过，关于模型的结构，可以看下面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入：1*28*28&lt;/li&gt;&lt;li&gt;卷积：20*5*5&lt;/li&gt;&lt;li&gt;非线性：&lt;/li&gt;&lt;li&gt;Max Pooling：2*2&lt;/li&gt;&lt;li&gt;卷积：50*5*5&lt;/li&gt;&lt;li&gt;非线性：&lt;/li&gt;&lt;li&gt;Max Pooling：2*2&lt;/li&gt;&lt;li&gt;全连接：500&lt;/li&gt;&lt;li&gt;非线性：&lt;/li&gt;&lt;li&gt;全连接：10&lt;/li&gt;&lt;li&gt;Softmax：&lt;/li&gt;&lt;/ul&gt;简单来说，就是2层卷积，2层max pooling，2层全连接。其中的非线性部分将分别换成上面提到的三种非线性函数。除此之外，其他的参数设定与例子中完全一致，大家也可以自己去实验下。&lt;p&gt;首先来看一下训练数据的Loss和迭代轮数的关系图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d725a22cf36142c591975774131644b7.png" data-rawwidth="805" data-rawheight="389"&gt;可以看出，在这个问题上，sigmoid明显会弱一些，tanh和relu相近。当然到最后sigmoid似乎差得也不多。&lt;/p&gt;&lt;p&gt;接下来是测试集的Loss：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f059e4c4ba214849de5be92e591d2b0f.png" data-rawwidth="797" data-rawheight="389"&gt;这个图的效果和前面的训练图类似。看到这里我们可以确定：对于这个问题，在其他变量保持一致的情况下，使用sigmoid的非线性函数训练效果会若些。那么上面提到的参数训练的问题呢？&lt;/p&gt;&lt;p&gt;关于观测参数更新这个问题，caffe已经为我们想好了。在solver配置文件中加入下面这一行配置就可以查看前向后向的数据和参数信息了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;debug_info: true&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里我们主要关注的是后向的参数。后向为我们展示的是bottom_data的diff和param的diff。这里我们主要关注的是param的diff，因为这是最终落到参数更新上的数量。Debug信息显示的是同一个参数的平均值。虽然是平均值，但是它也有一定的价值，而它也足以分析出问题来。&lt;/p&gt;&lt;p&gt;首先是sigmoid的四个主要层的参数更新量：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/37e84af9466eb0d4f88daecb301a289e.png" data-rawwidth="823" data-rawheight="390"&gt;从图中可以明显看出一个规律，那就是越靠上的层参数更新的量越大，越靠下的参数更新量越小。这和我们刚才的描述是一致的。那么其他的函数表现如何呢？接下来是tanh的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/86027fa212722f201ee64b373a6f1c38.png" data-rawwidth="811" data-rawheight="389"&gt;可以看出tanh的参数更新整体上好了许多。最起码不是越靠上越大了，这也说明在这个问题中tanh不存在梯度消失的问题，那么relu呢？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4e62528359cfe62785e4f4d583e6fcbd.png" data-rawwidth="814" data-rawheight="391"&gt;relu的结果同样没有出现梯度消失的问题，而且它的四条线距离和tanh相比更相近一些。&lt;/p&gt;&lt;p&gt;通过上面的分析，我们发现了sigmoid确实不适合深度学习模型，虽然它的函数曲线和人的神经反应很相似，但是在深度学习上它确实不太好用。这也是件无可奈何的事情，不过它还是在很多浅层模型上发挥着巨大的作用，所以它不会离开我们的视线的。&lt;/p&gt;&lt;h2&gt;计算简单&lt;/h2&gt;&lt;p&gt;关于这个部分……我想就不用多说了。上面表现良好的tanh和relu，哪个好算大家一目了然。在效果相近的情况下，自然是选择简单的函数。&lt;/p&gt;&lt;h2&gt;稀疏性&lt;/h2&gt;&lt;p&gt;很多前辈在提到了relu时，同时提到了它的稀疏性。因为它强行把小于0的部分截断，这样数据中就会出现很多0。这些0再乘以任何数都等于0，无形中相当于减少了数据的维度。再小的数据也比不上一个0来的实在，如果配上CNN中另一个重量级元素max pooling层，可以把这些为0的数据直接清除掉，这样数据的维度就可以进一步地压缩了。&lt;/p&gt;&lt;p&gt;relu函数还有一个很奇特的地方，那就是它看上去是一个非线性层，实际上操作起来却有点像一个线性层。对于线性部分的输出，最终的结果就好像左乘一个非0即1的对角阵：&lt;/p&gt;&lt;equation&gt;relu(X)=diag(1,1,0,0,1,...1,1)*X&lt;/equation&gt;&lt;p&gt;前面在分析非线性层的作用中，曾经有一条就是它使得深层网络变得有意义。这一点relu是可以做到的。但是它的效果还是像把输入数据做了线性投影一样，而不是非线性变换。relu看上去简单，可是实际上它能够在复杂的网络中有一席之地，还是有它存在的道理的。当然它也有它的问题，关于它的问题以后再说。&lt;/p&gt;&lt;p&gt;卷积层的基本结构也就到这了，后面内容更精彩～&lt;/p&gt;</description><author>冯超</author><pubDate>Tue, 26 Jul 2016 23:00:37 GMT</pubDate></item><item><title>卷积层（2）</title><link>https://zhuanlan.zhihu.com/p/21675422</link><description>这一回我们来看看卷积层的解法。我们将采用两种方法求解：&lt;ul&gt;&lt;li&gt;一种是实力派解法&lt;/li&gt;&lt;li&gt;一种是软件库中常用的套路——“整容”后的偶像派解法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里需要一个小例子，我们假定一个1*5*5的输入，卷积层的维度是1*1*3*3，同时stride=1,padding=0。最终的输出是1*3*3。&lt;/p&gt;&lt;p&gt;这里先画个详细的图，图像中对其中的变量做了定义：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d2a7f3e3fe73f0f5dbb2dd9c98ed20d6.jpg" data-rawwidth="1280" data-rawheight="960"&gt;其中：&lt;/p&gt;&lt;p&gt;X表示输入的矩阵，我们用一维的0-24表示下标。&lt;/p&gt;&lt;p&gt;K表示卷积核的矩阵，我们用一维的0-8表示下标。&lt;/p&gt;&lt;p&gt;Z表示卷积的结果，我们用一维的0-8表示下标。&lt;/p&gt;&lt;p&gt;由于前面我们提到操作中stride=1,padding=0，所以我们可以给出简单版和复杂版的输出维度计算公式：&lt;/p&gt;&lt;p&gt;首先是简单版：&lt;/p&gt;&lt;equation&gt;H_{out}=H_{in}-H_{kernel}+1&lt;/equation&gt;&lt;equation&gt;W_{out}=W_{in}-W_{kernel}+1&lt;/equation&gt;&lt;p&gt;然后是复杂版：&lt;/p&gt;&lt;equation&gt;H_{out}=\frac{H_{in}+2*H_{padding}-H_{kernel}}{H_{stride}}+1&lt;/equation&gt;&lt;equation&gt;W_{out}=\frac{W_{in}+2*W_{padding}-W_{kernel}}{W_{stride}}+1&lt;/equation&gt;&lt;p&gt;很显然，当stride=1,padding=0时，两个公式是等价的。&lt;/p&gt;&lt;h2&gt;实力派解法&lt;/h2&gt;&lt;p&gt;所谓的实力派解法就是用卷积定义（这里就用相关操作）去做前向计算，然后利用前向的算法去求反向。&lt;/p&gt;&lt;p&gt;接下来这张图上详细介绍了输出的每一个数值是利用哪一部分的信息计算出来的：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8948a8a27064aaf6594ebf9c47f45840.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;上面这张图上详细地讲述了每一个像素的前向计算公式，同时在计算的过程中，我们将每一个输出结果对应的输入和kernel的信息作了标记。实际上这和上一回我们说的算法是一样的。&lt;/p&gt;&lt;p&gt;刚才我们看了前向传播，下面我们看看反向传播的计算。反向传播中的偏置项bias这里就不说了，和全连接的反向算法一样，相对简单些。另外我们需要计算两个数值：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;给下一层传导的loss&lt;/li&gt;&lt;li&gt;本层卷积核参数的导数。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先是下一层参数的loss，也可以理解为Loss对输入数据的梯度。从卷积的过程中直接观察是比较困难的，我们需要讲观看的视角做一下转换。想要求出每个输入的导数，就需要列出每个输入元素参与的计算，这样我们用前面得到的Loss和与这个元素相乘的参数进行相乘再相加就可以得到想要的结果了。这个思路和全连接求导的思路是一致的，只是变换的过程需要费点脑筋：&lt;/p&gt;&lt;equation&gt;\frac{\partial Loss}{\partial X_i}=[\frac{\partial Loss}{\partial Z_j}, ...]^T * [\frac{\partial Z_j}{\partial K_l},...]&lt;/equation&gt;&lt;p&gt;为了更加清晰地展示算法，我画了下面这张图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/61ed8f283556a381e0d0f2c601905132.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;从这张图可以清楚地看出输入数据和输出loss的关系。右边的图主体上是由5*5的小图组成，每一个小图就相当于输入所在位置的数据参与卷积计算的过程。可以看出，每个输入数据在卷积过程的参与度是不同的。对于我们这个问题，最终的输出是9个数字，每个数字由大小为9的卷积计算得到，也就是说我们一共进行了81次乘加操作，我们用矩阵的形式把这个参与次数再写出来：&lt;/p&gt;&lt;p&gt;1   2   3   2   1&lt;/p&gt;&lt;p&gt;2   4   6   4   2&lt;/p&gt;&lt;p&gt;3   6   9   6   3&lt;/p&gt;&lt;p&gt;2   4   6   4   2&lt;/p&gt;&lt;p&gt;1   2   3   2   1&lt;/p&gt;&lt;p&gt;相加后发现确实是81。数字的数量是相符的。&lt;/p&gt;&lt;p&gt;然后我们还可以从延展的虚线中看出，如果将上面小图中的两部分标出的位置进行乘加操作后就可以得出输入所在点的梯度了，而且另外一个发现就是——把所有输入点的计算组合起来，是输出数据的梯度和卷积参数核的卷积操作，但是其中有2处不同：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;卷积核需要旋转180度&lt;/li&gt;&lt;li&gt;输出数据的梯度所在的矩阵需要在周围做padding，padding的数量等于卷积核的维度减1&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以这一步的求解最终可以转化成这个伪代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;residual_x=conv2(padding(residual_z,kernel.shape()-1), rot180(kernel))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过比较复杂的分析，我们最终得到了一个十分简洁的计算方法。&lt;/p&gt;&lt;p&gt;看完了给下一层传导的loss，下面是本层的参数导数。和之前类似，我们需要重新整理运算关系。同样地，我们再来一张图进行解释：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/adc4b77d8776e64ec03a10db4555d789.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;从这张图又可以很清楚地看出这个关系。这一次我们直接把输入数据和输出数据的残差做卷积，就可以得到参数w的导数。比上面的计算还要简单点。这里的求解也只需要一步就可以：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;residual_w=conv2(x,residual_z)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;实际上到此为止，卷积层的实力解法就到此结束了，实际上它与全连接层最大的不同就是线性部分，关于非线性部分的事情我们后面再说，但是由于线性部分和非线性部分相互独立，因此分来分析也是完全没有问题的。&lt;/p&gt;&lt;p&gt;所以从上面的分析中可以看出，想要采用实力派的解法需要能够清晰地画出下面三张图：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;前向图：标有&lt;b&gt;卷积核&lt;/b&gt;&lt;b&gt;id&lt;/b&gt;-&amp;gt;的&lt;b&gt;输入&lt;/b&gt;小图-&amp;gt;组成的&lt;b&gt;输出&lt;/b&gt;大图&lt;/li&gt;&lt;li&gt;下层Loss：标有&lt;b&gt;卷积核id&lt;/b&gt;-&amp;gt;的&lt;b&gt;输出&lt;/b&gt;小图-&amp;gt;组成的&lt;b&gt;输入&lt;/b&gt;大图&lt;/li&gt;&lt;li&gt;本层w导数：标有&lt;b&gt;输入id&lt;/b&gt;-&amp;gt;的&lt;b&gt;输出&lt;/b&gt;小图-&amp;gt;组成的&lt;b&gt;卷积核&lt;/b&gt;大图&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;刚才看到的是stride=1,padding=0的解法，相对来说省略了一些细节的考虑。如果把这些加上，这种方法也是可以解的，只不过比刚才要复杂一些。这里就不做更进一步的推导了，有志成为“实力派”的童鞋可以去尝试一下进一步地推导。但不管做怎么样的变换，都离不开上面三张图的推导。熟练推导三张图能够让我们更加深刻地理解卷积（相关）操作内部的过程，是绝对有好处的。&lt;/p&gt;&lt;h2&gt;“偶像派”解法&lt;/h2&gt;&lt;p&gt;说实话，像神经网络里面这样的推导所涉及到的数学能力并不高，但是想熟练掌握需要好好地刷一刷。但是如果不想把自己搞得这么痛苦，就可以试试偶像派的解法。&lt;/p&gt;&lt;p&gt;偶像派的解法是怎么做到的呢？我们前面说了卷积层的卷积计算是线性的，既然是线性的我们能不能把输入矩阵做一个变换，使运算变成一个矩阵和卷积核向量相乘的运算呢？当然可以了，这就涉及到我们的“整容”过程了。&lt;/p&gt;&lt;p&gt;整容的过程只需要用到上面的第一张图——前向图。前向图中每一个小图都可以表示输入数据的一部分和卷积核做点积的过程。最终我们求出了9个结果值，因此也就有9个输入的部分数据和卷积做了点积。这个过程同样可以画成一张图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0a6e403fe8e52dd87e871c8f9f8ab255.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;这样看来，前向计算和求解卷积核参数导数的计算就会变得容易得多。不过如果考虑了padding和stride，写这个转换的算法还是需要点细心的。至于向下一层传递的导数，做法也类似，转成矩阵乘法的形式即可，这里就不再赘述了。&lt;/p&gt;&lt;p&gt;这样，偶像派的解法也就完成了。借助了矩阵和向量的乘法，我们的大脑得到了极大的解放。那么问题来了，对于我们常用的软件库，大家是怎么实现这个算法的呢？&lt;/p&gt;&lt;p&gt;基本上大家都选择偶像的道路……倒不是因为偶像派的思路清晰，而是因为矩阵乘法这样的运算经过大家多年的研究，运算效率非常有保障。而卷积运算在实现过程中像cache友好性这样的问题会差些，所以最终被淘汰。&lt;/p&gt;&lt;p&gt;当然，现在被广大群众所爱戴的——CUDNN库的实现比我们偶像派的做法在细节上更为精细，可以算是偶像+实力兼备。感兴趣的童鞋可以去了解一下。&lt;/p&gt;&lt;p&gt;聊完了线性部分求解的事情，下次就来一起看看非线性部分的一些问题。 &lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 23 Jul 2016 12:09:44 GMT</pubDate></item><item><title>卷积层（1）</title><link>https://zhuanlan.zhihu.com/p/21609512</link><description>前面聊了3期全连接层，下面先扔下它，看看卷积神经网络的另外一个重量级组成部分——卷积层。&lt;p&gt;关于卷积层的具体计算方式在这里就不多说了，和全连接层类似，由线性部分和非线性部分组成，一会儿直接看代码就好。关于卷积层的计算方法，现在一般来说大家的实现方式都是用“相关”这个操作来进行的，为什么呢？当然是为了计算方便，减少一次把卷积核转一圈的计算。&lt;/p&gt;&lt;p&gt;以下是“卷积层”操作的基本代码，我们后面会做进一步地“升级”的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
import matplotlib.pyplot as plt
def conv2(X, k):
    x_row, x_col = X.shape
    k_row, k_col = k.shape
    ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1
    ret = np.empty((ret_row, ret_col))
    for y in range(ret_row):
        for x in range(ret_col):
            sub = X[y : y + k_row, x : x + k_col]
            ret[y,x] = np.sum(sub * k)
    return ret

class ConvLayer:
    def __init__(self, in_channel, out_channel, kernel_size):
        self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)
        self.b = np.zeros((out_channel))
    def _relu(self, x):
        x[x &amp;lt; 0] = 0
        return x
    def forward(self, in_data):
        # assume the first index is channel index
        in_channel, in_row, in_col = in_data.shape
        out_channel, kernel_row, kernel_col = self.w.shape[1], self.w.shape[2], self.w.shape[3]
        self.top_val = np.zeros((out_channel, in_row - kernel_row + 1, in_col - kernel_col + 1))
        for j in range(out_channel):
            for i in range(in_channel):
                self.top_val[j] += conv2(in_data[i], self.w[i, j])
            self.top_val[j] += self.b[j]
            self.top_val[j] = self._relu(self.topval[j])
        return self.top_val
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;到这里卷积层就介绍完了，谢谢……&lt;/p&gt;&lt;p&gt;卷积层内心OS：玩儿去，怎么可以戏份这么少……&lt;/p&gt;其实在卷积神经网络这个模型大火之前，卷积这个概念早已深入许多图像处理的小伙伴的内心了。在图像处理中，我们有大批图像滤波算法，其中很多都是像卷积层这样的线性卷积核。为了更清楚地介绍这些核，我们拿一张OCR的训练数据做例子（特别说明，本文字来自wikipedia，侵权删图）：&lt;pre&gt;&lt;code lang="text"&gt;import cv2
mat = cv2.imread('conv1.png',0)
row,col = mat.shape
in_data = mat.reshape(1,row,col)
in_data = in_data.astype(np.float) / 255
plt.imshow(in_data[0], cmap='Greys_r')&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个字显示出来是这个样子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c4ab7e19a8e712deabc5d244cf0c751d.png" data-rawwidth="264" data-rawheight="237"&gt;&lt;p&gt;首先是mean kernel，也就是均值滤波：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;meanConv = ConvLayer(1,1,5)
meanConv.w[0,0] = np.ones((5,5)) / (5 * 5)
mean_out = meanConv.forward(in_data)
plt.imshow(mean_out[0], cmap='Greys_r')
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/741d95ac24aeaef5ef4cf482dc317c3d.png" data-rawwidth="262" data-rawheight="229"&gt;&lt;p&gt;均值滤波在图像处理中可以起到对模糊图像的作用，当然由于卷积核比较小，所以效果不是很明显。&lt;/p&gt;&lt;p&gt;然后是梯度计算，请上Sobel kernel，我们定义一个计算纵向梯度的核，如果一个像素点的纵向梯度非常大，那么这个点的结果会非常大：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;sobelConv = ConvLayer(1,1,3)
sobelConv.w[0,0] = np.array([[-1,-2,-1],[0,0,0],[1,2,1]])
sobel_out = sobelConv.forward(in_data)
plt.imshow(sobel_out[0], cmap='Greys_r')
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;图像结果如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/60558b577bbd3a8aa9c33101ff2aab46.png" data-rawwidth="265" data-rawheight="232"&gt;&lt;p&gt;从结果来看，文字中横向比划的上端被保留了下来。&lt;/p&gt;&lt;p&gt;最后来一张Gabor filter的效果，现在的一些论文里面都宣称自己的模型能够学出Gabor filter的参数来（Gabor filter代码来自wikipedia）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def gabor_fn(sigma, theta, Lambda, psi, gamma):
    sigma_x = sigma
    sigma_y = float(sigma) / gamma
    (y, x) = np.meshgrid(np.arange(-1,2), np.arange(-1,2))
    # Rotation 
    x_theta = x * np.cos(theta) + y * np.sin(theta)
    y_theta = -x * np.sin(theta) + y * np.cos(theta)
    gb = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.cos(2 * np.pi / Lambda * x_theta + psi)
    return gb
print gabor_fn(2, 0, 0.3, 0, 2)
gaborConv = ConvLayer(1,1,3)
gaborConv.w[0,0] = gabor_fn(2, 0, 0.3, 1, 2)
gabor_out = gaborConv.forward(in_data)
plt.imshow(gabor_out[0], cmap='Greys_r')

[[-0.26763071 -0.44124845 -0.26763071]
 [ 0.60653066  1.          0.60653066]
 [-0.26763071 -0.44124845 -0.26763071]]
&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2bb3e17dd7a61ca01584c5aec347a2bd.png" data-rawwidth="250" data-rawheight="230"&gt;&lt;p&gt;从结果可以看出，Gabor filter同样可以做到边缘提取的作用，这和它本身的功能是相符的。&lt;/p&gt;&lt;p&gt;好了，以上三种处理完成之后，我们可以看出，不同的滤波核确实能起到不同的效果。实际上用于图像滤波的卷积核还有很多。&lt;/p&gt;&lt;p&gt;下面要试图做一些讲道理的事情，也就是说明：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;为什么要发明卷积层这种神经层？能不能用全连接层代替卷积层？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;很显然，用全连接层代替卷积层是完全没有问题的，但是这样做的代价实在太大了。原始图像的维度相对而言比较大，如果采用全连接的话参数将会有爆炸式的增长，参数的数量可能对于现在的电脑来说是一个灾难。试想一下对于MNist的数据，如果第一层是全连接层，即使把1*28*28数据映射到1*1024这样的输出上，其中的参数已经达到800万。而曾经的经典模型Lenet呢？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3a1b1a520801713b858dfdbe50213a67.png" data-rawwidth="748" data-rawheight="408"&gt;从上面的图可以看出（此处）：&lt;/p&gt;&lt;p&gt;第一层卷积的参数数量为：1*6*(5*5+1)=156，却可以得到6*28*28的输出。&lt;/p&gt;&lt;p&gt;两者的参数数量差距几万倍，而实际上两者效果绝对不会有如此大的差距。&lt;/p&gt;&lt;p&gt;既然如此，参数数量如此少的卷积层为什么可以有用？识别这样的问题总体来说是一种比较宏观的问题。像MNist这样的问题，每张图片上有784个像素，理论上可以有&lt;equation&gt;256^{784}&lt;/equation&gt;种组合，而实际的类别只有10种，基本上可以断定提供的信息是远远多于满足搜索需求的数量的。那么多出来的像素信息是以什么样的形式展现呢？在图像处理上，有一个词叫“局部相关性”就是指这样的问题。&lt;/p&gt;&lt;p&gt;我们是如何识别出一个数字的呢？当然是因为这个数字不同于别的数字的特点，对于像MNist这样的数据，特点自然来自于明暗交界的地方。一片黑的区域不会告诉我们任何有用的信息，同样地，一片白的区域也不会告诉我们任何有用的信息。同样，数字的笔画粗细对我们的识别也没有太大的作用。这些都是我们识别过程中会遇到的问题，而其中临近像素之间有规律地出现相似的状态就是局部相关性。&lt;/p&gt;&lt;p&gt;那么如何消除这些局部相关性呢，使我们的特征变得少而精呢？卷积就是一种很好地方法。它只考虑附近一块区域的内容，分析这一小片区域的特点，这样针对小区域的算法可以很容易地分析出区域内的内容是否相似。如果再加上Pooling层（可以理解为汇集，聚集的意思，后面不做翻译），从附近的卷积结果中再采样选择一些高价值的合成信息，丢弃一些重复低质量的合成信息，就可以做到对特征信息的进一步处理过滤，让特征向少而精的方向前进。&lt;/p&gt;&lt;h2&gt;卷积的另一种解释&lt;/h2&gt;&lt;p&gt;熟悉图像处理的童鞋一定都知道传说中的卷积定理，这个定理涉及到图像处理的一大黑科技——傅立叶变换。&lt;/p&gt;&lt;p&gt;关于傅立叶的高质量科普文章已经很多，这里就不多说了。套用之前看过的一个有赞的解释：如果把我们看到的图像比作一道做好的菜，那么傅立叶变换就是告诉我们这道菜具体的配料以及各种配料的用量。这个方法的神奇之处在于不管这道是如何做的（按类别码放还是大乱炖），它都能将配料清晰分出来。在图像处理中，这个配料被成为频率。其中有些信息被称作低频信息，有些被成为高频信息。&lt;/p&gt;&lt;p&gt;低频信息一般被我们是看作是整幅图像的基础，有点像一道菜中的主料。如果说这道菜是番茄炒蛋，那低频信息可以看作是番茄和蛋。高频信息一般是指那些变动比较大的，表达图像特点的信息，有点像一道菜的配料或者调料。对于番茄炒蛋，像是菜中调料和点缀的材料。&lt;/p&gt;&lt;p&gt;回到卷积中，卷积定理中说，两个矩阵的卷积的结果，等于两个矩阵在经过傅立叶变换后，进行元素级别的乘法（element-wise multiplication），然后再对结果进行反向傅立叶处理。如果我们用FFT表示傅立叶变换，IFFT表示反向傅立叶变换，那么下面两个过程结果是相同的（考虑到浮点数近似，可以认为相近就是相同）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;A = np.array()
B = np.array() # matrix
# method1
C = conv2(A,B)
#method2
FFT_A = FFT(A)
FFT_B = FFT(B)
C = IFFT(FFT_A * FFT_B)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;真实的代码不会像上面这样简单，但是大体来说结构也是相同的。看上去第二种方法比第一种方法啰嗦了不少，那么它的优势在哪里？&lt;/p&gt;&lt;p&gt;那我们就来看看上面提到的一些kernel在经过傅立叶变换后的样子：&lt;/p&gt;&lt;p&gt;首先是mean kernel：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1456f7f3cfb0466a31dadf577ae01d87.png" data-rawwidth="468" data-rawheight="467"&gt;然后是Sobel kernel:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/956c649ce09eff45999eb6980218f513.png" data-rawwidth="448" data-rawheight="451"&gt;最后是Gabor kernel:&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d15ed0e499eff7769702ab18bee8aecc.png" data-rawwidth="420" data-rawheight="423"&gt;&lt;p&gt;我们前面说过，傅立叶变换可以帮助分析出高低频信息。在上面的图像中，（经过fft_shift的）图像的正中心是低频信息，越靠近边缘频率越高。由于最终要进行元素级的乘法，如果kernel在某个频率的数据比较低，经过乘法后输入数据在这个频率的数据也会变小。&lt;/p&gt;&lt;p&gt;如果我们把上面的图片想象成中心为原点的直角坐标系图，所以我们可以看出：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mean kernel会把中心附近和坐标轴附近信息进行保留；&lt;/li&gt;&lt;li&gt;Sobel kernel会保留y轴上下的信息，丢弃中间的信息和x轴两边的信息；&lt;/li&gt;&lt;li&gt;Gabor kernel和Sobel kernel类似，但是保留的内容会更少些，更倾向于保留远离中心的像素。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以从这个角度来分析结果，sobel kernel和gabor kernel更倾向于保留高频信息而弱化低频信息，而mean kernel则是弱化高频信息而保留低频信息。所以mean kernel和另外两个kernel的作用不同。&lt;/p&gt;&lt;p&gt;另外，对于一些文章中提到利用可视化的方法发现了自己的参数像gabor filter，也说明了它的卷积核作用是保留高频牺牲低频。&lt;/p&gt;&lt;p&gt;那么从这个角度分析，卷积层意义在哪里？如果我们任务是给我们一盘番茄炒蛋，问我们这盘菜是哪位师傅做的（记得小时候在饭店吃饭的时候，菜盘边上经常会有一张小纸条写着“XX号厨师为您服务”）。那么盘子里的数量众多的番茄和鸡蛋不一定能帮助我们找到厨师，而其中佐料的分量多少更有助于我们找出厨师的做菜风格。当然这里我们假设厨师有自己的风格，且正常发挥没有失误了。而这个思路也是卷积层采取的一种策略。&lt;/p&gt;&lt;p&gt;关于傅立叶变换以及频域信息分析显然没有这么简单，更多的细节还需要更多的分析，这里只是抛砖作个引子。&lt;/p&gt;&lt;p&gt;下一回将说说卷积层具体求解的问题。&lt;/p&gt;</description><author>冯超</author><pubDate>Thu, 21 Jul 2016 19:18:11 GMT</pubDate></item><item><title>神经网络-全连接层（3）</title><link>https://zhuanlan.zhihu.com/p/21572419</link><description>&lt;p&gt;上一回我们聊完了算法，这回我们正式开始写代码。上回在做公式推导的时候，我们实际上只是针对一个数据样本进行推导，而实际中，计算和训练都是一批一批完成的。大多数机器学习训练都有batch的概念，而训练中batch的计算不是一个一个地算，而是一批数据集中算，那么就需要用上矩阵了。&lt;/p&gt;&lt;p&gt;首先给出Loss的代码，这里y和t都是按列存储的，每一列都是一个样本：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;class SquareLoss:
    def forward(self, y, t):
        self.loss = y - t
        return np.sum(self.loss * self.loss) /  self.loss.shape[1] / 2
    def backward(self):
        return self.loss&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了代码的简洁，我们在前向运算的时候就把一些后向计算的信息都保存起来，这样在后向计算的时候就能简单点。这样这个类就不能具备多线程的特性了，不过想支持多线程的功能还有别的办法。后面的全连接层也会采用同样的思路——前向为后向准备运算数据。&lt;/p&gt;&lt;p&gt;上一节我们讲了1个例子，输入有2个元素，第一层有4个输出，第2层有1个输出。我们假设训练数据有N个，我们对所有相关的训练数据和参数做以下的约定：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;所有的训练数据按列存储，也就是说如果把N个数据组成一个矩阵，那个矩阵的行等于数据特征的数目，矩阵的列等于N&lt;/li&gt;&lt;li&gt;线性部分的权值w由一个矩阵构成，它的行数为该层的输入个数，列数为该层的输出个数。如果该层的输入为2，输出为4，那么这个权值w的矩阵就是一个2*4的矩阵。&lt;/li&gt;&lt;li&gt;线性部分的权值b是一个行数等于输出个数，列数为1的矩阵。&lt;/li&gt;&lt;/ul&gt;基于上面的规则，我们把上一节的例子以批量数据的形式画成了下面一张图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/66d58d623e9956ec06b60f66e2f72cbf.jpg" data-rawwidth="4032" data-rawheight="3024"&gt;这张图从左往右有三个部分：&lt;ol&gt;&lt;li&gt;最左边是神经网络的结构图，可以看出里面的数据x,z和参数w,b都符合我们刚才对数据组织的定义。&lt;/li&gt;&lt;li&gt;中间是神经网络前向的过程。一共分为5步，其中最后一步用来计算Loss。&lt;/li&gt;&lt;li&gt;最右边是神经网络反向的过程。这里需要仔细看一下。为了表达上的简洁，我们用残差符号&lt;equation&gt;\delta&lt;/equation&gt;表达Loss对指定变量的偏导数。同时为了更加简洁地表达梯度计算的过程，在这个过程中我们对其中一个矩阵做了矩阵转置，这样可以确保最终输出维度的正确。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;对于上图右边的部分，需要认真地看几遍，最好能仔细地推导一遍，才能更好地掌握这个推导的过程，尤其是为了维度对矩阵做转置这部分。&lt;/p&gt;&lt;p&gt;看懂了上面的图，接下来要做的就是对上面的内容进行总结，写出最终的矩阵版后向传播算法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;class FC:
    def __init__(self, in_num, out_num, lr = 0.1):
        self._in_num = in_num
        self._out_num = out_num
        self.w = np.random.randn(in_num, out_num)
        self.b = np.zeros((out_num, 1))
        self.lr = lr
    def _sigmoid(self, in_data):
        return 1 / (1 + np.exp(-in_data))
    def forward(self, in_data):
        self.topVal = self._sigmoid(np.dot(self.w.T, in_data) + self.b)
        self.bottomVal = in_data
        return self.topVal
    def backward(self, loss):
        residual_z = loss * self.topVal * (1 - self.topVal)
        grad_w = np.dot(self.bottomVal, residual_z.T)
        grad_b = np.sum(residual_z)
        self.w -= self.lr * grad_w
        self.b -= self.lr * grad_b
        residual_x = np.dot(self.w, residual_z)
        return residual_x
&lt;/code&gt;&lt;/pre&gt;好了，现在我们有了Loss类和全连接类，我们还需要一个类把上面两个类串联起来，这里为了后面的内容我们定义了许多默认变量：&lt;pre&gt;&lt;code lang="text"&gt;class Net:
    def __init__(self, input_num=2, hidden_num=4, out_num=1, lr=0.1):
        self.fc1 = FC(input_num, hidden_num, lr)
        self.fc2 = FC(hidden_num, out_num, lr)
        self.loss = SquareLoss()
    def train(self, X, y): # X are arranged by col
        for i in range(10000):
            # forward step
            layer1out = self.fc1.forward(X)
            layer2out = self.fc2.forward(layer1out)
            loss = self.loss.forward(layer2out, y)
            # backward step
            layer2loss = self.loss.backward()
            layer1loss = self.fc2.backward(layer2loss)
            saliency = self.fc1.backward(layer1loss)
        layer1out = self.fc1.forward(X)
        layer2out = self.fc2.forward(layer1out)
        print 'X={0}'.format(X)
        print 't={0}'.format(y)
        print 'y={0}'.format(layer2out)
&lt;/code&gt;&lt;/pre&gt;代码是写完了，可是我们还需要验证一下自己的代码是不是正确的。一般来说我们会采用一些近似方法计算验证梯度是否正确，而现在，有一个博客为我们做了这件事情：&lt;p&gt;&lt;a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" data-editable="true" data-title="A Step by Step Backpropagation Example"&gt;A Step by Step Backpropagation Example&lt;/a&gt;&lt;/p&gt;&lt;p&gt;把我们的代码用博客上数据和结果做一下验证，就可以帮助我们修正代码做好debug。其实上面的代码本来也不多，可能犯错的地方也不多。&lt;/p&gt;&lt;h2&gt;一些具体的例子&lt;/h2&gt;一个经典的例子就是用神经网络做逻辑运算。我们可以用一个两层神经网络来模拟模拟与运算。下面就是具体的代码：&lt;pre&gt;&lt;code lang="text"&gt;# and operation
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T
y = np.array([[0],[0],[0],[1]]).T

net = Net(2,4,1,0.1)
net.train(X,y)
&lt;/code&gt;&lt;/pre&gt;以下是调用代码给出的结果，可以看出最终的结果效果还不错，经过10000轮的迭代，最终模型给出的结果和我们的期望结果十分相近，实际上如果我们继续进行迭代，这个算法的精度还可以进一步地提高，Loss可以进一步地减少：&lt;pre&gt;&lt;code lang="text"&gt;iter = 0, loss =0.105256639066
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.40930536  0.4617139   0.36923076  0.4299025 ]]
iter = 1000, loss =0.0229368486589
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.04445123  0.22684496  0.17747671  0.68605373]]
iter = 2000, loss =0.00657594469044
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.01057127  0.11332809  0.11016211  0.83411794]]
iter = 3000, loss =0.00322081318498
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00517544  0.07831654  0.07871461  0.88419737]]
iter = 4000, loss =0.00201059297485
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00336374  0.06171018  0.0624756   0.90855558]]
iter = 5000, loss =0.00142205310651
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00249895  0.05189239  0.05257126  0.92309992]]
iter = 6000, loss =0.00108341055769
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00200067  0.04532728  0.04585262  0.93287134]]
iter = 7000, loss =0.000866734887908
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00167856  0.04058314  0.04096262  0.9399489 ]]
iter = 8000, loss =0.000717647908313
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00145369  0.03696819  0.0372232   0.94534786]]
iter = 9000, loss =0.000609513241467
=== Label vs Prediction ===
t=[[0 0 0 1]]
y=[[ 0.00128784  0.03410575  0.03425751  0.94962473]]
=== Final ===
X=[[0 0 1 1]
 [0 1 0 1]]
t=[[0 0 0 1]]
y=[[ 0.00116042  0.03177232  0.03183889  0.95311123]]
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;记得初始化&lt;/h2&gt;&lt;p&gt;初始化是神经网络一个十分重要的事情，我就不说三遍了，来个实验，如果我们把所有的参数初始化成0，会发生一个可怕的事情：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T
y = np.array([[0],[0],[0],[1]]).T

net = Net(2,4,1,0.1)
net.fc1.w.fill(0)
net.fc2.w.fill(0)
net.train(X,y)
print "=== w1 ==="
print net.fc1.w
print "=== w2 ==="
print net.fc2.w
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;直接看结果：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;=== Final ===
X=[[0 0 1 1]
 [0 1 0 1]]
t=[[0 0 0 1]]
y=[[  3.22480024e-04   2.22335711e-02   2.22335711e-02   9.57711099e-01]]
=== w1 ===
[[-2.49072772 -2.49072772 -2.49072772 -2.49072772]
 [-2.49072772 -2.49072772 -2.49072772 -2.49072772]]
=== w2 ===
[[-3.373125]
 [-3.373125]
 [-3.373125]
 [-3.373125]]&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;不但没有训练出合理的结果，而且每一层的参数还都是一样的。&lt;/p&gt;&lt;p&gt;但是如果把每层参数设为不同的固定值呢？&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T
y = np.array([[0],[0],[0],[1]]).T

net = Net(2,4,1,0.1)
net.fc1.w.fill(1)
net.fc2.w.fill(0)
net.train(X,y)
print "=== w1 ==="
print net.fc1.w
print "=== w2 ==="
print net.fc2.w
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果竟然也不错：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;=== Final ===
X=[[0 0 1 1]
 [0 1 0 1]]
t=[[0 0 0 1]]
y=[[ 0.00399349  0.02830098  0.02830098  0.96924181]]
=== w1 ===
[[ 2.48265841  2.48265841  2.48265841  2.48265841]
 [ 2.48265841  2.48265841  2.48265841  2.48265841]]
=== w2 ===
[[ 3.231811]
 [ 3.231811]
 [ 3.231811]
 [ 3.231811]]&lt;/code&gt;&lt;/pre&gt;虽然每层的参数依然相同，但是训练得到了收敛。这又说明了什么呢？关于这个问题有机会再说。&lt;p&gt;全连接层就这样聊了三期，下回可以换个口味了。&lt;/p&gt;</description><author>冯超</author><pubDate>Fri, 15 Jul 2016 19:49:23 GMT</pubDate></item></channel></rss>