<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>深度学习大讲堂 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/dlclass</link><description>推送深度学习的最新消息，包括最新技术进展，使用以及活动</description><lastBuildDate>Wed, 17 Aug 2016 22:02:29 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Bengio教授的深度学习终极思考：文化、进化与迷因</title><link>https://zhuanlan.zhihu.com/p/22049891</link><description>&lt;p&gt;&lt;img src="https://pic2.zhimg.com/7138b514eda846f9aaca84e9ee650ebd_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;关于作者&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首次看到Bengio教授这篇论文是在[对话机器学习大神Yoshua Bengio（下）]&lt;/p&gt;&lt;br&gt;这篇论文是提问时候偶然提到的，按照Question中说法：&lt;strong&gt;您(Bengio)是机器学习领域唯一公开的以深度学习来研究社会学的科学家。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;所以相对于Hinton教授给我们带来生物神经方面的New Idea和Surprise（Hinton有剑桥认知心理学学士学位)从社会、文化角度扩展Deep Learning是非常有必要的，Bengio教授把重点放在了这个方向，其实和他的研究经历有关。&lt;br&gt;&lt;br&gt;他是神经网络(Forward-NN、Recurrent-NN)在自然语言处理(NLP)方向上推广的重要贡献人之一。[Bengio03]&lt;br&gt;&lt;br&gt;如果你曾经品读过他的100页大论文 [Learning Deep Architectures for AI] ，就会不得不佩服Bengio教授的思想。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;如果说Hinton创立了Deep Learning，那么Bengio就是Deep Learning最好的布道师和奠基人。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;作为一位坚持不加入任何商业公司的纯粹学者、Quora最受欢迎的机器学习专家，他对Deep Learning和AI的贡献非常大。&lt;img src="7138b514eda846f9aaca84e9ee650ebd.jpg" data-rawwidth="629" data-rawheight="247"&gt;&lt;b&gt;学习情结&lt;br&gt;&lt;/b&gt;&lt;br&gt;&lt;b&gt;1.1 学习的两大动机&lt;/b&gt;&lt;br&gt;&lt;br&gt;我们为何而学习？消磨人生？显然不是。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;人类的学习情结大致由两方面构成：&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;(I) Predictive Criterion(预测):&lt;/b&gt;&lt;br&gt;我们会不自觉的受到[时间访问局部性]的浸染，即当前遭遇的知识(Context)很有可能在近期再次相遇(Encounter)。&lt;br&gt;显然，如果不对当前的Context进行剖析，那么意味着未来多次相遇时，无法做出预测(Prediction)，得不偿失。&lt;br&gt;贪心机制会诱导我们尽早地对不确定的Context学习，用成语来讲就是“ 亡羊补牢，未为迟也”。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;(II) Reward Criterion(激励)：&lt;/b&gt;&lt;br&gt;一些与生存有关的问题会被独立诱导学习。&lt;br&gt;比如吃货在获取[如何满足口福]方面，显然有很强的积极性。&lt;br&gt;又比如，为了满足生理需求，人类在[性知识]方面的探索可谓是五花八门。&lt;br&gt;其中(I)广泛见于“监督学习”、“非监督学习”，(II)的相关工作由“强化学习”完成。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 预测学习算法&lt;/b&gt;&lt;br&gt;&lt;br&gt;“预测”显然在学习进程中处于主体地位，也是神经网络拟合模型的起源。&lt;br&gt; 预测的相对确定性建立在，以当前参数θ对不确定性样本评估的[误差]的[期望]:&lt;img src="abdf6d0a342a0ab6cdb2959cdd2d7b37.jpg" data-rawwidth="568" data-rawheight="59"&gt;期望积分形式不是很直观，它有更亲切的表达形式：&lt;br&gt;&lt;img src="fdd8584bc2634435dd364adf9bafa671.jpg" data-rawwidth="390" data-rawheight="72"&gt;我们熟知的大部分监督学习算法，其优化目标基本都是min Criterion(θ)&lt;br&gt;显然，当θOptimization≈θReal，我们认为Optimization过程引导Agent完成了学习任务。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 假设：优化&lt;/b&gt;&lt;br&gt;&lt;br&gt;你觉得(II)是否很功利？显然是，我们人类只会贪婪地朝满足自己的目标发展，这是骨子里的[目标性]。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;你觉得(I)是否很功利？显然也是，数学式不会给我们反驳的机会，这也是骨子里的[目标性]。&lt;br&gt;综合(I)(II)，Bengio提出了第一个假设：&lt;img src="b62eab0bab0ea60ae32b90e14b1a800a.jpg" data-rawwidth="587" data-rawheight="112"&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;推理、然后误入歧途&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;2.1 瞬时速度与平均速度&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;考虑一个高一物理题：&lt;br&gt;&lt;img src="90653ba9955547570b96ba59709839b5.jpg" data-rawwidth="573" data-rawheight="81"&gt;&lt;b&gt;解：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;平均速度=0/10=0m/s&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;瞬时速度=2330/10=233m/s&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这是一个有趣的问题，因为从微观上来讲，这个人很努力地跑，大家心知肚明。&lt;br&gt;但是，从宏观上来讲，他其实在原地踏步，浪费时间。&lt;br&gt;这样的问题在自然界是普遍存在的，贯穿[人类文明]的发展。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 微观视点：推理&lt;/b&gt;&lt;br&gt;&lt;br&gt;推理(Inference)过程是相当微观、细致的，起码它是神经元级别的变化。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;非线性分类模型的拓扑结构，基本都有隐变量(Latent variables)。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;隐变量经过非线性的激活(重视or不重视)，变成隐元(Hidden Units)，(注：SVM的隐元即支持向量)。&lt;/p&gt;&lt;p&gt;隐元介于输入和输出之间，包含着对输入的[编码信息]，解释(explain)着当前的观测样本。&lt;/p&gt;&lt;p&gt;在Forward-NN里，隐元只和上一层隐元(或输入)连接[前向连接]，在Recurrent-NN里，隐元还和当前层隐元连接[递归连接]。&lt;/p&gt;&lt;p&gt;推理是迭代的(iterative)，时间尺度为[秒级]，不停地修改[Configurations]，尽可能认同更多的(agree more)观测样本。&lt;br&gt;&lt;/p&gt;&lt;p&gt;每次瞬时的变化(change)，意味着心智状态(state of mind)的瞬变。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;变化的跳跃范围可能很大，因为神经元数量众多，且很敏感，容易牵一发而动全身。不像学习，是一个徐徐渐进的过程。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 宏观视点：学习&lt;/b&gt;&lt;br&gt;&lt;br&gt;学习是一个渐进、积累的过程，时间尺度为[分钟级]。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;推理算是学习的子过程，个人用一个有趣数学式来表达：Learning(t)=∫Inference(t)dt&lt;br&gt;即对推理过程函数中的[时间]进行积分，得到学习过程函数。&lt;br&gt;&lt;br&gt;回到瞬时速度和平均速度问题，众多推理过程可能是雄心勃勃的，但是构成的学习过程却可能是毫无意义的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;正如积分最倒霉的是积成了[零]，只要积分曲线(面)具有对称性。推理过程显然也可以呈现对称性，而且是大量地对称性。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;网络上的一个著名励志段子：&lt;br&gt;当你的所拥有的知识撑不起你的野心时，你只能 静下心去努力学习 原地打转。&lt;br&gt;闭门造车的推理，即便花了时间，得到的其实还是[零]。&lt;br&gt;&lt;br&gt;&lt;b&gt;2.4 局部最小值：上帝也无法逃离？&lt;/b&gt;&lt;br&gt;&lt;br&gt;[局部最小值]算是神经网络的经典宿敌了，自上个世纪90年代始，以Vapnik为首的统计机器学习数学家就不断对此炮轰猛击。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在今天看来，[局部最小值]或许并不是神经网络的过错，大量证据表明，人类在自然学习过程中本身就会出现局部最小值。    至于SVM为什么能绕开了[局部最小值]，和Vapnik这位大数学家有关，他用求解最优分隔平面巧妙地变换了神经网络的目标函数。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;从网络拓扑结构来看，SVM仅算是单隐层神经网络的特例。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;凸优化、自适应隐元(支持向量)、完备的数学体系，真的让SVM从本质上脱离了神经网络吗？&lt;br&gt;在[论文2.7节]中，Bengio对此发出的疑问：&lt;img src="4c16bef2ba96e3e72253b083553ba501.jpg" data-rawwidth="609" data-rawheight="96"&gt;&lt;b&gt;2.5 假设：局部下降&lt;/b&gt;&lt;br&gt;&lt;br&gt;既然人类自然学习本身就存在局部最小值，那么是如何掉进去的，又如何从中逃逸？&lt;br&gt;来看Bengio的第二个假设：&lt;img src="457e17cf7565f950b631f30f85e2b165.jpg" data-rawwidth="608" data-rawheight="71"&gt;即是说，学习过程依赖于对[误差]的不断局部下降，这是一个徐徐渐进的过程。&lt;br&gt;&lt;br&gt;[论文2.7节]最后有一个观点很突兀：&lt;img src="6cb612f0435941e6b07ba05dfb1ac14d.jpg" data-rawwidth="608" data-rawheight="71"&gt;即是说，神经推理过程不会陷入[局部最小值]，神经元可以随时大幅度变化自己，根本没有机会结识[局部最小值]。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;那么问题就肯定出在学习过程了，个人总结出&lt;b&gt;四个关键点：&lt;/b&gt;&lt;br&gt;&lt;b&gt;(I)&lt;/b&gt; 首先，根据Bengio的假设，学习过程显然是间断的，少有学习任务能够一气呵成。&lt;b&gt;(II) &lt;/b&gt;一旦间断，就肯定需要[断点续传]，每次选择一个局部的方向继续工作。&lt;br&gt;&lt;b&gt;(III) &lt;/b&gt;在学习后期，学习速率通常会变得很慢（比如厌学了)，导致学习几乎停滞不前了。&lt;br&gt;&lt;b&gt;(IV)&lt;/b&gt; 知识是有限的，尤其是在解决世界难题上(如NP完全问题)，一部分人仍然会固执地坚持似乎不正确的解(N=NP)。&lt;br&gt;&lt;br&gt;(II)显然不能保证，当前由贪心原则选择的[最速下降路径]一定是[通往全局最小值路径]的子路径。因为函数曲面必然存在大量[局部最小值]，所以搜索必然不会恰好选择[通往全局最小值路径]，而是在中间就落入局部最小值。&lt;br&gt; 这是为什么[局部最小值]不可避免的自然原因。当然，Bengio还指出的更坏的情况，当前[最速下降路径]，甚至不能保证会到达[局部最小值]。&lt;br&gt;依据是[随机梯度下降]与[批梯度下降]。&lt;br&gt;我们知道，[随机梯度下降]没有选择全部样本来学习，而是逐部分学习，最后会下降到离[批梯度下降]稍远的偏移位置。&lt;br&gt;这是[断点续传]策略造成的不可避免型[精度误差]，Bengio称该偏移位置为[Effective Local Minima]。&lt;br&gt;&lt;br&gt;(III)的解释可从二阶下降的牛顿法角度观察，学习速率由二阶Hessian矩阵控制：&lt;br&gt;Δxt=Ht^−1⋅gt随着[误差]的不断减小Ht^-1近似也在减小，学习速率的下降似乎是无法挽回的。&lt;br&gt;以至于最后，学习进程几乎处于停滞状态，此时很大可能在某个局部最小值附近，于是就出不来了。&lt;br&gt;从社会角度来看，出自[对话机器学习大神Yoshua Bengio（下）]中的Q&amp;amp;A：&lt;br&gt;&lt;img src="771a38734186825e91fd7b8067ef0c83.jpg" data-rawwidth="579" data-rawheight="185"&gt;对于(IV)，我们都知道： “失败乃成功之母”。失败是必然有的，说明你掌握的知识有限。物理学巨匠牛顿说过：“如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。”随着越来越多的先辈在[局部最小值]中摸爬滚打，我相信，在未来，我们的子孙肯定会到达[全局最小值]。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.6 逃逸策略&lt;/b&gt;&lt;br&gt;&lt;br&gt;尽管在自然学习过程中，陷入局部最小值不可避免，但人类目前至少存在两个途径来逃逸。&lt;br&gt;&lt;br&gt;&lt;b&gt;策略一：绞尽脑汁，豁然开朗&lt;/b&gt;&lt;br&gt;当我们被一个问题所困时，[绞尽脑汁]往往会从其他方向找到突破点。&lt;br&gt;该策略是有实验依据的，由Bengio组在2014年(本论文写成2年后)发现：当参数(神经元)的维度很高时，局部最小值会蜕变成鞍点：&lt;img src="f9fbc3b080a2e2887cd4b01369703c22.jpg" data-rawwidth="574" data-rawheight="250"&gt;参数高维，即每层中神经元个数很多，VC维超级大，会出现和直觉相悖的现象：几乎不存在局部最小值。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Bengio对此的解释： [2015蒙特利尔深度学习暑期学校之自然语言处理篇.哈工大SCIR]&lt;img src="8ce14b15597244d4436d6db8b1511b10.jpg" data-rawwidth="574" data-rawheight="185"&gt;Hinton教授的Dropout方法，实际上把已经神经网络变成了一个动态平均结构，这与生物神经网络是类似的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;尽管这时候模型总VC维已经庞大的无法直视，但是只要擅加稀疏和屏蔽，瞬时的结构风险是并不大的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;生物神经网络的Dropout稀疏率达95%以上，也就说，同时有95%的神经元被屏蔽，仅有5%是在工作的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[绞尽脑汁] 似乎能够强行降低稀疏率，立刻提高维度，寻求鞍点来突破。(仅个人假设)&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;当从局部最小值逃逸后，人会放松下来，又把稀疏率提高，服从结构风险最小化原则。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这时候，只要保持一个愉悦的心情，学习就会有效得多。如果继续保持重压，那么过拟合显然不可避免的。    正所谓：打骂(神经元数量使用特别多）出来的清华北大，不是残就是废（过拟合）。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;策略二：知识扩充&lt;/b&gt;&lt;br&gt;&lt;br&gt;这大概是最普遍的方法了，大量的局部最小值由于“知识有限，智商不足以解答”而产生。    “知识有限”不仅直接体现在观测样本上(如Cifar10相对于ImageNet)，还间接体现在“归纳的中间结果”上。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;比如在数学证明时，缺乏对前置定理的了解，你几乎是无法进行下一步推导的。&lt;br&gt;这也是本篇论文讨论的重点，[文化浸染是如何协助从局部最小值中逃逸的？]&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;深度结构与层次抽象表达&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;3.1 莫名其妙的功臣——Hubel&amp;amp;Wiesel&lt;/b&gt;&lt;br&gt;&lt;br&gt;几乎大部分关于Deep Learning资料，开篇必引1981年诺贝尔生理学或医学奖获得者, Hubel&amp;amp;Wiesel。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;他们在1974年经典之作[Visual-field representation in layer IV of monkey striate cortex]，可以说是万恶之源。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;至于深度神经网络的最初构想是否真的与Hubel&amp;amp;Wiesel的工作有关，并没有确切证据。但拿诺奖来贴金，这是非常划算的买卖。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2 抽象观点&lt;/b&gt;&lt;br&gt;&lt;br&gt;对RBM的可视化是简单的，但是对DBN的可视化确实艰难的。因为第二层以上参数学习的是经过non-linearity变换后的特征。    Bengio弟子之一Dumitru Erhan在2009年提出了对深度网络可视化的方法，用实验验证了深度结构的逐层抽象能力。[Erhan10]&lt;img src="85ccf40f558720e0c12822b5a20ffcf1.jpg" data-rawwidth="572" data-rawheight="202"&gt;&lt;b&gt;3.3 函数观点&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;神经元在每层时，都有一个对应的表达函数，深度越大，函数积链越长，后一层的函数链，其实是由前一层的函数链递推得到。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这像极了现代程序设计方法——按功能设计子函数、增大代码重用率。&lt;br&gt;尽管单隐层神经网络被证明可将任何函数拟合至精度为1/n [Barron 1993]，但是正如我们不会写出单函数代码一样，这并没有意义。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;单隐层结构本身就不是科学的，起码它缺少神经元[复用](re-use)机制，效果不会很好。&lt;br&gt;Bengio组的另一个研究热点即使探索深度的[等效性]，当前呼声较高的是这个假设：&lt;br&gt;Theorems on advantage of depth:&lt;br&gt;Some functions compactly represented with k layers may require exponential size with 2 layers.&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;(Hastad et al 86&amp;amp;91, Bengio et al 2007,Bengio&amp;amp;Delalleau 2011, Braverman 2011,Pascanu et al 2014, Montufar et al 2014)   以SVM为例，当搜索空间无限庞大时，K层神经网络的搜索范围与2n2n个支持向量等效，这时候选择SVM是不妥的。   &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;反之，要是研究如何拟合sinxsinx，SVM和K层神经网络几乎是难分伯仲。&lt;br&gt;这显然又回到了NFL(No Free Lunch)上，既不可盲目支持深度神经网络，更不可盲目排斥深度神经网络。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[复用]机制让神经网络结构变得很灵活，副作用也很明显，它服从于乘法原理。&lt;br&gt;&lt;br&gt;从图论观点来看，假设每个结点连出两条路径，那么到达深度nn的结点就有2n2n条路径，复杂度呈指数级增长。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;大量的可选择路径，让模型在搜索过程中，不停陷入形形色色的局部最小值当中，这似乎是无法避免的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.4 层次抽象与层次理解&lt;/b&gt;&lt;br&gt;&lt;br&gt;回忆一下，当你初逢《高等数学》，你是如何理解积分这样高级概念的：&lt;br&gt;&lt;img src="9b4f6e77b80f7ae395cc0e1b122f3146.jpg" data-rawwidth="571" data-rawheight="374"&gt;关于这个“世界”的表达，不同的人会产生不同的抽象，让自身去更好地理解。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;虽然抽象内容各不相同，但这种行为是共性的——层次结构、分布联系、深度计算。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;鉴于此，Bengio提出第三个假设：&lt;/p&gt;&lt;p&gt;Deep Abstractions Hypothesis：Higherlevel abstractions in brains are represented&lt;br&gt;by deeper computations (going through more areas or more computational steps in&lt;br&gt;sequence over the same areas).&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在计算模型中捕风捉影&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;4.1 非监督学习、监督学习&lt;/b&gt;&lt;br&gt;&lt;br&gt;数据挖掘最常见的思路：先聚类，定模式。后分析，精结果。这种主动意识的行为，真的没有潜意识在诱导嘛？    &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;考虑一个小学低年级的奥数题：&lt;/p&gt;&lt;p&gt;&lt;b&gt;找规律，并填空。&lt;br&gt;&lt;/b&gt;&lt;b&gt;0、1、1、2、3、5、8、13、21、34、__、__、__。&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;尽管我们对斐波那契数列耳熟能详，但世界上似乎还没有任何一种聚类算法能够发现这种规律。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;那些年，愚昧无知的我们又是如何解决这个问题的呢？&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;答案无非就是：&lt;/b&gt;&lt;b&gt;在数次失败、偶然恰好尝试[斐波那契公式]时，假设这是这种[概念]是对的，用给定数据计算多个实例x~，用x~去验证x，只要∑(x~−x)^2=0，即我们的[概念]发现的模式没有任何错误，则认为，解答正确，并且记忆这种解法。 &lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;很不凑巧，这恰是RBM/AutoEncoder的思路，而我们的潜意识似乎正好在使用它。    对斐波那契数列规律的五花八门探索，不仅基本不是最优的，更多还是错的(与学习任务南辕北辙)。&lt;br&gt;如果碰巧有个模式是对的，那么对后续的学习就轻松许多，或是不用学习、或是作为一种暗示和引导。&lt;br&gt;&lt;/p&gt;&lt;p&gt;据此两点，Bengio提出第一、第二观测现象：&lt;/p&gt;&lt;b&gt;4.2 搜索之殇&lt;/b&gt;&lt;br&gt;&lt;br&gt;神经网络并没有什么奇妙的内涵，它本质仍然是一个[启发式穷举搜索模型]。类似于A*，它的启发式方向是[贪心策略：最速下降]。    但这个穷举，是一个连续实数型的无尽穷举，是一个曲面复杂的连续函数的生成。&lt;br&gt;&lt;br&gt;&lt;p&gt;类比于[深度优先搜索]，我们都知道，随着深度的增加，会出现越来越恶劣的情况。&lt;br&gt;而神经网络同样有这样的厄运，这在上个世纪90年代，是被各界广为批判的，相关实验在[Erhan09]中。&lt;/p&gt;&lt;p&gt;据此，Bengio提出第三观测现象：&lt;br&gt;Observation O3: directly training all the layers together would not only make it difficult to exploit all the extra modeling power of a deeper architecture but would actually get worse results as the number of layers is increased.    O3仍然有一些其它的佐证，最形象的要属[Erhan09]中对同结构，随机初始化的各个神经网络，追踪多个训练阶段，&lt;br&gt;    剥离输出层，将最后的隐层输出降维，且2D可视化，得到轨迹线图：&lt;img src="782ce6616c72bec71159ece455cec31d.jpg" data-rawwidth="579" data-rawheight="285"&gt;这个轨迹线隐含着两点有趣的现象：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(I)&lt;/b&gt; 不同初始化的神经网络，选择了独立的搜索方向，陷入了独自的局部最小值中，彼此不会重合。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(II)&lt;/b&gt; 没有预训练的神经网络，杂乱无章地在乱跑。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;据此，Bengio提出第四、第五观测现象:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Observation O4: &lt;/b&gt;No two trajectories end up in the same local minimum. This suggests that the number of functional local minima (i.e. corresponding to different functions, each of which possibly corresponding to many instantiations in parameter space) must be huge.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Observation O5:  &lt;/b&gt;A training trick (unsupervised pre-training) which changes the initial&lt;br&gt;conditions of the descent procedure allows one to reach much better local minima, and these better local minima do not appear to be reachable by chance alone (note how the regions in function space associated with the two “flowers” have no overlap at all, in fact being at nearly 90 degrees from each other in the highdimensional function space).&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.3 假设：从模型走向人类&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Bengio根据以上的Observation作了三个伟大的假设，他认为：人类和现在的神经网络模型一样愚蠢，饱受[局部最小值]与[难以驾驭深度结构]的折磨。&lt;br&gt;&lt;/p&gt;&lt;p&gt;前提条件：[One Single Human Learner]，当然这个条件是不存在的，除非世界上除你以外的人都死光了&lt;br&gt;根据 Local Descent Hypothesis、O4、O5，Bengio提出局部最小值假设：Local Minima Hypothesis：Learning of a single human learner is limited by effective local minima.&lt;br&gt;结合O3，有对于训练深度结构的艰难假设:&lt;img src="b1405a38608fc6aba02f44886873cdd5.jpg" data-rawwidth="578" data-rawheight="76"&gt;最后，是一个关于徒有深度结构、却几乎不能利用之来逐层抽象的假设：Abstractions Harder Hypothesis：A single human learner is unlikely to discover high-level abstractions by chance because these are represented by a deep&lt;br&gt;sub-network in the brain.&lt;br&gt;这些看起来有些天方夜谭，人类怎么可能像机器学习模型那样愚蠢？&lt;br&gt;&lt;br&gt;但如果从史前时代开始，世界上就你一个人，你保持不死状态直到今天，没准今天真和模型一样愚蠢。&lt;br&gt;而之所以没有出现这种情况，是因为[社会文化浸染]与[有性繁殖]，让我们的进化地如此强大。&lt;br&gt;&lt;b&gt;社会：神经网络们的互联网E时代&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;5.1 双脑聊天&lt;/b&gt;&lt;br&gt;&lt;br&gt;考虑这样一个场景：那年，牛顿还没见过苹果长什么样子，也不知道什么为红色。他偶然来到果园，指着树上的红苹果，问山德士上校：“我听说果园里有苹果，这是苹果嘛？”上校回答道：“你看，这种颜色叫红色，而红色的球状的物体是就是苹果。”牛顿继续问：“味道怎么样？”上校冷笑道：“你来KFC品尝一下我们的苹果鸡腿堡不就行了。”    将这个场景用神经网络模型表示：&lt;img src="0a07c021dd59fd7d5ad6cdd1faadfdd3.jpg" data-rawwidth="597" data-rawheight="335"&gt;牛顿在未经过任何学习的情况下，直接从上校口中获取关于苹果的[颜色][形状]，这是一件非常不可思议的事。因为从目前的模型来看，要让神经网络监督训练[颜色][形状]，仍然需要大费周章。&lt;br&gt;&lt;/p&gt;&lt;p&gt;然而，此时牛顿还不知道苹果的[味道]，所以当他看见一个[红色的皮球]，会认为这是苹果，上前咬一口，咸咸的，所以牛顿可能得出了这样一个真理，[苹果是红色的、圆的、咸咸的]，而[红色的、圆的、甜甜的]的物体肯定不是苹果。&lt;br&gt;&lt;/p&gt;&lt;p&gt;看完上面的笑话，我们一般会觉得，牛顿其实也很蠢。&lt;br&gt;如果没人告诉他[苹果是甜甜的]，那么他可能一辈子会把[红色的皮球]错认为是[红色的苹果]，掉进一个局部最小值中。&lt;br&gt;&lt;/p&gt;&lt;p&gt;其实事情不比那么糟，牛顿起码有一个方法，让[红色的、圆的、甜甜的]的物体也被他认为是苹果:只要他不知道[苹果是可以吃的]，即，牛顿的神经网络中，扔掉关于[味道]的隐层，让网络深度变浅。&lt;/p&gt;&lt;p&gt;这不禁让我们想起了 O3 , 错误地直接训练多层网络，会让结果变糟，此时浅层结构胜于深度结构。&lt;/p&gt;&lt;p&gt;但，如果牛顿使用了正确的方法，比如真的去KFC吃了苹果鸡腿堡，那么就会认为[红色的、圆的、咸咸的]肯定不是苹果，这比直接舍弃[苹果是可以吃的]有效地多，此时深度结构远胜于浅层结构，符合O1。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.2 信息交流，让世界更美好&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    牛顿认知苹果，归功于山德士上校苦口婆心的教导，以口头语言形式。&lt;/p&gt;&lt;p&gt;从信息论观点来看，我们认为汉语的信息量比英语大，大部分语言的信息量要比行为动作要大。&lt;/p&gt;&lt;p&gt;即上校用四书五经而不是美式英语向牛顿解释，牛顿收获可能更大。&lt;/p&gt;&lt;p&gt;上校用美式英语而不是肢体动作向牛顿解释，牛顿收获可能更大。&lt;/p&gt;&lt;p&gt;但无论如何，只要[交流](Communication)能够传递到另一方的神经网络当中，成功消除神经元混沌，就足够了。&lt;/p&gt;&lt;p&gt;当然，我们还是希望能够实现信息量更丰富(Richer)的[交流]，也许偶然之中，就能突破自己的其它[局部最小值]。&lt;/p&gt;&lt;p&gt;[交流]不仅仅局限于人类，动物之间那些[危险警示]，也可视为[交流]，这样，愚蠢的动物才能有效学习如何保护自己。    据此，Bengio提出引导学习假设：brain can learn high-level abstractions if humans, which act as hints or indirect supervision for these high-level abstractions. &lt;/p&gt;&lt;p&gt;&lt;b&gt;5.3 交流背后的那些事&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在牛顿的识别苹果神经网络中，关于[颜色][形状]的隐层，被山德士上校给替换了。&lt;br&gt;这不过只是瞬时行为，然而仅仅就结束了嘛？显然不是。&lt;br&gt;&lt;br&gt;牛顿回去之后，将今天的收获见闻回想了一遍，对苹果有了更深的认识。&lt;br&gt;&lt;br&gt;在神经网络中，我们假设：&lt;strong&gt;[回想]&lt;/strong&gt;过程发出了一个训练信号(Train Signal)，牛顿的神经元立刻开始飞舞起来，经过多轮的推理(Inference)，将[颜色][形状][味道]三个隐层的值给[Fine-Tune]下，降低了识别错误率。显然，因为牛顿爱思考的性格，[交流]触发了牛顿的学习机制。这倒是能解释，为什么同样的老师上了同样的课，学生有的考上了清华，有的成了家里蹲。&lt;br&gt;&lt;br&gt;&lt;b&gt;5.4 交流引发强大的心智活动&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[交流]在神经系统中产生最频繁的效应就是[虚拟环境]，通俗点就是“不在场却能身临其境”。&lt;br&gt;&lt;/p&gt;&lt;p&gt;经[交流]直接修改的神经网络，上下协调性不佳，需要[Fine-Tuning]，可能还需要一些新样本来强化记忆理解。&lt;/p&gt;&lt;p&gt;Hinton教授认为生物神经元普遍包含两种方向的计算。&lt;/p&gt;&lt;p&gt;这是一个证明RBM和AutoEncoder合理性的突破点，因为大多数情况下，需要在“判别模型”和“生成模型”间快速切换。&lt;br&gt;&lt;br&gt;[虚拟环境]的产生可以用“生成模型”来解释，只要将前向传播的方向逆置，从[颜色]到[形状]、[味道]，生成“苹果”，就能在没有看见苹果的情况下，脑中浮现出苹果。&lt;/p&gt;&lt;p&gt;[交流]将关于世界的[概念](Concept)传播，传播通道的带宽是有限的，这意味着将产生[竞争]。&lt;/p&gt;&lt;p&gt;比如，我们在接受[地球是圆的]同时，会排斥[地球是方的]、[地球是三角形的]。&lt;/p&gt;&lt;p&gt;[竞争]是自然界的标准法则，所谓优胜劣汰，适者生存.最后留下的[概念]在族群(Population)神经网络的[局域网]中，成为霸主，此时[概念]进化为[信仰]。[信仰]同[深度结构]类似，是一把双刃剑。&lt;/p&gt;&lt;p&gt;正确的[信仰]，如哥白尼的[日心说]，有助于我们走出关于天体运动的[局部最小值]。错误的[信仰]，如亚里士多德的[力是维持物体运动的原因]，则会长期把我们囚禁在[局部最小值]中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.5 外传：合理的课程学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该部分是Bengio个人的一个小研究，基于 Guided Learning Hypothesis. , 即学习过程是可以被引导的。&lt;/p&gt;&lt;p&gt;课程顺序学习制度是人类在学习任务上的经验总结，一个实例如下：&lt;strong&gt;CTSC(国际信息学奥林匹克竞赛中国队选拔赛)1997 [选课]，背景描述：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在选修课程中，有些课程可以直接选修，有些课程需要一定的基础知识，必须在选了其它的一些课程的基础上才能选修。例如《Frontpage》必须在选修了《Windows操作基础》之后才能选修。我们称《Windows操作基础》是《Frontpage》的先修课。&lt;/p&gt;&lt;p&gt;从感觉上来看，先修课制度划定了学习进程的任务次序，具有[由浅入深性]、[无后效性]，保证学习进程循序渐进。&lt;/p&gt;&lt;p&gt;同时，让学习任务也具有深度结构，这是一个合理的层次抽象策略( 从简单抽象(子抽象)到复杂抽象(组合抽象) ) 。&lt;/p&gt;&lt;p&gt;据此，Bengio假设，如果在机器学习任务中，能够将样本的学习难度划分，重新安排学习顺序，那么就会有更好的效果。&lt;/p&gt;&lt;p&gt;因为学习是局部渐进的，所以低难度的学习样本，目标函数曲面较为简单，能够较为接近[全局最小值]。&lt;/p&gt;&lt;p&gt;再次，使用中、高难度的样本，函数曲面逐渐复杂，最后仍然会落入[局部最小值]中，不过位置更接近[局部最小值]。&lt;br&gt;&lt;img src="5846e100e3ab1b2ba0e1813bbee6116a.jpg" data-rawwidth="585" data-rawheight="358"&gt;&lt;b&gt;进化论 与 迷因论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;6 .1 理查德·道金斯的迷因论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;理查德·道金斯(Richard Dawkins)大概是世界上最野心勃勃、最不为人知的生物进化学家了。&lt;br&gt;在他的研究中，最令人难忘的工作就是提出“迷因论”，[果壳网]有一篇很好的翻译介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.2 搜索观点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;几乎世界万物的生命进程都可以看作是一个搜索进程。&lt;/p&gt;&lt;p&gt;基因的进化是在搜索——  不断舍弃劣等基因，探索高等基因。(历程：几十万年）&lt;/p&gt;&lt;p&gt;迷因的进化是在搜索——  在数量爆炸性的文化中，寻找传承不息的迷因子。(历程：几十年、几百年、几千年)&lt;/p&gt;&lt;p&gt;学习的进化是在搜索——  个体为将所掌握的知识统一，琢磨一个支点去平衡它们。(历程：几分钟、几小时、几天)&lt;/p&gt;&lt;p&gt;推理的进化是在搜索——  神经元为了协调神经关系，不断变化自己。（历程：几毫秒，几秒）&lt;/p&gt;&lt;p&gt;这些搜索策略，因为大自然的馈赠，获得了足以并行搜索的条件。&lt;br&gt;基因的并行在于[有性繁殖]，一个家族生生不息的繁衍，一起推动着这个家族基因的并行搜索进程。&lt;br&gt;&lt;br&gt;迷因的并行在于[宿主传播]，显然，知乎的一条回答，能够在短时间内进入数以千计人的脑中，加深“膜蛤”印象。&lt;/p&gt;&lt;p&gt;学习的并行在于[个体交流]，一个经典案例就是   “牛顿是如何认知苹果是红色的、圆圆的、甜甜的？”&lt;br&gt;&lt;br&gt;推理的并行在于[神经网络]，神经元彼此连接着，构成神经网络，而神经冲动无时无刻不在产生，瞬息万变。&lt;/p&gt;&lt;p&gt;自然界的四大并行搜索是可怕的，它们之间层层叠加，协助人类以最快的速度从[局部最小值]中逃逸，无限逼近[全局最小值]。&lt;img src="5100db87d58540df2b45ce59d51cd239.jpg" data-rawwidth="525" data-rawheight="322"&gt;&lt;b&gt;6.3 先有鸡，还是先有蛋&lt;/b&gt;&lt;/p&gt;&lt;p&gt;尽管迷因子看起来无所不能，但仔细分析，道金斯仅给出迷因的两种特性：[自由脑入侵]与[噪声复制] (noisy copy)这两种特性只能描述迷因子发展的中间状态，一个很严肃的问题必须被考虑：最早的迷因子无从复制，又如何而来？&lt;/p&gt;&lt;p&gt;该问题一定程度上等效于哲学上的“鸡生蛋，还是蛋生鸡？”&lt;/p&gt;&lt;p&gt;显然，要解决这个疑问，就必须从迷因论中跳出来，在其他领域寻找论据。&lt;/p&gt;&lt;p&gt;最先、也是最容易产生的一种假设，就是迷因起源于它的栖息地——脑，由神经网络而生。如果这种假设是正确的，那么迷因就获得第三种特性 [迷因重组]。&lt;/p&gt;&lt;p&gt;[迷因重组]的想法显然效仿于[基因重组]，[基因重组]是[有性繁殖]生物体内在的一种可怕技术人类的基因数量只有5万，且[基因突变]的频率太低，但是一旦这些基因排列组合，产生的基因型数量是庞大到无法计算的。&lt;/p&gt;&lt;p&gt;[迷因重组]更加可怕，人的一生接触的迷因子显然不止5万，所以[迷因重组]可以产生大量的崭新迷因子。&lt;/p&gt;&lt;p&gt;这似乎能解释迷因子的起源问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.4 迷因计算模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    [噪声复制]尽管看起来是像是一条毒瘤链，实际上它的速度是可怜的。&lt;/p&gt;&lt;p&gt;Bengio用了这样一个例子：一种迷因，在某轮传播中连接N个人，产生N个复制体。如果现在有M个人，且M&amp;gt;N，那么按照并行计算的概念，产生的M个复制体，会让下轮的迷因传播速度提升M/N倍。     这看起来很有道理，画成图应该是这样：&lt;img src="8ea7c00dcdb3e10b8a1ed3599f21b454.jpg" data-rawwidth="605" data-rawheight="363"&gt;从图上来看，显然是不符合上文关于人类“交流”的假设的，所以应该是这样：&lt;br&gt;&lt;img src="a7686d18c08ab7f94b5c16bca78fc6c3.jpg" data-rawwidth="583" data-rawheight="295"&gt;对于其中的某个人，他可能在短时间内，[连续]与[同轮]的人交换信息，这部分可以用一个子图描述：&lt;br&gt;&lt;img src="52cc5db55dddc71c084582b680a2ed07.jpg" data-rawwidth="599" data-rawheight="405"&gt;现在，让我们考虑一个更疯狂的想法，假如某人在每次交流时，对迷因的看法都不同，那么在这颗分治树上，叶子结点(最终的想法)与非叶结点(曾经的想法)对迷因都是有贡献的。 考虑对于深度为n的二叉树，总结点数的计算公式: N=2^n−1，这样，迷因子的传播速度，就可以呈指数级增长。&lt;/p&gt;&lt;p&gt;这只是理论上的最好传播效果，Bengio指出，实际交流跨度不会很大，所以这棵分治树的规模是有限的。&lt;/p&gt;&lt;p&gt;据此，Bengio提出迷因分治传播假设：Memes Divide-and-Conquer Hypothesis. Language, individual learning, and the recombination of memes constitute an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of memes, that helps humans build up better high-level internal representations of their world.   &lt;img src="8ba01f5eb8d4d242ab0e1c635595c5e9.jpg" data-rawwidth="581" data-rawheight="500"&gt;&lt;br&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;. &lt;/b&gt;&lt;br&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;br&gt;&lt;img src="5d3ffda1db1a21b6caabf20a1959a15b.jpg" data-rawwidth="81" data-rawheight="80"&gt;&lt;strong&gt;潘汀，&lt;/strong&gt;合肥工业大学计算机专业大三本科生，中科院计算所VIPL组2017级推免生。原ACM-ICPC算法竞赛选手，2015年获CCPC铜牌。2015年初开始研究机器学习，研究兴趣集中于对深度学习理论、应用(CV&amp;amp;NLP)及系统架构设计的综合探索。关于深度学习在面部情感分析方面应用的论文被《自动化学报》录用。&lt;br&gt;&lt;br&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325171&amp;amp;idx=1&amp;amp;sn=e721c69dffb6e4bb873b2c9f513e3a00&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325171&amp;amp;idx=1&amp;amp;sn=e721c69dffb6e4bb873b2c9f513e3a00&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;/p&gt;</description><author>程程</author><pubDate>Wed, 17 Aug 2016 12:36:42 GMT</pubDate></item><item><title>CVPR2016 论文快讯：人脸专题</title><link>https://zhuanlan.zhihu.com/p/22023527</link><description>&lt;p&gt;&lt;img src="https://pic2.zhimg.com/ac2953a54dccfdd3c98f36e7a954aa4d_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;前言&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;人脸识别，作为深度学习应用一个比较具有难度的方向，近几年一直得到工业界和学术界的广泛关注。目前大部分人了解到的人脸识别的性能都是从LFW数据库说起，大家一谈起人脸识别，都会认为目前人脸识别已经足够好了，好到确认性能已经到了 99.77%。但是人脸识别真的做到了我们看到的数字那样完善吗？远远没有！&lt;br&gt;&lt;/p&gt;&lt;p&gt;LFW数据库仅包含了部分场景的姿态、背景等变化，而且大部分实验结果都是基于严格提取关键点、人脸矫正后的训练样本和测试样本（今年CVPR做face alignment仍然是个很热门的方向）得到的。对于实际应用中的光照、对比度、抖动、焦点、模糊、遮挡、分辨率、姿态等因素影响人脸识别的复杂因素[12]依然没有得到完全解决。因此，一些更具有挑战性的人脸数据库也发布出来，比如MegaFace、IJB-A等数据库、微软百万名人数据库（不过这个数据库比较noisy）。&lt;br&gt;&lt;/p&gt;&lt;p&gt;此外，视频人脸识别也是目前仍然比较难的一个方向，今年没有出现LSTM或者attention model去做视频中人脸识别的论文，反倒是光流、LSTM做视频中Events, Actions, and Activity Recognition的论文比较多。&lt;br&gt;&lt;/p&gt;&lt;p&gt;本次会议大家可以关注人脸的以下几个点：人脸老龄化预测[1]、人脸的表情捕捉和复现[2]、人脸alignment（偏向于三维alignment、姿态变化较大情况下的alignment）、同时训练的级联CNN做人脸检测[3]、大规模人脸检索问题（度量学习）[4]、深度度量学习（常用于学习得到人脸的具有区分性的特征）[5]、人脸识别问题[6,7,8,9,10,11]、更具挑战性的人脸数据集MegaFace的提出等。&lt;/p&gt;&lt;p&gt;接下来对CVPR2016上与人脸相关的部分文章进行介绍。&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;一、深度度量学习&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.Deep Metric Learning via Lifted Structured Feature Embedding&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章来自斯坦福大学 Hyun Oh Song等人的工作。之所以介绍这篇论文，是因为deep metric learning是深度学习应用于人脸识别领域的一个最常用的方法，利用比较好的目标函数可以学到更具有区分性的人脸特征。目前比较经典的几篇人脸识别的论文都有用到deep metric learning的方法，比如facebook公司的DeepFace[13]里面有用到加权卡方距离和contrastive loss[15]两种度量方式。香港中文的DeepID2[14]里面有用到contrastive loss的度量。Google公司的Facenet里面有用到triplet loss[16]的度量。而这篇论文作为CVPR2016的oral paper，提出了利用训练的batch里面所有相同label的人脸对和不同label的人脸对的信息进行语义特征映射，来使得同类之间的距离更小，异类之间的距离更大。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="ad74f366c069b2a3d82afa11caa31030.jpg" data-rawwidth="312" data-rawheight="283"&gt;上图中，c是作者论文中挑选数据的示意图，红色表示相同label，蓝色表示不同label。不同于适用于verification的contrastive loss，和利用hard neg和hard positive的做identification的triplet loss，该论文的优化目标如下图，可以看到在选择数据进行训练的时候，作者实际上是利用了pair (i,j)的对应的所有的不同label的人脸信息。这样我们可以在当前batch的优化中，挑选出距离当前对(i,j)最小的负样本，从而使得其距离最大化。同时，也加了使得同类之间距离最小化的限制。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="7941b2c2710dddb33581671faf5d25b9.jpg" data-rawwidth="361" data-rawheight="122"&gt;&lt;b&gt;2. CP-mtML: Coupled Projection Multi-Task Metric Learning for Large Scale Face Retrieval&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;二、人脸识别&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Pose-Aware Face Recognition in the Wild&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这篇文章来自南加州大学Iacopo Masi，主要关注点在于人脸识别中的大姿态变化问题。不同于当前大部分利用单一模型通过大量训练数据，或者矫正人脸到正脸来学习姿态不变性的方法。作者通过使用五个指定角度模型和渲染人脸图片的方法处理姿态变化。作者主要利用的数据集是IJB-A数据库，同时对比了其与LFW的挑战性不同。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="1bfba5ac7201e520a6926a6d0fc96da6.jpg" data-rawwidth="574" data-rawheight="515"&gt;给定一个需要验证的模版对，每张图片都经过一个姿态分类器，然后不同的姿态输入到不同的CNN模型，从而提取到特征，并且匹配以得到分数。对于正面和侧面都有一个平面内对齐，对于0度角、40独角侧面、75度角侧面都有一个平面外旋转矫正。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="0b4a69a515baa17730f207c85ac568a0.jpg" data-rawwidth="608" data-rawheight="214"&gt;&lt;b&gt;2.Multi-view Deep Network for Cross-view Classification&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这篇文章来自中科院计算所山世光老师组Meina Kan的工作。类似于上篇论文，也是针对人脸识别中的跨视图或跨姿态问题提出对应的解决办法，这篇论文尝试移除人脸数据之间的跨模态差异性，并且找寻跨模态之间的非线性的差异性和模态不变性表达。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="14c28b46cf4a7f8f79a6cc84f5691cbe.jpg" data-rawwidth="461" data-rawheight="410"&gt;作者提出的MvDN模型，由两个子网络组成。模态特定子网络(view-specific subnetwork)用于移除指定模态的差异性，注意这里的多个自网络1,2，...,v是多路复用的方式，也就是说公共子网络独立的连接到指定模态的子网络。接下来的公共子网络(common subnetwork)用于获取所有模态的公共特征表达。作者使用Rayleigh quotient objective来学习整个网络。目标函数如下，&lt;br&gt;&lt;/p&gt;&lt;img src="3ae6593b225c32ba722a046cb64156ad.jpg" data-rawwidth="345" data-rawheight="61"&gt;其中样本类内离散度表示为下图，最小化类内离散度矩阵确保了跨模态之间的同类样本间的距离更近。&lt;img src="e29bd479bf7988a86284528265f5d663.jpg" data-rawwidth="345" data-rawheight="72"&gt;样本类间离散度表示为下图，这样可以最大化跨模态不同类之间的距离。&lt;br&gt;&lt;img src="39248be0d5d251cd86c08b34da70a5d2.jpg" data-rawwidth="317" data-rawheight="53"&gt;&lt;b&gt;3. Sparsifying Neural Network Connections for Face Recognition&lt;/b&gt;&lt;br&gt;&lt;p&gt;这篇文章来自香港中文汤晓鸥老师组Yi Sun大神的作品，在此膜一膜。早在DeepID2+里面，作者就做过sparse的一些解释，认为稀疏性对于卷积神经网络应用于人脸识别效果有提升。最近一年多，关于pruning（英伟达和斯坦福合作的论文[17]）和sparse应用于深度学习的文章比较多，也是神经网络优化的一个重要方向。这篇文章实际上是应该有类似于stacked Auto-Encoder的逐层单独训练得到初始化参数的灵感。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="8c2061bdcd89b35916d20be0aaba31dc.jpg" data-rawwidth="485" data-rawheight="492"&gt;作者以迭代的方式来稀疏convnet，每次仅仅对其中一层加稀疏限制，得到的整个模型作为下次迭代的初始化参数。作者从最后一个卷积层开始加稀疏限制，并且固定前面几层的参数。然后对倒数第二层局部连接层加稀疏限制，固定其他层的参数。依次从后往前。作者之所以先删除高层的连接的原因是因为，全连接层和局部连接层在深度模型中有大量的参数，而这些层里面的大量参数都是冗余的。同时Yi Sun也提到了具体如何用caffe去实现相关操作。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. The MegaFace Benchmark: 1 Million Faces for Recognition at Scale&lt;/b&gt;&lt;br&gt;&lt;/p&gt;这篇论文来自华盛顿大学的大规模人脸识别测试数据集。MegaFace资料集包含一百万张图片，代表690000个独特的人。这是第一个在一百万规模级别的面部识别算法测试基准。&lt;br&gt;现有脸部识别系统仍难以准确识别超过百万的数据量。为了比较现有公开脸部识别算法的准确度，华盛顿大学在去年年底开展了一个名为“MegaFace Challenge”的公开竞赛。这个项目旨在研究当数据库规模提升数个量级时，现有的脸部识别系统能否维持可靠的准确率。&lt;br&gt;&lt;br&gt;下图是人脸识别常用数据库的规模介绍。&lt;img src="f499cbac05c3dfd1ad0c5233242ee156.jpg" data-rawwidth="587" data-rawheight="121"&gt;&lt;b&gt;5. Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition&lt;/b&gt;&lt;br&gt;&lt;p&gt;这篇论文来自中国科学院深圳先进技术研究院，主要介绍如何年龄不变性人脸识别（AIFR）。作者在几个常用的人脸老龄化数据集上面做了实验，比如MORPH Album2, FGNET, CACD-VS。在CACD-VS数据库上超过了人类投票识别的结果。&lt;br&gt;&lt;/p&gt;&lt;img src="e8e17d3aa645f1c79b93635e5d774df8.jpg" data-rawwidth="611" data-rawheight="327"&gt;上图介绍了作者提出的LF-CNN以及训练过程，前面三个卷积层是正常的卷积，后面的两个卷积层是局部卷积层(最先在deepface论文中提出)，作者用的激活函数是PReLU,同时使用Latent Identity Analysis (LIA)方法来学习全连接层部分的参数。&lt;br&gt;作者分别使用了两部分数据来训练这两个并行的网络，第一部分是用于学习全连接层参数标注年龄和身份label的数据，第二部分是用于学习卷积层参数的只标注了身份label的数据。整个训练过程中，学习卷积层参数的时候，全连接层参数固定，并且最后既使用softmax loss，又使用contrastive loss。在学习全连接层参数的时候，卷积层的参数固定。具体的全连接层参数的学习过程可以参见论文以及下图，具体不做赘述。&lt;img src="c8d0890dfcbb7b527268fbc31003a11a.jpg" data-rawwidth="502" data-rawheight="474"&gt;&lt;b&gt;三、人脸老龄化&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;1. Recurrent Face Aging&lt;br&gt;&lt;br&gt;&lt;/b&gt;这篇文章是意大利特伦托大学的论文，也是CVPR2016的oral paper，主要是做人脸老龄化预测。以下图是作者论文模型的效果示意图，最左边一列是输入的图片，其他的几列分别是模型产生的更老龄化的人脸。&lt;br&gt;&lt;img src="ac2953a54dccfdd3c98f36e7a954aa4d.jpg" data-rawwidth="592" data-rawheight="268"&gt;作者认为传统的将年龄分组成离散组合，然后对于每个来源于相邻的年龄段组成的人脸对进行单步的特征映射方法忽略了相邻年龄段之间的in-between evolving states。由于人脸老龄化是一个平缓的过程，所以作者认为通过平缓的转换变换更合适。因此，作者利用两层的门循环单元作为基本循环模块，其中的底层将一个年轻的人脸编码成隐式表达，顶层用于将隐式特征表达解码成相应的更老的人脸。&lt;br&gt;&lt;img src="2660878b9a269be1cb3e727270fde943.jpg" data-rawwidth="598" data-rawheight="221"&gt;&lt;br&gt;作者使用两个步骤来进行操作。第一步是人脸归一化，第二步是老龄化模式学习。作者通过迭代优化特征脸和光流估计的方法来做人脸归一化。循环人脸老化模块如上图所示，利用RNN来建模相邻年龄段之间的老化模块。RFA通过之前状态人脸来产生进一步老化的人脸。训练好后，我们可以通过0-5岁年龄段图片的输入，一步步得到61-80年龄段的人脸老龄化预测结果。&lt;p&gt;&lt;b&gt;2. Ordinal Regression With Multiple Output CNN for Age Estimation&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;四、表情捕捉、复现&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Face2Face: Real-time Face Capture and Reenactment of RGB Videos&lt;/b&gt;&lt;br&gt;&lt;/p&gt;先来看段振奋人心的demo展示吧。实现表情捕捉，然后复现input video的表情。http://weibo.com/p/23044490fdc7728d1859aff62fb4ca62f2eba8&lt;br&gt;[一个小故事，当时cvpr2016现场，作者打算演示下demo，结果打开visual studio之后，就崩了o(〃'▽'〃)o]  &lt;br&gt;女生的表情作为输入源，将其表情map到施瓦辛格脸上。&lt;br&gt;&lt;img src="a1a88da6e8ab881b663df89efff55253.jpg" data-rawwidth="597" data-rawheight="197"&gt;这篇论文也是CVPR2016的oral paper。论文中能够实时重现一个人说话时的动作和表情，并将其映射到（视频中）另外一个人的脸上。该软件有一个强大的研究团队，包括来自普朗克信息学研究所（Max Planck Institute for Informatics）、埃朗根纽伦堡大学（University of Erlangen-Nuremberg）和斯坦福大学的研究人员。&lt;br&gt;这个技术的原理是通过一种密集光度一致性办法（Dense Photometric Consistency measure），达到跟踪源和目标视频中脸部表情的实时转换，由于间隔的时间很短，使得“复制”面部表情成为可能，但现在还没办法实现声音也一样模仿出来。[由于对这部分不是很了解，所以部分摘自新闻信息（〜^㉨^)〜]&lt;br&gt;&lt;br&gt;&lt;b&gt;五、人脸检测&lt;/b&gt;&lt;br&gt;&lt;b&gt;1.Joint Training of Cascaded CNN for Face Detection&lt;/b&gt;&lt;br&gt;&lt;b&gt;2. WIDER FACE: A Face Detection Benchmark&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;六、人脸对齐&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;1. Face Alignment Across Large Poses: A 3D Solution.&lt;/b&gt;&lt;br&gt;&lt;b&gt;2. Unconstrained Face Alignment via Cascaded Compositional Learning.&lt;/b&gt;&lt;br&gt;&lt;b&gt;3. Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders.&lt;/b&gt;&lt;br&gt;&lt;b&gt;4. Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment.&lt;/b&gt;&lt;br&gt;&lt;b&gt;5. Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting.&lt;/b&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;七、人脸重建&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;b&gt;1. Automated 3D Face Reconstruction From Multiple Images Using Quality Measures.&lt;/b&gt;&lt;br&gt;&lt;b&gt;2. A Robust Multilinear Model Learning Framework for 3D Faces.&lt;/b&gt;&lt;br&gt;&lt;b&gt;3. Adaptive 3D Face Reconstruction From Unconstrained Photo Collections.&lt;/b&gt;&lt;br&gt;&lt;b&gt;4. A 3D Morphable Model Learnt From 10,000 Faces.&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;结语&lt;/b&gt;&lt;br&gt;&lt;br&gt;总的来说，CVPR2016会议中关于人脸的论文仍然有很多，涉及到计算机视觉，图形学，深度学习等等方面，CVPR的工业界展示上面，也有很多令人振奋的demo。很多厂商都参展了，比如百度IDL，腾讯优图，商汤，格灵深瞳，旷视科技等。除了本文提到的论文，感兴趣的同&lt;br&gt;学和老师可以在CVPR2016官网查询更多论文：http://cvpr2016.thecvf.com/program/main_conference&lt;p&gt;所有pdf版本友善版下载链接：http://www.cv-foundation.org/openaccess/CVPR2016.py&lt;/p&gt;&lt;p&gt;已开源的所有论文code的下载链接：https://tensortalk.com/?cat=conference-cvpr-2016&amp;amp;t=type-code&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;致谢: &lt;/b&gt;&lt;b&gt;本文作者特别感谢中科院计算所阚美娜副研究员对本文的修改和建设性意见。&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;br&gt;参考文献&lt;/b&gt;&lt;br&gt;[1] Wang W, Cui Z, Yan Y, et al. Recurrent Face Aging[J].&lt;/p&gt;&lt;p&gt;&lt;br&gt;[2] Thies J, Zollhöfer M, Stamminger M, et al. Face2face: Real-time face capture and reenactment of rgb videos[J]. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2016, 1.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[3] Qin H, Yan J, Li X, et al. Joint Training of Cascaded CNN for Face Detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 3456-3465.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[4]CP-mtML: Coupled Projection Multi-Task Metric Learning for Large Scale Face Retrieval.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[5] Song H O, Xiang Y, Jegelka S, et al. Deep metric learning via lifted structured feature embedding[J]. arXiv preprint arXiv:1511.06452, 2015.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[6] Masi I, Rawls S, Medioni G, et al. Pose-Aware Face Recognition in the Wild[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4838-4846.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[7] Kan M, Shan S, Chen X. Multi-view Deep Network for Cross-view Classification[J].&lt;/p&gt;&lt;p&gt;&lt;br&gt;[8] Sun Y, Wang X, Tang X. Sparsifying Neural Network Connections for Face Recognition[J]. arXiv preprint arXiv:1512.01891, 2015.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[9] Feng Q, Zhou Y, Lan R. Pairwise Linear Regression Classification for Image Set Retrieval[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4865-4872.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[10] Kemelmacher-Shlizerman I, Seitz S, Miller D, et al. The megaface benchmark: 1 million faces for recognition at scale[J]. arXiv preprint arXiv:1512.00596, 2015.&lt;/p&gt;&lt;p&gt;&lt;br&gt; [11] Wen Y, Li Z, Qiao Y. Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4893-4901.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[12] Abaza A, Harrison M A, Bourlai T. Quality metrics for practical face recognition[C]//Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012: 3103-3107.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[13] Taigman Y, Yang M, Ranzato M A, et al. Deepface: Closing the gap to human-level performance in face verification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014: 1701-1708.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[14]Sun Y, Chen Y, Wang X, et al. Deep learning face representation by joint identification-verification[C]//Advances in Neural Information Processing Systems. 2014: 1988-1996.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[15]Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping[C]//2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE, 2006, 2: 1735-1742.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[16] Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 815-823.&lt;/p&gt;&lt;p&gt;&lt;br&gt;[17] Han S, Pool J, Tran J, et al. Learning both weights and connections for efficient neural network[C]//Advances in Neural Information Processing Systems. 2015: 1135-1143.&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;img src="55880c7614be01e1d1fd6760f25d6944.jpg" data-rawwidth="81" data-rawheight="89"&gt;&lt;strong&gt;汤旭，&lt;/strong&gt;上海科技大学信息学院研究生二年级，导师为“青年千人”高盛华教授。百度深度学习研究院人脸组实习生。研究方向为深度学习与计算机视觉（人脸识别等），个人邮箱：tangxu@shanghaitech.edu.cn&lt;br&gt;&lt;br&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325063&amp;amp;idx=1&amp;amp;sn=8430ef3dbd2d871c63f2b7fbac90c0b4&amp;amp;scene=4#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325063&amp;amp;idx=1&amp;amp;sn=8430ef3dbd2d871c63f2b7fbac90c0b4&amp;amp;scene=4#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;</description><author>程程</author><pubDate>Mon, 15 Aug 2016 17:29:43 GMT</pubDate></item><item><title>CVPR 2016论文快讯：目标检测领域的新进展</title><link>https://zhuanlan.zhihu.com/p/22022960</link><description>&lt;p&gt;&lt;img src="https://pic2.zhimg.com/a2e150fb5bf08078c99c3ecd5acf68c9_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！ &lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;前言&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;2016年的CVPR会议目标检测（在这里讨论的是2D的目标检测，如图1所示）的方法主要是基于卷积神经网络的框架，代表性的工作有ResNet[1]（Kaiming He等）、YOLO[5]（Joseph Redmon等）、LocNet[7]（Spyros Gidaris等）、HyperNet[3]（Tao Kong等）、ION[2]（Sean Bell等）、G-CNN[6]（Mahyar Najibi等）。在这里之所以把ResNet也放进来，是因为有效的特征对于目标检测领域是极为重要的。&lt;/p&gt;&lt;p&gt;图1：2D目标检测示意图&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="bab62fa2fd6dd70e3318dfe8b6d54c6d.jpg" data-rawwidth="432" data-rawheight="171"&gt;在目标检测中，以下几个指标非常重要：（a）识别精度；（b）识别效率；（c）定位准确性。以上的几个工作或者侧重识别率和效率，或者通过某种方式提高定位的准确性，下面分别展开进行描述。&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;一、识别精度&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;说起识别精度，不得不提目标检测中衡量检测精度的指标mAP(mean average precision)。简单来讲就是在多个类别的检测中，每一个类别都可以根据recall和precision绘制一条曲线，那么AP就是该曲线下的面积，而mAP是多个类别AP的平均值，这个值介于0到1之间，且越大越好。具有代表性的工作是ResNet、ION和HyperNet。&lt;br&gt;&lt;/p&gt;&lt;p&gt;ResNet：何凯明的代表作之一，获得了今年的bestpaper。文章不是针对目标检测来做的，但其解决了一个最根本的问题：更有力的特征。检测时基于Faster R-CNN的目标检测框架，使用ResNet替换VGG16网络可以取得更好的检测结果。（实际上，使用ResNet网络代替ZF, VGG, GoogleNet等网络模型无论在图像分类、目标检测还是图像分割等任务上都可以大大提高识别的准确率）&lt;br&gt;&lt;/p&gt;图2：ResNet获得CVPR2016的best paper&lt;img src="6bf560ca0dfcdd8c5f091e5f0e7e841b.jpg" data-rawwidth="578" data-rawheight="433"&gt;ION（inside-outside-network）：这个工作的主要贡献有两个，第一个是如何在Fast R-CNN的基础之上增加context信息，所谓context在目标检测领域是指感兴趣的ROI周围的信息，可以是局部的，也可以是全局的。为此，作者提出了IRNN的概念，这也就是outside-network。第二个贡献是所谓skip-connection，通过将deep ConvNet的多层ROI特征进行提取和融合，利用该特征进行每一个位置的分类和进一步回归，这也就是inside-network。&lt;br&gt;&lt;br&gt;依靠这两个改进，ION可以在Pascal VOC 2007数据集上边提高大约5个百分点。同时也获得了COCO 2015 detection竞赛的best student entry。&lt;br&gt;&lt;br&gt;图3：Inside-Outside Net (ION)&lt;p&gt;&lt;img src="e40c74629d5b98b5857be6945f11367f.jpg" data-rawwidth="545" data-rawheight="277"&gt;HyperNet：文章的出发点为一个很重要的观察：神经网络的高层信息体现了更强的语义信息，对于识别问题较为有效；而低层的特征由于分辨率较高，对于目标定位有天然的优势，而检测问题恰恰是识别+定位，因此作者的贡献点在于如何将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。&lt;br&gt;&lt;/p&gt;&lt;p&gt;通过以上的改进策略，HyperNet可以在产生大约100个region proposal的时候保证较高的recall，同时目标检测的mAP相对于Fast R-CNN也提高了大约6个百分点&lt;br&gt;&lt;/p&gt;&lt;p&gt;图4：HyperNet 框架&lt;br&gt;&lt;/p&gt;&lt;img src="c2b542ff22bd90b0d7d3bf89a402509d.jpg" data-rawwidth="611" data-rawheight="217"&gt;&lt;b&gt;二、识别效率&lt;/b&gt;&lt;br&gt;YOLO：这是今年的oral。这个工作在识别效率方面的优势很明显，可以做到每秒钟45帧图像，处理视频是完全没有问题的。YOLO最大贡献是提出了一种全新的检测框架——直接利用CNN的全局特征预测每个位置可能的目标，相比于R-CNN系列的region proposal+CNN 这种两阶段的处理办法可以大大提高检测速度。今年新出来的SSD[11]方法虽然在识别率上边有了很大的提升，但YOLO的先驱作用是显而易见的。&lt;br&gt;&lt;br&gt;图5：YOLO识别框架&lt;br&gt;&lt;img src="e957b3706fba3912b74cf0861443cb6a.jpg" data-rawwidth="578" data-rawheight="143"&gt;G-CNN：不管是Fast R-CNN[9]，还是Faster R-CNN，或者像HyperNet这样的变种，都需要考虑数以万计的潜在框来进行目标位置的搜索，这种方式的一个潜在问题是负样本空间非常大，因此需要一定的策略来进行抑制（不管是OHEM[8]还是region proposal方法，其本质上还是一种抑制负样本的工作）。G-CNN从另一个角度来克服这个问题。G-CNN在在初始化的时候不需要那么多框，而是通过对图像进行划分（有交叠），产生少量的框（大约180个），通过一次回归之后得到更接近物体的位置。然后以回归之后的框作为原始窗口，不断进行迭代回归调整，得到最终的检测结果。&lt;br&gt;&lt;br&gt;经过五次调整之后，G-CNN可以达到跟Fast R-CNN相当的识别性能，但速度是Fast R-CNN的5倍（3fps）。&lt;br&gt;&lt;br&gt;图6：G-CNN示意图&lt;br&gt;&lt;img src="98a66cdcfa6778e6daa9f03b19ae4a7a.jpg" data-rawwidth="593" data-rawheight="188"&gt;&lt;br&gt;图7：G-CNN训练过程&lt;br&gt;&lt;img src="f356fbd846d8c43f77a380e5482d4bdf.jpg" data-rawwidth="613" data-rawheight="287"&gt;&lt;b&gt;三、准确性&lt;/b&gt;&lt;br&gt;&lt;br&gt;LocNet：以上提到的工作都是在做整个目标检测的框架，而LocNet在做另一件事情—如何产生更准确的bounding box? &lt;br&gt;&lt;br&gt;图8：LocNet 示意图&lt;br&gt;&lt;img src="cc8801f58de473ffb652f573914caaa1.jpg" data-rawwidth="600" data-rawheight="247"&gt;在目标检测的评价体系中，有一个参数叫做IoU，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。在Pascal VOC中，这个值为0.5。而2014年以来出现的MS COCO竞赛规则把这个IoU变成了0.5-1.0之间的综合评价值，也就是说，定位越准确，其得分越高，这也侧面反映了目标检测在评价指标方面的不断进步。&lt;br&gt;&lt;br&gt;回到这个话题，如何产生更准确的目标位置呢？LocNet的解决方案是：针对每一个给定的初始框进行适当的放大，然后用一个CNN的网络回归出这个放大后的框包含的那个正确框的位置。为了达到这个目标，需要定义回归方式，网络以及模型，具体的细节参见[7]。&lt;br&gt;&lt;br&gt;经过把原始的框（比如selective search生成的）进行再一次回归之后，再放入Fast R-CNN进行检测，在IoU=0.5的情况下，在Pascal VOC 数据集上mAP可以提升大约5个百分点，而IoU=0.7时可以达到13个百分点的提升，效果还是挺惊人的。&lt;br&gt;&lt;br&gt;&lt;b&gt;结语&lt;/b&gt;&lt;br&gt;&lt;br&gt;目标检测是计算机视觉中基础而且热门的领域，最近两年的由于深度学习的影响产生了巨大的进步，相信在未来的一两年时间有更优秀的工作出现。&lt;br&gt;&lt;b&gt;注：&lt;/b&gt;以上提到的内容仅代表作者自己的观点，如有错误之处欢迎批评指正。&lt;br&gt;&lt;b&gt;致谢：本文作者特此感谢中科院计算所博士生王斌对本文提出的建设性修改意见。&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;br&gt;&lt;/p&gt;[1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. In CVPR 2016&lt;br&gt;[2] Bell S, Zitnick C L, Bala K, et al. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR 2016&lt;br&gt;[3] Kong T, Yao A, Chen Y, et al. HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection. In CVPR 2016&lt;br&gt;[4] Cheng M M, Zhang Z, Lin W Y, et al. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR 2014&lt;br&gt;[5] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection. In CVPR 2016&lt;br&gt;[6] Najibi M, Rastegari M, Davis L S. G-CNN: an Iterative Grid Based Object Detector. In CVPR 2016&lt;br&gt;[7] Gidaris S, Komodakis N. LocNet: Improving Localization Accuracy for Object Detection. In CVPR 2016&lt;br&gt;[8] Shrivastava A, Gupta A, Girshick R. Training region-based object detectors with online hard example mining. In CVPR 2016&lt;br&gt;[9] Girshick R. Fast R-CNN. In ICCV 2015&lt;br&gt;[10] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS 2015&lt;br&gt;[11] Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector[J]. arXiv preprint arXiv:1512.02325, 2015.&lt;br&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;br&gt;&lt;img src="779430dbcf60f7002a47c3f6465f4184.jpg" data-rawwidth="82" data-rawheight="84"&gt;&lt;strong&gt;孔涛, &lt;/strong&gt;清华大学计算机系博士生二年级。研究兴趣为计算机视觉、机器学习和机器人。相关成果发表在CVPR、ICRA等期刊和会议。个人邮箱：kt14@mails.tsinghua.edu.cn，个人主页：&lt;a href="https://taokong.github.io/" class=""&gt;https://taokong.github.io/&lt;/a&gt;。&lt;br&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325043&amp;amp;idx=1&amp;amp;sn=bd016d98a40e8cf7d53ee674f201b4a7&amp;amp;scene=4#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325043&amp;amp;idx=1&amp;amp;sn=bd016d98a40e8cf7d53ee674f201b4a7&amp;amp;scene=4#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;</description><author>程程</author><pubDate>Mon, 15 Aug 2016 17:04:19 GMT</pubDate></item><item><title>【CVPR2016论文快讯】面部特征点定位的最新进展</title><link>https://zhuanlan.zhihu.com/p/21955390</link><description>&lt;p&gt;&lt;img src="https://pic3.zhimg.com/87d645fd2f6e5084d92616ecdfce1c2e_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;CVPR2016刚刚落下帷幕，本文对面部特征点定位的论文做一个简单总结，让大家快速了解该领域最新的研究进展，希望能给读者们带来启发。CVPR2016相关的文章大致可以分为三大类：&lt;strong&gt;处理大姿态问题，处理表情问题，处理遮挡问题。&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;1.姿态鲁棒的人脸对齐方法&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&lt;strong&gt;1.1 Face Alignment Across Large Poses: A 3D Solution [1]&lt;/strong&gt;&lt;br&gt;这里首先介绍一篇大会口头报告文章，来自中国科学院自动化研究所Xiangyu Zhu等人的工作。极端姿态下（如侧脸），一些特征点变了不可见，不同姿态下的人脸表观也存在巨大差异，这些问题都导致大姿态下面部特征点定位任务极具挑战性。为了解决以上问题，本文提出一种基于3D人脸形状的定位方法3DDFA，算法框架如下图所示：&lt;img src="a547fb4ee01b6a9542090d56e231617d.jpg" data-rawwidth="595" data-rawheight="171"&gt;算法输入为100x100的RGB图像和PNCC （Projected Normalized Coordinate Code） 特征，PNCC特征的计算与当前形状相关，可以反映当前形状的信息；算法的输出为3D人脸形状模型参数。使用卷积神经网络拟合从输入到输出的映射函数，网络包含4个卷积层，3个pooling层和2个全连接层。通过级联多个卷积神经网络直至在训练集上收敛，PNCC特征会根据当前预测的人脸形状更新，并作为下一级卷积神经网络的输入。此外，卷积神经网络的损失函数也做了精心的设计，通过引入权重，让网络优先拟合重要的形状参数，如尺度、旋转和平移；当人脸形状接近ground truth时，再考虑拟合其他形状参数。实验证明该损失函数可以提升定位模型的精度。由于参数化形状模型会限制人脸形状变形的能力，作者在使用3DDFA拟合之后，抽取HOG特征作为输入，使用线性回归来进一步提升2D特征点的定位精度。&lt;br&gt;训练3DDFA模型，需要大量的多姿态人脸样本。为此，作者基于已有的数据集如300W，利用3D信息虚拟生成不同姿态下的人脸图像，核心思想为：先预测人脸图像的深度信息，通过3D旋转来生成不同姿态下的人脸图像，如下图所示：&lt;img src="d96cf84376c555577418e5bdc7382428.jpg" data-rawwidth="605" data-rawheight="347"&gt;&lt;p&gt;（a）为原始图像，（b,c,d）为生成的虚拟样本，yaw方向的角度依次增加20°，30°和40°。生成虚拟人脸图像的code和3DDFA的code可以在以下链接下载：&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm" class=""&gt;http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;strong&gt;1.2 Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting [2]&lt;/strong&gt;&lt;br&gt;这篇文章是来自密西根州立大学的Amin Jourabloo和Xiaoming Liu的工作。和上一篇文章的出发点一样，作者试图使用3D人脸建模解决大姿态下面部特征点定位问题。2D的人脸形状U可以看成是3D人脸形状A通过投影变化m得到，如下图所示：&lt;img src="d1331dff2d6aaccda5fa163a9c22e1f3.jpg" data-rawwidth="587" data-rawheight="339"&gt;&lt;p&gt;3D人脸形状模型可以表示为平均3D人脸形状A0与若干表征身份、表情的基向量Aid和Aexp通过p参数组合而成。面部特征点定位问题（预测U）可以转变为同时预测投影矩阵m和3D人脸形状模型参数p。算法的整体框架图如下所示：&lt;/p&gt;&lt;img src="5adc0cffe3192a3527ca758139ca4607.jpg" data-rawwidth="605" data-rawheight="392"&gt;作者通过级联6个卷积神经网络来完成这一任务。首先以整张人脸图像作为输入，来预测投影矩阵的更新。使用更新后的投影矩阵计算当前的2D人脸形状，基于当前的2D人脸形状抽取块特征作为下一级卷积神经网络的输入，下一级卷积神经网络用于更新3D人脸形状。基于更新后的3D人脸形状，计算可得当前2D人脸形状的预测。根据新的2D人脸形状预测，抽取块特征输入到卷积神经网络中来更新投影矩阵，交替迭代优化求解投影矩阵m和3D人脸形状模型参数p，直到在训练集收敛。值得一提的是，该方法在预测3D人脸形状和投影矩阵的同时也考虑到计算每一个特征点是否可见。如果特征点不可见，则不使用该特征点上的块特征作为输入，这是普通2D人脸对齐方法难以实现的。此外，作者提出两种pose-invariant的特征Piecewise Affine-Warpped Feature (PAWF)和Direct 3D Projected Feature (D3PF)，可以进一步提升特征点定位的精度。&lt;br&gt;&lt;br&gt;&lt;strong&gt;1.3 Unconstrained Face Alignment via Cascaded Compositional Learning [3]&lt;/strong&gt;&lt;br&gt;这篇文章是来自香港中文大学的Shizhan Zhu等人的工作。和前面两篇工作不同，本文提出的方法Cascaded Compositional Learning (CCL)没有从3D人脸建模出发来解决大姿态下人脸对齐问题，而是将所有人脸样本划分成多个域（Domain）来分别处理，并通过学习组合系数，融合不同域的结果来得到最终的定位结果。方法的出发点与GSDM[4]类似，不过GSDM依赖视频中上一帧的人脸对齐结果来选择域，所以不能处理静态图片的人脸对齐问题。本文提出的方法巧妙地学习组合系数来自动完成域的选择，从而有效地解决GSDM的局限性。CCL算法的示意图如下所示：&lt;img src="88a65e76298ecf1cf4e7e64f82893bb2.jpg" data-rawwidth="598" data-rawheight="178"&gt;算法整体框架为级联形状回归，每一级包含三块，分别是特征提取模块，形状回归模块和组合系数预测模块。其中特征提取模块在LBF [5]特征的基础上引入特征点是否可见的信息，为后续预测组合系数提供重要线索，当出现自遮挡情况时（Self-occlusion）比LBF特征更加鲁棒。形状回归模块包含K个形状回归器，分别对应于K个域。组合系数预测模块融合K个形状回归器的预测，生成最终的定位结果。该方法在AFW和AFLW数据集上均取得了State-of-the-art的结果，在单核的台式机上达到350 FPS，方法简单、高效。&lt;br&gt;&lt;strong&gt;2.表情鲁棒的人脸对齐方法&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;2.1 Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection [6]&lt;/strong&gt;&lt;br&gt;前面给大家介绍了三篇主要解决大姿态下人脸对齐问题的文章，接下来给大家带来一篇联合处理表情识别和面部特征点检测的文章。这篇文章是来自Rensselaer Polytechnic Institute的Yue Wu和Qiang Ji的工作。考虑到表情识别和人脸对齐是两个非常相关的人脸感知任务，作者在级联形状回归算法框架的基础上，提出新的Constrained Joint Cascade Regression Framework (CJCRF)来联合预测表情（这里是识别脸部运动单元（Facial Action Unit））和面部特征点定位。下图为算法框架图：&lt;img src="a7ae5e998156797ba96ec73d01545299.jpg" data-rawwidth="543" data-rawheight="231"&gt;算法分两步，首先使用受限玻尔兹曼机模型，建模脸部运动单元与人脸形状之间的联系。下图（a）蓝色人脸形状展示了不同的脸部运动单元（AU12，AU15和AU25）对应的人脸形状先验（红色为平均人脸形状）。下图（b）：给定一个特定的人脸形状（蓝色），不同的脸部运动单元（AU）被激活的概率分布情况。&lt;img src="661722a59102c780f2f1aec56d348de8.jpg" data-rawwidth="552" data-rawheight="120"&gt;&lt;br&gt;接着，以脸部运动单元与人脸形状之间的联系作为约束，嵌入到级联形状回归框架下来联合估计特征点的位置和脸部运动单元。实验表明，Constrained Joint Cascade Regression Framework (CJCRF)可以同时提升特征点定位任务和脸部运动单元识别任务的精度。下图展示了不引入脸部运动单元信息（图a）和引入脸部运动单元信息（图b）的定位结果，可以看出引入脸部运动单元信息可以提升面部特征点定位模型对于夸张表情的鲁棒性。&lt;img src="1cf12a9574a7b47329c25cebff140e9e.jpg" data-rawwidth="511" data-rawheight="137"&gt;&lt;strong&gt;3.遮挡鲁棒的人脸对齐方法&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;3.1 Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders [7]&lt;/strong&gt;&lt;br&gt;最后介绍本人的一篇工作，主要是处理遮挡问题。面部特征点定位系统在出现遮挡时往往会性能退化。为此，本文提出一个新的算法框架Deep Regression Networks Coupled WithDe-corrupt Autoencoders（DRDA）来&lt;strong&gt;显示&lt;/strong&gt;处理面部特征点定位任务中的遮挡问题。算法总体框架如下所示：&lt;img src="f0e8664190b423d7766e0b65c4fd61a4.jpg" data-rawwidth="621" data-rawheight="347"&gt;去遮挡网络（De-corrupt Autoencoders）用于自动恢复被遮挡区域的人脸信息。由于姿态、表情的影响，人脸表观千差万别，很难仅使用一个去遮挡网络来很好地恢复人脸表观细节。为了恢复较为精细的人脸表观，本文依据当前预测的人脸形状，将人脸划分为若干个区域，对每个区域学习一个去遮挡网络，来去除遮挡物。深度回归网络（Deep Regression Networks）使用去遮挡后的人脸作为输入，来预测人脸形状。通过级联多个去遮挡网络和深度回归网络，逐步优化人脸去遮挡结果和特征点定位的结果。该方法不但可以预测出特征点是否被遮挡（如图a所示），而且能定位出遮挡物区域，并最终得到“干净的”人脸（如图b所示）。&lt;br&gt;&lt;img src="f1f4bdbe73d49c73282ac6a0b5b3813c.jpg" data-rawwidth="529" data-rawheight="206"&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;br&gt;&lt;br&gt;以上介绍的几个工作分别从姿态、表情、遮挡等因素出发设计算法，提升特征点定位模型的鲁棒性。所有方法或多或少都和级联形状回归框架有关，足见级联形状回归方法的有效性。但级联形状回归框架下的每一级回归模型都是独立训练的，并不是一个端到端（End-to-End）的方法。英国帝国理工大学的George Trigeorgis等人提出使用Convolutional Recurrent Neural Network 来解决特征点定位问题 [8]，可以端到端地训练特征点定位模型，比传统的级联回归方法有显著的性能提升。此外，姿态估计、表情识别以及遮挡检测与特征点定位任务有很强的依赖关系，联合考虑这些任务或许是人脸分析应用里不错的解决方案。马里兰大学的Rama Chellappa教授在CVPR2016 ChaLearn Looking at People and Faces Workshop的特邀报告上介绍了HyperFace。这一工作的核心思想也是融合卷积神经网络不同层的feature map来同时完成人脸检测、面部特征点定位、姿态预测和性别识别等任务。再者，以上介绍的大部分工作与深度模型相关，如何学习低复杂度的定位网络，能在手持终端上高效准确地定位面部关键点也是一个值得探索的问题。&lt;br&gt;&lt;br&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;[1]&lt;/strong&gt; Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, Stan Z. Li. Face Alignment Across Large Poses: A 3D Solution. CVPR 2016.&lt;br&gt;&lt;strong&gt;[2] &lt;/strong&gt;Amin Jourabloo, Xiaoming Liu. Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting. CVPR 2016.&lt;br&gt;&lt;strong&gt;[3]&lt;/strong&gt; Shizhan Zhu, Cheng Li, Chen-Change Loy, Xiaoou Tang. Unconstrained Face Alignment via Cascaded Compositional Learning. CVPR 2016.&lt;br&gt;&lt;strong&gt;[4]&lt;/strong&gt; Xuehan Xiong, De la Torre Fernando. Global supervised descent method. CVPR 2015.&lt;br&gt;&lt;strong&gt;[5]&lt;/strong&gt; Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun. Face Alignment at 3000 FPS via Regressing Local Binary Features. CVPR 2014.&lt;br&gt;&lt;strong&gt;[6]&lt;/strong&gt; Yue Wu, Qiang Ji. Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection. CVPR 2016.&lt;br&gt;&lt;strong&gt;[7]&lt;/strong&gt; Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen. Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders. CVPR 2016.&lt;br&gt;&lt;strong&gt;[8]&lt;/strong&gt; George Trigeorgis, Patrick Snape, Mihalis A. Nicolaou, Epameinondas Antonakos, Stefanos Zafeiriou. Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment. CVPR 2016.&lt;br&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;img src="afe2c7f37f48ba656d5d71715dd38af3.jpg" data-rawwidth="104" data-rawheight="108"&gt;&lt;strong&gt;张杰&lt;/strong&gt;，中科院计算技术研究所VIPL课题组博士生，专注于深度学习技术及其在人脸识别领域的应用。相关研究成果发表在计算机视觉国际顶级学术会议ICCV， CVPR和ECCV，拥有两篇关于人脸跟踪和对齐方面的专利，并担任国际顶级期刊TPAMI，TIP和TNNLS审稿人。&lt;br&gt;&lt;br&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;b&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325127&amp;amp;idx=1&amp;amp;sn=945584eb2409ebbb48c7d9236a63094e&amp;amp;scene=0#wechat_redirect"&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325127&amp;amp;idx=1&amp;amp;sn=945584eb2409ebbb48c7d9236a63094e&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;/b&gt;</description><author>程程</author><pubDate>Wed, 10 Aug 2016 17:20:28 GMT</pubDate></item><item><title>深度学习你不可不知的技巧（下）</title><link>https://zhuanlan.zhihu.com/p/21952042</link><description>&lt;p&gt;&lt;img src="https://pic2.zhimg.com/bdfc0dccd44c1d539c8b9d497c4f6781_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;Sec. 5: Activation Functions&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;One of the crucial factors in deep networks is activation function, which brings the&lt;strong&gt; non-linearity&lt;/strong&gt; into networks. Here we will introduce the details and characters of some popular activation functions and give advices later in this section.&lt;/p&gt;&lt;p&gt;&lt;img src="baa57af6b7a5d14702788c191a2b433a.jpg" data-rawwidth="595" data-rawheight="190"&gt;&lt;b&gt;1.&lt;/b&gt;&lt;b&gt;Sigmoid&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="1c04a0787df09293c1972416802381d1.jpg" data-rawwidth="568" data-rawheight="346"&gt;The sigmoid non-linearity has the mathematical form &lt;br&gt;&lt;/p&gt;&lt;img src="e1d20dc43e0f34f9f80df9e887b0c41e.jpg" data-rawwidth="154" data-rawheight="26"&gt; It takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1).&lt;br&gt;    In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:&lt;br&gt;    1. Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during back-propagation, this (local) gradient will be multiplied to the gradient of this gate's output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.        2. Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g., x&amp;gt;0 element wise in &lt;img src="f7d1cc570647a97159289b4d9a497f69.jpg" data-rawwidth="116" data-rawheight="29"&gt;), then the gradient on the weights  w will during back-propagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.&lt;br&gt;&lt;br&gt;&lt;b&gt;2.tanh(x)&lt;/b&gt;&lt;img src="940791ec295424363324dd378288758c.jpg" data-rawwidth="568" data-rawheight="372"&gt;The tanh non-linearity squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.&lt;br&gt;&lt;br&gt;&lt;b&gt;3.Rectified Linear Unit&lt;/b&gt;&lt;img src="5725c4f80843dd7517430c80dd953933.jpg" data-rawwidth="609" data-rawheight="380"&gt;The Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function &lt;img src="e607340f51941c220265774cf8abec76.jpg" data-rawwidth="139" data-rawheight="30"&gt;, which is simply thresholded at zero.&lt;br&gt;There are several pros and cons to using the ReLUs:&lt;br&gt;    1. (Pros) Compared to sigmoid/tanh neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero. Meanwhile, ReLUs does not suffer from saturating.&lt;br&gt;    2. (Pros) It was found to greatly accelerate (e.g., a factor of 6 in [1]) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.&lt;br&gt;    3. (Cons) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e., neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.&lt;br&gt;&lt;b&gt;4.Leaky ReLU&lt;/b&gt;&lt;p&gt;&lt;img src="fcac5804f4c607f97e9f7cda5cf6d12d.jpg" data-rawwidth="600" data-rawheight="345"&gt;Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when&lt;/p&gt;&lt;p&gt;&lt;img src="aa489ee850d0ce4a7fc02a3822e78e66.jpg" data-rawwidth="58" data-rawheight="23"&gt;, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes&lt;img src="c397c3b6542b2f7fc08d1f40d93c483c.jpg" data-rawwidth="99" data-rawheight="37"&gt;if  &lt;img src="aa489ee850d0ce4a7fc02a3822e78e66.jpg" data-rawwidth="58" data-rawheight="23"&gt;and &lt;img src="1698d78532d4e1c19f8ada034c924cce.jpg" data-rawwidth="77" data-rawheight="32"&gt;if&lt;img src="d7e946e0246337266f19f85577bb557a.jpg" data-rawwidth="52" data-rawheight="36"&gt; , where a is a small constant. Some people report success with this form of activation function, but the results are not always consistent.&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Parametric ReLU&lt;/b&gt;&lt;/p&gt;Nowadays, a broader class of activation functions, namely the &lt;strong&gt;rectified unit family&lt;/strong&gt;, were proposed. In the following, we will talk about the variants of ReLU.&lt;img src="1febf2296bfea0837392236f278a7bf3.jpg" data-rawwidth="578" data-rawheight="161"&gt;ReLU, Leaky ReLU, PReLU and RReLU. In these figures, for PReLU, ai is learned and for Leaky ReLU ai is fixed. For RReLU, aji is a random variable keeps sampling in a given range, and remains fixed in testing.&lt;br&gt;    The first variant is called parametric rectified linear unit (PReLU) [4]. In PReLU, the slopes of negative part are learned from data rather than pre-defined. He et al. [4] claimed that PReLU is the key factor of surpassing human-level performance on ImageNet(http://www.image-net.org/) classification task. The back-propagation and updating process of PReLU is very straightforward and similar to traditional ReLU, which is shown in Page. 43 of the slides.、&lt;p&gt;&lt;b&gt;6.Randomized ReLU&lt;/b&gt;&lt;/p&gt; The second variant is called randomized rectified linear unit (RReLU). In RReLU, the slopes of negative parts are randomized in a given range in the training, and then fixed in the testing. As mentioned in [5], in a recent Kaggle National Data Science Bowl (NDSB) competition(https://www.kaggle.com/c/datasciencebowl), it is reported that RReLU could reduce overfitting due to its randomized nature. Moreover, suggested by the NDSB competition winner, the random ai in training is sampled from &lt;img src="12ffc37e7d4a3e4fd154fc9ff1ab9497.jpg" data-rawwidth="84" data-rawheight="32"&gt;and in test time it is fixed as its expectation, i.e.,&lt;img src="c4f2fbc7958bbf1188f793a4f19d1da1.jpg" data-rawwidth="135" data-rawheight="41"&gt;In [5], the authors evaluated classification performance of two state-of-the-art CNN architectures with different activation functions on theCIFAR-10, CIFAR-100 and NDSB data sets, which are shown in the following tables. Please note that, for these two networks, activation function is followed by each convolutional layer. And the 1/a  in these tables actually indicates , where a is the aforementioned slopes.&lt;img src="535b87dae48359db1ee846c3269c8e5a.jpg" data-rawwidth="599" data-rawheight="95"&gt;From these tables, we can find the performance of ReLU is not the best for all the three data sets. For Leaky ReLU, a larger slope  will achieve better accuracy rates. PReLU is easy to overfit on small data sets (its training error is the smallest, while testing error is not satisfactory), but still outperforms ReLU. In addition, RReLU is significantly better than other activation functions on NDSB, which shows RReLU can overcome overfitting, because this data set has less training data than that of CIFAR-10/CIFAR-100. In conclusion, three types of ReLU variants all consistently outperform the original ReLU in these three data sets. And PReLU and RReLU seem better choices. Moreover, He et al. also reported similar conclusions in [4].&lt;br&gt;&lt;b&gt;Sec. 6: Regularizations&lt;/b&gt;&lt;br&gt;There are several ways of controlling the capacity of Neural Networks to prevent overfitting:&lt;br&gt;&lt;strong&gt;L2 regularization&lt;/strong&gt; is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight w in the network, we add the term &lt;img src="cf9f44c0b65455a6e6bbd4624251829a.jpg" data-rawwidth="64" data-rawheight="33"&gt;to the objective, where&lt;img src="c124921cfbc2cb5666b33a441829885e.jpg" data-rawwidth="41" data-rawheight="30"&gt;is the regularization strength. It is common to see the factor of 1/2 in front because then the gradient of this term with respect to the parameter w is simply &lt;img src="9922a3789ca4cafdd9e8d9ac97923e82.jpg" data-rawwidth="49" data-rawheight="20"&gt;instead of &lt;img src="2228d4b9d6d2d9a2ffd5ae9f46a27430.jpg" data-rawwidth="57" data-rawheight="23"&gt;. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors.&lt;br&gt;&lt;strong&gt;L1 regularization&lt;/strong&gt; is another relatively common form of regularization, where for each weight w we add the term &lt;img src="018617a829ec17322bb7c51df7f93990.jpg" data-rawwidth="39" data-rawheight="35"&gt;to the objective. It is possible to combine the L1 regularization with the L2 regularization:&lt;img src="db2840da4f6af592ac8d95625a3d6386.jpg" data-rawwidth="102" data-rawheight="31"&gt; (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.&lt;br&gt;&lt;strong&gt;Max norm constraints.&lt;/strong&gt; Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vectorfor every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector &lt;img src="0f56bfa0b311b2c24ac33dede0965ed5.jpg" data-rawwidth="44" data-rawheight="25"&gt;of every neuron to satisfy&lt;img src="f2af30da5c59d81cb5458ee982ba7362.jpg" data-rawwidth="83" data-rawheight="30"&gt; . Typical values of c are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.&lt;br&gt;&lt;strong&gt;Dropout &lt;/strong&gt;is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in [6] that complements the other methods (L1, L2, maxnorm). During training, dropout can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. (However, the exponential number of possible sampled networks are not independent because they share the parameters.) During testing there is no dropout applied, with the interpretation of evaluating an averaged prediction across the exponentially-sized ensemble of all sub-networks (more about ensembles in the next section). In practice, the value of dropout ratio &lt;img src="8ae8d6d77ef55c5f6d8468638622a95d.jpg" data-rawwidth="74" data-rawheight="28"&gt;is a reasonable default, but this can be tuned on validation data.&lt;img src="2585f8a987bdc52e4ec24de4dedfaf0e.jpg" data-rawwidth="596" data-rawheight="166"&gt;The most popular used regularization techniquedropout [6]. While training, dropout is implemented by only keeping a neuron active with some probability p (a hyper-parameter), or setting it to zero otherwise. In addition, Google applied for a US patent(https://www.google.com/patents/WO2014105866A1) for dropout in 2014.&lt;br&gt;&lt;b&gt;Sec. 7: Insights from Figures&lt;/b&gt;&lt;br&gt;&lt;br&gt;Finally, from the tips above, you can get the satisfactory settings (e.g., data processing, architectures choices and details, etc.) for your own deep networks. During training time, you can draw some figures to indicate your networks’ training effectiveness.&lt;br&gt;1. As we have known, the learning rate is very sensitive. From Fig. 1 in the following, a very high learning rate will cause a quite strange loss curve. A low learning rate will make your training loss decrease very slowly even after a large number of epochs. In contrast, a high learning rate will make training loss decrease fast at the beginning, but it will also drop into a local minimum. Thus, your networks might not achieve a satisfactory results in that case. For a good learning rate, as the red line shown in Fig. 1, its loss curve performs smoothly and finally it achieves the best performance.&lt;br&gt;&lt;br&gt;2.  Now let’s zoom in the loss curve. The epochs present the number of times for training once on the training data, so there are multiple mini batches in each epoch. If we draw the classification loss every training batch, the curve performs like Fig. 2. Similar to Fig. 1, if the trend of the loss curve looks too linear, that indicates your learning rate is low; if it does not decrease much, it tells you that the learning rate might be too high. Moreover, the “width” of the curve is related to the batch size. If the “width” looks too wide, that is to say the variance between every batch is too large, which points out you should increase the batch size.&lt;br&gt;3. Another tip comes from the accuracy curve. As shown in Fig. 3, the red line is the training accuracy, and the green line is the validation one. When the validation accuracy converges, the gap between the red line and the green one will show the effectiveness of your deep networks. If the gap is big, it indicates your network could get good accuracy on the training data, while it only achieve a low accuracy on the validation set. It is obvious that your deep model overfits on the training set. Thus, you should increase the regularization strength of deep networks. However, no gap meanwhile at a low accuracy level is not a good thing, which shows your deep model has low learnability. In that case, it is better to increase the model capacity for better results. &lt;p&gt;&lt;img src="95f6094ce5962a3cab74db8719eeccb2.jpg" data-rawwidth="590" data-rawheight="144"&gt;&lt;b&gt;Sec. 8: Ensemble&lt;/b&gt;&lt;/p&gt;In machine learning, ensemble methods [8] that train multiple learners and then combine them for use are a kind of state-of-the-art learning approach. It is well known that an ensemble is usually significantly more accurate than a single learner, and ensemble methods have already achieved great success in many real-world tasks. In practical applications, especially challenges or competitions, almost all the first-place and second-place winners used ensemble methods.&lt;br&gt;Here we introduce several skills for ensemble in the deep learning scenario.&lt;br&gt;&lt;strong&gt;Same model, different initialization.&lt;/strong&gt; Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.   &lt;strong&gt;Top models discovered during cross-validation. &lt;/strong&gt;Use cross-validation to determine the best hyperparameters, then pick the top few (e.g., 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it does not require additional retraining of models after cross-validation. Actually, you could directly select several state-of-the-art deep models from Caffe Model Zoo (https://github.com/BVLC/caffe/wiki/Model-Zoo) to perform ensemble.   &lt;strong&gt;Different checkpoints of a single model.&lt;/strong&gt; If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.    &lt;strong&gt;Some practical examples. &lt;/strong&gt;If your vision tasks are related to high-level image semantic, e.g., event recognition from still images, a better ensemble method is to employ multiple deep models trained on different data sources to extract different and complementary deep representations. For example in the Cultural Event Recognition (https://www.codalab.org/competitions/4081#learn_the_details) challenge in associated with ICCV’15 (http://pamitc.org/iccv15/) , we utilized five different deep models trained on images of ImageNet (http://www.image-net.org/), Place Database (http://places.csail.mit.edu/) and the cultural images supplied by the competition organizers (http://gesture.chalearn.org/). After that, we extracted five complementary deep features and treat them as multi-view data. Combining “early fusion” and “late fusion” strategies described in [7], we achieved one of the best performance and ranked the 2nd place in that challenge. Similar to our work, [9] presented the Stacked NN framework to fuse more deep networks at the same time.&lt;br&gt;&lt;b&gt;References &amp;amp; Source Links&lt;/b&gt;&lt;br&gt;[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks.(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) In NIPS, 2012&lt;br&gt;[2] A Brief Overview of Deep Learning (http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html/) , which is a guest post by Ilya Sutskever.&lt;br&gt;[3] CS231n: Convolutional Neural Networks for Visual Recognition of Stanford University, held by Prof. Fei-Fei Li and Andrej Karpathy.&lt;br&gt;[4] K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.（http://arxiv.org/abs/1502.01852）InICCV, 2015.&lt;br&gt;[5] B. Xu, N. Wang, T. Chen, and M. Li. Empirical Evaluation of Rectified Activations in Convolution Network（http://arxiv.org/abs/1505.00853）. In ICML Deep Learning Workshop, 2015.&lt;br&gt;[6] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. （http://jmlr.org/papers/v15/srivastava14a.html）JMLR, 15(Jun):1929−1958, 2014.&lt;br&gt;[7] X.-S. Wei, B.-B. Gao, and J. Wu. Deep Spatial Pyramid Ensemble for Cultural Event Recognition. （&lt;a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf"&gt;http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf&lt;/a&gt;）In ICCV ChaLearn Looking at People Workshop, 2015.&lt;br&gt;[8] Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms（https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/9781439830031）. Boca Raton, FL: Chapman &amp;amp; HallCRC/, 2012. (ISBN 978-1-439-830031)&lt;br&gt;[9] M. Mohammadi, and S. Das. S-NN: Stacked Neural Networks. Project in Stanford CS231n Winter Quarter, 2015.（http://cs231n.stanford.edu/reports/milad_final_report.pdf）&lt;br&gt;[10] P. Hensman, and D. Masko. The Impact of Imbalanced Training Data for Convolutional Neural Networks.（http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf） Degree Project in Computer Science, DD143X, 2015.&lt;br&gt;&lt;br&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;img src="a88ed14b0a577bc5117e41ef3d938a7e.jpg" data-rawwidth="76" data-rawheight="76"&gt;&lt;/p&gt;&lt;strong&gt;魏秀参，&lt;/strong&gt;南京大学计算机系机器学习与数据挖掘所（LAMDA）博士生，研究方向为计算机视觉和机器学习，特别是深度学习和弱监督学习。曾在国际顶级期刊和会议发表学术论文。个人主页：http://lamda.nju.edu.cn/weixs/，微博ID：Wilson_NJUer&lt;br&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324989&amp;amp;idx=2&amp;amp;sn=c70ec361350fefc7693d2231879dfd49&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324989&amp;amp;idx=2&amp;amp;sn=c70ec361350fefc7693d2231879dfd49&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;</description><author>程程</author><pubDate>Wed, 10 Aug 2016 15:26:34 GMT</pubDate></item><item><title>深度学习你不可不知的技巧(上)</title><link>https://zhuanlan.zhihu.com/p/21931937</link><description>&lt;p&gt;&lt;img src="https://pic2.zhimg.com/bdfc0dccd44c1d539c8b9d497c4f6781_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;br&gt;&lt;img src="bdfc0dccd44c1d539c8b9d497c4f6781.jpg" data-rawwidth="412" data-rawheight="300"&gt;Deep Neural Networks, especially &lt;strong&gt;Convolutional Neural Networks&lt;/strong&gt; (CNN), allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-arts in visual object recognition, object detection, text recognition and many other domains such as drug discovery and genomics.&lt;br&gt;    In addition, many solid papers have been published in this topic, and some high quality open source CNN software packages have been made available. There are also well-written CNN tutorials or CNN software manuals. However, it might lack a recent and comprehensive summary about the details of how to implement an excellent deep convolutional neural networks from scratch. Thus, we collected and concluded many implementation details for DCNNs.&lt;strong&gt; Here we will introduce these extensive implementation details, i.e., tricks or tips, for building and training your own deep networks.&lt;/strong&gt;&lt;br&gt;&lt;b&gt;Introduction&lt;/b&gt;&lt;br&gt;&lt;br&gt;We assume you already know the basic knowledge of deep learning, and here we will present the implementation details (tricks or tips) in Deep Neural Networks, especially CNN for image-related tasks, mainly in &lt;strong&gt;eight aspects:&lt;/strong&gt;     1) data augmentation;     2) pre-processing on images;     3) initializations of Networks;     4) some tips during training;     5) selections of activation functions;     6) diverse regularizations;     7)some insights found from figures and finally     8) methods of ensemble multiple deep networks.&lt;br&gt;    Additionally, the corresponding slides are available at [slide]（http://lamda.nju.edu.cn/weixs/slide/CNNTricks_slide.pdf）. If there are any problems/mistakes in these materials and slides, or there are something important/interesting you consider that should be added, just feel free to contact me（http://lamda.nju.edu.cn/weixs/?AspxAutoDetectCookieSupport=1）.&lt;br&gt;Sec. 1: Data Augmentation&lt;br&gt;&lt;br&gt;Since deep networks need to be trained on a huge number of training images to achieve satisfactory performance, if the original image data set contains limited training images, it is better to do data augmentation to boost the performance. Also, data augmentation becomes the thing must to do when training a deep network.&lt;br&gt;&lt;br&gt;1.There are many ways to do data augmentation, such as the popular horizontally flipping, random crops and color jittering. Moreover, you could try combinations of multiple different processing, e.g., doing the rotation and random scaling at the same time. In addition, you can try to raise saturation and value (S and V components of the HSV color space) of all pixels to a power between 0.25 and 4 (same for all pixels within a patch), multiply these values by a factor between 0.7 and 1.4, and add to them a value between -0.1 and 0.1. Also, you could add a value between [-0.1, 0.1] to the hue (H component of HSV) of all pixels in the image/patch.&lt;br&gt;2.Krizhevsky et al. [1] proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images. In practice, you can firstly perform PCA on the set of RGB pixel values throughout your training images. And then, for each training image, just add the following quantity to each RGB image pixel (i.e.,&lt;img src="c7b22284eb7fa5547a87818bd226338d.jpg" data-rawwidth="151" data-rawheight="30"&gt; ): &lt;img src="c336acf04331d645fe507676e8099a69.jpg" data-rawwidth="224" data-rawheight="29"&gt;where , pi and &lt;img src="1b500a373d05e6c5d97489cdc38c42fc.jpg" data-rawwidth="24" data-rawheight="23"&gt;are the i-th eigenvector and eigenvalue of the 3*3 covariance matrix of RGB pixel values, respectively, and ai  is a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Please note that, each ai is drawn only once for all the pixels of a particular training image until that image is used for training again. That is to say, when the model meets the same training image again, it will randomly produce another ai  for data augmentation. In [1], they claimed that “fancy PCA could approximately capture an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination”. To the classification performance, this scheme reduced the top-1 error rate by over 1% in the competition of ImageNet 2012.&lt;br&gt;&lt;br&gt;&lt;b&gt;Sec. 2: Pre-Processing&lt;/b&gt;&lt;br&gt;&lt;br&gt;Now we have obtained a large number of training samples (images/crops), but please do not hurry! Actually, it is necessary to do pre-processing on these images/crops. In this section, we will introduce several approaches for pre-processing.&lt;br&gt;    The first and simple pre-processing approach is zero-center the data, and then normalize them, which is presented as two lines Python codes as follows:&lt;img src="ef09f7231766634ab9135205dafd21b8.jpg" data-rawwidth="309" data-rawheight="48"&gt;where, X is the input data (NumIns×NumDim). Another form of this pre-processing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this pre-processing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm. In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional pre-processing step.&lt;br&gt;&lt;br&gt;    Another pre-processing approach similar to the first one is PCA Whitening. In this process, the data is first centered as described above. Then, you can compute the covariance matrix that tells us about the correlation structure in the data:&lt;img src="b47aa2b6005b0e968fb2e5ec449e2e62.jpg" data-rawwidth="468" data-rawheight="48"&gt;After that, you decorrelate the data by projecting the original (but zero-centered) data into the eigenbasis:&lt;img src="dede68a06ace4aef9bf8a59fb3187864.jpg" data-rawwidth="583" data-rawheight="47"&gt;The last transformation is whitening, which takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale:&lt;img src="895e6624c70a5c90722fd0701cf86b6f.jpg" data-rawwidth="578" data-rawheight="27"&gt;Note that here it adds 1e-5 (or a small constant) to prevent division by zero. One weakness of this transformation is that it can greatly exaggerate the noise in the data, since it stretches all dimensions (including the irrelevant dimensions of tiny variance that are mostly noise) to be of equal size in the input. This can in practice be mitigated by stronger smoothing (i.e., increasing 1e-5 to be a larger number).&lt;br&gt;    Please note that, we describe these pre-processing here just for completeness. In practice, these transformations are not used with Convolutional Neural Networks. However, it is also very important to zero-center the data, and it is common to see normalization of every pixel as well.&lt;br&gt;&lt;b&gt;Sec. 3: Initializations&lt;/b&gt;&lt;br&gt;&lt;br&gt;Now the data is ready. However, before you are beginning to train the network, you have to initialize its parameters.&lt;b&gt;1.All Zero Initialization&lt;/b&gt;&lt;br&gt;In the ideal situation, with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which you expect to be the “best guess” in expectation. But, this turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during back-propagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.&lt;br&gt;&lt;br&gt;&lt;b&gt;2.Initialization with Small Random Numbers&lt;/b&gt;&lt;br&gt;Thus, you still want the weights to be very close to zero, but not identically zero. In this way, you can random these neurons to small numbers which are very close to zero, and it is treated as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for weights might simply look like&lt;img src="53b51b8c9a87b66da467031daa88caa0.jpg" data-rawwidth="194" data-rawheight="23"&gt;, where&lt;img src="a0a381fa6a18623dc1f11d1358d385c4.jpg" data-rawwidth="63" data-rawheight="23"&gt;is a zero mean, unit standard deviation gaussian. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.&lt;br&gt;&lt;br&gt;&lt;b&gt;3.Calibrating the Variances&lt;/b&gt; One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that you can normalize the variance of each neuron's output to 1 by scaling its weight vector by the square root of its fan-in (i.e., its number of inputs), which is as follows:&lt;img src="075a4301a21f81202656c9b3d03aa4c2.jpg" data-rawwidth="540" data-rawheight="35"&gt;where “randn” is the aforementioned Gaussian and “n” is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence. The detailed derivations can be found from Page. 18 to 23 of the slides. Please note that, in the derivations, it does not consider the influence of ReLU neurons.&lt;br&gt;&lt;br&gt;&lt;b&gt;4.Current Recommendation&lt;/b&gt;&lt;br&gt;As aforementioned, the previous initialization by calibrating the variances of neurons is without considering ReLUs. A more recent paper on this topic by He et al. [4] derives an initialization specifically for ReLUs, reaching the conclusion that the variance of neurons in the network should be 2.0/n  as:&lt;img src="8ca19a7107f8c61e7481f0aff3aab29c.jpg" data-rawwidth="459" data-rawheight="30"&gt;&lt;br&gt;&lt;b&gt;Sec. 4: During Training&lt;/b&gt;&lt;br&gt;&lt;br&gt;Now, everything is ready. Let’s start to train deep networks!&lt;br&gt;&lt;br&gt;&lt;b&gt;Filters and pooling size. &lt;/b&gt;During training, the size of input images prefers to be power-of-2, such as 32 (e.g., CIFAR-10), 64, 224 (e.g., common used ImageNet), 384 or 512, etc. Moreover, it is important to employ a small filter (e.g., 3*3 ) and small strides (e.g., 1) with zeros-padding, which not only reduces the number of parameters, but improves the accuracy rates of the whole deep network. Meanwhile, a special case mentioned above, i.e., 3*3 filters with stride 1, could preserve the spatial size of images/feature maps. For the pooling layers, the common used pooling size is of 2*2.    &lt;b&gt;Learning rate.&lt;/b&gt;In addition, as described in a blog by Ilya Sutskever [2], he recommended to divide the gradients by mini batch size. Thus, you should not always change the learning rates (LR), if you change the mini batch size. For obtaining an appropriate LR, utilizing the validation set is an effective way. Usually, a typical value of LR in the beginning of your training is 0.1. In practice, if you see that you stopped making progress on the validation set, divide the LR by 2 (or by 5), and keep going, which might give you a surprise.&lt;br&gt;&lt;b&gt;Fine-tune on pre-trained models. &lt;/b&gt;Nowadays, many state-of-the-arts deep networks are released by famous research groups, i.e.,Caffe Model Zoo(https://github.com/BVLC/caffe/wiki/Model-Zoo) and VGG Group(http://www.vlfeat.org/matconvnet/pretrained/). Thanks to the wonderful generalization abilities of pre-trained deep models, you could employ these pre-trained models for your own applications directly. For further improving the classification performance on your data set, a very simple yet effective approach is to fine-tune the pre-trained models on your own data. As shown in following table, the two most important factors are the size of the new data set (small or big), and its similarity to the original data set. Different strategies of fine-tuning can be utilized in different situations. For instance, a good case is that your new data set is very similar to the data used for training pre-trained models. In that case, if you have very little data, you can just train a linear classifier on the features extracted from the top layers of pre-trained models. If your have quite a lot of data at hand, please fine-tune a few top layers of pre-trained models with a small learning rate. However, if your own data set is quite different from the data used in pre-trained models but with enough training images, a large number of layers should be fine-tuned on your data also with a small learning rate for improving performance. However, if your data set not only contains little data, but is very different from the data used in pre-trained models, you will be in trouble. Since the data is limited, it seems better to only train a linear classifier. Since the data set is very different, it might not be best to train the classifier from the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier on activations/features from somewhere earlier in the network.&lt;img src="9ef6065676ec806cd835339a3af57b9a.jpg" data-rawwidth="576" data-rawheight="184"&gt;Fine-tune your data on pre-trained models. Different strategies of fine-tuning are utilized in different situations. For data sets, Caltech-101 is similar toImageNet, where both two are object-centric image data sets; while Place Database is different from ImageNet, where one is scene-centric and the other is object-centric.&lt;br&gt;&lt;br&gt;&lt;b&gt;References &amp;amp; Source Links&lt;br&gt;&lt;/b&gt;&lt;br&gt;[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks.(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) In NIPS, 2012&lt;br&gt;[2] A Brief Overview of Deep Learning (http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html/) , which is a guest post by Ilya Sutskever.&lt;br&gt;[3] CS231n: Convolutional Neural Networks for Visual Recognition of Stanford University, held by Prof. Fei-Fei Li and Andrej Karpathy.&lt;br&gt;[4] K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.（http://arxiv.org/abs/1502.01852）InICCV, 2015.&lt;br&gt;[5] B. Xu, N. Wang, T. Chen, and M. Li. Empirical Evaluation of Rectified Activations in Convolution Network（http://arxiv.org/abs/1505.00853）. In ICML Deep Learning Workshop, 2015.&lt;br&gt;[6] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. （http://jmlr.org/papers/v15/srivastava14a.html）JMLR, 15(Jun):1929−1958, 2014.&lt;br&gt;[7] X.-S. Wei, B.-B. Gao, and J. Wu. Deep Spatial Pyramid Ensemble for Cultural Event Recognition. （http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf）In ICCV ChaLearn Looking at People Workshop, 2015.&lt;br&gt;[8] Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms（https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/9781439830031）. Boca Raton, FL: Chapman &amp;amp; HallCRC/, 2012. (ISBN 978-1-439-830031)&lt;br&gt;[9] M. Mohammadi, and S. Das. S-NN: Stacked Neural Networks. Project in Stanford CS231n Winter Quarter, 2015.（http://cs231n.stanford.edu/reports/milad_final_report.pdf）&lt;br&gt;[10] P. Hensman, and D. Masko. The Impact of Imbalanced Training Data for Convolutional Neural Networks.（http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf） Degree Project in Computer Science, DD143X, 2015.&lt;br&gt;&lt;br&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;br&gt;&lt;img src="a88ed14b0a577bc5117e41ef3d938a7e.jpg" data-rawwidth="76" data-rawheight="76"&gt;&lt;strong&gt;魏秀参, &lt;/strong&gt;南京大学计算机系机器学习与数据挖掘所（LAMDA）博士生，研究方向为计算机视觉和机器学习，特别是深度学习和弱监督学习。曾在国际顶级期刊和会议发表学术论文。个人主页：&lt;a href="http://lamda.nju.edu.cn/weixs/"&gt;http://lamda.nju.edu.cn/weixs/&lt;/a&gt;，微博ID：Wilson_NJUer&lt;br&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;b&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324989&amp;amp;idx=1&amp;amp;sn=dba2bef393de4f395a265691e82a4d37&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324989&amp;amp;idx=1&amp;amp;sn=dba2bef393de4f395a265691e82a4d37&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;img src="a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;</description><author>程程</author><pubDate>Tue, 09 Aug 2016 11:31:33 GMT</pubDate></item><item><title>【机器人的双眸】视觉SLAM导论</title><link>https://zhuanlan.zhihu.com/p/21500015</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;版权声明：&lt;/b&gt;本文原载于作者博客园博客，经作者修改增加最新进展后授权发表于深度学习大讲堂微信公众号。&lt;/p&gt;&lt;p&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/p&gt;开始做SLAM（机器人同时定位与建图）研究已经近三年了。从博士一年级开始对这个方向产生兴趣，到现在为止，也算是对这个领域有了大致的了解。然而越了解，越觉得这个方向难度很大。总体来讲有以下几个原因： &lt;p&gt; 1. 入门资料很少。虽然国内也有不少人在做，但这方面现在没有太好的入门教程。《SLAM for dummies》可以算是一篇。中文资料几乎没有。 &lt;/p&gt;&lt;p&gt; 2. SLAM研究已进行了三十多年，从上世纪的九十年代开始。其中又有若干历史分枝和争论，要把握它的走向就很费工夫。 &lt;/p&gt;&lt;p&gt; 3. 难以实现。SLAM是一个完整的系统，由许多个分支模块组成。现在经典的方案是“图像前端，优化后端，闭环检测”的三部曲，很多文献看完了自己实现不出来。 &lt;/p&gt;&lt;p&gt; 4. 自己动手编程需要学习大量的先决知识。首先你要会C和C++，网上很多代码还用了11标准的C++。第二要会用Linux。第三要会cmake，vim/emacs及一些编程工具。第四要会用openCV, PCL, Eigen等第三方库。只有学会了这些东西之后，你才能真正上手编一个SLAM系统。如果你要跑实际机器人，还要会ROS。&lt;/p&gt;&lt;p&gt;当然，困难多意味着收获也多，坎坷的道路才能锻炼人（比如说走着走着才发现Linux和C++才是我的真爱之类的。）鉴于目前网上关于视觉SLAM的资料极少，我于是想把自己这三年的经验与大家分享一下。说的不对的地方请大家批评指正。&lt;/p&gt;&lt;p&gt;这篇文章关注视觉SLAM，专指用摄像机，Kinect等深度像机来做导航和探索，且主要关心室内部分。到目前为止，视觉SLAM的框架已基本成熟，可以在运动稳定、光照充足的环境中，在厘米级别的误差范围内进行定位与建图。但是，离充满动态光照和人群密集的现实环境，要应用视觉SLAM技术，仍有很长的路要走。以下，我会介绍SLAM的历史、理论以及实现的方式，且主要介绍视觉（Kinect）的实现方式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;SLAM问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;SLAM，全称叫做Simultaneous Localization and Mapping，中文叫做同时定位与建图。大体说来，一个SLAM系统分为四个模块（除去传感器数据读取），如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a437944e5c4b41bb5d0cf5c4e3f7cd35.png" data-rawwidth="718" data-rawheight="261"&gt;啊不行，这么讲下去，这篇文章肯定没有人读，所以我们换一个讲法，引入机器人“小萝卜”同学。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小萝卜的故事&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从前，有一个机器人叫“小萝卜”。它长着一双乌黑发亮的大眼睛，叫做Kinect。有一天，它被邪恶的科学家关进了一间空屋子，里面放满了杂七杂八的东西。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4ae83cd42362b37d2c89fbffe07732aa.png" data-rawwidth="708" data-rawheight="451"&gt;小萝卜感到很害怕，因为这个地方他从来没来过，一点儿也不了解。让他感到害怕的主要是三个问题：　　&lt;p&gt;1. 自己在哪里？　　&lt;/p&gt;&lt;p&gt;2.  这是什么地方？　　&lt;/p&gt;&lt;p&gt;3.  怎么离开这个地方？&lt;/p&gt;&lt;p&gt;在SLAM理论中，第一个问题称为定位 (Localization)，第二个称为建图 (Mapping)，第三个则是随后的路径规划。我们希望借助Kinect工具，帮小萝卜解决这个难题。各位同学有什么思路呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;Kinect数据&lt;/b&gt;&lt;/p&gt;要打败敌人，首先要了解你的武器。不错，我们先介绍一下Kinect。众所周知这是一款深度相机，你或许还听说过别的牌子，但Kinect的价格便宜，测量范围在3m-12m之间，精度约3cm，较适合于小萝卜这样的室内机器人。它采到的图像是这个样子的（从左往右依次为rgb图，深度图与点云图）：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8db1c9cbcfb522514315b6db317990ba.png" data-rawwidth="717" data-rawheight="192"&gt;Kinect的一大优势在于能比较廉价地获得每个像素的深度值，不管是从时间上还是从经济上来说。OK，有了这些信息，小萝卜事实上可以知道它采集到的图片中，每一个点的3d位置。只要我们事先标定了Kinect，或者采用出厂的标定值。&lt;/p&gt;&lt;p&gt;我们把坐标系设成这个样子，这也是openCV中采用的默认坐标系。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/def7c60ac8aa46b7119924d84411601f.png" data-rawwidth="588" data-rawheight="588"&gt;o’-uv是图片坐标系，o-xyz是Kinect的坐标系。假设图片中的点为(u,v)，对应的三维点位置在(x,y,z)，那么它们之间的转换关系是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/684aaaa877aa06c1a0720c550ed5f866.png" data-rawwidth="353" data-rawheight="94"&gt;或者更简单的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/36be4cbc9718f38a5b1b4c10a70602f5.png" data-rawwidth="470" data-rawheight="253"&gt;后一个公式给出了计算三维点的方法。先从深度图中读取深度数据（Kinect给的是16位无符号整数），除掉z方向的缩放因子，这样你就把一个整数变到了以米为单位的数据。然后，x,y用上面的公式算出。一点都不难，就是一个中心点位置和一个焦距而已。f代表焦距，c代表中心。如果你没有自己标定你的Kinect，也可以采用默认的值：s=5000, cx = 320, cy=240, fx=fy=525。实际值会有一点偏差，但不会太大。&lt;/p&gt;&lt;p&gt;&lt;b&gt;定位问题&lt;/b&gt;&lt;/p&gt;知道了Kinect中每个点的位置后，接下来我们要做的，就是根据两帧图像间的差别计算小萝卜的位移。比如下面两张图，后一张是在前一张之后1秒采集到的：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/546f40f8c9f7e721d1620f9c6679dcd2.png" data-rawwidth="617" data-rawheight="232"&gt;你肯定可以看出，小萝卜往右转过了一定的角度。但究竟转过多少度呢？这就要靠计算机来求解了。这个问题称为相机相对姿态估计，经典的算法是ICP（Iterative Closest Point，迭代最近点）。这个算法要求知道这两个图像间的一组匹配点，说的通俗点，就是左边图像哪些点和右边是一样的。你当然看见那块黑白相间的板子同时出现在两张图像中。在小萝卜看来，这里牵涉到两个简单的问题：特征点的提取和匹配。&lt;p&gt;如果你熟悉计算机视觉，那你应该听说过SIFT, SURF之类的特征。不错，要解决定位问题，首先要得到两张图像的一个匹配。匹配的基础是图像的特征，下图就是SIFT提取的关键点与匹配结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/81483842c135d4045b2390e70c71b542.png" data-rawwidth="715" data-rawheight="558"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f558202613a3961a16b4a73a6e3fc07a.png" data-rawwidth="710" data-rawheight="280"&gt;对实现代码感兴趣的同学请Google“opencv 匹配”即可，在openCV的教程上也有很明白的例子。上面的例子可以看出，我们找到了一些匹配，但其中有些是对的（基本平等的匹配线），有些是错的。这是由于图像中存在周期性出现的纹理（黑白块），所以容易搞错。但这并不是问题，在接下来的处理中我们会将这些影响消去。&lt;/p&gt;&lt;p&gt;得到了一组匹配点后，我们就可以计算两个图像间的转换关系，也叫PnP问题。它的模型是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/92cb6bc089fb3595d195e3cab93a4649.png" data-rawwidth="365" data-rawheight="393"&gt;R为相机的姿态，C为相机的标定矩阵。R是不断运动的，而C则是随着相机做死的。ICP的模型稍有不同，但原理上也是计算相机的姿态矩阵。原则上，只要有四组匹配点，就可以算这个矩阵。你可以调用openCV的SolvePnPRANSAC函数或者PCL的ICP算法来求解。openCV提供的算法是RANSAC（Random Sample Consensus，随机采样一致性）架构，可以剔除错误匹配。所以代码实际运行时，可以很好地找到匹配点。以下是一个结果的示例。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ed1e0ed810c1563d0320e4e77ec596a3.png" data-rawwidth="715" data-rawheight="266"&gt;上面两张图转过了16.63度，位移几乎没有。&lt;/p&gt;有同学会说，那只要不断匹配下去，定位问题不就解决了吗？表面上看来，的确是这样的，只要我们引入一个关键帧的结构（发现位移超过一个固定值时，定义成一个关键帧）。然后，把新的图像与关键帧比较就行了。至于建图，就是把这些关键帧的点云拼起来，看着还有模有样，煞有介事的：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f7b9858ddc01dc16348bbc4eadfda6b8.png" data-rawwidth="711" data-rawheight="454"&gt;然而，如果事情真这么简单，SLAM理论就不用那么多人研究三十多年了（它是从上世纪90年代开始研究的）（上面讲的那些东西简直随便哪里找个小硕士就能做出来……）。那么，问题难在什么地方呢？&lt;p&gt;&lt;b&gt;SLAM端优化理论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最麻烦的问题，就是“噪声”。这种渐近式的匹配方式，和那些惯性测量设备一样，存在着累积噪声。因为我们在不断地更新关键帧，把新图像与最近的关键帧比较，从而获得机器人的位移信息。但是你要想到，如果有一个关键帧出现了偏移，那么剩下的位移估计都会多出一个误差。这个误差还会累积，因为后面的估计都基于前面的机器人位置……哇！这后果简直不堪设想啊（例如，你的机器人往右转了30度，再往左转了30度回到原来的位置。然而由于误差，你算成了向右转29度，再向左转31度，这样你构建的地图中，会出现初始位置的两个“重影”）。我们能不能想办法消除这个该死的误差呢？&lt;/p&gt;&lt;p&gt;朋友们，这才是SLAM的研究，前面的可以说是“图像前端”的处理方法。我们的解决思路是：如果你和最近的关键帧相比，会导致累计误差。那么，我们最好是和更前面的关键帧相比，而且多比较几个帧，不要只比较一次。&lt;/p&gt;我们用数学来描述这个问题。设：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e21696d38a7a8c802aa10d09e86d293c.png" data-rawwidth="499" data-rawheight="216"&gt;不要怕，只有借助数学才能把这个问题讲清楚。上面的公式中，xp是机器人小萝卜的位置，我们假定由n个帧组成。xL则是路标，在我们的图像处理过程中就是指SIFT提出来的关键点。如果你做2D SLAM，那么机器人位置就是x, y加一个转角theta。如果是3D SLAM，就是x,y,z加一个四元数姿态（或者rpy姿态）。这个过程叫做参数化（Parameterization）。&lt;/p&gt;&lt;p&gt;不管你用哪种参数，后面两个方程你都需要知道。前一个叫运动方程，描述机器人怎样运动。u是机器人的输入，w是噪声。这个方程最简单的形式，就是你能通过什么方式（码盘等）获得两帧间的位移差，那么这个方程就直接是上一帧与u相加即得。另外，你也可以完全不用惯性测量设备，这样我们就只依靠图像设备来估计，这也是可以的。&lt;/p&gt;&lt;p&gt;后一个方程叫观测方程，描述那些路标是怎么来的。你在第i帧看到了第j个路标，产生了一个测量值，就是图像中的横纵坐标。最后一项是噪声。偷偷告诉你，这个方程形式上和上一页的那个方程是一模一样的。&lt;/p&gt;&lt;p&gt;在求解SLAM问题前，我们要看到，我们拥有的数据是什么？在上面的模型里，我们知道的是运动信息u以及观测z。用示意图表示出来是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/588ab235d3c73468e266985fbdac0e5f.png" data-rawwidth="618" data-rawheight="435"&gt;我们要求解的，就是根据这些u和z，确定所有的xp和xL。这就是SLAM问题的理论。从SLAM诞生开始科学家们就一直在解决这个问题。最初，我们用Kalman滤波器，所以上面的模型（运动方程和观测方程）被建成这个样子。直到21世纪初，卡尔曼滤波器仍在SLAM系统占据最主要的地位，Davison经典的单目SLAM就是用EKF做的。但是后来，出现了基于图优化的SLAM方法，渐渐有取而代之的地位[1]。我们在这里不介绍卡尔曼滤波器，有兴趣的同学可以在wiki上找卡尔曼滤波器，另有一篇中文的《卡尔曼滤波器介绍》也很棒。由于滤波器方法存储n个路标要消耗n平方的空间，在计算量上有点对不住大家。尽管08年有人提出分治法的滤波器能把复杂度弄到O(n) [2]，但实现手段比较复杂。我们要介绍那种新兴的方法: Graph-based SLAM。&lt;/p&gt;&lt;p&gt;图优化方法把SLAM问题做成了一个优化问题。学过运筹学的同学应该明白，优化问题对我们有多么重要。我们不是要求解机器人的位置和路标位置吗？我们可以先做一个猜测，猜想它们大概在什么地方。这其实是不难的。然后呢，将猜测值与运动模型／观测模型给出的值相比较，可以算出误差：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8a5b69cb9101880eadb893007c8229ab.png" data-rawwidth="598" data-rawheight="277"&gt;通俗一点地讲，例如，我猜机器人第一帧在(0,0,0)，第二帧在(0,0,1)。但是u1告诉我机器人往z方向（前方）走了0.9米，那么运动方程就出现了0.1m的误差。同时，第一帧中机器人发现了路标1，它在该机器人图像的正中间；第二帧却发现它在中间偏右的位置。这时我们猜测机器人只是往前走，也是存在误差的。至于这个误差是多少，可以根据观测方程算出来。&lt;/p&gt;我们得到了一堆误差，把这些误差平方后加起来（因为单纯的误差有正有负，然而平方误差可以改成其他的范数，只是平方更常用），就得到了平方误差和。我们把这个和记作phi，就是我们优化问题的目标函数。而优化变量就是那些个xp, xL。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/243f19e0f12f064c43c8db3f049eb4e2.png" data-rawwidth="462" data-rawheight="63"&gt;改变优化变量，误差平方和（目标函数）就会相应地变大或变小，我们可以用数值方法求它们的梯度和二阶梯度矩阵，然后用梯度下降法求最优值。这些东西学过优化的同学都懂的。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/79423da5f175a3c1cde754fe38545f42.png" data-rawwidth="466" data-rawheight="71"&gt;注意到，一次机器人SLAM过程中，往往会有成千上万帧。而每一帧我们都有几百个关键点，一乘就是几百万个优化变量。这个规模的优化问题放到小萝卜的机载小破本上可解吗？是的，过去的同学都以为，Graph-based SLAM是无法计算的。但就在21世纪06，07年后，有些同学发现了，这个问题规模没有想象的那么大。上面的J和H两个矩阵是“稀疏矩阵”，于是呢，我们可以用稀疏代数的方法来解这个问题。“稀疏”的原因，在于每一个路标，往往不可能出现在所有运动过程中，通常只出现在一小部分图像里。正是这个稀疏性，使得优化思路成为了现实。&lt;/p&gt;&lt;p&gt;优化方法利用了所有可以用到的信息（称为full-SLAM, global SLAM），其精确度要比我们一开始讲的帧间匹配高很多。当然计算量也要高一些。&lt;/p&gt;&lt;p&gt;由于优化的稀疏性，人们喜欢用“图”来表达这个问题。所谓图，就是由节点和边组成的东西。我写成G={V,E}，大家就明白了。V是优化变量节点，E表示运动/观测方程的约束。什么，更糊涂了吗？那我就上一张图，来自[3]。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/9589f79d22e80c95e2e449a918775301.png" data-rawwidth="485" data-rawheight="227"&gt;图有点模糊，而且数学符号和我用的不太一样，我用它来给大家一个图优化的直观形象。上图中，p是机器人位置，l是路标，z是观测，t是位移。其中呢，p, l是优化变量，而z,t是优化的约束。看起来是不是像一些弹簧连接了一些质点呢？因为每个路标不可能出现在每一帧中，所以这个图是蛮稀疏的。不过，“图”优化只是优化问题的一个表达形式，并不影响优化的含义。实际解起来时还是要用数值法找梯度的。这种思路在计算机视觉里，也叫做Bundle Adjustment。它的具体方法请参见一篇经典文章[4]。&lt;/p&gt;&lt;p&gt;不过，BA的实现方法太复杂，不太建议同学们拿C来写。好在2010年的ICRA上，其他的同学们提供了一个通用的开发包：g2o [5]。它是有图优化通用求解器，很好用，我改天再详细介绍这个软件包。总之，我们只要把观测和运动信息丢到求解器里就行。这个优化器会为我们求出机器人的轨迹和路标位置。如下图，红点是路标，蓝色箭头是机器人的位置和转角（2D SLAM）。细心的同学会发现它往右偏转了一些。：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5f62b425357b66cd5d9f69c2dee20ac4.png" data-rawwidth="706" data-rawheight="403"&gt;&lt;b&gt;闭环检测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面提到，仅用帧间匹配最大的问题在于误差累积，图优化的方法可以有效地减少累计误差。然而，如果把所有测量都丢进g2o，计算量还是有点儿大的。根据我自己测试，约10000多条边，g2o跑起来就有些吃力了。这样，就有同学说，能把这个图构造地简洁一些吗？我们用不着所有的信息，只需要把有用的拿出来就行了。&lt;/p&gt;&lt;p&gt;事实上，小萝卜在探索房间时，经常会左转一下，右转一下。如果在某个时刻他回到了以前去过的地方，我们就直接与那时候采集的关键帧做比较，可以吗？我们说，可以，而且那是最好的方法。这个问题叫做闭环检测。&lt;/p&gt;&lt;p&gt;闭环检测是说，新来一张图像时，如何判断它以前是否在图像序列中出现过？有两种思路：一是根据我们估计的机器人位置，看是否与以前某个位置邻近；二是根据图像的外观，看它是否和以前关键帧相似。目前主流方法是后一种，因为很多科学家认为前一种依靠有噪声的位置来减少位置的噪声，有点循环论证的意思。后一种方法呢，本质上是个模式识别问题（非监督聚类，分类），常用的是Bag-of-Words (BOW)。但是BOW需要事先对字典进行训练，因此SLAM研究者仍在探讨有没有更合适的方法。&lt;/p&gt;&lt;p&gt;在Kinect SLAM经典大作中[6]，作者采用了比较简单的闭环方法：在前面n个关键帧中随机采k个，与当前帧两两匹配。匹配上后认为出现闭环。这个真是相当的简单实用，效率也过得去。&lt;/p&gt;&lt;p&gt;高效的闭环检测是SLAM精确求解的基础。研究者也在尝试利用深度学习技术提高闭环检测的精度，例如本文作者发表在Autonomous Robot期刊上的论文Unsupervised Learning to Detect Loops Using Deep Neural Networks for Visual SLAM System采用了无监督的深度自动编码机从原始输入图像中学习紧凑的图像表示，相比于传统的Bag of Word方法提高了闭环检测的鲁棒性。方法流程图如下[7]：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5d86fbd436bd03310940f79efe243fac.png" data-rawwidth="685" data-rawheight="182"&gt;&lt;b&gt;小结&lt;/b&gt;&lt;/p&gt;本文我们介绍了SLAM的基本概念，重点介绍了图优化解决SLAM问题的思路。本文作者编写了一个基于RGB-D相机的SLAM程序，它是一个Linux下基于cmake的工程，github地址是：&lt;a href="https://github.com/gaoxiang12/rgbd-slam-tutorial-gx" class=""&gt;https://github.com/gaoxiang12/rgbd-slam-tutorial-gx&lt;/a&gt; 。&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;[1] Visual SLAM: Why filter? Strasdat et. al., Image and Vision Computing, 2012.&lt;p&gt;[2] Divide and Conquer: EKF SLAM in O(n), Paz Lina M et al., IEEE Transaction on Robotics, 2008&lt;/p&gt;&lt;p&gt;[3] Relative bundle adjustment, Sibley, Gabe, 2009&lt;/p&gt;&lt;p&gt;[4] Bundle adjustment - a Modern Synthesis. Triggs B et. el., Springer, 2000&lt;/p&gt;&lt;p&gt;[5] g2o: A General Framework for Graph Optimization, Kummerle Rainer, et. al., ICRA, 2011&lt;/p&gt;&lt;p&gt;[6] 3-D Mapping with an RGB-D Camera, IEEE Transaction on Robotics, Endres et al., 2014&lt;/p&gt;&lt;p&gt;[7] Xiang Gao, Tao Zhang, Unsupervised Learning to Detect Loops Using Deep Neural Networks for Visual SLAM System, Autonomous Robot, 2015. &lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/da2c45572e3f53a0bf5ffc57210439af.png" data-rawwidth="95" data-rawheight="94"&gt;&lt;strong&gt;高翔，&lt;/strong&gt;清华大学自动化学院博士研究生，主要研究兴趣为基于RGB-D相机的视觉SLAM技术。先后获得清华大学新生奖学金、张明为奖学金并三次获得国家励志奖学金，相关研究成果发表于Robotics and Autonomous Systems、Autonomous Robot、CCC等期刊和会议。个人博客地址：&lt;a href="http://cnblogs.com/gaoxiang12"&gt;http://cnblogs.com/gaoxiang12&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324941&amp;amp;idx=1&amp;amp;sn=f1ec0018e80d94ea0fbd75af27961c98&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324941&amp;amp;idx=1&amp;amp;sn=f1ec0018e80d94ea0fbd75af27961c98&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/499b12b5d7aa6804560ef81a692b6637.jpg" data-rawwidth="327" data-rawheight="55"&gt;&lt;/p&gt;</description><author>程程</author><pubDate>Wed, 06 Jul 2016 17:06:57 GMT</pubDate></item><item><title>深度强化学习导引</title><link>https://zhuanlan.zhihu.com/p/21498750</link><description>&lt;p&gt;&lt;img src="https://pic4.zhimg.com/07dfccb1b1645b43cbca9aceb9f5bfeb_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;前言&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;深度强化学习可以说是人工智能领域现在最热门的方向，吸引了众多该领域优秀的科学家去发掘其能力极限。而深度强化学习本身也由于其通用性备受各个应用领域推崇，从端对端游戏控制、机器人手臂控制、推荐系统，甚至也来到了自然语言对话系统。然而如何在日新月异，几乎每日都在更新迭代的深度强化学习的进展中保持好节奏，那是这篇文章带给大家的建议和思考。&lt;br&gt;&lt;/p&gt;&lt;p&gt;我们首先简要介绍一下深度学习和强化学习技术，以及在两者融合两者过程可能会出现的问题，接着探讨了深度强化学习的几种范式，然后介绍近期有意思的一些工作和应用，最后给出总结和展望。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;基础&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一.深度学习&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习是人工神经网络 2006 年后重获新生的名称，伴随着其实际应用中的超越式效果而风靡全球。使之成为可行的方法的计算设备 GPU 也因此大卖特卖，成为深度学习研究必备利器。&lt;br&gt;&lt;/p&gt;&lt;p&gt;人工神经网络已经可以实现任意复杂度连续函数的逼近，这个可以在 Michael Nielsen 的《神经网络和深度学习》书中看到神经网络可以计算任何函数的具体化的证明。而深度学习则可以利用超多的隐藏层来提升表示的能力（浅层网络需要指数级的隐藏元个数才能达到相当的深层网络的表达能力）。深度学习的表示其实是大量函数的复合，并可以通过反向传播进行训练，参见下图。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="5efa70e84d4f1761bf74245484331594.png" data-rawwidth="710" data-rawheight="449"&gt;现在深度学习已经席卷了语音识别、图像识别、计算机视觉、自然语言处理乃至视频预测等领域，主要的两种网络 CNN 和 RNN 完成了空间和时间的完备。但由于对于深度学习本身仍旧有太多的认知空白，一部分人仍然对其无法完全接受。尽管这样，我还是想建议大家去了解它，你可以从书本开始，比如说前面提到的 &lt;b&gt;《神经网络和深度学习》&lt;/b&gt; 还有来自 Montreal University 的 &lt;b&gt;《深度学习》&lt;/b&gt;，来走进这个领域。这本书包含了深度学习学习、研究及应用所有需要的概念和直觉（并不含强化学习）。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;二.强化学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;强化学习，现在常常将其看作机器学习领域的一个分支，但如果细细去看，你会发现，强化学习本身也有完整的一条发展的脉络。从动物行为研究和优化控制两个领域独立发展最终经 Bellman 之手汇集抽象为 MDP 问题而完成形式化。之后经很多的科学家的不断扩大，形成了相对完备的体系——常被称为近似动态规划，参看 MIT 教授 Dimitri P. Bertsekas 的 动态规划系列，Dynamic Programming and Optimal Control, Vol. II, 4th Edition: Approximate Dynamic Programming。&lt;br&gt;&lt;/p&gt;&lt;p&gt;强化学习是非常严谨的领域，适合各类人享受/被折磨（数学重起来可以直接 KO 一般的非数学系本科生）。但往往应用起来却非常困难，首先维度灾难的存在使得我们很难高效地求解最优的策略或者计算最优行动值。另外深度学习其中包含的思想——贪婪、动态规划、近似等等都是算法中最为关键的部分，也是这些方法使用得比较极致的地方。因此，才有不少人持续在其上很多年不断地推进研究的深入和一般性。（这里，其实要说一句，国内的强化学习研究并不是特别领先，也要引发我们的思考。另一个有趣的现象是，作为强化学习研究的重镇 Alberta 大学，也就是 Richard Sutton 等计算机科学家领衔的强化学习中心，同样是在加拿大。这种感觉让人想到了 Geoffrey Hinton 在 Toronto 领导的深度学习复兴。个人感觉，国内强化学习研究不能够兴起的原因是研究者本身相对狭窄的视角，与不同学科和思想的连接甚弱，乃至于不敢想象——一句话概括的话，我觉得是勇气和想象力的缺失吧！在现在的研究中看到得更多是很多想法的全方位连接，交叉科学的研究是切切实实地交叉。）&lt;br&gt;&lt;/p&gt;&lt;p&gt;在 Warren B. Powell 的一篇短文中说道，很多来自不同领域的人，都在忙着自己的一亩三分地上耕耘，自得其乐；实际上，大多人做出来同样的工作，因此他提出了 10 条意见。简言之：建议大家从一个全貌看待问题和学科，找到相通联的点，以此出发，找到潜在的连线，最终形成整体的面的认知。&lt;br&gt;&lt;/p&gt;&lt;p&gt;这里结合 David Silver 的强化学习课程给出一个强化学习的概貌：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="07dfccb1b1645b43cbca9aceb9f5bfeb.png" data-rawwidth="689" data-rawheight="405"&gt;&lt;b&gt;三.&lt;/b&gt;&lt;b&gt;深度强化学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习模型的简单（实际上带来了更多的不可控制的难度）刚刚好是降低了一些使用的难度，短短数十行代码，便能够解决之前需要花费大量精力才可以设计出来的系统。所以，各个应用领域（语音、图像、视觉、自然语言理解等）现在都把资源往深度学习上倾斜，在这里我们不去评判这会造成的未发生的不良后果，从乐观的角度来看，深度学习确实让人工智能领域重新焕发活力。当然如何去疏导人们的激情是相当重要的事情，我相信过上一段时间后，大家都会找到合适的路径发展下去的。&lt;br&gt;&lt;/p&gt;&lt;p&gt;一蹴而就的成功在科学领域往往是非常难以实现的。存在的若干重要的数论、图论问题，也都是经过一代代科学家继往开来、在前人工作上不断推进的。说完了历史，现在来看看最为激动人心的进展。我们介绍深度强化学习的范式和相关算法。看看究竟什么才是最为关键的因素。 实际上关键在于我们如何去应用这些技术解决问题——适合的问题建模，解决手段的提升。&lt;br&gt;&lt;/p&gt;&lt;p&gt;强化学习之前并不能实用的原因在于面对过大的状态或者行动空间，很难有效地处理这些情形，往往看到的例子都是相对简化的场景。深度学习的出现让人们能够去处理真正的问题，比如说视觉识别准确率的大幅提高至 ImageNet 数据急的 top-5 错误率下降到了 4% 以内，现在语音识别已经真正变得比较成熟，并且被广泛商用，且目前所有的商用语音识别算法没有一个不是基于深度学习的。这些都是说明深度学习能成为一些实际应用的基础。而现在深度强化学习的研究和应用也基本上针对上面的问题展开。&lt;br&gt;&lt;/p&gt;&lt;p&gt;根据 Berkeley 的深度强化学习课程我们可以其分成近似动态规划方法（Approximate Dynamic Programming Methods）策略梯度方法（Policy Gradient Methods）和 搜索+监督学习（Search + Supervised Learning）三类。我们这里挑几个代表性的方法简要介绍一下，如 Deep Q- Network、Double Q-Network 和 DDPG 等方法及现在的一些应用，如机器人手臂控制、对话生成和游戏控制等等。这些研究也不是突然一下子就出现的，他们的产生可以说伴随着强化学习的发展而恰好到深度学习的出现又产生了巨大的能量。先看看近似动态规划方法，Deep Q-Network。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;四.Deep Q-Network&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DQN 实际上在 2013 年就已经发表，后经 DeepMind 众人改进成发表在 Nature 上的经典文章，由于现在已经有大量的文章介绍过，我们这里略过。DQN 是一种基于 Q-学习的神经网络版本。通过神经网络来近似 Q 函数，但是并不是简单地替换，否则在 2006 年应该就能够产生一定的影响了。DQN 解决了三个困难，DQN 为深度基于值的强化学习问题提供了一种稳定解决方案：&lt;br&gt;&lt;/p&gt;&lt;p&gt;1. 使用经验回放将数据之间的关联打破，重回独立同分布的设定下，从过去的策略中学习，使用 免策略 Q-学习&lt;br&gt;&lt;/p&gt;&lt;p&gt;2.目标 Q-网络避免振荡，将 Q-网络和目标网络之间的关联打破&lt;br&gt;&lt;/p&gt;&lt;p&gt;3.截断奖励或者正规化网络，适应到合适的范围内可以得到健壮的梯度&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;五.Double Q-Network&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;http://arxiv.org/pdf/1509.06461v3.pdf &lt;/p&gt;&lt;p&gt;在某些随机环境中，Q-学习表现很糟糕。罪魁祸首是很大的行动值的过估计(overestimations)。这些过估计是由于 Q学习使用最大的行动值作为最大期望行动值的估计产生了正的偏差。这里有另外一种方式来近似对于任意随机变量集的最大期望行动值。所谓的双估计方法某些事件会欠估计而不是过估计。将这种思想应用在 Q-学习上可以得到双 Q-学习方法，一种免策略强化学习方法。这个算法可以收敛到最优策略上，并在某些设置下表现得要超过 Q-学习算法。&lt;br&gt;&lt;/p&gt;&lt;p&gt;Double Q-Network 则是融合 Q-学习和深度学习的结果，在某些 Atari 游戏中 DQN 本身其实也会受到过估计的影响，通过双 Q-学习的引入，就能够处理大规模的函数近似问题。最终的算法不仅仅降低了观察值过估计，而且在某些游戏中有着相当好的表现。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;六.策略梯度方法&lt;/b&gt;&lt;br&gt;&lt;/p&gt;尽管现存若干本强化学习相关的书籍，但是对于策略梯度部分的介绍确实不够的。已有的强化学习（RL）课本没有给出足够的关于如何使用函数近似的指导；基本上都是聚焦在离散状态空间的领域。而且，现有 RL 课本并没有对无导数优化和策略梯度方法给出充分讲述，而这些技术在很多的任务上都是相当重要的.&lt;p&gt;策略梯度算法通过梯度下降进行优化。就是说，通过重复计算策略的期望回报梯度的噪声估计，然后按照梯度方向来更新策略。该方法比其他 RL 方法（如 Q-学习）更有利，原因是我们可以直接优化感兴趣的量——策略的期望总收益。该类方法由于梯度估计的高方差长期被认为不太实用，直到最近，Schulman 等人和 Mnih 等人的工作展示了神经网络策略在困难的控制问题上的采用策略梯度方法的成功应用。&lt;/p&gt;&lt;p&gt;你可能比较熟悉概率模型的监督学习，其中目标是最大化给定输入(x) 时的输出 (y) 的对数概率。&lt;/p&gt;&lt;p&gt;&lt;img src="da1a3ebd4c126e8d1bc89114742d9d4c.png" data-rawwidth="209" data-rawheight="40"&gt;策略梯度方法通常需要假设一个随机策略，该策略给出了对每个状态 (s) 的行动 (a) 上的概率分布；我们将此分布写作&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="732ff3d66fdc7dbe2344bee142e14528.png" data-rawwidth="88" data-rawheight="28"&gt;如果我们知道对每个状态正确的行动&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="5f59e92cebc22bae417043511395bf5f.png" data-rawwidth="29" data-rawheight="23"&gt;我们可以简单地最大化监督学习的目标函数：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="fddedeaec7ed56214705e958398527fa.png" data-rawwidth="207" data-rawheight="33"&gt;然而，我们并不知道正确的行动。相反，我们会尝试对行动好坏进行粗略的猜测，试着去增加好的行动的概率。更加具体地讲，假设我们刚收集完 agent 和环境一个 agent 和环境回合的交互，所以我们有了一个状态、行动和收益的序列：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="b2271ebef3f6a6411bc40474372dc8d4.png" data-rawwidth="388" data-rawheight="36"&gt;令&lt;img src="ec7d017fa08affac5f1e4484b81db22c.png" data-rawwidth="26" data-rawheight="27"&gt;表示收益的和：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="e1c894e0ff510cbeca87477f37d11bf7.png" data-rawwidth="116" data-rawheight="42"&gt;最简单的策略梯度公式就是：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="90caca9d6c3fafcc60f45a87cf24fa08.png" data-rawwidth="459" data-rawheight="35"&gt;使用这个梯度的估计&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="f9d7d65dbf828916541eb3e93e2dd436.png" data-rawwidth="39" data-rawheight="30"&gt;我们可以用一个梯度上升的步骤，&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="debda400f2af05cfd8f3d000de0eedc8.png" data-rawwidth="103" data-rawheight="27"&gt;进行策略的更新，其中&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="f38d1b690cbf5235f299dd6af36bb363.png" data-rawwidth="40" data-rawheight="27"&gt;为学习率，我们会收集所有的回合，对那个回合中所有的行动的对数概率按照回合的总收益&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="ec7d017fa08affac5f1e4484b81db22c.png" data-rawwidth="26" data-rawheight="27"&gt;为比例进行增加。换言之，如果我们收集了大量的回合数据，其中一些是好的（凭借运气），另外一些是差的。我们本质上是在进行监督学习——最大化好的回合的概率。&lt;br&gt;&lt;/p&gt;&lt;p&gt;尽管我们现在还没有给出对上述策略梯度公式的数学上的验证，但实际上已经给出了一个对策略梯度的无偏估计.&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="4cc23ff1836b460d09695c185e057e20.png" data-rawwidth="126" data-rawheight="35"&gt;策略梯度定义为右式的策略期望总收益的梯度.&lt;br&gt;&lt;/p&gt;&lt;p&gt;如果我们用充足的样本（充足的回合），那么就可以任意精度计算出策略梯度。然而，估计量&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="f9d7d65dbf828916541eb3e93e2dd436.png" data-rawwidth="39" data-rawheight="30"&gt;通常噪声很大，即有很高的方差。你可以想象，这里存在很大的提升空间。与其提高好的轨迹（trajectory）的概率，我们应该提高好的行动的概率，也就是说，我们应试着去推断哪些行动影响轨迹的好坏.&lt;br&gt;&lt;/p&gt;&lt;p&gt;有一系列形如下式的策略梯度估计量：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="ad30ed0ddeb61b1841e416dbae7a310e.png" data-rawwidth="237" data-rawheight="34"&gt;其中&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="14f65702a8290221a658894860592978.png" data-rawwidth="37" data-rawheight="30"&gt;是行动&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="f74de3924c1f551f83f7f7e257d99f67.png" data-rawwidth="32" data-rawheight="25"&gt;的有利度 (advantage)的估计——比平均值好还是坏的程度.&lt;br&gt;下面的有利度估计量更有效率，也更常见：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="9293cb7a4a5fcf1b045b8ab7182487ce.png" data-rawwidth="307" data-rawheight="37"&gt;其中&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="b8083a0a563928830acb37526b90dc91.png" data-rawwidth="30" data-rawheight="27"&gt;是折扣因子，&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="d7a01a2404fb59d49ddd82a161a43bf8.png" data-rawwidth="51" data-rawheight="35"&gt;是 状态-值 函数&lt;br&gt;&lt;/p&gt;&lt;img src="5ff938444c7ed9f170b17c9ba496f29a.png" data-rawwidth="204" data-rawheight="32"&gt;的近似.&lt;img src="b8083a0a563928830acb37526b90dc91.png" data-rawwidth="30" data-rawheight="27"&gt;用来定义一个有效时间区域，其中你忽略所有可能的超过未来&lt;br&gt;&lt;img src="29e78468b8046611d43e7905d8823a40.png" data-rawwidth="98" data-rawheight="32"&gt;的时间步的影响。&lt;br&gt;我们的策略其实是&lt;img src="60897c8af2785a741464adaae036e67c.png" data-rawwidth="79" data-rawheight="35"&gt;那么我们如何使用一个神经网络进行表示？实际上，我们仅仅需要将&lt;br&gt;&lt;img src="b3319e5326f032d75fd1c05e0959392e.png" data-rawwidth="33" data-rawheight="27"&gt;映射到某个向量&lt;br&gt;&lt;img src="55edd4eee71060674f24ad89c3b00517.png" data-rawwidth="30" data-rawheight="22"&gt;上，向量描述了行动&lt;br&gt;&lt;img src="f38d1b690cbf5235f299dd6af36bb363.png" data-rawwidth="40" data-rawheight="27"&gt;上的分布。例如，如果&lt;br&gt;&lt;img src="f38d1b690cbf5235f299dd6af36bb363.png" data-rawwidth="40" data-rawheight="27"&gt;来自一个离散的集合，那么我们设计一个神经网络将&lt;br&gt;&lt;img src="b3319e5326f032d75fd1c05e0959392e.png" data-rawwidth="33" data-rawheight="27"&gt;到一个概率向量上。（我们一般在神经网络的最后层使用一个 softmax 函数）这完全就是我们用来进行分类器学习的形式.如果&lt;br&gt;&lt;img src="f38d1b690cbf5235f299dd6af36bb363.png" data-rawwidth="40" data-rawheight="27"&gt;是连续的，那么我们可以将&lt;br&gt;&lt;img src="b3319e5326f032d75fd1c05e0959392e.png" data-rawwidth="33" data-rawheight="27"&gt;映射到一个高斯分布的均值和方差上。一般情况我们使用一个不依赖于&lt;br&gt;&lt;img src="b3319e5326f032d75fd1c05e0959392e.png" data-rawwidth="33" data-rawheight="27"&gt;的对角协方差.如果&lt;br&gt;&lt;img src="f38d1b690cbf5235f299dd6af36bb363.png" data-rawwidth="40" data-rawheight="27"&gt;是二值的，那么我们可以使用一个单个输出的网络，表示输出 1 的概率.&lt;br&gt;&lt;b&gt;七.DDPG 深度确定型策略梯度方法&lt;/b&gt;&lt;br&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/ddpg.pdf&lt;br&gt;&lt;br&gt;这是 DPG 确定型策略梯度方法的深度学习化，利用 DQN 的思想将 DPG 进行改造。DDPG 可以解决连续行动空间上的强化学习问题。在实验中，DDPG 给出了稳定的表现，并且在不同环境上都不需要做出改动。另外，DDPG 在所有实验中都是以比 DQN 学习使用更少时间步的经验发现 Atari 游戏的解的，大概是性能 20 倍的差距。给定更多模拟时间，DDPG 可能解决比现在 Atari 游戏更加困难的问题。DDPG 的未来方向应该是利用基于模型的方法来减少训练的回合次数，因为模型无关的强化学习方法通常需要大量的训练才能找到合理的解。&lt;br&gt;&lt;br&gt;DDPG 实际上是 Actor-Critic 结构，融合了策略和值函数两者信息进行学习。对 Actor 和 Critic 均使用深度神经网络进行近似。&lt;br&gt;&lt;br&gt;使用一个权重为&lt;br&gt;&lt;img src="b2da1eaadff524fce5fbc67edfde28c6.png" data-rawwidth="31" data-rawheight="23"&gt;的深度神经网络&lt;br&gt;&lt;img src="ad8e27cbc2cca27baee0f6492975de2d.png" data-rawwidth="98" data-rawheight="34"&gt;来表示策略，定义目标函数为总折扣奖励&lt;br&gt;&lt;img src="1af6815c4846888677c200fc3e3e1507.png" data-rawwidth="253" data-rawheight="36"&gt;然后使用 SGD 来端对端优化目标函数，也即是说调整策略参数&lt;br&gt;&lt;img src="b2da1eaadff524fce5fbc67edfde28c6.png" data-rawwidth="31" data-rawheight="23"&gt;来达到更大的奖励&lt;br&gt;&lt;br&gt;确定型策略梯度是 David Silver 在 2014 年的工作，刚好为此铺垫，他们证明了确定型策略梯度算法给出的期望恰好就是策略梯度（这里可以参考 DPG 论文中的证明），策略的梯度由下式给出&lt;br&gt;&lt;br&gt;&lt;img src="e00a9e7ff6bbb12bef8a36ac4f3b14db.png" data-rawwidth="308" data-rawheight="42"&gt;策略梯度是最大化提升&lt;br&gt;&lt;img src="adf75ff57d7f6ca9407f3935c1607fcf.png" data-rawwidth="32" data-rawheight="29"&gt;的方向，确定型 Actor-Critic，使用两个网络，Actor 是参数为&lt;br&gt;&lt;img src="b2da1eaadff524fce5fbc67edfde28c6.png" data-rawwidth="31" data-rawheight="23"&gt;的策略&lt;br&gt;&lt;img src="2b3ca8b8f8d4f5f72b908dea064d3149.png" data-rawwidth="125" data-rawheight="66"&gt;Critic 是参数为&lt;img src="63feb317771556843b6f1fa6ac52529d.png" data-rawwidth="35" data-rawheight="23"&gt;的值函数&lt;br&gt;&lt;img src="ee4d48123970c8856d1ab93cfc7e5bb0.png" data-rawwidth="144" data-rawheight="65"&gt;&lt;br&gt;Critic 为 Actor 提供损失函数，&lt;br&gt;&lt;img src="2275b5c5497f9321b184d6ed63680769.png" data-rawwidth="229" data-rawheight="39"&gt;梯度从 Critic 到 Actor 反向传播，&lt;br&gt;&lt;img src="677bbf2e3f010dcc2ce8ba8d358a0d42.png" data-rawwidth="214" data-rawheight="38"&gt;Critic 通过 Q-学习估计当前策略的值&lt;br&gt;&lt;img src="3bc31d36066a5b8a0fad62c84aa149b1.png" data-rawwidth="395" data-rawheight="40"&gt;而 Actor 按照提升 Q 的方向更新策略&lt;br&gt;&lt;img src="7f7d7d75f271a41653a66bbdec8705ab.png" data-rawwidth="215" data-rawheight="40"&gt;确定型深度策略梯度（DDPG）由于基本的 actor-critic 使用神经网络会振荡或者发散，DDPG 给出了稳定解，采取了 DQN 中的技巧对 actor 和 critic 均使用经验回放并冻结目标网络来避免振荡&lt;br&gt;&lt;img src="553daa797f0e0e180eeb4230622c24e4.png" data-rawwidth="498" data-rawheight="40"&gt;&lt;b&gt;八.基于记忆的 DRL 架构&lt;/b&gt;&lt;br&gt;&lt;br&gt;http://arxiv.org/abs/1605.09128&lt;br&gt;&lt;br&gt;近期 Michigan 大学的研究组一篇论文提出了一种基于记忆的深度强化学习架构，专门设计了可控制的机制来处理第一人称视角的场景、延迟奖励及高维视觉信息，并引入主动感知能力，从而能够较好地完成既定任务。上面提到的问题或者要求同时具备是现有的深度强化学习架构并不能完全应付。这个新框架在实验中相比其他的深度强化学习模型表现出了较好的泛化能力。&lt;br&gt;&lt;img src="11410dd6eae9901499a1d2e41dd67f1a.png" data-rawwidth="509" data-rawheight="445"&gt;其结构示例如图：&lt;br&gt;&lt;img src="9ba9340e44c21035f2e8b9196e549657.png" data-rawwidth="562" data-rawheight="550"&gt;这两幅图展示了记忆操作的过程和不同的网络整体结构.&lt;br&gt;&lt;br&gt;MQN 仅仅依赖当前观察，除了当前输入用来做强化学习问题中的时态上下文的内存检索类似于 MemNN，是一个单纯的前驱网络结构构造了上下文环境；RMQN 则是循环结构使用 LSTM 从观察的历史信息中刻画了空间和时间信息，保证能够从 LSTM 和外部记忆中获得时态信息；FRMQN 则包含了一个从检索得到的记忆中反馈到上下文向量的链接。如图&lt;br&gt;&lt;img src="1eea73db58bfdcd1e8e6275b0c55d485.png" data-rawwidth="363" data-rawheight="287"&gt;最终使用的 FRMQN 网络架构包含了用来抽取图像特征的卷积网络、获取历史观察的记忆单元和一个上下文向量用于记忆查询和行动值的估计。其中提及的 FRQMN 对于未曾见过的环境在学习值函数的时候能够表现出更好的泛化能力.&lt;br&gt;&lt;br&gt;在https://sites.google.com/a/umich.edu/junhyuk-oh/icml2016-minecraft可以看到在实际的 Minecraft 中的 agent 行为的效果视频.&lt;br&gt;&lt;br&gt;&lt;b&gt;九.大规模离散行动空间上的深度强化学习&lt;/b&gt;&lt;br&gt;&lt;br&gt;https://arxiv.org/pdf/1512.07679.pdf&lt;br&gt;&lt;br&gt;这项工作建立在 DeepMind 之前的 DDPG 等工作之上，杂糅了若干模型，并使用嵌入的方式来大幅度降低行动空间的维数，其主要过程在下图中给出：&lt;br&gt;&lt;img src="29c72d9908c76213b6ebfae2203ba3ec.png" data-rawwidth="416" data-rawheight="680"&gt;&lt;b&gt;十.博弈均衡求解的深度强化学习方法&lt;/b&gt;&lt;br&gt;&lt;br&gt;https://arxiv.org/pdf/1603.01121.pdf&lt;br&gt;&lt;br&gt;NFSP 就是引入神经网络近似函数的 FSP，是一种利用强化学习技术来从自我博弈中学习近似纳什均衡的方法，解决了三个问题：&lt;br&gt;&lt;br&gt;1. 无先验知识 NFSP agent 学习&lt;br&gt;&lt;br&gt;2. 运行时不依赖局部搜索&lt;br&gt;&lt;br&gt;3. 收敛到自我对局的近似纳什均衡&lt;p&gt;这是一般的不完美信息二人零和博弈。虚拟对弈同样也会收敛到合作、势力场博弈的纳什均衡。所以 NFSP 也能够成功应用在这些博弈上。另外，近期的研究关于连续空间行动的强化学习（Lillicrap et al. 2015）也能够应用在连续行动博弈中，目前的博弈论方法并不能直接处理这样的情形。所以说，这系列工作是具有重要的意义的，揭示了可以完成部分真实场景博弈的均衡求解。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;用于对话生成的深度强化学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;http://arxiv.org/pdf/1606.01541.pdf&lt;br&gt;&lt;/p&gt;&lt;p&gt;循环神经网络在对话生成上的应用确实有所进展，可以为对话机器人生成回应的语句，但是这些反应相当地短视，常常就忽略了对未来产生的后果。为对话的未来方向进行建模是产生连贯有趣的对话的关键，这也是传统 NLP 对话模型要采用强化学习的缘故。这个工作，将这些目标进行整合，应用深度强化学习来建模机器人对话的未来奖励。这个对话模型模拟了两个虚拟 agent 之间的对话，使策略梯度方法在包含三个有用的对话属性（信息量、连贯性和易答性）的奖励序列上。实验在 diversity、长度和人类评判上进行，结果表明算法产生了更具交互性的答复并刺激出更加持久的对话模拟。这也是基于对话长期成功的学习神经网络对话模型的第一次尝试。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;未来发展&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;现在的深度强化学习中很多的模型是，强化学习中部分研究成果深度学习化的结果。但最令人兴奋的是，一些新的想法，例如强化变分推断，在Theophane Weber 等人的论文（http://arkitus.com/files/nips-15-weber-reinforced-inference.pdf）中，就将 VI 和 RL 进行了联系。参见下图的对比：&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="6e8ac6647a5eb587d8a15d089513a311.png" data-rawwidth="714" data-rawheight="135"&gt;他们给出了一种将推断看作是强化学习的视角，这样其实可以让变分推断的研究者们受强化学习技术启发创造出新的推断技术。基线和值函数的方式来进行解释。很多强化学习中其他的概念可用在变分推断中，如时间差分方法或者探索方法，未来这两者间的关系应该能够挖掘到更深的层次，这也使得我们能够找到更多的微分模型和关联技术.&lt;br&gt;&lt;/p&gt;&lt;p&gt;而这篇文章中作者之一 John Schulman 和他 Berkeley 的合作者也有一个进行从计算方法的角度统一化工作，Gradient Estimation Using Stochastic Computation Graphs，将监督学习、非监督学习和强化学习中出现的共同问题进行提炼——损失函数由一个随机变量集上的期望定义，这些随机变量可能是概率模型的变量或者是外部环境的变量。那么使用样本来估计损失函数的梯度就是基于梯度学习的算法的核心。该文给出了随机计算图的形式化定义——包含确定型函数和条件概率分布的有向无环图，并解释如何自动推导出损失函数梯度的无偏估计。得到的算法是对标准反向传播算法的微小改进。该框架可以帮助研究者们开发复杂微妙的模型，方便地加入随机和确定型的操作，如注意力、记忆和行动控制等。&lt;br&gt;&lt;/p&gt;&lt;p&gt;另外深度强化学习在博弈均衡求解中的应用也是令人兴奋的方向之一，随着这些技术的细化和深入，我们将理论计算机和更为实用的机器学习等等技术之间的鸿沟进一步缩小。&lt;br&gt;&lt;/p&gt;&lt;p&gt;未来深度强化学习的发展必定是理论探索和应用实践的深入，这一方面取决于我们深度学习的认识，另一方面则倚重不断地实践。&lt;br&gt;&lt;/p&gt;&lt;p&gt;最后，我想推荐一下 OpenAI 的 gym，这是一个强化学习算法测试的环境，可以在上面去尝试自己解决一些问题，同时也可以比对自己方法的优劣。现在也是相当活跃的一个项目，OpenAI 的成员正在不断扩展这个环境，使之满足现在强化学习需要的环境，另外也在征求大家的意见列出最关键的一些相关问题。深度学习有很多的标准的任务可以供大家测试算法，强化学习领域实际上在前几年并不是非常方便进行测试，现在的 Gym 可以算作深度强化学习算法的试金石了。&lt;br&gt;&lt;/p&gt;&lt;p&gt;OpenAI 处于快速发展阶段，其中涉及的 POMDP 环境不断增加：&lt;br&gt;&lt;/p&gt;&lt;p&gt;1. 经典控制和玩具文本：强化学习文献中的小规模的任务&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. 算法：执行诸如多位数字相加，序列逆变等等计算。 大多数这样的任务需要记忆，而难度可通过序列长度调整&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Atari 游戏：屏幕图像或者 RAM 作为输入，使用的是 Arcade Learning Environment 作为底层支撑棋盘游戏：当前包括围棋的 9X9 和 19X19 棋盘，Pachi 作为对手&lt;/p&gt;&lt;p&gt;4. 2D 和 3D 机器人：在模拟环境中控制机器人，这些任务使用了 MuJoCo 物理引擎，还有部分来自&lt;b&gt;RLLAB&lt;/b&gt;（http://rllab.readthedocs.io/en/latest/）&lt;br&gt;&lt;/p&gt;&lt;p&gt;根据 OpenAI 发布的信息，他们也在扩展 Gym 中其他的环境，如：&lt;br&gt;&lt;/p&gt;&lt;p&gt;1. 多 agent 场景，这些场景中的 agent 之间可以合作或者竞争&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Curriculum 学习和迁移学习。当前这些任务还只是初期，后面会形成任务的序列，这样算法可以一个接一个任务地进行训练。这里的设计师创建不断提升难度的任务序列，来适应所需的场景。&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. 真实世界操作：最终目标是将 Gym API 和机器人硬件进行结合，在真实世界中检验强化学习算法&lt;br&gt;&lt;/p&gt;&lt;p&gt;所以从 OpenAI Gym 开始你可以逐步走近到走进这个有意思的领域了，通过实现那些 tricky 的算法来掌握它，对于很多人来说，实现了可以运作的算法代码才是真的懂了（我觉得可能还不够，仍旧有很多的指引需要我们去探索，也许数学证明才是真的理解象征……）&lt;br&gt;&lt;/p&gt;&lt;p&gt;很开心能够有这样的一群人去实践人工智能技术的开放化，对此，我非常的钦佩，也希望能够借助自己的力量来帮助这个项目的成长。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="c5c903b16a6f8d6aa26c5b052ef86e9d.png" data-rawwidth="679" data-rawheight="291"&gt;在 Gym 变得更加稳定后， OpenAI 近期向大家征求未来的研究项目，这里可以看到相应的项目和评分。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="e5fd0917b63c71333fd7a4369f5c6a48.png" data-rawwidth="735" data-rawheight="327"&gt;&lt;b&gt;学习建议&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;现在网络上其实遍布了可以学习深度强化学习的资源，建议大家可以选择下面的课程：&lt;br&gt;&lt;/p&gt;&lt;p&gt;1. Neural Networks for Machine Learning — Geoff Hinton (Coursera)&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Neural Nets — Andrej Karpathy’s CS231N (Stanford)&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Reinforcement Learning — David Silver UCL&lt;br&gt;&lt;/p&gt;4. Advanced Robotics (the MDP / optimal control lectures) — Pieter Abbeel’s CS287 (Berkeley) Deep RL — John Schulman’s CS294-112 (Berkeley)&lt;br&gt;&lt;br&gt;&lt;br&gt;5. Deep RL — David Silver RLDM&lt;br&gt;&lt;br&gt;6. Deep RL — John Schulman MLSS&lt;br&gt;&lt;br&gt;除了课程外，也有一些书籍，比如 Richard Sutton 等人的《Reinforcement Learning: An Introduction》，还有 Pieter 推荐了 Cover 和 Thomas 的《信息论》和 Nocedal 和 Wright 写的 nonlinear optimization 书，David Barber 的 Bayesian Reasoning and Machine Learning 等等&lt;br&gt;如果你爱编程实现，那么 Ilya Sutskever 建议你从实现简单的 MNIST 分类器开始，卷积网络，重新实现 char-rnn，然后玩玩大的卷积网络。同样还可以选择一些竞赛，比如说 Kaggle 的 Knowledge 系列。不断地去寻找训练和实现的手感。在 OpenAI 中也有很多的资源帮助你快速找到感觉。&lt;br&gt;&lt;br&gt;但实际上，深度学习和强化学习及深度强化学习需要有理论和实践的结合，所以理论课程和实践经历都是非常重要的。通过读书听课可以找到自己的位置，还有知识本身所处的位置以及你在整个知识地图的位置，从而不至于迷失在日新月异的环境中，慢慢地你会发现自己感兴趣的那些目的地，然后逐步地探索这些有趣的目的地，最终能够取得令自己满意的成就。而实现模型的重要性在于你可以将那些抽象的概念和理论转化为实实在在可以触及的经验，你飞动的指尖会慢慢告诉你什么是正确的节奏，哪些是让你兴奋的点。通常理论和实践之间的差异之大会超乎想象，所以只能够通过实际应用去填补这些罅隙，实现完模型后往往需要成天成天的调试你才能达到满意的效果。这个过程是非常痛苦的，但这种痛苦也是短暂的：当你最终完成了一个真正可用的模型时，那种快感无与伦比.。我想这也是很多人坚持挑战自己的缘故吧。Ilya Suskever 说道：“But each time you suffer, know that you've built a little bit of skill that will be invaluable for the future.”&lt;br&gt;&lt;br&gt;&lt;b&gt;后记&lt;/b&gt;&lt;br&gt;&lt;br&gt;两周前答应刘昕博士，匆匆写完这篇，越到 deadline 越是能发现自己的漏洞，为了填补这些常常查资料到深夜，不过这个过程还是非常享受，乐在其中。从深度学习基本的 MLP、SGD 的理解和掌握，到 RNN LSTM 及 NTM 和 MM 等网络的理解，再到慢慢地发现生成式模型的魅力所在，随之与变分推断的结合，然后后来的强化学习重新进入我的视野，这个过程跌宕起伏，也让我对这个庞大领域的脉络逐渐清晰。可以说深度学习重新激活了我对机器学习的热情。而且随着理解的深入，你会发现深度强化学习会将各个领域有趣的问题放在同样的一个框架内进行思考和处理，这是以前只有博弈论和复杂网络能够带来给我的体验。相较于单纯掌握某个领域的知识和技术，我倾向于从更广的层面来理解它们，然后吸收进入自己的知识体系。&lt;br&gt;&lt;br&gt;这个过程中，我们需要的一方面是接受新的技术，同时也要辨别清楚那些重要的东西，那么最好的方式就是去研习它们，观察它们，使用它们，这样才有最真切的体会，也能够告诉自己需要的究竟是什么。&lt;br&gt;&lt;br&gt;我们给某个事物取了名字，一方面界定清楚了它，但另一方面也限制住了它。实际上，并不需要太多严苛地将注意力限制在一个有“名”的物上，而是应该去感知它所能够触及的领域和问题，通过用之来增加认知。对于技术或者理论均是如此，敢于突破前人列下的规则，才是我们创新的动力之源.&lt;br&gt;&lt;br&gt;现在这个开放的时代，让技术的进步可以方便地获取，只要你有热情和兴趣，就能够找到释放的地方。尽管有着诸多鸿沟拦在人类的面前，但是勇敢者必定能够迈出坚定的步伐去探知未来的奥秘！既然如此，大家尽情发挥吧~&lt;br&gt;&lt;br&gt;记住，跟随你的好奇心，它会指引你找到属于自己的路！&lt;br&gt;&lt;br&gt;“I am a pessimist because of intelligence, but an optimist because of will.”&lt;br&gt;&lt;br&gt;&lt;b&gt;相关资料&lt;/b&gt;&lt;br&gt;Parallel and Distributed Computation:Numerical Methods:https://dspace.mit.edu/handle/1721.1/3719#files-areaOpenAI Gym: http://arxiv.org/pdf/1606.01540.pdfDQN https://www.cs.toronto.edu/~vmnih/docs/dqn.pdfDouble DQN http://arxiv.org/pdf/1509.06461v3.pdfDPG http://jmlr.org/proceedings/papers/v32/silver14.pdfDDPG http://arxiv.org/abs/1509.02971GAE http://arxiv.org/pdf/1506.02438v4Nervana: http://www.nervanasys.com/demystifying-deep-reinforcement-learning/David Silver drl:  http://videolectures.net/site/normal_dl/tag=968058/rldm2015_silver_reinforcement_learning.pdfJohn Schulman drl: http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/2016-MLSS-RL.pdfReinforced Variational Inference: http://arkitus.com/files/nips-15-weber-reinforced-inference.pdfGradient Estimation Using Stochastic Computation Graphs:https://arxiv.org/pdf/1506.05254.pdfControl of Memory, Active Perception, and Action in Minecraft:  http://arxiv.org/abs/1605.09128Deep Reinforcement Learning in Large Discrete Action Spaces:https://arxiv.org/pdf/1512.07679.pdfDeep Reinforcement Learning from Self-Play in Imperfect-Information Games:https://arxiv.org/pdf/1603.01121.pdfContinuous control with deep reinforcement learning:http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/ddpg.pdf&lt;br&gt;&lt;br&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果" class="" data-editable="true"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;b&gt;&lt;img src="27a17f73e18709f79257c34e1708a380.png" data-rawwidth="97" data-rawheight="95"&gt;朱小虎&lt;/b&gt;，简书数据科学家，负责推荐系统业务。目前专注于深度学习在自然语言理解及社会网络分析中的应用，并与传统机器学习算法结合来改善业务流程和提升用户体验。上海深度学习线下群发起人，深度学习国际群联合发起人。致力于推广深度学习和机器学习技术，共同完成了该领域一本入门教程《神经网络与深度学习》的翻译工作。&lt;p&gt;&lt;br&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324914&amp;amp;idx=1&amp;amp;sn=0baaf404b3d8132243d08b55310de210&amp;amp;scene=0#wechat_redirect" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650324914&amp;amp;idx=1&amp;amp;sn=0baaf404b3d8132243d08b55310de210&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;br&gt;&lt;img src="499b12b5d7aa6804560ef81a692b6637.jpg" data-rawwidth="327" data-rawheight="55"&gt;&lt;/p&gt;</description><author>程程</author><pubDate>Wed, 06 Jul 2016 16:01:52 GMT</pubDate></item><item><title>面部特征点定位概述及最近研究进展</title><link>https://zhuanlan.zhihu.com/p/21456877</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;面部特征点定位任务即根据输入的人脸图像，自动定位出面部关键特征点，如眼睛、鼻尖、嘴角点、眉毛以及人脸各部件轮廓点等，如下图所示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a2ae516b4db1afcdf028cda341976234.jpg" data-rawwidth="622" data-rawheight="359"&gt;这项技术的应用很广泛，比如自动人脸识别，表情识别以及人脸动画自动合成等。由于不同的姿态、表情、光照以及遮挡等因素的影响，准确地定位出各个关键特征点看似很困难。我们简单地分析一下这个问题，不难发现这个任务其实可以拆分出三个子问题：&lt;/p&gt;&lt;p&gt; 1. 如何对人脸表观图像（输入）建模&lt;/p&gt;&lt;p&gt; 2. 如何对人脸形状（输出）建模&lt;/p&gt;&lt;p&gt; 3.如何建立人脸表观图像（模型）与人脸形状（模型）的关联&lt;/p&gt;&lt;p&gt;以往的研究工作也离不开这三个方面。人脸形状建模典型的方法有可变形模板（Deformable Template）、点分布模型（主动形状模型Active Shape Model）、图模型等。&lt;/p&gt;&lt;p&gt;人脸表观建模又可分为全局表观建模和局部表观建模。全局表观建模简单的说就是考虑如何建模整张人脸的表观信息，典型的方法有主动表观模型Active Appearance Model（产生式模型）和Boosted Appearance Model（判别式模型）。对应的局部表观建模则是对局部区域的表观信息建模，包括颜色模型、投影模型、侧剖线模型等。&lt;/p&gt;&lt;p&gt;近来，级联形状回归模型在特征点定位任务上取得了重大突破，该方法使用回归模型，直接学习从人脸表观到人脸形状（或者人脸形状模型的参数）的映射函数，进而建立从表观到形状的对应关系。此类方法不需要复杂的人脸形状和表观建模，简单高效，在可控场景（实验室条件下采集的人脸）和非可控场景（网络人脸图像等）均取得不错的定位效果。此外，基于深度学习的面部特征点定位方法也取得令人瞩目的结果。深度学习结合形状回归框架可以进一步提升定位模型的精度，成为当前特征定位的主流方法之一。下面我将具体介绍级联形状回归和深度学习这两大类方法的研究进展。&lt;/p&gt;&lt;p&gt;&lt;b&gt;级联线性回归模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;面部特征点定位问题可以看作是学习一个回归函数&lt;em&gt;F&lt;/em&gt;，以图象&lt;em&gt;I&lt;/em&gt;作为输入，输出&lt;em&gt;θ&lt;/em&gt;为特征点的位置（人脸形状）：&lt;em&gt;θ = F（I）&lt;/em&gt;。&lt;/p&gt;&lt;p&gt;简单的说，级联回归模型可以统一为以下框架：学习多个回归函数{&lt;em&gt;f1&lt;/em&gt; ,…, &lt;em&gt;fn-1&lt;/em&gt;,&lt;em&gt; fn&lt;/em&gt;}来逼近函数&lt;em&gt;F&lt;/em&gt;：&lt;/p&gt;&lt;p&gt;&lt;em&gt;θ &lt;/em&gt;=&lt;em&gt; F&lt;/em&gt;（&lt;em&gt;I&lt;/em&gt;）=&lt;em&gt;  fn &lt;/em&gt;(&lt;em&gt;fn-1 &lt;/em&gt;(…&lt;em&gt;f1&lt;/em&gt;(&lt;em&gt;θ&lt;/em&gt;0, &lt;em&gt;I&lt;/em&gt;) ,&lt;em&gt;I&lt;/em&gt;) ,&lt;em&gt; I&lt;/em&gt;)&lt;/p&gt;&lt;p&gt;&lt;em&gt;θi= fi &lt;/em&gt;(&lt;em&gt;θi-1, I&lt;/em&gt;),   &lt;em&gt; i=1,…,n&lt;/em&gt;&lt;/p&gt;&lt;p&gt;所谓的级联，即当前函数fi的输入依赖于上一级函数&lt;em&gt;fi-1&lt;/em&gt;的输出&lt;em&gt;θi-1&lt;/em&gt;，而每一个&lt;em&gt;fi&lt;/em&gt;的学习目标都是逼近特征点的真实位置&lt;em&gt;θ&lt;/em&gt;，&lt;em&gt;θ&lt;/em&gt;0为初始形状。通常情况，&lt;em&gt;fi&lt;/em&gt;不是直接回归真实位置&lt;em&gt;θ&lt;/em&gt;，而回归当前形状&lt;em&gt;θi-1&lt;/em&gt;与真实位置&lt;em&gt;θ&lt;/em&gt;之间的差：Δ&lt;em&gt;θi &lt;/em&gt;= &lt;em&gt;θ &lt;/em&gt;- &lt;em&gt;θi-1&lt;/em&gt;。&lt;/p&gt;&lt;p&gt;接下来我将详细介绍几个典型的形状回归方法，他们根本的不同点在于函数fi的设计不同以及输入特征不同。&lt;/p&gt;&lt;p&gt;在加州理工学院从事博士后研究的Piotr Dollár于2010年首次提出级联形状回归模型CascadedPose Regression（CPR），来预测物体的形状，该工作发表在国际计算机视觉与模式识别会议CVPR上。如下图所示，如下图所示，给定初始形状&lt;em&gt;θ&lt;/em&gt;0，通常为平均形状，根据初始形状&lt;em&gt;θ&lt;/em&gt;0提取特征（两个像素点的差值）作为函数&lt;em&gt;f1&lt;/em&gt;的输入。每个函数fi建模成Random Fern回归器，来预测当前形状&lt;em&gt;θi-1&lt;/em&gt;与目标形状&lt;em&gt;θ&lt;/em&gt;的差Δ&lt;em&gt;θi&lt;/em&gt;，并根据Δ&lt;em&gt;Ӫi&lt;/em&gt;预测结果更新当前形状得&lt;em&gt;θ i = θi-1+&lt;/em&gt;Δ&lt;em&gt;Ӫ&lt;/em&gt;&lt;em&gt;i&lt;/em&gt;，作为下一级函数&lt;em&gt;fi+1&lt;/em&gt;的输入。该方法在人脸、老鼠和鱼三个数据集上取得不错的实验结果，通用的算法框架亦可用于其他形状估计任务，比如人体姿态估计等。该方法的不足之处在于对初始化形状&lt;em&gt;θ&lt;/em&gt;0比较敏感，使用不同的初始化做多次测试并融合多次预测结果可以一定程度上缓解初始化对于算法的影响，但并不能完全解决该问题，且多次测试会带来额外的运算开销。当目标物体被遮挡时，性能也会变差。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e8b6928be7a686ae7f9e3a78d9b70775.jpg" data-rawwidth="648" data-rawheight="335"&gt;与上一个工作来自同一课题组的Xavier P. Burgos-Artizzu，针对CPR方法的不足，进一步提出Robust Cascaded Pose Regression（RCPR）方法，并发表在2013年国际计算视觉会议ICCV上。为了解决遮挡问题，Piotr Dollár提出同时预测人脸形状和特征点是否被遮挡的状态，即&lt;em&gt;fi&lt;/em&gt;的输出包含Δθi和每个特征点是否被遮挡的状态&lt;em&gt;pi&lt;/em&gt;：&lt;/p&gt;&lt;p&gt; {Δ&lt;em&gt;θi&lt;/em&gt; ,&lt;em&gt; pi &lt;/em&gt;}&lt;em&gt;= fi&lt;/em&gt;(&lt;em&gt;θi-1, I&lt;/em&gt;),   &lt;em&gt; i=1,…,n&lt;/em&gt;&lt;/p&gt;&lt;p&gt;当某些特征点被遮挡时，则不选取该特征点所在区域的特征作为输入，从而避免遮挡对定位的干扰。此外，作者提出智能重启技术来解决形状初始化敏感的问题：随机初始化一组形状，运行{&lt;em&gt;f1&lt;/em&gt; ,…,&lt;em&gt;fn-1&lt;/em&gt;,&lt;em&gt; fn&lt;/em&gt;}的前10%的函数，统计形状预测的方差，如果方差小于一定阈值，说明这组初始化不错，则跑完剩下的90%的级联函数，得到最终的预测结果；如果方差大于一定阈值，则说明初始化不理想，选择重新初始化一组形状。该策略想法直接，但效果很不错。&lt;/p&gt;&lt;p&gt;另外一个很有趣的工作Supervised Descent Method（SDM），从另一个角度思考问题，即考虑如何使用监督梯度下降的方法来求解非线性最小二乘问题，并成功地应用在面部特征点定位任务上。不难发现，该方法最终的算法框架也是一个级联回归模型。与CPR和RCPR不同的地方在于：&lt;em&gt;fi&lt;/em&gt;建模成了线性回归模型；&lt;em&gt;fi&lt;/em&gt;的输入为与人脸形状相关的SIFT特征。该特征的提取也很简单，即在当前人脸形状&lt;em&gt;θi-1&lt;/em&gt;的每个特征点上提取一个128维的SIFT特征，并将所有SIFT特征串联到一起作为fi的输入。该方法在&lt;strong&gt;LFPW&lt;/strong&gt;和&lt;strong&gt;LFW-A&amp;amp;C&lt;/strong&gt;数据集上取得不错的定位结果。同时期的另一个工作DRMF则是使用支持向量回归SVR来建模回归函数fi，并使用形状相关的HOG特征（提取方式与形状相关的SIFT类似）作为&lt;em&gt;fi&lt;/em&gt;输入，来级联预测人脸形状。与SDM最大的不同在于，DRMF对于人脸形状做了参数化的建模。&lt;em&gt;fi&lt;/em&gt;的目标变为预测这些形状参数而不再是直接的人脸形状。这两个工作同时发表在CVPR 2013上。由于人脸形状参数化模型很难完美地刻画所有形状变化，SDM的实测效果要优于DRMF。&lt;/p&gt;&lt;p&gt;微软亚洲研究院孙剑研究员的团队在CVPR 2014上提出更加高效的级联形状回归方法Regressing LocalBinary Features（LBF）。和SDM类似，&lt;em&gt;fi&lt;/em&gt;也是建模成线性回归模型；不同的地方在于，SDM直接使用SIFT特征，LBF则基于随机森林回归模型在局部区域学习稀疏二值化特征。通过学习稀疏二值化特征，大大减少了运算开销，比CRP、RCPR、SDM、DRMF等方法具有更高的运行效率（LBF可以在手机上跑到300FPS），并且在&lt;strong&gt;IBUG&lt;/strong&gt;公开评测集上取得优于SDM、RCPR的性能。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/40a9bd07a25a2ed924bc302e1bd0fe27.jpg" data-rawwidth="641" data-rawheight="171"&gt;级联形状回归模型成功的关键在于：&lt;/p&gt;&lt;p&gt;1. 使用了形状相关特征，即函数&lt;em&gt;fi&lt;/em&gt;的输入和当前的人脸形状&lt;em&gt;θi-1&lt;/em&gt;紧密相关；&lt;/p&gt;&lt;p&gt;2. 函数&lt;em&gt;fi&lt;/em&gt;的目标也与当前的人脸形状&lt;em&gt;θi-1&lt;/em&gt;相关，即fi的优化目标为当前形状&lt;em&gt;θi-1&lt;/em&gt;与真实位置&lt;em&gt;θ&lt;/em&gt;之间的差Δ&lt;em&gt;θi。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;此类方法在可控和非可控的场景下均取得良好的定位效果，且具有很好的实时性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;深度模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以上介绍的级联形状回归方法每一个回归函数fi都是浅层模型（线性回归模型、Random Fern等）。深度网络模型，比如卷积神经网络（CNN）、深度自编码器（DAE）和受限玻尔兹曼机（RBM）在计算机视觉的诸多问题，如场景分类，目标跟踪，图像分割等任务中有着广泛的应用，当然也包括特征定位问题。具体的方法可以分为两大类：使用深度模型建模人脸形状和表观的变化和基于深度网络学习从人脸表观到形状的非线性映射函数。&lt;/p&gt;&lt;p&gt;主动形状模型ASM和主动表观模型AAM使用主成分分析（PCA）来建模人脸形状的变化。由于姿态表情等因素的影响，线性PCA模型很难完美地刻画不同表情和姿态下的人脸形状变化。来自伦斯勒理工学院JiQiang教授的课题组在CVPR2013提出使用深度置信网络（DBN）来刻画不同表情下人脸形状的复杂非线性变化。此外，为了处理不同姿态的特征点定位问题，进一步使用3向RBM网络建模从正面到非正面的人脸形状变化。最终该方法在表情数据库&lt;strong&gt;CK+&lt;/strong&gt;上取得比线性模型AAM更好的定位结果。该方法在同时具备多姿态多表情的数据库&lt;strong&gt;ISL&lt;/strong&gt;上也取得较好的定位效果，但对同时出现极端姿态和夸张表情变化的情况还不够理想。&lt;/p&gt;&lt;p&gt;下图是深度置信网络（DBN）：建模不同表情下的人脸形状变化的示意图。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1d97c3fee165f9110ae2c6258fb6e3c4.jpg" data-rawwidth="607" data-rawheight="368"&gt;香港中文大学汤晓鸥教授的课题组在CVPR 2013上提出3级卷积神经网络DCNN来实现面部特征点定位的方法。该方法也可以统一在级联形状回归模型的大框架下，和CPR、RCPR、SDM、LBF等方法不一样的是，DCNN使用深度模型-卷积神经网络，来实现&lt;em&gt;fi&lt;/em&gt;。第一级&lt;em&gt;f1&lt;/em&gt;使用人脸图像的三块不同区域（整张人脸，眼睛和鼻子区域，鼻子和嘴唇区域）作为输入，分别训练3个卷积神经网络来预测特征点的位置，网络结构包含4个卷积层，3个Pooling层和2个全连接层，并融合三个网络的预测来得到更加稳定的定位结果。后面两级&lt;em&gt;f2&lt;/em&gt;,&lt;em&gt; f3&lt;/em&gt;在每个特征点附近抽取特征，针对每个特征点单独训练一个卷积神经网络（2个卷积层，2个Pooling层和1个全连接层）来修正定位的结果。该方法在&lt;strong&gt;LFPW&lt;/strong&gt;数据集上取得当时最好的定位结果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/40dd129eac25691ae9dbc86cf9b622c5.jpg" data-rawwidth="653" data-rawheight="196"&gt;借此机会也介绍本人发表在欧洲视觉会议ECCV2014的一个工作：即提出一种由粗到精的自编码器网络（CFAN）来描述从人脸表观到人脸形状的复杂非线性映射过程。该方法级联了多个栈式自编码器网络&lt;em&gt;fi&lt;/em&gt;，每一个fi刻画从人脸表观到人脸形状的部分非线性映射。具体来说，输入一个低分辨率的人脸图像&lt;em&gt;I&lt;/em&gt;，第一层自编码器网络&lt;em&gt;f1&lt;/em&gt;可以快速地估计大致的人脸形状，记作基于全局特征的栈式自编码网络。网络&lt;em&gt;f1&lt;/em&gt;包含三个隐层，隐层节点数分别为1600,900,400。然后提高人脸图像的分辨率，并根据&lt;em&gt;f1&lt;/em&gt;得到的初始人脸形状&lt;em&gt;θ1&lt;/em&gt;，抽取联合局部特征，输入到下一层自编码器网络&lt;em&gt;f2&lt;/em&gt;来同时优化、调整所有特征点的位置，记作基于局部特征的栈式自编码网络。该方法级联了3个局部栈式自编码网络{&lt;em&gt;f2&lt;/em&gt; , &lt;em&gt;f3&lt;/em&gt;,&lt;em&gt; f4&lt;/em&gt;}直到在训练集上收敛。每一个局部栈式自编码网络包含三个隐层，隐层节点数分别为1296,784,400。得益于深度模型强大的非线性刻画能力，该方法在&lt;strong&gt;XM2VTS，LFPW，HELEN&lt;/strong&gt;数据集上取得比DRMF、SDM更好的结果。此外，CFAN可以实时地完成人脸面部特征点定位（在I7的台式机上达到23毫秒/张），比DCNN（120毫秒/张）具有更快的处理速度。&lt;/p&gt;&lt;p&gt;下图是CFAN：基于由粗到精自编码器网络的实时面部特征点定位方法的示意图。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ce5df3f2cef3ae3e8b6becad34e76bfe.jpg" data-rawwidth="659" data-rawheight="435"&gt;&lt;p&gt;以上基于级联形状回归和深度学习的方法对于大姿态（左右旋转-60°~+60°）、各种表情变化都能得到较好的定位结果，处理速度快，具备很好的产品应用前景。针对纯侧面（±90°）、部分遮挡以及人脸检测与特征定位联合估计等问题的解决仍是目前的研究热点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/3df99ac3d9feb7d52710296d94702bf8.jpg" data-rawwidth="641" data-rawheight="430"&gt;&lt;p&gt;&lt;b&gt;张杰&lt;/b&gt;，中科院计算技术研究所VIPL课题组博士生，专注于深度学习技术及其在人脸识别领域的应用。相关研究成果发表在计算机视觉国际顶级学术会议ICCV, CVPR和ECCV，并担任国际顶级期刊TIP和TNNLS审稿人。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;tempkey=6BuGzbZobsctK9UqgdPOFXICSWv%2BpjzsPT8tOZX4%2BilWKGUZIBEvUwLVXN6sRKP%2FfhbvlxP%2F6UPAFblkLy9vp4QWAL0uZPyL5TtVOiuEQhW1tCk6OgS6cUKBUStIpDE%2Fbx9nu1NtKIebww88xpUCqA%3D%3D&amp;amp;#rd" class=""&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;tempkey=6BuGzbZobsctK9UqgdPOFXICSWv%2BpjzsPT8tOZX4%2BilWKGUZIBEvUwLVXN6sRKP%2FfhbvlxP%2F6UPAFblkLy9vp4QWAL0uZPyL5TtVOiuEQhW1tCk6OgS6cUKBUStIpDE%2Fbx9nu1NtKIebww88xpUCqA%3D%3D&amp;amp;#rd&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/499b12b5d7aa6804560ef81a692b6637.jpg" data-rawwidth="327" data-rawheight="55"&gt;</description><author>程程</author><pubDate>Thu, 30 Jun 2016 11:07:12 GMT</pubDate></item><item><title>人脸识别简史与近期进展</title><link>https://zhuanlan.zhihu.com/p/21465605</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7777c979e5fc4bd05b25d276fe4cad50.jpg" data-rawwidth="700" data-rawheight="516"&gt;自动人脸识别的经典流程分为三个步骤：人脸检测、面部特征点定位（又称Face Alignment人脸对齐）、特征提取与分类器设计。一般而言，狭义的人脸识别指的是"特征提取+分类器"两部分的算法研究。&lt;p&gt;在深度学习出现以前，人脸识别方法一般分为高维人工特征提取（例如：LBP, Gabor等）和降维两个步骤，代表性的降维方法有PCA, LDA等子空间学习方法和LPP等流行学习方法。在深度学习方法流行之后，代表性方法为从原始的图像空间直接学习判别性的人脸表示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/63829295ca4fb9e9e434c9f13eb29c89.jpg" data-rawwidth="727" data-rawheight="526"&gt;一般而言，人脸识别的研究历史可以分为三个阶段。在第一阶段（1950s-1980s），人脸识别被当作一个一般性的模式识别问题，主流技术基于人脸的几何结构特征。在第二阶段（1990s）人脸识别迅速发展，出现了很多经典的方法，例如Eigen Face, Fisher Face和弹性图匹配，此时主流的技术路线为人脸表观建模。在第三阶段（1990s末期到现在），人脸识别的研究不断深入，研究者开始关注面向真实条件的人脸识别问题，主要包括以下四个方面的研究：1）提出不同的人脸空间模型，包括以线性判别分析为代表的线性建模方法，以Kernel方法为代表的非线性建模方法和基于3D信息的3D人脸识别方法。2）深入分析和研究影响人脸识别的因素，包括光照不变人脸识别、姿态不变人脸识别和表情不变人脸识别等。3）利用新的特征表示，包括局部描述子（Gabor Face, LBP Face等）和深度学习方法。4）利用新的数据源，例如基于视频的人脸识别和基于素描、近红外图像的人脸识别。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8d23bddd3e5194cb6c78ad704fc35c29.jpg" data-rawwidth="707" data-rawheight="522"&gt;2007年以来，LFW数据库成为事实上的真实条件下的人脸识别问题的测试基准。LFW数据集包括来源于因特网的5,749人的13,233张人脸图像，其中有1680人有两张或以上的图像。LFW的标准测试协议包括6000对人脸的十折确认任务，每折包括300对正例和300对反例，采用十折平均精度作为性能评价指标。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2c19c13ef54bc95e2b4bd5a926354246.jpg" data-rawwidth="702" data-rawheight="516"&gt;自从LFW发布以来，性能被不断刷新。2013年之前，主要技术路线为&lt;b&gt;人造或基于学习的局部描述子+测度学习&lt;/b&gt;。2014年之后，主要技术路线为深度学习。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4bd9e2ea1cda7a69490c2947714eb793.jpg" data-rawwidth="704" data-rawheight="521"&gt;&lt;p&gt;2014年以来，深度学习+大数据（海量的有标注人脸数据）成为人脸识别领域的主流技术路线，其中两个重要的趋势为：1）网络变大变深（VGGFace16层，FaceNet22层）。2）数据量不断增大（DeepFace 400万，FaceNet2亿），大数据成为提升人脸识别性能的关键。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/464af944f98c87ff36a26f4ccb59aed8.jpg" data-rawwidth="704" data-rawheight="523"&gt;在前DL时代，以VIPL实验室三代半SDK为例，关键技术点包括1）分块人脸特征融合：Gabor特征+LPQ特征。 2）子空间学习进行特征降（PCA+LDA）。3）融合多尺度的人脸归一化模板。SDK3.5的相关技术在FRGC实验4上取得了0.1%错误接受率条件下96%的确认率，至今依然是FRGC数据集上最好结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/cd37a1ab79e97ffefbfec4801822d59a.jpg" data-rawwidth="695" data-rawheight="521"&gt;&lt;p&gt;需要指出的是，虽然深度学习强调特征学习，但学习特征并不是DL的专利。在前DL时代，利用浅层模型从图像中直接学习表示和基于人造描述子学习语义表示（例如学习中层属性表示的Attributes and Simile Classifier和学习高层语义表示的Tom-vs-Pete）的工作都见于相关文献。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5e72eecfcd2935b6bd72fc98fb0fde0d.jpg" data-rawwidth="711" data-rawheight="520"&gt;&lt;p&gt;2014年，Facebook发表于CVPR14的工作DeepFace将大数据（400万人脸数据）与深度卷积网络相结合，在LFW数据集上逼近了人类的识别精度。其中DeepFace还引入了一个Local Connected卷积结构，在每个空间位置学习单独的卷积核，缺点是会导致参数膨胀，这个结构后来并没有流行起来。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a904840380fc200ce9990dbe4f292e34.jpg" data-rawwidth="704" data-rawheight="525"&gt;&lt;p&gt;DeepID家族可以看作是DL时代人脸识别领域的一组代表性工作。最早的DeepID网络包括四个卷积层，采用softmax损失函数。DeepID2在DeepID网络的基础上，同时考虑了分类损失（identity loss) 和确认损失（verification loss），这两种损失在Caffe深度学习框架中分别可以采用softmaxwithloss层和contrastive loss层来实现。DeepID2+网络则是在DeepID2的基础上，增加了每一层的辅助损失函数（类似Deep Supervised Network)。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d3abd5fe65650cd8879c8bf2885274db.jpg" data-rawwidth="712" data-rawheight="516"&gt;Google发表于CVPR2015的工作FaceNet采用了22层的深层卷积网络和海量的人脸数据（800万人的2亿张图像）以及常用于图像检索任务的Triplet Loss损失函数。值得一提的是，由于人脸类别数达到800万类，如果使用softmax loss，输出层节点将达到800万个，需要至少32GB显存（假设上一个隐层节点1024个，采用单精度浮点数），而Triplet Loss则不需要额外占用显存。FaceNet在LFW数据集上十折平均精度达到99.63%，这也是迄今为止正式发表的论文中的最好结果，几乎宣告了LFW上从2008年到2015年长达8年之久的性能竞赛的结束。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果" class="" data-editable="true"&gt;@果果是枚开心果&lt;/a&gt;.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6da8793a5f7d285f150479affefc63d5.jpg" data-rawwidth="245" data-rawheight="387"&gt;&lt;b&gt;刘昕&lt;/b&gt;，中国科学院计算技术研究所人脸识别研究组博士研究生，导师山世光研究员。博士阶段主要从事人脸识别与深度学习技术的研究与工业化应用。作为第一主力或并列第一主力获得 ICCV 2015 年龄估计竞赛亚军、ICCV 2015 文化事件识别竞赛冠军和 2015 年度阿里大规模图像检索竞赛总决赛冠军。&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=402656652&amp;amp;idx=1&amp;amp;sn=a254c2c9c8493c6e845d19dbbfea93de&amp;amp;scene=4#wechat_redirect"&gt;http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=402656652&amp;amp;idx=1&amp;amp;sn=a254c2c9c8493c6e845d19dbbfea93de&amp;amp;scene=4#wechat_redirect&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/499b12b5d7aa6804560ef81a692b6637.jpg" data-rawwidth="327" data-rawheight="55"&gt;</description><author>程程</author><pubDate>Fri, 01 Jul 2016 10:59:17 GMT</pubDate></item></channel></rss>