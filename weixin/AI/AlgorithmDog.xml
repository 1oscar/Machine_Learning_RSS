<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>AlgorithmDog</title>
    <link>http://www.iwgc.cn/list/5566</link>
    <description>AlgorithmDog 讲述机器学习和系统研发的轶事,希望把这些事讲得生动有趣.每周日更新哦.欢迎关注.</description>
    <item>
      <title>强化学习系列之九:Deep Q Network (DQN)</title>
      <link>http://www.iwgc.cn/link/2654181</link>
      <description>&lt;p&gt;&amp;nbsp; &amp;nbsp;我们终于来到了深度强化学习。&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-2410" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUheodPu55vXXQibaDLsibxeibwr6X07YAej3JbkQicWYyw87hjHz5Gn6oegA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;1. 强化学习和深度学习结合&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;机器学习=目标+表示+优化。&lt;/span&gt;&lt;span&gt;目标层面的工作关心应该学习到什么样的模型，强化学习应该学习到使得激励函数最大的模型。表示方面的工作关心数据表示成什么样有利于学习，深度学习是最近几年兴起的表示方法，在图像和语音的表示方面有很好的效果。深度强化学习则是两者结合在一起，深度学习负责表示马尔科夫决策过程的状态，强化学习负责把控学习方向。&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;深度强化学习有三条线：分别是基于价值的深度强化学习，基于策略的深度强化学习和基于模型的深度强化学习。这三种不同类型的深度强化学习用深度神经网络替代了强化学习的不同部件。基于价值的深度强化学习本质上是一个 Q Learning 算法，目标是估计最优策略的 Q 值。 不同的地方在于 Q Learning 中价值函数近似用了深度神经网络。比如 DQN 在 Atari 游戏任务中，输入是 Atari 的游戏画面，因此使用适合图像处理的卷积神经网络（Convolutional Neural Network，CNN）。下图就是 DQN 的框架图。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3343" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhIfUBaxwpqpYyqf25iaMWWfMzG1CYquIVmJ8cw9X9M0xq2KzWdp4xENA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;2. Deep Q Network (DQN) 算法&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;当然了基于价值的深度强化学习不仅仅是把 Q Learning 中的价值函数用深度神经网络近似，还做了其他改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这个算法就是著名的 DQN 算法，由 DeepMind 在 2013 年在 NIPS 提出。DQN 算法的主要做法是 Experience Replay，其将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3341" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhIwg2whD3AShmqIIW9X38EMq5gSVMQ7Ey93KLvJoNQnUZCGKBEK11Hw/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Experience Replay 的动机是：1）深度神经网络作为有监督学习模型，要求数据满足独立同分布，2）但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DeepMind 在 2015 年初在 Nature 上发布了文章，引入了 Target Q 的概念，进一步打破数据关联性。Target Q 的概念是用旧的深度神经网络&amp;nbsp;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;nobr style="transition: none; border: 0px; max-width: none; max-height: none; min-width: 0px; min-height: 0px; vertical-align: 0px;"&gt;&lt;/nobr&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;w&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&amp;nbsp;去得到目标值，下面是带有 Target Q 的 Q Learning 的优化目标。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhR9V2FDlmLwTLbIj8bJLnzZxPcdFhPNh7KIbibykkT4BWWhkbXfulShw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;下图是 Nature 论文上的结果。可以看到，打破数据关联性确实很大程度地提高了效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3348" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhH9ET4wpMjKiaLfUl0K35wBRL3yPj6y1Xe9udIVyiaOrjf8BzYZdjOKhg/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;3. 总结&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;本来想把基于价值的深度强化学习的 Double DQN, Prioritised replay 和 Duelling network 也写了，但就这点东西写的晚上 2 点。先这样吧，中秋会补上。&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 欢迎关注 AlgorithmDog~&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz/Q3H1TCddfvPE2JpYR7tQ2eqqCLpF5ncuTx6MNCib7Qd3FF67BsxicqGoLo1Y44rRGAah5gUfUrV6QTjIVaanBsDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 12 Sep 2016 02:16:30 +0800</pubDate>
    </item>
    <item>
      <title>游戏中的人工智能系列:一些准备工作</title>
      <link>http://www.iwgc.cn/link/2559124</link>
      <description>&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 一直想开游戏人工智能的专题。只是最近刚刚用 Spark, 踩了一些坑，没有时间看东西，就延误了。虽然现在还没有看什么东西，但拖不了了，下定决心这个专题。&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3322" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/Q3H1TCddfvMmlUawRUxCw2Wlrjg6TGCt1ykAs0xPZicLicUcx6TPbo4BSEcTXaCkpVaHjCk0icLwhrc8b9OU6LpHw/0?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 因为我对游戏中的人工智能也不了解，只能边看边写。才疏学浅难免有些纰漏，欢迎大家指出。这篇博客只介绍一些学习游戏 AI 的准备工作。最重要的准备工作就是选择好游戏啦。我们现在有两个选项，一个是打飞机的游戏 AIsteroids, 一个是即时策略游戏 0.A.D。&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;1. AIsteroids&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;AIsteroids 其实不是正儿八经的游戏，而是专门的游戏 AI 模拟平台。在AIsteroids 游戏中，开始时只有若干礁石（由圆圈表示）、一个 AI 或者人类控制的飞船（由三角形表示）、增加玩家射击力的宝物（由方块表示）。飞船可以转弯和推进（前向或者逆向）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;在 GitHub 上有 Javascript 实现 AIsteroids, 我已经把它 fork 到&lt;a style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;自己的项目&lt;/a&gt;里了~___~。先安装 node.js, 然后从 GitHub 下载或者克隆 AIsteroids，在项目主目录运行 “node server.js”，就可以用浏览器访问 http://127.0.0.1:1337 看到下面的画面了。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3327" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvMmlUawRUxCw2Wlrjg6TGCtwnltqEh8BJGlc3mUqV5jC3icLFRJeJafeibwyp6XjCpy30JliaGPR6wnQ/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从上图，我们发现 AIsteroids 确实很适合模拟游戏 AI。我们只需要在浏览器上填上 AI 的代码，就能看到 AI 操纵飞机和敌人对战了。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;2. 0.A.D&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;0.A.D 是由 WIldfire Games 开发的开源游戏。0 A.d 是一个实时的战略游戏，与微软的帝国时代类似。在游戏中用户可以建立文明，定义他们反对的敌人，这个游戏以公元前500年至公元500年之间为背景时间，包括了六个独特的文明和若干个多角色模式。0.A.D 的&lt;a style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;主页&lt;/a&gt;&amp;nbsp;介绍了游戏的详细资料，包括玩法、开发和&amp;nbsp;&lt;a style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;AI&lt;/a&gt;&amp;nbsp;相关的内容。游戏界面如下所示。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3334" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/Q3H1TCddfvMmlUawRUxCw2Wlrjg6TGCtqV2EuD2PG9GtZ1Q7cG94EiayBfX5AiaqjFSO8SoB1fBxPOnWSHqRRHXg/0?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;选择 0.A.D 作为游戏 AI 模拟平台的原因有两点：1）0.A.D 的 AI 做得比较好，2）0.A.D 提供了 AI 接口。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;3. 总结&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;游戏中的人工智能系列会以 AIsteroids 为主要模拟平台讲解传统的游戏 AI 算法，比如有限状态机和行为树。后期的文章可能会涉及 0.A.D。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;不过要等我再看下相关的资料~_~。下周先不更这个系列，会更新强化学习系列介绍 DQN 相关的算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;欢迎关注 AlgorithmDog~&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz/Q3H1TCddfvPE2JpYR7tQ2eqqCLpF5ncuTx6MNCib7Qd3FF67BsxicqGoLo1Y44rRGAah5gUfUrV6QTjIVaanBsDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 02:34:57 +0800</pubDate>
    </item>
  </channel>
</rss>
