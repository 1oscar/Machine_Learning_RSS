<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>AlgorithmDog</title>
    <link>http://www.iwgc.cn/list/5566</link>
    <description>AlgorithmDog 讲述机器学习和系统研发的轶事,希望把这些事讲得生动有趣.每周日更新哦.欢迎关注.</description>
    <item>
      <title>强化学习系列之九:Deep Q Network (DQN)</title>
      <link>http://www.iwgc.cn/link/2654181</link>
      <description>&lt;p&gt;&amp;nbsp; &amp;nbsp;我们终于来到了深度强化学习。&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-2410" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUheodPu55vXXQibaDLsibxeibwr6X07YAej3JbkQicWYyw87hjHz5Gn6oegA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;1. 强化学习和深度学习结合&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;机器学习=目标+表示+优化。&lt;/span&gt;&lt;span&gt;目标层面的工作关心应该学习到什么样的模型，强化学习应该学习到使得激励函数最大的模型。表示方面的工作关心数据表示成什么样有利于学习，深度学习是最近几年兴起的表示方法，在图像和语音的表示方面有很好的效果。深度强化学习则是两者结合在一起，深度学习负责表示马尔科夫决策过程的状态，强化学习负责把控学习方向。&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;深度强化学习有三条线：分别是基于价值的深度强化学习，基于策略的深度强化学习和基于模型的深度强化学习。这三种不同类型的深度强化学习用深度神经网络替代了强化学习的不同部件。基于价值的深度强化学习本质上是一个 Q Learning 算法，目标是估计最优策略的 Q 值。 不同的地方在于 Q Learning 中价值函数近似用了深度神经网络。比如 DQN 在 Atari 游戏任务中，输入是 Atari 的游戏画面，因此使用适合图像处理的卷积神经网络（Convolutional Neural Network，CNN）。下图就是 DQN 的框架图。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3343" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhIfUBaxwpqpYyqf25iaMWWfMzG1CYquIVmJ8cw9X9M0xq2KzWdp4xENA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;2. Deep Q Network (DQN) 算法&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;当然了基于价值的深度强化学习不仅仅是把 Q Learning 中的价值函数用深度神经网络近似，还做了其他改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这个算法就是著名的 DQN 算法，由 DeepMind 在 2013 年在 NIPS 提出。DQN 算法的主要做法是 Experience Replay，其将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3341" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhIwg2whD3AShmqIIW9X38EMq5gSVMQ7Ey93KLvJoNQnUZCGKBEK11Hw/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Experience Replay 的动机是：1）深度神经网络作为有监督学习模型，要求数据满足独立同分布，2）但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DeepMind 在 2015 年初在 Nature 上发布了文章，引入了 Target Q 的概念，进一步打破数据关联性。Target Q 的概念是用旧的深度神经网络&amp;nbsp;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;nobr style="transition: none; border: 0px; max-width: none; max-height: none; min-width: 0px; min-height: 0px; vertical-align: 0px;"&gt;&lt;/nobr&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;w&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&amp;nbsp;去得到目标值，下面是带有 Target Q 的 Q Learning 的优化目标。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhR9V2FDlmLwTLbIj8bJLnzZxPcdFhPNh7KIbibykkT4BWWhkbXfulShw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;下图是 Nature 论文上的结果。可以看到，打破数据关联性确实很大程度地提高了效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3348" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvPqOU7lTxHiavGqFpLhWEbUhH9ET4wpMjKiaLfUl0K35wBRL3yPj6y1Xe9udIVyiaOrjf8BzYZdjOKhg/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;3. 总结&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style="border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em;"&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;本来想把基于价值的深度强化学习的 Double DQN, Prioritised replay 和 Duelling network 也写了，但就这点东西写的晚上 2 点。先这样吧，中秋会补上。&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 欢迎关注 AlgorithmDog~&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz/Q3H1TCddfvPE2JpYR7tQ2eqqCLpF5ncuTx6MNCib7Qd3FF67BsxicqGoLo1Y44rRGAah5gUfUrV6QTjIVaanBsDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 12 Sep 2016 02:16:30 +0800</pubDate>
    </item>
  </channel>
</rss>
