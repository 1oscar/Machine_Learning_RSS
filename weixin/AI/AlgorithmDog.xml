<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>AlgorithmDog</title>
    <link>http://www.iwgc.cn/list/5566</link>
    <description>AlgorithmDog 讲述机器学习和系统研发的轶事,希望把这些事讲得生动有趣.每周日更新哦.欢迎关注.</description>
    <item>
      <title>游戏智能系列之二:再次进行准备</title>
      <link>http://www.iwgc.cn/link/2837423</link>
      <description>&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 上次准备用 AIsteroids 和 0.A.D 平台去学习游戏中的人工智能，现在换成 Clashjs。&lt;span&gt;&lt;/span&gt;AIsteroids 和 0.A.D 作模拟平台碰到了一个严重问题：1）AIsteroids 平台太简单了，只要让飞机不停右转同时发射子弹，就能很大概率赢得胜利；0.A.D 则太复杂了，复杂的即时策略游戏 AI 不适合初学者学习，陷入特定游戏的细节也不符合我们学习的本意。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3392" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvNY3gpq81sPUlMbypnrxj2yVAicfItxBZq0clphyLkuTMXDIVEmVZpdZyA2ghybzwOKZUlzItTGwJg/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;学习会有反复和曲折，我们需要拥抱变化。因此，我在网上搜寻，找到了一个新平台来实践我们学习的知识。&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;1. Clashjs 游戏&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Clashjs 是我在&amp;nbsp;&lt;a style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;GitHub&lt;/a&gt;&amp;nbsp;上找到开源项目。从下图可以看出来，Clashjs 也是一个飞机对战游戏。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3394" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvNY3gpq81sPUlMbypnrxj2yopkx9hPpFrRQnE0BMOLqs2QlqpSPOibfghDysFrnYeFic1wDr5n7yWTw/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Clashjs 比 AIsteroids 复杂。在 Clashjs 游戏中，有多台飞机参与战斗，最后只有一家获胜。并且参与战斗的飞机必须先吃到红色的镭射包，才能发射镭射打击敌人。这两点让 AI 设计者需要考虑更精细的策略才能取胜。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;2. Clashjs 游戏简明教程&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;对一款模拟平台，我们要做的第一件事就是把它启动起来。启动 Clashjs 很简单。&lt;/p&gt;&lt;p&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;）安装&lt;/span&gt;&lt;span&gt; node &lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span&gt; npm&lt;/span&gt;&lt;span&gt;;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;把&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;Clashjs&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;项目从&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;Github&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;上克隆下来;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;进入&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;Clashjs&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;项目运行&lt;/span&gt;&lt;span&gt; npm install &lt;/span&gt;&lt;span&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span&gt; npm run dev&lt;/span&gt;&lt;span&gt;;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;在浏览器地址栏输入&lt;/span&gt;&lt;span&gt; http&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt;//127.0.0.1:8080 &lt;/span&gt;&lt;/pre&gt;&lt;p&gt;项目地址：https://github.com/javierbyte/clashjs。值得注意的是，如果您不是在本地运行 Clashjs，在您输入 npm install &amp;amp;&amp;amp; npm run dev 之前，您需要 package.json 中的 “webpack-dev-server --progress --profile --colors --hot” 语句中加入 “--host 0.0.0.0” 从而允许通过 IP 访问。如果您对 Docker 了解，也可以下载我做的镜像 /lietal/game_ai，配置好的 clashjs 放在主目录下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;模拟平台运行起来之后，我们要做的第二件事就是加入自己的 Bot。您可以在$clash_home/src/players/you.js 写入自己的算法，然后在 $clash_home/src/Players.js 引入，重新运行 npm run dev。这样就把自己的 Bot 加入模拟平台啦，是不是很方便。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;当然了编写 Bot 的时候，我们需要遵守一定的规范。Bot 写出了如下所示，其中最关键的就是 ai 函数。这个 ai 函数输入玩家信息、敌人信息和游戏信息，输出要采取的动作。&lt;/p&gt;&lt;p&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;info&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;
 &amp;nbsp; &amp;nbsp;name&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;'javierbyte'&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;
 &amp;nbsp; &amp;nbsp;style&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;// one of the 6 styles (0 to 5)&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;&lt;/span&gt;&lt;span&gt;},&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;ai&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;playerState&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt; enemiesStates&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt; gameEnvironment&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;
 &amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;// think...&lt;/span&gt;&lt;span&gt;
 &amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;'move'&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;&lt;/span&gt;&lt;span&gt;}&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;}&lt;/span&gt;&lt;/pre&gt;&lt;p&gt;玩家信息都用 PlayerState 表示，敌人信息则用 PlayerState 的数组表示。游戏信息则用 GameEnvironment 表示。&lt;/p&gt;&lt;p&gt;&lt;span&gt;PlayerState&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;position&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;`[&amp;lt;number&amp;gt;, &amp;lt;number&amp;gt;]`&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;direction&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;`&amp;lt;string&amp;gt;`&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;// One of 'north', 'east', 'south' or 'west'&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;ammo&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;`&amp;lt;number&amp;gt;`&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;isAlive&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;`&amp;lt;bool&amp;gt;`&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;}&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;GameEnvironment&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;gridSize&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;[&amp;lt;&lt;/span&gt;&lt;span&gt;number&lt;/span&gt;&lt;span&gt;&amp;gt;,&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;&amp;lt;number&amp;gt;&lt;/span&gt;&lt;span&gt;],&lt;/span&gt;&lt;span&gt;
 &amp;nbsp;ammoPosition&lt;/span&gt;&lt;span&gt;:&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;array of &lt;/span&gt;&lt;span&gt;[&amp;lt;&lt;/span&gt;&lt;span&gt;number&lt;/span&gt;&lt;span&gt;&amp;gt;,&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;&amp;lt;number&amp;gt;&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt; arrays&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;}&lt;/span&gt;&lt;/pre&gt;&lt;p&gt;输出的动作一共有 6 个。其中四个是 north、east、south 和 west，表示转向的方向。另两个则是 shoot 表示射击和 move 表示前景。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;3. 总结&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;游戏智能系列文章会以 Clashjs 为平台介绍游戏智能算法，比如有限状态机和行为树。之前选用的两个游戏平台 AIsteroids 和 0.A.D，一个太简单一个太复杂，重新选择 Clashjs 作为模拟平台。学习过程曲折和反复还是会经常出现的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;欢迎关注 AlgorithmDog。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz/Q3H1TCddfvPE2JpYR7tQ2eqqCLpF5ncuTx6MNCib7Qd3FF67BsxicqGoLo1Y44rRGAah5gUfUrV6QTjIVaanBsDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 02:59:20 +0800</pubDate>
    </item>
    <item>
      <title>强化学习系列之九:Deep Q Network (DQN) 补全版</title>
      <link>http://www.iwgc.cn/link/2727655</link>
      <description>&lt;p&gt;&amp;nbsp; &amp;nbsp; 我们终于来到了深度强化学习。&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-2410" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zC7r2syGp2dt8D94kSQqDgjmTLOU4JxHXWicLRLFtZaESEMmTIB78SZA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;1. 强化学习和深度学习结合&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;机器学习=目标+表示+优化。目标层面的工作关心应该学习到什么样的模型，强化学习应该学习到使得激励函数最大的模型。表示方面的工作关心数据表示成什么样有利于学习，深度学习是最近几年兴起的表示方法，在图像和语音的表示方面有很好的效果。深度强化学习则是两者结合在一起，深度学习负责表示马尔科夫决策过程的状态，强化学习负责把控学习方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;深度强化学习有三条线：分别是基于价值的深度强化学习，基于策略的深度强化学习和基于模型的深度强化学习。这三种不同类型的深度强化学习用深度神经网络替代了强化学习的不同部件。基于价值的深度强化学习本质上是一个 Q Learning 算法，目标是估计最优策略的 Q 值。 不同的地方在于 Q Learning 中价值函数近似用了深度神经网络。比如 DQN 在 Atari 游戏任务中，输入是 Atari 的游戏画面，因此使用适合图像处理的卷积神经网络（Convolutional Neural Network，CNN）。下图就是 DQN 的框架图。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3343" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z79glicdE60yTenWApvjU4wpiangdvYRjx5PiapsjnWaJo3stZKXDwyNPg/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;2. Deep Q Network (DQN) 算法&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;当然了基于价值的深度强化学习不仅仅是把 Q Learning 中的价值函数用深度神经网络近似，还做了其他改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这个算法就是著名的 DQN 算法，由 DeepMind 在 2013 年在 NIPS 提出。DQN 算法的主要做法是 Experience Replay，其将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3341" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zmfiaohUGWibtibAZZatZ7AT1T9XZTJMbHFPcicOwvVFg79FicGQ7hHDWsRA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Experience Replay 的动机是：1）深度神经网络作为有监督学习模型，要求数据满足独立同分布，2）但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DeepMind 在 2015 年初在 Nature 上发布了文章，引入了 Target Q 的概念，进一步打破数据关联性。Target Q 的概念是用旧的深度神经网络&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8ztcxf4nVMuAZNtuIhyZqIdgCOCI1vT6l7V0CbXvPRQgrIsuJBibMiaEMQ/0?wx_fmt=png"/&gt;&amp;nbsp;去得到目标值，下面是带有 Target Q 的 Q Learning 的优化目标。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zz7d0vS4WuzzicUGibHmINE1Of0y81vAG0oc2C9XYLcFXRjg6q7dCeTEg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;下图是 Nature 论文上的结果。可以看到，打破数据关联性确实很大程度地提高了效果。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3348" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zRcH7jATgAwukE3ZcUib550T7NTXIRz7xsSra3JMo9wL2MlE5RF4eEhA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;3. 后续发展&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DQN 是第一个成功地将深度学习和强化学习结合起来的模型，启发了后续一系列的工作。这些后续工作中比较有名的有 Double DQN, Prioritized Replay 和 Dueling Network。&lt;/p&gt;&lt;h4 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 24px; ; ; ; ; "&gt;&lt;span&gt;3.1 Double DQN&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Thrun 和 Schwartz 在古老的 1993 年观察到 Q-Learning 的过优化 (overoptimism) 现象 [1]，并且指出过优化现象是由于 Q-Learning 算法中的 max 操作造成的。令&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z5EICTibSyFYHa9RZxuWFy24ia4PQEcVc8qCgtxDBKm9yomMboFuicl99A/0?wx_fmt=png"/&gt;&amp;nbsp;是目标 Q 值；我们用了价值函数近似，&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zEKr5ibTuz6qF6LpZtAyCSiceibpiblQHUMd3jjN8oGSmLEXia9UoGTKF5Ig/0?wx_fmt=png"/&gt;&amp;nbsp;是近似 Q 值；令 Y 为近似值和目标之间的误差，即&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z6zT8C40uwVkRjFQjo76GvMGhQA1Yw4xD8HAPwFtvVAYynCCvdDvibUQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;Q-learning 算法更新步骤将所有的 Q 值更新一遍，这个时候近似值和目标值之间的差值&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zOibTvxXAqVJhhyvhNFeqRE15yZnjCz7nJLEANyz0Zwawvy7iaeiaf7W9w/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;其中&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z2l6XqAh5GedIPoPWYfCz5BuhyXlGgPEKicHic6j6CqpN0ibXafhc79W7g/0?wx_fmt=png"/&gt;。这时候我们发现，即使&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zcYiaubc8GhcCte7uBmZSzaFFSlPMwef2Pcia70P7aZzA3kgLlRlOjIDA/0?wx_fmt=png"/&gt;&amp;nbsp;也就是一开始是无偏的近似， Q Learning 中的 max 操作也会导致 E[Z] &amp;gt; 0。这就是过优化现象。为了解决这个问题，Thrun 和 Schwartz 提出了 Double Q 的想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Hasselt 等进一步分析了过优化的现象，并将 Double Q 的想法应用在 DQN 上，从而提出了 Double DQN。下图是 Hasselt 在论文中报告的实验结果。从实验结果来看，Double DQN 拥有比 DQN 好的效果。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3363" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zQQjZicLtTpVeSy14tzZQicFy3wibOYLE2b97S5HtC736ickzSxemFdEO4A/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h4 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 24px; ; ; ; ; "&gt;&lt;span&gt;3.2 Prioritized Replay&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;DQN 用了 Experience Replay 算法，将系统探索环境获得的样本保存起来，然后从中采样出样本以更新模型参数。对于采样，一个常见的改进是改变采样的概率。Prioritized Replay [3] 便是采取了这个策略，采用 TD-err 作为评判标准进行采样。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8ztHGSFGTOOmj3P3bibgW9icWgTIS3nGhNTibEVHdJicTgoH4yeU1AkxDnKQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;下图是论文中采用的例子。例子中有 n 个状态，在每个状态系统一半概率采取 “正确” 或者一半概率 “错误”，图中红色虚线是错误动作。一旦系统采取错误动作，游戏结束。只有第 n 个状态 “正确” 朝向第 1 个状态，系统获得奖励 1。在这个例子训练过程中，系统产生无效样本，导致训练效率底下。如果采用 TD-err 作为评判标准进行采样，能够缓解这个问题。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3377" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zXfNlUhUjhmFJ6zyUiaVgYW8j0MD83I8LCJ58xypvuWM2Ww3BvevCIAA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;论文报告了 Prioritized Replay 算法效果。从下图来看，Prioritized Replay 效果很好。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3364" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zmm7ccu6V8HTB5UavIWnHy0q2xNIxib5pQe7PvibOCI89lX0IPxgN739g/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h4 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 24px; ; ; ; ; "&gt;&lt;span&gt;3.3 Dueling Network&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Baird 在 1993 年提出将 Q 值分解为价值 (Value) 和优势 (Advantage) [4]。&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zNRunXibIQdhuHas0qnSL2X79j959Vcw1up8aNVoYlu0wotBm2thRGUQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;这个想法可以用下面的例子说明 [5]。上面两张图表示，前方无车时，选择什么动作并不会太影响行车状态。这个时候系统关注状态的价值，而对影响动作优势不是很关心。下面两张图表示，前方有车时，选择动作至关重要。这个时候系统需要关心优势了。这个例子说明，Q 值分解为价值和优势更能刻画强化学习的过程。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3361" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zCNwl5tVeljfrwSuxcj5ianibaujWRz2gPGOnHppOEhwJM7icA5ZwGRQzA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Wang Z 将这个 idea 应用在深度强化学习中，提出了下面的网络结构 [5]。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3379" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z13kDoibk65iadCbDGRC3ggE1C7J7ytnQ29tp76SCepBbClsCqQHyJqXw/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这种网络结构很简单，但获得了很好的效果。&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-3362" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8zEU9hStqVLLsgGNzLBLbqyib3DDMEEaTI8gxgAHvLcIGKmbuQrRZ2gZA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Dueling Network 是一个深度学习的网络结构。它可以结合之前介绍的 Experience Replay、 Double DQN 和 Prioritized Replay 等方法。 作者在论文中报告 Dueling Network 和 Prioritized Replay 结合的效果最好。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; ; "&gt;&lt;span&gt;4. 总结&lt;/span&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3 style=" border: 0px; margin-bottom: 10px; vertical-align: baseline; clear: both; line-height: 1.5em; ; ; ; ; "&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 上次本来想把基于价值的深度强化学习的 Double DQN, Prioritized Replay 和 Dueling Network 也写了的，写到晚上 2 点。现在补上这部分内容。这周作业算交啦，明天就不更啦。&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从上面介绍来看，DQN、 Double DQN、Prioritized Replay 和 Dueling Network 都能在深度学习出现之前的工作找到一些渊源。深度学习的出现，将这些方法的效果提高了前所未有的高度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;文章结尾欢迎关注我的公众号 AlgorithmDog，每次更新就会有提醒哦~&lt;/p&gt;&lt;p&gt;&lt;a rel="attachment wp-att-2360" style="border: 0px; vertical-align: baseline; color: rgb(116, 51, 153); background: transparent;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/Q3H1TCddfvOq5o6RcuhfxFzZAT0W2P8z1V3e1JtngZic1zuiaZu6LVFfre5m6Aicew3ALokf8diaHqcXfvicVOpicZibA/0?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;[1] S. Thrun and A. Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors, Proceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Lawrence Erlbaum.&lt;br/&gt;[2] Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep reinforcement learning with double Q-learning." CoRR, abs/1509.06461 (2015).&lt;br/&gt;[3] Schaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv preprint arXiv:1511.05952, 2015.&lt;br/&gt;[4] Baird, L.C. Advantage updating. Technical Report WLTR-93-1146,&lt;br/&gt;Wright-Patterson Air Force Base, 1993.&lt;br/&gt;[5] Wang Z, de Freitas N, Lanctot M. Dueling network architectures for deep reinforcement learning[J]. arXiv preprint arXiv:1511.06581, 2015.&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 18 Sep 2016 01:25:58 +0800</pubDate>
    </item>
  </channel>
</rss>
