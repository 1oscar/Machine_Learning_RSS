<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 神经网络架构演进史：全面回顾从LeNet5到ENet十余种架构（附论文）</title>
      <link>http://www.iwgc.cn/link/2565649</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Github&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Eugenio Culurciello&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：孙雨辰、李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;深度神经网络与深度学习是非常强大的流行算法。许多运用它们的成功典范依赖于对神经网络架构的精心设计。作者在这篇文章中&lt;span&gt;重新回顾一下最近几年的深度学习背景下神经网络设计的历史。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LeNet5&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。自从 1988 年开始，在许多次成功的迭代后，这项由 Yann LeCun 完成的开拓性成果被命名为 LeNet5（参见：Gradient-Based Learning Applied to Document Recognition）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4yRTJwBXiaXnZINv4GJSR12xf9UJ1sGsKPrYrjunGR0KcSTxGTAUtv2w/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 的架构基于这样的观点：（尤其是）图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。在那时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢。因此，能够保存参数以及计算过程是一个关键进展。这和将每个像素用作一个大型多层神经网络的单独输入相反。LeNet5 阐述了那些像素不应该被使用在第一层，因为图像具有很强的空间相关性，而使用图像中独立的像素作为不同的输入特征则利用不到这些相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 特征能够总结为如下几点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积神经网络使用 3 个层作为一个序列：卷积、池化、非线性 → 这可能是自从这篇 paper 起图像深度学习的关键特征！&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用卷积提取空间特征&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用映射到空间均值下采样（subsample）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;双曲正切（tanh）或 S 型（sigmoid）形式的非线性&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层神经网络（MLP）作为最后的分类器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;层与层之间的稀疏连接矩阵避免大的计算成本&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体来看，这个网络是最近大量架构的起点，并且也给这个领域的许多带来了灵感。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;间隔&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 1998 年到 2010 年神经网络处于孵化阶段。大多数人没有意识到它们不断增长的力量，与此同时其他研究者则进展缓慢。由于手机相机以及便宜的数字相机的出现，越来越多的数据可被利用。并且计算能力也在成长，CPU 变得更快，GPU 变成了多种用途的计算工具。这些趋势使得神经网络有所进展，虽然速度很慢。数据和计算能力使得神经网络能完成的任务越来越有趣。之后一切变得清晰起来......&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Dan Ciresan Net&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年的时候，Dan Claudiu Ciresan 和 Jurgen Schmidhuber 发布了最早的 GPU 神经网络的一个实现。这个实现是在一块 NVIDIA GTX 280 图形处理器上运行 9 层的神经网络，包含前向与反向传播。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;AlexNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2012 年，Alex Krizhevsky 发表了 Alexet（参见：ImageNet Classification with Deep Convolutional Neural Networks），它是 LeNet 的一种更深更宽的版本，并以显著优势赢得了困难的 ImageNet 竞赛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4lD9IbppYP6aYhUibXU4m6WlT0vs8a9PTAWFODVtPblp8A6FPSc0zyjg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AlexNet 将 LeNet 的思想扩展到了更大的能学习到远远更复杂的对象与对象层次的神经网络上。这项工作的贡献有：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用修正的线性单元（ReLU）作为非线性&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在训练的时候使用 Dropout 技术有选择地忽视单个神经元，以避免模型过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;覆盖进行最大池化，避免平均池化的平均化效果&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 GPU NVIDIA GTX 580 减少训练时间&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在那时，GPU 相比 CPU 可以提供更多数量的核，训练时间可以提升 10 倍，这又反过来允许使用更大的数据集和更大的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AlexNet 的成功掀起了一场小革命。卷积神经网络现在是深度学习的骨干，它已经变成了「现在能解决有用任务的大型神经网络」的代名词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Overfeat&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2013 年的 12 月，纽约大学的 Yann LeCun 实验室提出了 AlexNet 的衍生——Overfeat（参见：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks）。这篇文章也提出了学习边界框（learning bounding box），并导致之后出现了很多研究这同一主题的论文。我相信学习分割对象比学习人工边界框更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;VGG&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自牛津大学的 VGG 网络（参见：Very Deep Convolutional Networks for Large-Scale Image Recognition）是第一个在各个卷积层使用更小的 3×3 过滤器（filter），并把它们组合作为一个卷积序列进行处理的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这看来和 LeNet 的原理相反，其中是大的卷积被用来获取一张图像中相似特征。和 AlexNet 的 9×9 或 11×11 过滤器不同，过滤器开始变得更小，离 LeNet 竭力所要避免的臭名昭著的 1×1 卷积异常接近——至少在该网络的第一层是这样。但是 VGG 巨大的进展是通过依次采用多个 3×3 卷积，能够模仿出更大的感受野（receptive field）的效果，例如 5×5 与 7×7。这些思想也被用在了最近更多的网络架构中，如 Inception 与 ResNet。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4xjjrwABFNBEuArUBXhKcvjYoqF39Com0X8JSp0vWwPQWJKWTBnc5bg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 网络使用多个 3×3 卷积层去表征复杂特征。注意 VGG-E 的第 3、4、5 块（block）：256×256 和 512×512 个 3×3 过滤器被依次使用多次以提取更多复杂特征以及这些特征的组合。其效果就等于是一个带有 3 个卷积层的大型的 512×512 大分类器。这显然意味着有大量的参数与学习能力。但是这些网络训练很困难，必须划分到较小的网络，并逐层累加。这是因为缺少强大的方式对模型进行正则化，或者或多或少约束大量由于大量参数增长的搜索空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 在许多层中都使用大特征尺寸，因为推断（inference）在运行时是相当耗费时间的。正如 Inception 的瓶颈（bottleneck）那样，减少特征的数量将节省一些计算成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;网络中的网络（Network-in-network）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;网络中的网络（NiN，参见论文：Network In Network）的思路简单又伟大：使用 1×1 卷积为卷积层的特征提供更组合性的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NiN 架构在各个卷积之后使用空间 MLP 层，以便更好地在其他层之前组合特征。同样，你可以认为 1×1 卷积与 LeNet 最初的原理相悖，但事实上它们可以以一种更好的方式组合卷积特征，而这是不可能通过简单堆叠更多的卷积特征做到的。这和使用原始像素作为下一层输入是有区别的。其中 1×1 卷积常常被用于在卷积之后的特征映射上对特征进行空间组合，所以它们实际上可以使用非常少的参数，并在这些特征的所有像素上共享！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4wkfBwsSdhCOBcBKUB9gIhjb8tzo2IYdQjTDiaVBpicicRXxTIFLDWlPQw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MLP 的能力能通过将卷积特征组合进更复杂的组（group）来极大地增加单个卷积特征的有效性。这个想法之后被用到一些最近的架构中，例如 ResNet、Inception 及其衍生技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NiN 也使用了平均池化层作为最后分类器的一部分，这是另一种将会变得常见的实践。这是通过在分类之前对网络对多个输入图像的响应进行平均完成的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;GoogLeNet 与 Inception&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自谷歌的 Christian Szegedy 开始追求减少深度神经网络的计算开销，并设计出 GoogLeNet——第一个 Inception 架构（参见：Going Deeper with Convolutions）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是在 2014 年秋季，深度学习模型正在变得在图像与视频帧的分类中非常有用。大多数怀疑者已经不再怀疑深度学习与神经网络这一次是真的回来了，而且将一直发展下去。鉴于这些技术的用处，谷歌这样的互联网巨头非常有兴趣在他们的服务器上高效且大规模庞大地部署这些架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Christian 考虑了很多关于在深度神经网络达到最高水平的性能（例如在 ImageNet 上）的同时减少其计算开销的方式。或者在能够保证同样的计算开销的前提下对性能有所改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他和他的团队提出了 Inception 模块：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4biaF4FoYXaibicicovTM3lAqdAdxszyum8M2KCbNeo9Yu9EdTDLo2qEBuw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;初看之下这不过基本上是 1×1、3×3、5×5 卷积过滤器的并行组合。但是 Inception 的伟大思路是用 1×1 的卷积块（NiN）在昂贵的并行模块之前减少特征的数量。这一般被称为「瓶颈（bottleneck）」。这部分内容将在下面的「瓶颈层（bottleneck layer）」部分来解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GoogLeNet 使用没有 inception 模块的主干作为初始层，之后是与 NiN 相似的一个平均池化层加 softmax 分类器。这个分类器比 AlexNet 与 VGG 的分类器的运算数量少得多。这也促成一项非常有效的网络设计，参见论文：An Analysis of Deep Neural Network Models for Practical Applications。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;瓶颈层（Bottleneck layer）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;受到 NiN 的启发，Inception 的瓶颈层减少了每一层的特征的数量，并由此减少了运算的数量；所以可以保持较低的推理时间。在将数据通入昂贵的卷积模块之前，特征的数量会减少 4 倍。在计算成本上这是很大的节约，也是该架构的成功之处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们具体验证一下。现在你有 256 个特征输入，256 个特征输出，假定 Inception 层只能执行 3×3 的卷积，也就是总共要完成 256×256×3×3 的卷积（将近 589,000 次乘积累加（MAC）运算）。这可能超出了我们的计算预算，比如说，在谷歌服务器上要以 0.5 毫秒运行该层。作为替代，我们决定减少需要进行卷积运算的特征的数量，也就是 64（即 256/4）个。在这种情况下，我们首先进行 256 -&amp;gt; 64 1×1 的卷积，然后在所有 Inception 的分支上进行 64 次卷积，接而再使用一个来自 64 -&amp;gt; 256 的特征的 1×1 卷积，现在运算如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;256×64 × 1×1 = 16,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;64×64 × 3×3 = 36,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;64×256 × 1×1 = 16,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比于之前的 60 万，现在共有 7 万的计算量，几乎少了近 10 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而且，尽管我们做了更好的运算，我们在此层也没有损失其通用性（generality）。事实证明瓶颈层在 ImageNet 这样的数据集上已经表现出了顶尖水平，而且它也被用于接下来介绍的 ResNet 这样的架构中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它之所以成功是因为输入特征是相关联的，因此可通过将它们与 1×1 卷积适当结合来减少冗余。然后，在小数量的特征进行卷积之后，它们能在下一层被再次扩展成有意义的结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Inception V3（还有 V2）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Christian 和他的团队都是非常高产的研究人员。2015 年 2 月，Batch-normalized Inception 被引入作为 Inception V2（参见论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）。Batch-normalization 在一层的输出上计算所有特征映射的均值和标准差，并且使用这些值规范化它们的响应。这相当于数据「增白（whitening）」，因此使得所有神经图（neural maps）在同样范围有响应，而且是零均值。在下一层不需要从输入数据中学习 offset 时，这有助于训练，还能重点关注如何最好的结合这些特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 12 月，该团队发布 Inception 模块和类似架构的一个新版本（参见论文：Rethinking the Inception Architecture for Computer Vision）。该论文更好地解释了原始的 GoogLeNet 架构，在设计选择上给出了更多的细节。原始思路如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;通过谨慎建筑网络，平衡深度与宽度，从而最大化进入网络的信息流。在每次池化之前，增加特征映射。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当深度增加时，网络层的深度或者特征的数量也系统性的增加。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用每一层深度增加在下一层之前增加特征的结合。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;只使用 3×3 的卷积，可能的情况下给定的 5×5 和 7×7 过滤器能分成多个 3×3。看下图&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4SbBX8mGVALOPTvfdoYX26M3KDlmwcEm6ck25gYzQLgTjicDfTXpK5mw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因此新的 Inception 成为了：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4eZISZibeQDCia3POWvz9fOyoz0icmZpEp5chmeRFsP6Wtfb9wO3h6HoAg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;也可以通过将卷积平整进更多复杂的模块中而分拆过滤器：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ibJqzib225ovv3Xav5jK7doiaMNuctfZPG4lwVHCPFpBvCGOqjQgN65ibA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在进行 inception 计算的同时，Inception 模块也能通过提供池化降低数据的大小。这基本类似于在运行一个卷积的时候并行一个简单的池化层：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4D8LePSPdI8KKGFdscAdC8KOueqZOqYQ09OkI0iaCiaLglyianON3ZMXEA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception 也使用一个池化层和 softmax 作为最后的分类器。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ResNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 12 月又出现了新的变革，这和 Inception V3 出现的时间一样。ResNet 有着简单的思路：供给两个连续卷积层的输出，并分流（bypassing）输入进入下一层（参见论文：Deep Residual Learning for Image Recognition）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf49E043MMV0uDXQlia1ibjOO1Ulko5EKZKcaED9yghb3yAMjCGGCrq5BwQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这和之前的一些旧思路类似。但 ResNet 中，它们分流两个层并被应用于更大的规模。在 2 层后分流是一个关键直觉，因为分流一个层并未给出更多的改进。通过 2 层可能认为是一个小型分类器，或者一个 Network-In-Network。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是第一次网络层数超过一百，甚至还能训练出 1000 层的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有大量网络层的 ResNet 开始使用类似于 Inception 瓶颈层的网络层：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4OtCPCF1riahRAKnQ5Eyib67FV3Ub3QcJwWlpGUMYAb0X6gqavBweDe7w/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种层通过首先是由带有更小输出（通常是输入的 1/4）的 1×1 卷积较少特征的数量，然后使用一个 3×3 的层，再使用 1×1 的层处理更大量的特征。类似于 Inception 模块，这样做能保证计算量低，同时提供丰富的特征结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ResNet 在输入上使用相对简单的初始层：一个带有两个池的 7×7 卷基层。可以把这个与更复杂、更少直觉性的 Inception V3、V4 做下对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ResNet 也使用一个池化层加上 softmax 作为最后的分类器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于 ResNet 的其他洞见每天都有发生：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ResNet 可被认为既是平行模块又是连续模块，把输入输出（inout）视为在许多模块中并行，同时每个模块的输出又是连续连接的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ResNet 也可被视为并行模块或连续模块的多种组合（参见论文：Residual Networks are Exponential Ensembles of Relatively Shallow Networks）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;已经发现 ResNet 通常在 20-30 层的网络块上以并行的方式运行。而不是连续流过整个网络长度。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当 ResNet 像 RNN 一样把输出反馈给输入时，该网络可被视为更好的生物上可信的皮质模型（参见论文：Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Inception V4&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是 Christian 与其团队的另一个 Inception 版本，该模块类似于 Inception V3：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf44Pw1eJaLicsWRmrI2iafLur9uGhibc2u9vged32NWRGLdZ0VlgK23qiaRw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception V4 也结合了 Inception 模块和 ResNet 模块：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf40MCqn2WCvyyGmdCCkN9ZI1uIvrLonJyq2q85OkQrUv23BfeibD5YthQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认为该架构不太简洁，但也满满都是较少透明度的启发法（heuristics）。很难理解里面的选择，对作者们而言也难以解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;考虑到网络的简洁性，可被轻易的理解并修正，那 ResNet 可能就更好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;SqueezeNet&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SqueezeNet（参见论文：SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size）是最近才公布的，该架构是对 ResNet 与 Inception 里面概念的重新处理。一个更好的架构设计网络型号要小，而且参数还不需要复杂的压缩算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;ENet&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的团队计划结合近期公开的架构的所有特征，做出一个非常高效、低重的网络，使用较少的参数和计算就能达到顶尖结果。该网络架构被称为 ENet，由 Adam Paszke 设计。我们已经使用它进行过单像素标记和场景解析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;详细了解 ENet 可参见论文 ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation。ENet 是一个编码加解码的网络。编码器是一个常规的 CNN 设计进行分类。解码器是一个增采样（upsampling）网络，将分类反向传播给原始图像进行分割。这只使用了神经网络，没有其他算法进行图像分割。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ENet 被设计为在开始时尽可能使用最小数量的资源。正是如此它有着如此小的脚本，编码器和解码器网络共占有 0.7 MB，16 fp 精度。即使这么小的型号，ENet 在分割的准确度上也类似于或者高于其他神经网络解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;模块分析&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对 CNN 模块的分析，该论文（Systematic evaluation of CNN advances on the ImageNet）已经做过了，里面的发现是非常有帮助的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用没有 batchnorm 的 ELU 非线性或者有 batchnorm 的 ReLU。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用一个学习到的 RGB 的彩色空间转换。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用线性学习率衰退策略。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用平均和最大池化层的和。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用大约 128 到 256 的 mini-batch 大小。如果这对你的 GPU 而言太大，将学习率按比例降到这个大小就行。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用完全连接层作为卷积，并为做最后预测平均所有预测。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当研究增加训练集大小的时候，检测有一个 plateau 是否没有达到&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;数据的整洁要比数据大小更重要。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果你不能增加输入图像的大小，在随后的层上减少步幅（stride），这样做有同样的效果。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果你的网络有复杂和高度优化的架构，像是 GoogLeNet，那修改一定要谨慎。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;其他值得关注的架构&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FractalNet（参见论文：FractalNet: Ultra-Deep Neural Networks without Residuals）使用递归架构，它在 ImageNet 上没有进行测试。该架构是 ResNet 的衍生或者更通用的 ResNet。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;未来&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们相信制作神经网络架构是深度学习领域发展的头等大事。我们团队高度推荐仔细阅读并理解文中提到的论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但有人可能会想为什么我们要投入如此多的时间制作架构？为什么不是用数据告诉我们使用什么？如何结合模块？这些问题很好，但仍在研究中，有一篇论文可以参考：Neural networks with differentiable structure。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要注意到，我们在本文中谈到的大部分架构都是关于计算机视觉的。类似神经网络架构在其他领域内也有开发，学习其他所有任务中的架构变革也是非常有趣的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你对神经网络架构和计算性能的比较有兴趣，可参见论文：An Analysis of Deep Neural Network Models for Practical Applications。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击「阅读原文」，下载文中提到的论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>ACM 9 月刊 | 深度学习的黄金搭档：GPU正重塑计算方式</title>
      <link>http://www.iwgc.cn/link/2565650</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 CACM&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Samuel Greengard&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜雪&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着神经网络和深度学习研究的不断深入——尤其是语音识别和自然语言处理、图像与模式识别、文本和数据分析，以及其他复杂领域——研究者们不断在寻找新的更好的方法来延伸和扩展计算能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几十年来，这一领域的黄金标准一直是高性能计算（HCP）集群，它解决了大量处理能力的问题，虽然成本有点过高。但这种方法已经帮助推动了多个领域的进步，包括天气预测、金融服务，以及能源勘探。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，2012 年，一种新的方法出现了。伊利诺伊大学的研究者之前已经研究过在台式超级计算机中使用 GPUs 来加速处理任务（如图像重建）的可能性，现在多伦多大学的一组计算机科学家和工程师证明了一种在 GPUs 上运行深度神经网络来极大推进计算机视觉技术的方法。插上 GPUs（之前主要用在图形中）后，计算神经网络的性能会立即获得巨大提升，这种提升反映在了计算机视觉效果的明显改善上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一次革命性进步&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「仅仅在几年之后，GPUs 已经出现在深度学习的核心位置，」加州大学伯克利分校电子工程和计算机科学系教授 Kurt Keutzer 说到。「GPUs 的使用正在成为主流，通过在一个应用程序中使用几十到数百个处理器，GUP 正在从根本上改变计算。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伊利诺伊大学厄巴纳-香槟分校电子与计算机工程 Walter J. Sanders III–Advanced Micro Device 的名誉主席 Wen-Mei W. Hwu 也说过，「GPU 是卓越的吞吐量计算设备。如果你只有一项任务，就没必要用到 GPUs，因为速度也快不到哪去。但是，如果你有大量的相互之间独立的任务，用 GPUs 就对了。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一个深度视角&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GPU 架构起源于基础的图形渲染操作，比如给图形加阴影。1999 年，Nvida 推出了 GeForce 256，这是世界上第一个 GPU。简单来说，这个专用的电路——-可内置在视频卡或主板中——主导并优化了计算机内存以加快显示器的渲染速度。今天，GPUs 用在更加广泛的设备中，包括个人计算机、平板电脑、手机、工作站、电子标示、游戏机，以及嵌入式系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，「计算机视觉和深度学习中很多新应用的内存都是有限带宽，」Keutzer 解释道，「在这些应用中，应用程序的速度往往最终取决于它从内存中提取数据以及流入和通过处理器要花多少时间。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;部署 GPU 的一个常常被忽视的巨大优势是其 processor-to-memory 的超级带宽。Keutzer points 指出，这样的结果是，「在带宽有限的应用中，这个 processor-to-memory 带宽的相对优势直接转化成超级应用性能。」关键是 GPUs 用更少的电力提供了更快的浮点运算（FLOPs，每秒浮点运算次数）通过支持 16 位的浮点数扩大了能效优势，比单精度（32 位）或双精度（64 位）浮点数的能效更高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多核 GPU 要依赖更大量的 32 位到 64 位这样更简单的处理器内核的大量部署。相比之下，使用更小的传统的微处理器，通常是 2 位到 4 位到 8 位时，效果如何会呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「使用微处理器的 GPUs 实现了更优越的性能，并为深度神经网络提供了更好的架构支持。GPUs 在深度神经网络上表现出的性能优势逐渐被转化到更多种类的应用中。」Keutzer 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，一个典型的 GPU 集群包含了 8 到 16 个 GPU，而像 Keutzer 这样的研究人员正在尝试使用数百个 GPU 在超大数据集上同时训练多个深度神经网络，否则将需要几周的训练时间。这个训练需要运行大量数据通过该系统以让它达到能解决问题的状态。那时，它或许就可以在一个 GPU 或者混合处理器中运行了。「这不是一次学术训练。」Keutzer 指出。「我们训练用于像自动驾驶汽车这种新应用的神经网络时，就需要这样的速度。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;使用 GPU 正在成为主流，通过在单个应用中使用多个处理器，能从根本上改变计算。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GPU 技术现在的进展速度远比传统的 CPU 快，凭借强劲的浮点马力和较低的能耗，GPU 的可扩展性能让深度学习和机器学习任务的效率得到飞速提升，效果堪比给汽车装上涡轮增压发动机，百度高级研究员 Bryan Catanzaro 说到。「深度学习不是新鲜事物。GPUs 也不是。但是这个领域在计算能力得到极大提升和有丰富数据可供使用之后，才开始真正起航。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大部分进展来自 Nvidia，这家公司不断推出更加复杂的 GPUs，包括刚推出的专为解决训练和推理这类特殊任务的 Pascal 架构。在这款最新的 GPU 系统中，Tesla P100 芯片实现了在一片硅片上封包 150 亿个晶体管，数量是之前处理器的两倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个例子，百度正在推进语言识别研究的新前沿。它的「Deep Speech」项目，依赖一个端到端的神经网络，在英语和汉语的短音频剪辑中使语音识别的精确度达到了人类水平。这家公司还在探索自动驾驶汽车中的 GPU 技术；它一直在研发能在北京大街上自动导航的自动驾驶汽车，并做了改变车道、超车、停车和启动的演习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，微软亚洲的研究员使用 GPUs 和一种深度神经网络的变体——深度残差网络，来在计算机视觉中的对象分类和识别的任务中实现更高精确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌，也在使用这些技术来持续改进图像识别算法。前谷歌人工智能研究员，现 Open AI 研究室主任 Ilya Sutskever 说到：「神经网络正在复兴。神经网络和深度学习的核心理念已经被讨论和思考多年了，但是正是通用 GPU 的研发才是神经网络和深度学习成功的关键。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一步超越&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「虽然 GPU 技术正在深度学习领域推进到新的前沿，但很多计算性的挑战仍然存在。首先，像 GPU 这样的独立程序化多核设备的高效实现仍然很困难；并且，这种困难会随着多 GPU 并行的加剧而恶化。」Keutzer 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不幸的是，他补充道，「这些设备的许多高效程序化的专业技术都被限制在公司内部，许多已被开发的技术细节仍未被广泛地应用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同样地，Keutzer 认为，关于深度神经网络的设计仍被广泛地描述为「黑科技」，构建一种新型的深度神经网络架构同构建一种新型的微处理器架构一样复杂。更糟糕的是，一旦这种深度神经网络架构被构建，「就会产生很多类似超参数的 knobs，在应用在训练中时，只有当这些 knobs 被合理设置时才会产生应有的精确度。所有的这些造成了这些已知和未知间的知识鸿沟」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「不论是在深度神经网络领域还是 GPU 编程领域，拥有专业知识的个人都是十分匮乏的，而那些对两方面都了如指掌的人才则是更为罕见。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一大挑战则是理解如何最高效地使用 GPU。举个例子，百度需要 8-16 个 GPU 去训练一个模型，从而在整个应用中达到 40%-50% 的浮点峰值。「这就意味着表现得效果十分有限。」Catanzaro 说道，「现实是我们需要更大规模地使用 GPU，8 个或 16 个远远不够，我们可能需要的是 128 个 GPU 并行。」这就需要更好的连接，以及支持由 32 位浮点支持到 16 位浮点支持的能力。Nvidia 的下一代 GPU——Pascal 有可能可以解决这些问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有一大障碍在于让 GPU 更好地同其他 GPU 和 CPU 集成。Hwu 指出，这两种类型的处理器并不常会集成在一起，并且他们之间也很少拥有足够高的带宽。这就最终转化成了有限数量的应用和系统运行的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「你十分需要让你的 GPU 具备运行大数据任务的能力；同时，你的 GPU 还能适时暂停使卸载进程比较合算。」Catanzaro 解释道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在的 Nvidia GPU 存在于不同的芯片上，他们通常通过一个 I/O bus (PCIe) 连接到 CPU 上。这就是它们能够向 GPU 发送大量任务的一个原因。未来的系统会将 GPU 和 CPU 集成在一个统一的包里，并且它能承担更高带宽和更小的风险，以及通过 GPU 和 CPU 来保持共享的一致性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Keutzer 希望随着时间的推移，CPU 和 GPU 能够得到更好的集成，这两者间更强的一致性与同步性也随之能够实现。事实上，Nvidia 和 Intel 也都在关注着这一领域。Keutzer 注意到一种名为 Knight's Landing (KNL) 的新型 Intel 芯片在 Xeon Phi 72-core super-computing 处理器中提供了前所未有的计算能力，并且，它同时集成了 CPU 和 GPU 的特性。同时，这款芯片还提供了每秒 500 GB processor-to-memory 的带宽需求，这也将侵蚀 GPU 在这一领域的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hwu 注意到 KNL 的 72 个核彼此都能执行「一个宽泛的向量指令（512 字节）。当转化到双重精度（8 字节）和单一精度（4 字节）的时候，向量宽带就将会是 64 和 128。在这个层面上，它和 GPU 有着类似的执行模型。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Keutzer 希望随着时间的推移，CPU 和 GPU 能够得到更好的集成，这两者间更强的一致性与同步性也随之能够实现。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KNL chip 的编程模型是传统的 x86 模型，所以，Hwu 认为，程序员们「需要通过 Intel C Compiler 编写代码来使得芯片变得可向量化，或是使用 Intel AVX 向量的本质库函数。」他补充道，GPU 的编程模型需要依托于一个核心的编程模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，X86 的内核对所以高速缓存的层次结构具有高速缓存的一致性，Hwu 说道，「但是，GPU 的第一层缓存并不清晰一致，它会伴随着一些减少的储存带宽。」然而，他补充道，「就深度学习的应用来说，高速缓存的一致性对大多数算法的第一层缓存并没有那么重要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;未来十年，所有这些的通用性在于大的行业环境如何发展。Hwu 表示，他坚信摩尔定律能够在超过三代的时间内继续发挥作用，设计师和程序员也能够从几乎离散的 CPU 和 GPU 系统过渡到集成的设计上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果摩尔定律停止运转，它也将显著地影响未来的这些系统，以及人们在深度学习和其他任务上使用硬件和软件的方式。」Hwu 指出，「但是，即使我们解决了硬件层面的问题，特定深度学习方面的任务仍需要大量的标签化数据。在某些层面，我们需要在标签化数据方面取得突破，从而让我们能够具备从事必要领域训练的能力，尤其是在自动驾驶领域。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在未来几年，Sutskever 说道，机器学习将会广泛地应用到 GPU。「随着机器学习的方法不断提升，它们会被应用到远超今天使用范围的领域并影响到其他所有方面，从医疗保健、机器人到金融服务和用户体验。这些进步依托于更快 GPU 的发展，这也将会使机器学习具备研究的能力。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adds Catanzaro 说：GPU 是通往未来计算之门。深度学习令人兴奋是因为当你增加更多数据时，它可以规模化。在这一点上，我们会永不满足的追求更多的数据和计算资源来解决复杂问题。在拓展计算极限方面，GPU 技术是非常重要的一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 从输入到输出的「黑箱」：我们能够理解深度神经网络吗？</title>
      <link>http://www.iwgc.cn/link/2565651</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Nautilus&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：AARON M. BORNSTEIN&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、Quantum、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Dmitry Malioutov 无法详细解释他构建的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Malioutov 是 IBM 的一名研究科学家，他的部分工作是开发能为 IBM 的企业客户解决所面临的难题的机器学习系统。其中一个项目是为一家大型保险公司建立的。这是一项极具挑战的任务，需要一个精密复杂的算法。然而当要向客户解释结果的时候，他却遇到了障碍。「我们无法向他们解释这个模型，因为他们并没有接受过机器学习的训练」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，即使他们是机器学习专家可能也没用。这是由于该模型是一个人工神经元网络，这个系统接受给定类型的输入数据——在这个案例中即保险公司的客户纪录——然后找出其中的特定模式。这种网络在实际中的应用已经有半个多世纪的历史了，但最近这个领域出现了突破性的进展——从语音识别和语言翻译到机器人围棋棋手和自动驾驶汽车。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4POvh803AVfjcFy9utNJBPAObJ68o7e4F7zyU3dyuMYNUics1vy9HhvQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;「隐藏」意义：在神经网络（neural networks）中，数据在层与层之间传递，并在每一步经历简单的转变。在输入层和输出层之间的是隐藏层（hidden layers），这些层中包含大量节点和连接，它们遵循着人类无法解释的模式，或者与输入输出层之间并没有明显的联系。「深度（Deep）」网络就是指那些包含很多隐藏层的网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它们有着极佳的性能，可产出激动人心的结果，但现代神经网络有一个麻烦的问题：没人能真正搞清楚它们是如何工作的。而这就意味着没人能预测到它们会在什么情况下失效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;举个例子，机器学习研究者 Rich Caruana 和他的同事最近报道了一个事件。匹兹堡大学医学中心有一个团队在用机器学习预测肺炎患者会不会发展出严重的并发症。他们的目标是把那些低并发症风险的病人送到门诊病房，以节省医院床位并减轻医务人员负担。这个团队尝试了好几种不同的方法，包括很多种神经元网络，以及能产生清晰可读的规则的软件生成的决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的神经网络比其他任何方法得到正确结论的次数都更多。但当这些研究人员和医生查看那些人类可读的规则时，他们发现了这样的描述：有一条规则指示医生把有哮喘的肺炎患者送回家，但谁都知道哮喘患者是很容易患并发症的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型所做的只是人类分配给它的任务：找出一个数据中存在的准确模式。它给出的糟糕建议是数据中的漏洞所致。因为医院的政策是把所有患哮喘的肺炎病人送入重症监护，而正因为这个政策的良好成效，几乎所有哮喘患者都不会患上并发症。如果没有改变了医院患者纪录的额外加护工作，结果会与现在有很大不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这则医院轶事说明了可解释性（interpretability）的实际价值所在。Caruana 及同事写道：「如果那个基于规则的系统 （rule-based system）能知道哮喘降低了风险，神经网络当然也会学习到这一点。」但是神经网络并不能被人类理解，而且它对于哮喘患者得出的怪异结论的原因也难以确认。如果不是有那个可解释的模型，Malioutov 警告说：「你可能真会害死人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是为什么很多人不愿意把赌注押在神经网络的不确定性上。当 Malioutov 把他那准确但不可解释的神经网络模型呈现给客户时，他同时也提供了一个基于规则的备选模型，这个模型的工作方式是可以用简单的术语进行沟通的。第二种可解释的模型其实没有第一种准确，但即便如此客户还是选择了第二种——虽然对一个具备高度数学复杂性的保险公司，准确性的每个百分点都很重要。Malioutov说：「他们更加认可它（第二种模型），他们真的非常注重直观。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;甚至政府都开始关注不可解释的神经网络预言日趋增加的影响力。欧盟最近提出的新法案要建立「解释权（right to explanation）」，它让市民有权要求算法决策的透明化。然而这项立法很可能是难以实行的，因为立法者并未明确界定「透明（transparency）」的含义。我们并不清楚这种疏忽是源于对问题的无知，还是由于立法者的确深知其中的复杂性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，有些人认为这种清晰的定义是不可能的。目前对于神经网络的运作方式，虽然我们已经知道了所有能知道的事情，但它们毕竟只是计算机程序，我们对它们工作的方式和原因所知甚少。这个网络是由许多（有时是几百万个）独立的单元组成的，它们叫做神经元（neuron）。每个神经元都会把多个数字输入转化成一个数字输出，然后再把它传递给另一个或很多个其它神经元。就像在大脑中一样，这些神经元被分成很多「层 （layer）」——一些可以获取下层的输入数据并把它们的输出传递给上层的细胞团。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络的训练通过数据的馈送进行，然后不断调整层级间的连接，直到网络计算出与已知输出（一般由很多子类组成）尽可能接近的输出为止。过去几年取得的惊人成果要归功于一系列新技术，它们使得训练在输入和最终输出之间有很多层级的深度网络成为可能。一种受欢迎的深度网络叫做 AlexNet，它被用来给照片归类，分类的标准是看照片中是否有狮子狗或者博美犬。它包含了六千多万个「权重（weight）」，神经元通过权重得知要对每个输入数据给予多少关注。「如果想理解这个网络，你必须要对这六千万个数字有一定的理解才行。」一位任职于康奈尔大学和 Geometric Intelligence 的计算机科学家 Jason Yosinski 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使有可能加强这种可解释性，也并不总能得到满意的结果。对于可解释性的需要也可以被看作是另一套约束条件，这会妨碍一个模型得到只与输入数据和给定的输出结果相关的「纯粹」结果，而且有可能会降低准确性。在今年早些时候的一次 DARPA 会议上，项目经理 David Gunning 用一个表格总结了相关的利弊，结果显示深度网络是现代技术中最不可理解的。而与之相对的技术是决策树（decision tree）——一种对解释的关注重于效率的基于规则的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4LDaL1NaB8Uibugjh12kAQcxzhia7qqsYcf0YXLURQKfB0x7fFic6khDFQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;WHAT VS. WHY：现代学习算法需要在人类可解释性（或可解读性）和准确性之间做出权衡。而深度学习是最准确的，同时也是最不可解释的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结果是现代机器学习系统给了我们一个选择：我们是想准确的知道会发生「什么」，还是想知道「为什么」会发生某件事，而这要以牺牲准确度为代价？「为什么」帮助我们制定决策、适应环境、以及了解模型何时可能崩溃。而「什么」则能帮助我们解决当下的实际问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个艰难的选择。但有些科学家希望能消除选择的需要——让我们在享受多层计算的成果的同时还能理解它。令人吃惊的是，有些最具前景的研究方向把神经网络视为实验对象——模仿了最初激励他们的生物科学——而非分析性的、纯数学的对象。比如 Yosinski 就表示他正尝试「像理解动物甚至理解人类那样」去理解深度网络。他和其他计算机科学家正致力于引进生物学研究中的技术来窥视网络深处，这模仿了神经科学家窥视大脑内部的方法：探查单个组成部分，纪录下它们如何响应输入中的微小改变，甚至移除部分碎片看其它成分如何补偿。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从零开始构建一个新的智能系统以后，现在科学家们又要把它拆开了，对这些虚拟有机体用了显微镜和解刨刀的数字等同物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Yosinski 坐在一台计算机终端前，对着一个网络摄像头侃侃而谈。网络摄像头的数据被传人一个深度神经元网络中，而该网络自身也正被实时分析着——用的是 Yosinski 和同事开发的叫做 Deep Visualization（深度可视化）的工具包。在屏幕上点击了几下， Yosinski 放大了网络中的一个神经元。在这次交互的视频录像中，他说道「这个神经元似乎会对脸部图像做出反应。」我们知道人类的大脑中也有这种神经元，许许多多这样的神经元聚集的区域被称为梭状回面部区（fusiform face area）。这个区域是在一项始于1992年的多研究项目中被发现的，是人类神经科学中最可靠的观测结果。但这些研究需要用到像正电子放射断层造影（positron emission tomography） 这种先进技术才能进行，Yosinski 仅通过代码就能窥视他的人造神经元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4JAp9OibGOKeVCZ6n2kzqXdmwZ104nrQx9iasFjicQiaicAKicicL4PCOptNxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;大脑活跃性：深度神经网格（绿色方框标记）中的一个神经元对 Yosinski 的脸作出响应，就像是人类大脑中那个对脸部作出可靠响应的特别部分一样（黄色强调）。左：来自 2015年国际机器学习大会（ICML）上的深度学习研讨会（ Deep Learning Workshop）中 Yosinski 等人的演讲： Understanding Neural Networks Through Deep Visualization。右：来自乔治城大学医学中心 Maximilian Riesenhuber。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种方法让他能把特定的人造神经元绘制成人类可理解的概念或对象，比如脸，这能帮助神经元网络转变成直观的工具。他的项目也能找出图片的哪一方面对面部神经元的刺激最显著。他说：「我们能看到如果有颜色更深的眼睛或更红的嘴唇，它们的响应会更强烈。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在杜克大学的计算机科学与电子和计算机工程教授 Cynthia Rudin 看来，这种「事后分析」的解释本质上是很有问题的。她的研究集中在构建基于规则的机器学习系统上，主要应用在像罪犯量刑和医学诊断这种人类可读解释是有可能而且也重要的领域中。但她认为，对于像视觉识别这样的问题，「人类对其结果的解释完全是主观的。」我们可以把网络响应简化成面部神经元的识别，但我们如何确定这就是它所寻找的呢？Rudin 的担心对应着那则著名的格言，也许并没有比视觉系统更简洁的模型，除了视觉模型本身。「对于一个复杂模型在做什么，你可以有很多解释，那你是不是就挑出那个你想要的，然后认为它是对的呢？」她说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Yosinski 的工具包能够部分反驳这些担忧，通过逆向工程的方式探索出网络自身「想要的」正确结果，这是一种理想的人为方法。这个项目从原始静态开始，进而一个像素接一个像素的调整，通过训练网络的逆向过程捣鼓图像。最终它找到一张能让给定神经元得到最大可能响应的图像。当将这种方法用于 AlexNet 神经元上时，它产生的计算机图像鬼魅般的、准确无误地产生了标记类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ibicjNDTfQoick4ica2CXrj27glAQj90iaUHrhxmLyHpw1nBhVib3Q8l563w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;理想化的猫：计算机合成的理想猫脸的例子，由 Deep Visualization 工具包产生。这些猫脸的产生是通过对初始图像住个像素的调整，直到 AlexNet 的面部神经元得到一个最大响应为止。来自 2015年国际机器学习大会（ICML）上的深度学习研讨会（ Deep Learning Workshop）中 Yosinski 等人的演讲： Understanding Neural Networks Through Deep Visualization。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在非常普遍的意义上，这似乎能支持他面部神经元确实是在寻找脸部图像的理论。但这里有个问题。为了生成这些图像， Yosinski 的程序依赖于一个统计学约束（叫做自然图像优先（natural image prior）），这会限定它产生出与真实世界物体类似的结果相匹配的图像。当他移除这些规则后，工具包仍旧会选定它标记为最大可信度的图像，但这个图像就是纯静态的了。事实上，Yosinski 在很多案例中都展示过这点，AlexNet 神经元更倾向于呈现给人类的图像绝大部分都是静态的。他很乐意承认「很容易搞清楚如何让这种网络说一些极端的话。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了避免这种陷阱，弗吉尼亚理工学院的一位电子和计算机工程助理教授 Dhruv Batra 采取了一种更高级别的试验性方法来破译深度网络。他并未试图在神经网络的内部结构中寻找模式——对此他辩驳说「因为比我更聪明的人已经在这样做了」——而是用一种机器人版本的眼跟踪技术来探索神经网络的工作机理。他的团队，在一个由研究生 Abhishek Das 和 Harsh Agrawal 带头的项目中，向深度网络提出关于图像的问题，比如给定的房间图片中的窗户上有没有窗帘。不像 AlexNet 或类似的系统，Das 的网络设计就是一次只关注图像上一块小区域。它在整个图像上移动虚拟眼睛，直到它认为获得了足够多回答问题的信息为止。经过充分的训练后，这种深度网络能取得很棒的表现，回答精确度与人类最佳水平相当。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后Das、Batra 及其同事尝试去发现网络如何通过调查选择查看的图片位置来做决策。他们的发现令人惊讶：在回答关于窗帘的问题时，网络甚至没有费心去找窗户。相反，它首先查看图像的底部，如果找到一张床则停止寻找。看来，在用来训练这个神经网络的数据集中，有窗帘的窗户可能会出现在卧室里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管这种方法确实揭示了一些深度网络的内部运作，但它也增加了可解释性方面的挑战。「机器所拾取的不是关于这个世界的真相，」Batra 说。「它们是有关数据集的真相。」机器与被供给的数据紧密调谐，这使得提取关于机器运行的一般规则变得困难。更重要的是，他警告说，如果你不知道它是如何工作的，你便不知道它会如何失败。而当它们真的失败了，以 Batra 的经验，「它们会败得壮观且丢脸。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类似 Yosinski 和 Batra 这样的研究人员所面临的一些障碍，对于研究人类大脑的科学家来说会比较熟悉。例如有关神经影像学的解释问题在今天已经非常常见，尽管还没有得到普遍重视。在 &amp;nbsp;2014 年的一篇回顾有关该领域的文章中，认知神经科学家 Martha Farah 写道：「忧虑在于……（功能型大脑/functional brain）图像更像是研究者的发明，而不是研究者的观察。」在非常不同的智能系统中出现的这些问题表明，它们可能会成为障碍。这不是根据对于这种或那种大脑的研究，而是根据对智能本身的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;探究可解释性是一件傻事吗？来自圣地亚哥加利福尼亚大学的 Zachary Lipton，质疑了解释神经网络的企图以及建立可解释型机器学习模型的价值。他给今年的国际机器学习大会（International Conference on Machine Learning /ICML）中一个关于人类可解释性（ Human Interpretability）的专题讨论会（由 Malioutov 及其两个同事组织）提交了一份煽动性的论文。该文标题为「有关模型可解释性的神话（The Mythos of Model Interpretability）」，这篇文章挑战了可解释性的定义以及研究人员寻找它的理由。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Lipton 指出许多学者不同意可解释性的概念，这让他意识到：可解释性要么并不存在——要么就有多种可能含义。他认为不应当信任这种解释冲动，而研究者应该使用神经网络来解放自己去「探索有野心的模型。」他说可解释性阻止了模型去充分发挥其全部潜力。他认为该领域的一个目的是「构建能从超过人类处理能力的更多特征中学习的模型。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这种能力既是特点也是缺陷：如果我们不了解网络输出是如何产生的，那么我们就不知道输入的哪些方面是必要的，或者甚至连什么可以被当做输入都不知道。案例：1996 年萨塞克斯大学的 Adrian Thompson 通过应用类似于今天那些训练深度网络的技术来使用软件设计电路。该电路是用来执行一个简单的任务：区分两个音频的音调。在对电路元件数千次的洗牌和重新安排之后，该软件找到了一个能够近乎完美地完成任务的配置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而 Thompson 惊讶地发现，该电路使用了比任何人类工程师所需要的更少的组件——包括几个没有与剩余部分进行物理连接的组件，但不知何故它们对于电路的正常工作来说仍然是必要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他着手解剖电路。经过几次实验，他了解到之前的成功是利用了相邻组件之间微妙的电磁干扰。未连接的组件是通过引起局部电场的小波动来影响电路。人类工程师通常会防范这些干扰，因为它们不可预测。果然，当 Thompson 将相同的电路布局拷贝到另一批组件中时——或者甚至改变了环境温度——它彻底失败了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该电路表现出了训练后的机器的一个标志性特征：它们可以非常紧凑和简化，非常适合其环境——但不适应任何其他环境。它们能够获取工程师看不见的模式；但不知道哪一个模式在别的任何地方都不存在。机器学习的研究人员竭尽全力去避免这种被称为「过拟合（overfitting）」的现象，但是这些算法正被应用于越来越多的动态情况中，其脆弱性会不可避免地被暴露出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于普林斯顿大学的计算机科学教授 Sanjeev Arora 来说，这种现象是他寻找那些允许人类干预和调整网络的可解释性模型的主要动机。Arora 指出了两个可能代表不可理解型机器的硬性限制问题。一个是「可组合性（composability）」——当手头的任务涉及到许多不同决策（比如有关围棋或自动驾驶汽车）时，网络无法有效地学习哪个决定导致了失败。「通常当我们设计东西时，我们了解不同的组件然后把它们组装在一起，」他说，这允许人类调整那些不适合给定环境的组件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个使得可解释性悬而未决的难题被 Arora 称为「域适应性（domain adaptability）」——从一种环境到另一种环境灵活应用知识的能力。这个任务对于人类学习者来说可以做得很好，而机器却会以令人惊讶的方式失败。Arora 描述了程序是如何灾难性地无法适应于那些甚至是微妙的环境变化，而人类则可以轻松处理。例如一个被训练的通过阅读正式文件（如维基百科）来分析人类语言的网络，会在更口语化的语境（如 Twitter）中全盘失效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以这种观点来看，可解释性看起来似乎很重要。但我们明白自己所说的话是什么意思吗？先驱计算机科学家 Marvin Minsky 创造了「suitcase word（手提箱词）」一词来描述许多术语——比如「意识」或者「情绪」——当我们谈论自己的智能时会使用它们。他提出，这些词反映了许多被锁在「手提箱」中的不同潜在过程的工作原理。只要我们继续调查这些词汇，代替那些更加基本的概念，那么争论就会过去，我们的见解将被我们的语言所限制。在有关智能的研究中，「可解读性（interpretability）」本身也可能是这样一个手提箱单词吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然与我谈话的许多研究人员都乐观地认为，总有一天理论家们会打开行李箱并发现一套类似于牛顿定律那样单一、统一的原则或定律，主宰机器学习（可能也包括人类学习），但是其他人告诫到没有理由做这样的期望。纽约城市大学的哲学教授 Massimo Pigliucci 提醒道，自然科学以及扩展到人工智能的「理解（understanding）」——可能会成为 Ludwig Wittgenstein （先于Minsky）所说的一个「集群概念（cluster concept）」，它承认许多（部分有区别的）定义。他说，如果该领域中的「理解（understanding）」确实实现了，则它可能不会在物理学中被发现，而是在进化生物学中。他说我们或许更期待物种起源而非统一定律。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然这并不意味着深度网络预示着某一种新的自主生命体。但它们可能会像生命一样难以理解。该领域越来越多的实验方法及事后解释，可能不是某种身处黑暗渴望理论之光的绝望感。相反它们可能会是我们所能期待的唯一光明。可解释性可能会逐渐作为一组按照（生物）分类学排列的「物种」原型实例出现，其由推理所定义，并取决于特定语境的解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 &amp;nbsp;ICML 的专题讨论会结束时，一些出现在舞台上的主持人尝试去定义「可解释性」。下面的响应声与辩论小组的数量一样多。一些讨论过后，各小组似乎找到了共识——「简洁性（simplicity）」对可解释型模型来说是必要的。但是在为简洁性下定义时，小组又出现了分歧。「最简单的」那个模型是不是依赖于最少特征的那一个？是不是得到最大差异的那一个？是不是程序体积最小的那一个？专题讨论会结束时没有给出一个统一答案，那个未完成的概念定义被替换成了另一个。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如 Malioutov 所说的那样：「简洁性并不简单。」&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌技术论文：用于YouTube推荐的深度神经网络</title>
      <link>http://www.iwgc.cn/link/2565652</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 arxiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Paul Covington, Jay Adams, Emre Sargin&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：孙宇辰、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4H7t4c8sOWwHv0cPIs07IdhrLXNRlZpicibV6t8fKEjkIaADUiaxzia103g/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;YouTube 所使用的推荐系统是现在最大规模的、最先进的业界的推荐系统之一。在这篇论文中，我们在较高层面上描述这个系统，并重点关注了深度学习所带来的巨大的性能提升。本论文根据典型的两阶段信息检索的二分法（two-stage information retrieval dichotomy）分为两部分：首先，我们详细描述了一种深度候选生成模型（deep candidate generation model），接着描述了一种分离的深度排名模型（deep ranking model）。通过设计、迭代、维护一个带有巨量面向用户的影响的巨型推荐系统，我们还提供了实用的经验教训和见解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;系统概述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的推荐系统的整体结构如图 2 所示。系统由两个神经网络组成：一个用于候选生成，一个用于排名。其中候选生成网络从用户的 YouTube 活动历史中提取事件作为输入，然后从一个大的视频库中检索出一个小数据集（上百个视频）。这些候选被认为通常与用户有很精准的相关性。这个候选生成网络仅通过协同过滤（collaborative filtering）提供广泛的个性化。用户之间的相似性可以通过粗粒度特征（例如视频观看的 ID、搜索查询单词以及人口特征统计）表达。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个推荐列表中出现的一些「最好」的推荐需要一种良好的表征，以在具有高召回率（recall）的候选集中区分相对的重要性。排名网络通过使用一个描述视频与用户的特征集合的期望目标函数来给每个视频打分，从而完成排名的任务。根据它们的得分，然后将最高分的视频展现给用户。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两阶段的推荐方法允许我们从一个很大（数百万）的语料库中进行推荐，与此同时还仍有在设备上出现的少量视频是个性化的吸引用户的内容。此外，这个设计能够和其他源生成的候选进行混合，例如在这一项早期工作[3]中描述的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在开发过程中，我们广泛地使用了非网络的指标（准确度、召回率、ranking loss）来引导我们的系统的迭代改进。然而，为了最终测定一个算法或模型的效果，我们依靠于通过实时实验进行 A/B 测试。在一个实时实验中，我们能度量在点击率、观看时间与许多度量用户参与度的指标中不易察觉的变化。这是非常重要的，因为实时 A/B 测试结果不总是与离线实验有相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4f2kibHohjCC7rKN5LS4ZRtNzZfCl6MNsMhlN322OyRFDasbOJp1mp9Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：推荐系统架构：候选视频通过「漏斗」状的流程从大量视频中被检索出来并进行排名，然后再将其中一小部分展示给用户。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4csbBJMLIia77uosYRgLneeGLpKiamH34P7iauLKKb0zFcQ6mVtHxOrMmg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图3：深度候选生成模型架构：嵌入的稀疏特征是和稠密特征连结在一起的。在级联（concatenation）将可变大小的稀疏 ID 转换成适合隐藏层输入的固定宽度的向量之前，嵌入被取了平均。所有隐藏层是全连接的。在训练中，使用取样的 softmax 的输出之上的梯度下降对交叉熵损失进行最小化。在服务中，用一个近似最近邻（approximate nearest neighbor ）查询生成数以百计的候选视频推荐。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ILKh0lwaoIibeiaFNoviawOBiaoFVhM26ia3Cvl5K8Oy51nGRQAecWj73kg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图4：对于一个给定的视频，模型用样本年龄（example age）作为一个特征训练，能够精准表达出数据中的上传时间和依赖时间的受欢迎程度。如果没有这一特征，该模型会在训练窗口近似地预测平均似然（ average likelihood）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4mfhNjC7w9F0BLSw8P1PKicMFnhG7Ax5DlMHz2SI0EYfsQFVDkrNibpmA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图5：给模型选择标签和输入上下文对离线评估来说很有挑战性，但是对实时性能有巨大的影响。如图，实心圆点•是网络的输入特征，空心圆点◦是被去除的。我们发现在 A/B 测试上预测未来观看（5b）的表现更好。如 5b 所示，样本年龄表示为 tmax − tN，其中 tmax 是训练数据中的最大观测时间。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf43LDnEpibjdQ5kRyASjauX16RcPK1btTUgFxYt0GSFAueNMkbqJ9dHtA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图7：描绘了嵌入的分类特征（包括一价特征和多价特征）的深度排名系统架构，这些特征带有共享的嵌入和规范化的连续特征的乘幂。所有层都是全连接的。在实践中，需要给网络馈送数百个特征。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们描述了我们用于推荐 YouTube 视频的深度神经网络架构，划分为两个不同的问题：候选生成与排名。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的深度协同过滤模型能够吸收很多信号并使用深度的层对它们的交互进行建模，其性能优于 YouTube 原来使用的矩阵分解方法。比起科学，选择推荐的代理问题（surrogate problem）更像是一门艺术；而且我们发现通过获取不对称的联合观看行为（co-watch behavior ）和预防未来信息的泄露，对未来观看的分类可以在实时评估中表现良好。抑制来自分类器的判别信号也是获得好的结果的关键，否则模型将会对代理问题过拟合，不能很好地转换到主页。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们发现使用训练样本的年龄作为输入特征，移除了相对于过去的固有偏差（bias），并允许模型表达受欢迎视频的时间依赖行为。这种改进的离线保持了精确率，同时在 A/B 测试中显著地增加了最近上传视频的观看时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;排名是更经典的机器学习问题，但是我们深度学习方法在性能上超过了之前对观看时间预测的线性与基于树的方法。推荐系统尤其受益于用户过去和事物之间的行为这样专门的特征。深度神经网络需要对类别和连续特征的特殊表征，我们对其分别使用嵌入与分位数标准化（quantile normalization）进行变换。我们发现深度的层可以有效地对数百个特征的非线性交互建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;逻辑回归（Logistic regression）根据给训练样本赋予权重进行修改，其中给观看时间正样本，没有观看的是负样本，从而让我们可以学习接近模型预期观看时间的几率。这种方式相比于直接预测点击率，可以在观看时间权重排名评估指标上表现得远远更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击「阅读原文」，下载此论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>特稿 | 如何让深度学习突破数据瓶颈？这家创业公司直接挑战生物神经元的计算模型</title>
      <link>http://www.iwgc.cn/link/2555446</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Feature Article&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL2USVJONEbWmpLUZ0fzvfhjDfyg7mBErykt3erfsI37ubXWLJDbP6mw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;Demiurge Technologies 是一家位于瑞士的人工智能创业公司，他们致力于研究生物神经元的计算原理，开发下一代深度学习，以解决小样本学习和与物理世界交互的难题。他们的深度学习系统将应用于第四级别自动驾驶和探索机器人等领域。与大部分人工智能公司不同的是，&lt;span&gt;Demiurge Technologies 希望从根源解决目前深度学习存在的问题，面对这样一个不论在神经科学领域，还是人工智能领域都同样重要的问题，他们的勇气、方法和视野都令人尊敬。也希望 &lt;span&gt;Demiurge 的创业思路和运作模式能够给从业者带来灵感和启发。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文分为四大部分，共 &lt;span&gt;11,483 &lt;/span&gt;字，阅读预计 &lt;span&gt;20&lt;/span&gt; 分钟&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一、基于生物神经元的下一代深度学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;二、游戏规则制定者&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三、自动驾驶&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;四、宏大目标下的生存追问&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;瑞士，有着覆盖国土面积 60% 的阿尔卑斯山脉和超过 1,500 个湖泊，玛丽·雪莱所著的西方文学史上首部科幻小说&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400276879&amp;amp;idx=1&amp;amp;sn=7b6c18c6c6e0be66d7feb9b9cee27048#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400276879&amp;amp;idx=1&amp;amp;sn=7b6c18c6c6e0be66d7feb9b9cee27048#wechat_redirect"&gt;《弗兰肯斯坦》&lt;/a&gt;就诞生于日内瓦湖畔，它讲述了一位天才科学家从零到一创造出智能生命体的故事，成为此后 200 年间讨论人类与机器、生命与智能的哲学模板。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge Technologies 也是一家希望从生命中获得线索并以此来开发通用人工智能的创业公司，位于瑞士一个依山傍湖的小镇——静谧但充满力量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司的办公地点高度保密，从不接受无关访客，即便是新员工面试也会先安排在其他地方，在正式录用后才会被邀请来公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「公司的物理空间是精心打造的文化载体，我们利用环境甄别对的人，依靠对的人进一步强化环境的凝聚力。」Demiurge 联合创始人兼 CEO 刘思宜（Idonae Lovetrue）表示。从这个看似有些神秘的规定中足以看出一种他们对实现通用智能所需人才和文化的独到视角及判断标准，令人心生敬畏，以至于在去公司的路上我忍不住问她「我要不要带个头套？」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司院落的历史要远远超过人工智能的历史，除了 Xbox 等科技公司里常见的娱乐设备之外，他们的员工还有一项令人羡慕的放松方式——经过后院一个 100 英尺长的木码头去湖里划船，Idonae 说：「我们鼓励大家在湖中思考和放松。这里如同梭罗笔下的瓦尔登湖，我们切身感悟亲近自然，必然会汲取沉静的力量，发现更本质的自然规律。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 可能拥有全世界最美的办公地点，像极了托尔金笔下描绘的霍比特人的家园。Idonae 坚定的认为「我们就是一群霍比特人，以无畏严谨的脚步去追逐梦想（实现通用人工智能）。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于生物神经元的下一代深度学习&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「虽然目前的深度学习在语音识别和图像识别方面取得了突破性进步，但如果把深度学习用于绝大多数的其他领域，比如说自动驾驶、实体机器人等，就会面临一个来自于真实世界的非常大的挑战，那就是训练数据量严重不足。」Demiurge 联合创始人、CTO 任志攀（Bragi Lovetrue）表示。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLLNfSSD0CKIcgX1bFZD2wJEjfTJg2b9wpQOHvvZr3rU7zvzA5MlRUgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em&gt;&lt;span&gt;人工智能不同应用场景的数据需求和数据供给对比，图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拿开发消费级别的全自动驾驶来说，最大挑战在于要开发出在交通事故的预判和预防上远超人类驾驶员的软件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果用现有的深度学习去实现这一点，那就需要大量的事故数据，但这方面的数据供给非常有限，而采集数据又难度很大。首先，没有人能够准确预测何时何地会发生何种事故，因此无法系统地提前部署以采集真实事故数据；其次，从法律上来说我们不能靠人为制造事故来采集数据；第三，也无法模拟数据，因为事故更多涉及实时的传感以及与物理世界的互动，模拟出来的数据与真实数据差距很大，这从 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400004983&amp;amp;idx=3&amp;amp;sn=1b719d273cac2a9daee46019e9691b74&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400004983&amp;amp;idx=3&amp;amp;sn=1b719d273cac2a9daee46019e9691b74&amp;amp;scene=21#wechat_redirect"&gt;DARPA 机器人挑战赛&lt;/a&gt;就能看出来；最后，像 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402117079&amp;amp;idx=1&amp;amp;sn=00acc2259bac0536471620c365988179&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402117079&amp;amp;idx=1&amp;amp;sn=00acc2259bac0536471620c365988179&amp;amp;scene=21#wechat_redirect"&gt;AlphaGo&lt;/a&gt; 那样，在规则定义明确的简单环境下自行创造大量训练数据的方式，在复杂的真实环境中难以发挥作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果遇到数据量不足的情况，同时又很难通过之前那些行之有效的方式去增加数据供给，那就无法发挥出深度学习的优势。而更重要的是，我们还会遇到数据类型不一样的问题，物理世界中是不同传感器获取的实时数据流，而现在深度学习在信息世界中的应用，比如说图像识别，使用的数据都是基于图片的数据点，而非数据流，所以这也是将深度学习现有的成功延伸到真实物理世界应用的一个底层障碍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于这个原因，Demiurge 专注于开发一种系统方法从源头解决真实世界诸多领域中数据量严重不足的问题——既然很难有效增加数据供给，为何不设法大幅降低对数据的需求？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;降低对数据量的需求、实现小样本学习甚至 one-shot learning，是目前深度学习研究中的关键问题，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717572&amp;amp;idx=1&amp;amp;sn=8265c0994d752907c5b99d40a1cd2e4b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717572&amp;amp;idx=1&amp;amp;sn=8265c0994d752907c5b99d40a1cd2e4b&amp;amp;scene=21#wechat_redirect"&gt;Yann LeCun&lt;/a&gt;、 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716884&amp;amp;idx=1&amp;amp;sn=7ab39a8590cddae0d8b8247df5a29b20&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716884&amp;amp;idx=1&amp;amp;sn=7ab39a8590cddae0d8b8247df5a29b20&amp;amp;scene=21#wechat_redirect"&gt;Yoshua Bengio&lt;/a&gt; 等深度学习专家也多次在演讲中提到解决深度学习中 one-shot learning 问题的重要性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在今年斯德哥尔摩的全球机器人顶级学术会议 ICRA 上，Bragi 在 Industry Forum 演讲中介绍了 Demiurge 的方法，从神经科学里寻找关键线索，「&lt;strong&gt;比起深度学习的点神经元，生物神经元所擅长的是从多模的实时数据流中提取多维度的时空信息来实现 one-shot learning，这是现有的深度学习很难做到的。生物神经元不仅能够做这种特征提取，而且是以一种非常高效的方式，效果和效率都很出色。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度神经网络的确从神经科学领域的研究中获取了一些灵感，但其工作原理与人脑截然不同（诚然，我们对大脑的工作原理还没有弄清楚），Yann LeCun 表示，他最不喜欢的对深度学习的定义就是「它像我们的大脑」，谷歌 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717709&amp;amp;idx=1&amp;amp;sn=1edfa7e52ee07d65d11e60c77dbbd74a&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717709&amp;amp;idx=1&amp;amp;sn=1edfa7e52ee07d65d11e60c77dbbd74a&amp;amp;scene=21#wechat_redirect"&gt;Jeff Dean&lt;/a&gt; 认为深度神经网络是对大脑神经网络的简单抽象，并非是模拟人类神经元如何工作。神经科学专注的点包括计算的细节实现，还有对神经编码以及神经回路的研究。然而，在机器学习领域，人工神经网络则倾向于避免出现这些，而是往往使用简单和相对统一的初始结构，以支持成本函数（cost funcion）的蛮力最优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 从历史的角度分析了深度学习和神经科学的关系，「现在的深度学习从神经科学中获得的灵感非常有限，这是因为深度学习的理论基础是上世纪 80 年代基本定型的，那时之前的神经科学也发展比较慢，无法为深度学习提供更多灵感。而从 80 年代至今，神经科学的发展速度远远超过了之前，过去 30 年产生的神经科学知识是 80 年代以前的 46 倍，而且现在每年神经科学获得新发现的速度是 80 年代以前的 100 倍。所以，对于深度学习来说，如今的神经科学已经是一个非常巨大的宝库，为提升现有深度学习的学习能力提供重要线索。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 表示，越来越多的深度学习专家开始研究如何从神经科学中获取更多的线索，「 Yoshua Bengio 做的非常前沿，一方面研究深度学习的反向传播算法在生物神经元上是如何实现的，另一方面研究生物神经元的 STDP 学习算法如何提升现有的深度神经网络的学习能力 。位于深度学习与神经科学交汇的最前沿，我们很深刻地体会到现在正在发生着的转型，从深度学习和神经科学没有太大关系的这一代（深度学习1.0），过度到深度学习重新从神经科学获得重要启发的下一代（深度学习 2.0 ）。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTJr5lEmwciaTImJge1IUicCO11HRWwBXjEGHM1d4hFIo8SbkrK5Yg7Eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;深度学习 2.0 ，图片由来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在近期谷歌 DeepMind 和 MIT 媒体实验室的合著论文&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716140&amp;amp;idx=1&amp;amp;sn=3d74ee2545c20cba8189d445202f6f31&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716140&amp;amp;idx=1&amp;amp;sn=3d74ee2545c20cba8189d445202f6f31&amp;amp;scene=21#wechat_redirect"&gt;《Towards an integration of deep learning and neuroscience》&lt;/a&gt;中提到，近期出现的结构化、成本函数和训练程度的复杂化这两项机器学习方面的进展或许会将神经科学和机器学习两个研究领域看似不同的视角连接起来。此外，硬件方面，IBM Zurich 在 8 月首次用低成本高性能的相变材料实现了生物神经元计算的关键机制——&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717770&amp;amp;idx=1&amp;amp;sn=9f33fc6a5a3cabd55f26bd31e871769d&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717770&amp;amp;idx=1&amp;amp;sn=9f33fc6a5a3cabd55f26bd31e871769d&amp;amp;scene=21#wechat_redirect"&gt;神经薄膜&lt;/a&gt; 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更重要的是，面向物理世界的移动人工智能的各种应用需求（识别、避障、抓取等），与各类生物在物理环境的各种生存需求是高度吻合的。Bragi 表示，斯坦福大学人工智能实验室主任李飞飞教授就特别重视深度学习在机器人上的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生物神经元，经过上亿年的演化，是自然找到的最优解决方案。对于 Demiurge 来说，理解生物神经元的计算模型是找到降低数据需求的通用算法，开发通用移动人工智能核心技术的关键。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这与其他解决数据量不足的思路有着本质不同。「比如说 UC Berkely 的 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402669894&amp;amp;idx=3&amp;amp;sn=be901c55ed7118c37be520853d3752a1&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402669894&amp;amp;idx=3&amp;amp;sn=be901c55ed7118c37be520853d3752a1&amp;amp;scene=21#wechat_redirect"&gt;Pieter Abbeel&lt;/a&gt; 和 Google 的 Sergey Levine ，他们都是在用深度强化学习来开发基于自我监督学习（self-supervised learning）的通用算法，但这种自动的数据收集和标记本质上依然是增加数据供给。此外，NYU 的 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401148927&amp;amp;idx=1&amp;amp;sn=71dfd63b7e2f56f28c77db1107e9e74f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401148927&amp;amp;idx=1&amp;amp;sn=71dfd63b7e2f56f28c77db1107e9e74f&amp;amp;scene=21#wechat_redirect"&gt;Brenden Lake&lt;/a&gt; 等用贝叶斯程序学习的方法针对特定问题开发出专门的数学模型。虽然能够在特定任务中大幅降低了数据需求，实现了 one-shot learning , 但这不是通用方法，」Bragi 说，「实际应用中需要的是降低数据需求的通用方法，深度学习的通用性无疑是最佳的。对于深度学习来说，如果不从生物神经元原理入手的话，是很难解决这些问题的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 表示，目前深度神经网络与生物神经网络最本质的区别在于神经元的类型。目前深度神经网络用的是点神经元，其计算模型是把信号加权平均的结果输入到一个非线性函数。这种点神经元是对生物神经元的极度简化，没有基于时间的变量。而生物神经元则利用脉冲进行基于多维时空变量的计算。单个生物神经元的计算模型是神经科学领域的一个关键问题，而这个问题的答案正是设计下一代深度学习的关键线索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 就是完全专注在这个问题上，所做的结果也走在世界最前沿，「我们已经开发出了生物神经元的计算模型，现在正在非常严谨的对它进行测试。 」Bragi 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLpBLmhEtLoSIWWIvFzlesvbJeozhTXHpfeJk9qDy9sAtiamJM2VXp1HA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;点神经元和脉冲神经元，图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型的关键在于理解脉冲如何以非常少量的计算步骤和能耗能够准确抓取极高维度的时空信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「没有基于脉冲的计算模型—仅仅像 IBM TrueNorth 那样，简单模仿一些生物神经元的硬件特点，或者像 Numenta 和 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=3&amp;amp;sn=f7438d205d1ec70454134d59802a3326&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=3&amp;amp;sn=f7438d205d1ec70454134d59802a3326&amp;amp;scene=21#wechat_redirect"&gt;Vicarious&lt;/a&gt; 的 HTM（Hierarchical Temporal Memory）那样，简单借鉴一些生物神经元的软件特点—消费级别大脑芯片的硬件开发也就无从谈起。 对于实现生物神经元计算模型的软硬件要求的掌握，是 Demiurge 最重要的核心优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种计算模型的提出是多尺度跨领域研发的结果，不仅需要对跨领域的基础理论和前沿算法进行研究，还要从应用角度来分析真实世界的需求和需要满足的限制，来缩小算法搜索的空间。因为真实世界中有很多限制，比如说提供的数据量非常少，但为了应用成功或者让物种生存，就必须快速学习来了解整个环境，而在整个过程中又不能耗能太多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生物智能给 Demiurge 提供了非常重要的线索，他们从跨物种的通用智能系统出发，理解要满足什么样的条件才能最大化它们的生存，这是从生存追问的一种智能系统设计的思路。不管设计出何种模型，都要满足这些限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前深度学习领域从实验室结果到产品级应用的演化进程，对于真实世界的诸多限制一开始是尽量回避的，即首先选择那些可以不太涉及物理限制的简单场景，尽力实现在该场景下深度神经网络的最优化表现后，再开始逐条考虑开发应用时必须面对的各种物理限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这种演化可能适合学术研究，但不适合产品研发。Demiurge 的研发从一开始就充分考虑真实世界应用的所有限制，开发出来的计算模型和大脑芯片能在真实世界的各种限制条件下完成出色稳定的应用表现。 」Idonae 进一步解释了这背后的决策依据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于采用了同时满足技术突破和应用表现的双重评估标准，Demiurge 的研发风格是极为大胆和严谨的。提出的计算模型首先要在从数学理论上完整论证，同时还要用神经科学最新的发现和数据去做验证。这部分数学理论与神经科学的验证之后， Demiurge 会开始软件的模拟和硬件的实施，最终把自动驾驶作为首个测试平台，通过实现第四级别的无人驾驶测试他们的大脑芯片产品在对交通事故的学习、预判和预防的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 虽然是从神经科学中寻找深度学习突破的密码，但他们所做的技术依然可以称之为深度学习，最终的产品形态也是利用深度神经网络，也利用很多的隐含层和反向传播算法，只不过是将深度神经网络中的点神经元替换成了脉冲神经元，是计算单元的区别，在整个计算架构上区别很少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，可以在充分利用了生物神经元优势的情况下同时还继承了这代深度学习的所有优势，比如说具有通用性，以及从训练的角度上是 model-free ，这依然是一个以数据和经验来驱动的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 说：「我们和 DeepMind 、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716227&amp;amp;idx=1&amp;amp;sn=ad2807981602ddabff37d235aa6d2810&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716227&amp;amp;idx=1&amp;amp;sn=ad2807981602ddabff37d235aa6d2810&amp;amp;scene=21#wechat_redirect"&gt;OpenAI&lt;/a&gt; 等最大的区别是，我们很清楚脉冲神经网络在感知数据流计算上的巨大优势，并知道如何从软件上和硬件上实现它。对这一代深度学习来说，正如 Google 资深研究员 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect"&gt;Greg Corrado&lt;/a&gt; 在 Brain Forum 上所说，他们尚不清楚如何利用脉冲进行计算，在算法层面和应用层面发挥脉冲的优势&amp;nbsp; 。我们与 IBM 区别是，IBM 的最新突破用 GST 相变材料首次完整第实现了单一神经薄膜，这是基于对生物神经元物理性质的深入理解与再现，但要开发应用于物理世界的大脑芯片， 仅靠复制生物神经元的物理性质是不够的，根本上仍然需要对生物神经元计算原理的掌握，后者是 Demiurge 的核心优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;游戏规则制定者&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;理解生物神经元的脉冲计算原理，是神经科学领域的世界级难题，同时对人工智能界的下一次突破也意义重大，面对这样一个不论是从科研还是从应用上都将带来巨大价值的命题，Demiurge 作为一个资源有限的创业公司是如何做到的？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「不应该是艾伦实验室、索尔科研究所、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=403048122&amp;amp;idx=2&amp;amp;sn=1baead76eee66f84f989c7d4d2f0dea4&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=403048122&amp;amp;idx=2&amp;amp;sn=1baead76eee66f84f989c7d4d2f0dea4&amp;amp;scene=21#wechat_redirect"&gt;HBP&lt;/a&gt; 等世界级脑科学研究机构，或者谷歌、Facebook 等科技巨头才有动力和能力去解决这样一个世界难题吗？」我非常直接的向 Bragi 询问。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「就像阿基米德那个用杠杆去撬动地球的比喻，对于撬动这个世界级难题来说（理解生物神经元的脉冲计算原理），有很多不同支点（探索方法）可供选择。比如说各国脑计划的研究重点主要集中在提高探测设备和研究手段，使得我们能够尽可能收集从局部到全部、从单个时间点到更大时间尺度上的尽可能多的关于神经元的数据，他们大多是从收集数据的角度来努力。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「艾伦研究所在这方面做了很多贡献，不仅提供了系统化数据收集的标准和数据收集的设备，同时还把收集上来的数据加以整理并免费开放，他们的思路是，更多的数据可能会帮助我们最终解决算法的问题，这是大数据驱动的对算法的理解。而欧盟脑计划（HBP）的思路不一样，他们认为，即便是收集足够多的数据，但缺少模拟的过程，对数据的利用效率也不够高，所以他们特别强调建立一个全尺度、高精度的虚拟大脑，这样就能保证在虚拟大脑里重现已经观察到的大脑的现象和特征，从而让我们更加准确的提出测试各类神经元的计算模型，这也是从蓝脑计划到欧盟人脑计划的一个重点。」Bragi 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;Idonae 补充到：&lt;/span&gt;「以上这些研究更多的是提供了基础设施，支点都离问题比较远，而不是直接去解决这个问题。而 Demiurge 选择了最近的支点（完全专注于单个生物神经元），并打造出了最长的杠杆（提出了通用的脉冲计算模型），所以能够以有限的资源撬动无限的潜力。」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;她认为「下一代深度学习是一个底层应用问题，不是一个表层应用问题。底层问题则需要对多领域深入的理解和灵感来寻求突破，还需要对应用核心痛点的深入理解，所以预测和管理更具挑战性。而表层问题可以用循序渐进改良的方式推动，产出和时间相对容易预测。在学术界和大企业机构，相关评审机制的设计和运作有利于解决表层应用问题，但对解决底层应用问题的机制缺乏动力和经验。因此在解决底层应用问题上，Demiurge 量身打造的文化制度和评审机制就会显示出独特的优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTx3W9DEHDOaz6REwxicy1cicftyC4TuAAFsm9uSaur5DZ2iboib1G1qpqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Demiurge 联合创始人、CEO Idonae Lovetrue 在 TEDx Hochschule Luzern 演讲，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此前的 TED 演讲上，Idonae 也提到了 Demiurge 相对于科技巨头的优势，「大企业显然有多种优势：充足的资源、雄厚的财力和强大的网路，但开发应用于物理世界的人工智能最重要的事情是生&lt;/span&gt;&lt;span&gt;存本能，但这与大企业的属性相悖，企业一旦做大，保持其生存本能就会极其困难。但生存却是创业的一切，并且它在每个人的血管中流淌，我何时何地都能感受到它。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「所以，Demiurge 为自己创造了一个非常独特的位置，掌握了一个从科学的利益和动力、产品的利益和动力的完美契合点，」Bragi 表示，「单个神经元计算模型这个问题既是从深度学习应用需求来说必须要解决的根本问题，同时也是神经科学领域一个诺贝尔奖级别的问题。比起学术界，Demiurge 离应用最近，可以获得一些额外的关键启发和应用场景下的限制条件， 更有能力去做这个事情。比起工业界，Demiurge 离科学最近，能够非常专注地去彻底解决应用的底层问题，更有定力去完成这个事情。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而恰恰是因为 Demiurge 所坚持的这个目标也是神经科学家一直以来的终极目标，所以神经科学领域的顶尖机构和专家非常支持他们，为他们提供研究成果、数据和人才。所以从这方面来说，Demiurge 和神经科学领域的大机构是一种合作关系，而非直接的竞争关系，而这种合作关系也是平等的优势互补，这些科学家不是在通常情况下的单方面付出，而是可以获得反馈。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「&lt;strong&gt;我们就像魔戒远征队，而那些在神经科学领域已走得最远的专家就像是精灵族的长老，用他们的的知识和智慧来帮助我们。&lt;/strong&gt;」Idonae 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年 5 月份举办的世界顶级神经科学会议 Brain Forum 也邀请了 Demiurge ，他们的展台就在神经科学巨无霸项目「欧盟人脑计划（HBP）」的旁边，Brain Forum CEO Jamil El-Imad 对 Demiurge 赞赏有加，他在大会开始前的媒体见面上唯一介绍的一家创业公司就是 Demiurge ，「在所有的创业公司中，Demiurge 的视野（vision）与我内心所想最为接近。」Jamil 在私下交流时说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就连不可一世的欧盟人脑计划和瑞士蓝脑计划的负责人、全球顶级神经学家 Henry Markram 也在一直积极且慷慨为 Demiurge 提供支持——将他了解的科学家与他的学生介绍给 Demiurge 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLJSlzHqRn5ITXNJn32s1Vtl7uBf3wGyUcVUqM2oHzQ4yEdEicdwgDV6w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Brain Forum 会场，欧洲脑计划（HBP）发起人 Henry Markram 和 Idan Segev与 Demiurge 创始人 Idonae 和 Bragi 交流，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这次远征中守护他们前行的不仅有科学家，还有投资人，Demiurge 目前完成了两轮投资，投资人包括对技术有着极强前瞻性的投资机构和知名企业家。Idonae 说：「深度学习是一个生态系统，不仅跨越了很多应用，还突破了传统应用和研发之间的壁垒，我们需要很多背景去了解和应对这个全新的生态系统，投资人的经验和视野在这方面可以给我们很多帮助。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;各类人才的汇集让 Idonae 充满信心，「我们只要坚持走在解决这个问题的路径上，最适合的人会一个一个陆续登场，而每个人都必然是在相关领域深耕良久，因为只有有了很深的积累之后才有足够的眼光看到我们解决这个问题的必然性。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而对于 Demiurge 来说，他们不仅希望自己创造的这套新的游戏规则能够帮助他们解决具体问题，还希望这个规则本身可以为后来者提供一种史无前例的参照。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们在创造一个先例，从来没有人说过创业公司不可以通过解决一个诺贝尔奖级别的问题来直接开发出堪比互联网基础的人工智能技术，只不过是很少有人有勇气做这方面尝试，而我们非常清楚我们的目标是什么，我们存在的意义是什么。我们希望自己是启发性的，也可以让后面的人有一个新的参考体系。」Idonae 表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「为什么是我们？我们对现有的游戏规则很了解，并且非常清楚做到什么程度才算是真正的成功，」Idonae说，「通用人工智能的成功标准，高于在 ImageNet 竞赛中取得高分，高于实现完全的自动驾驶，而是能够实现人人可居的智慧城市，人人可获益的地外探索。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;自动驾驶&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 基于生物神经元计算模型所提出的下一代深度学习及相关的软硬件平台， 可以做到高性能、低成本的解决小样本学习和自适应学习等人工智能在真实世界中所面临的诸多问题。从目前来看，这项技术最直接、也是最有市场需求的应用就是自动驾驶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 和 Idonae 五月份的行程非常密集，他们需要去瑞士中部的卢塞恩进行 TED 演讲，然后当天赶到西南部城市洛桑参加 Brain Froum ，会议结束后再返回公司。Bragi 驾驶着一辆 Model S 在四天里行驶了超过 800 英里，沿途再美的风景也会屈服于驾驶员的时间成本和精力消耗，这也是所有人期待自动驾驶早日实现并积极参与其中的原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 20 世纪 80 年代卡耐基梅隆大学的 Navlab 计划，到谷歌自动驾驶项目，再到如今所有相关公司的强势布局，众多参与者都走在追求这个终极目标的路上，每个参与者都会基于自己的优势规划发展路径，神秘的自动驾驶创业公司 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=3&amp;amp;sn=79401d6c07b5ba73330e9c0a69b9f0fd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=3&amp;amp;sn=79401d6c07b5ba73330e9c0a69b9f0fd&amp;amp;scene=21#wechat_redirect"&gt;Drive.ai&lt;/a&gt; 就完全押宝于深度学习，将深度学习应用于全自动集成驾驶堆栈，改变用规则去应对各种场景，让汽车完全自行通过理解数据去学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;而 Demiurge 的方案不是循序渐进，而是从自动驾驶场景下的小样本学习和与真实物理世界交互的两大限制出发，用生物神经元的计算模型从根源上解决这个问题。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「比如说蝗虫，它们的翅膀非常孱弱，任何撞击对它们来说都是非常致命的，但它们在高速飞行中有着几乎完美的自动避障能力，这背后的机制如果用在自动驾驶汽车上，将会实现第四级别的自动驾驶。最令人吃惊的是，蝗虫的自动避障系统只用了两个生物神经元，一个用来探测障碍，一个用来执行避障的行为，这说明生物神经元在处理物理世界的任务时，从小数据和数据流中的学习和决策能力非常出色，这对我们降低数据需求提供了重要线索。」Bragi 说。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL1xxhEXUkibX9g7JQaqfEm74ESx39WE2P8Ap7LCySFFnyd0tNWJfFDnQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;基于深度学习的自动驾驶和蝗虫自动避障的对比，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在产品方面，软件依然是第一位，但如果现有的自动驾驶平台无法与他们的软件相适应时，Demiurge 也会重新设计硬件，「我们要设计的深度学习芯片也是基于脉冲神经元，所以从硬件实施上也与现在的硬件有所不同」，但 Bragi 没有透露更多具体细节，「这两种方法都是可能的，至于选择哪一种，则是看工程上的需要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在众多自动驾驶领域的参与者中，Demiurge 认为公司最大的潜在竞争对手是&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716988&amp;amp;idx=1&amp;amp;sn=65ad0b7c47c2b6c3f1b0b89e6fab1ff5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716988&amp;amp;idx=1&amp;amp;sn=65ad0b7c47c2b6c3f1b0b89e6fab1ff5&amp;amp;scene=21#wechat_redirect"&gt;特斯拉&lt;/a&gt;。特斯拉在去年 10 月通过软件升级增加了辅助驾驶功能，这个功能在研发时使用了特斯拉车主过去 18 个月积累的 7.8 亿英里行驶数据。在该功能上线后的短短六个月内就积累了 4,700 万英里数据，远远超过谷歌历时 6 年积累的 150 万英里，而近期特斯拉的这个数据已经增加到 1 亿英里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 把特斯拉视为头号竞争对手的原因在于，目前只有特斯拉充分认识到现有深度学习对于数据需求量过大的底层问题，并且后者正在用不同方式来逼近这个目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 说：「特斯拉在收集数据上有着垄断性的巨大优势，所以能够利用现有深度学习做自动驾驶，在与大多数同行竞争中已然遥遥领先。但特斯拉并没有满足这一状态，Elon Musk 同时通过成立 Open AI 在本质上寻求能够实现第四级别自动驾驶的下一代的深度学习算法，完全超越竞争，这和 Demiurge 的思路是一样的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;宏大目标下的生存追问&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 Demiurge 的两位创始人交流其实是一件压力很大的事情，不管他们在介绍 Demiurge ，讨论人工智能行业，还是随意的聊天，我都需要努力让自己的思维保持活跃，以跟上他们的谈话节奏和思维逻辑。他们有一种独特的谈话方式，总是会抽茧剥丝般的从现象聊到本质，并会将不同事物联系起来推理出通用的规律并进行演绎，既可以自下而上的将具体抽象成理论，也可以自上而下的将理论落实到具体事物或者一个形象的比喻上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在深入了解他们的创业理念后，才发现他们的所有行为，小到一个聊天的话题，大的公司的研究方向，都可以用一种独特的气质支撑起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而这种独特的气质就是被他们视为竞争对手的特斯拉创始人伊隆·马斯克的「秘密武器」——第一性原理，马斯克自己对此的解读是：我认为，人们的思维过程通常都束缚在常规或类似的经历中。很少会试着在第一原则的基础上考虑某些事。他们习惯说：「因为我们以前那样做过，所以我们会这样做。」或者，他们不做这事是因为「好吧，之前没人做过，所以情况肯定不太妙。」但是，这是一种荒谬的思维方式。你必须从头开始推理，你注视着那些基本条件，然后从中建构你的推理，之后你可以看看是否得出一个有效或是无效的结论——可能会和人们以往的结论相同，也可能不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这种思维落实到行为上，便是基于问题的根源来设定目标，并保证之后做出的所有行为都与这个终极目标息息相关，并是以一种效用最大的方式去实施。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）基于核心问题提出目标&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 意识到现有深度学习无法解决小样本学习和与物理世界实时交互的问题，而这恰恰是生物神经元所擅长的，所以他就直接去研究生物神经元的计算模型，以创造下一代深度学习并付诸实用。相信很多人工智能领域的从业者都可以发现目前深度学习的问题，也经常有人表示需要让机器像人类一样学习，但很少有人、尤其是创业公司会把这个问题作为自己专注的研发目标，更多的是去选择一些权宜之计。&lt;strong&gt;但对于 Demiurge 来说，他们不仅理性的发现了这个问题，同时又有着足够的勇气去挑战它，而从来不会考虑此前是否有创业公司这样做过&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 会让人不自觉的想到另外一家如日中天的人工智能创业公司 DeepMind ，都是学术与应用相结合，都是专注于通用人工智能，且创始人都有着神经科学和人工智能的复合背景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当被问到和 DeepMind 的异同时，Bragi 表示：「被谷歌收购之前的 DeepMind 和我们很像，我认为他们接受被谷歌收购是『the best scenario of failture for DeepMind（最好的失败）』，因为如果他们率先解决了通用人工智能这个问题，那么谷歌被 DeepMind 收购将会是『the best scenario of failture for Google』 。我不知道他们是在研究了三四年之后遇到了瓶颈，还是说一直把这个（被收购）当成备选方案。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当被问到谷歌所提供的数据和资源支持是否会成为 DeepMind 的竞争优势时，Bragi 依然是从目标出发给予了否定，「我们看的非常清楚，解决物理世界的问题，大数据不是一个帮助，而是一个掣肘，想要利用谷歌的资源和数据优势，我们认为是没有真正理解解决这个问题的方法。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我判断的标准是一个公司是否把最核心的资源、人和时间放在最关键的问题上， DeepMind 被谷歌收购前是这样，但现在有点不一样了。」Bragi 非常坚定地认为，「比如说 AlphaGo ，它很好，但不是我们想要的」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 创始人表示，他们只想去解决一个问题：让人工智能不再只是成为谈资，也不是炫目的展示，而是让大家在生活中能够获得便利和切实的好处，或者通过平价地外探索去获得人类文明新维度的可能性，这个是他们投身人工智能的目标，他们只想达到这一个目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）围绕目标的生存法则&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 认为，Demiurge 看到了一个人工智能应用的关键问题，并拥有对这个问题的兴趣以及解决这个问题的决心。所以确定了这个目标之后，Demiurge 的每一个动作都极其严苛的围绕它去实施，而不在乎常规做法是什么，以及通行的游戏规则是怎样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「和生物一样，我们所做的事情也是生存驱动的，」Idonae 说，「我们这个旅程就是要抵达终点，我们要知道每一天如何更逼近自己的目标，并以一种能耗最低的方式往前跑。如果在这个过程的每一步都要符合主流的预期，做一个好的表现者，同时又要以最快的速度抵达终点，那这个预期本身就是矛盾、不切实际的 。有人擅长把每一步都做的漂亮，每一步都有掌声，这是一种做法，但这不是我们擅长的做法；我们不在乎这一过程的孤寂，只在乎是否抵达真正的终点， 所以，我们或许只会攒到一次掌声，那就是当绝大多数人都因这项基础技术而受益时为我们的产品而鼓掌。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种状态 从Demiurge 创立之前就已经体现了出来，Bragi 在纽约大学读本科时，为了弄清楚如何实现通用人工智能，他就向学校申请自主设计了一个全新的本科专业——覆盖计算机科学、人工智能、神经科学、物理学和哲学，设计并完成了大量的博士级别的研究课题。后来在计算机名校卡耐基梅隆读研期间，他也没有按照大多数人的通行研究路径选择继续读博。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「先弄清楚到底怎么样做才能实现通用人工智能，然后从解决这个问题来追问所有的决策，而不是说会做任何形式上的妥协。这个是我们每做一次看似不寻常的选择时背后的逻辑，其实就是对问题本身的探求和理解。」Bragi 解释到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 还做了一件在大众看来不太寻常的事情。今年 3 月，谷歌计划出售机器人公司&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715759&amp;amp;idx=1&amp;amp;sn=004c78b7cf919b0e9b7171720633aac5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715759&amp;amp;idx=1&amp;amp;sn=004c78b7cf919b0e9b7171720633aac5&amp;amp;scene=21#wechat_redirect"&gt;波士顿动力&lt;/a&gt;，Demiurge 立即在自己的 Medium 主页发布官方声明，表示希望收购波士顿动力公司，将波士顿动力公司的相关技术整合到基于 Demiurge 深度神经网络的新一代机器人操作系统中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;采访期间，谷歌宣布将波士顿动力出售给丰田。「卖给丰田是波士顿团队第二好的选择（注：&lt;span&gt;Bragi 认为，&lt;/span&gt;最好的选择是被 &lt;span&gt;Demiurge 收购&lt;/span&gt;），波士顿动力本身含有不同声音，有人坚持基于控制论的模型设计，但面对复杂的人与外部真实环境的交互，开发模型是极其困难的。有人相信深度学习能够模拟任意函数的建模优势，但是用深度学习进行端对端建模， 需要远超谷歌能力的海量的传感和控制数据 。他们所面临的最核心的难题恰恰是我们可以为他们解决的，就是与真实物理世界交互的深度学习系统。」Bragi 说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然交易未能达成，而 Demiurge 作为一个初期创业公司所发出的这个声明也没有得到大范围的媒体关注，但这件看起有些不可思议的事情恰恰体现出 Demiurge 的理念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司把这个作为唯一标准来筛选人才、投资人，甚至是公司的地理位置。在一年多前的一个内部讨论会上，当 &lt;span&gt;Idonae&lt;/span&gt; 提到要把公司设在瑞士时，在场的所有投资人和专家都表示很难理解。当今天再回顾这个问题时，现实的回报让 Demiurge 的创始人确信当初选择的明智，「公司的选址取决于你要做什么类型的创业，我们是要解决深度学习的核心挑战，同时也是脑科学的难题，解决这个问题之后还要将一系列技术产品化，把这些结合起来看，瑞士是最合适的。瑞士有着全球领先的神经科学和机器人技术，而且瑞士特别适合全球市场的高科技企业的成长与发展（比如罗氏、诺华以及 ABB ）。而我们的技术应用是一个万亿级别的全球市场，在行业合作、产品和安全标准制定、甚至是国际间的多重合作方面，瑞士都有着稳固的优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 还说「我们最看中的还是人与人的吸引，什么样的问题吸引什么样的人进来，而欧洲的人才池对我们这个问题最感兴趣，也是最了解及最能提供贡献的。同时，我们也看中瑞士的视野和地缘里的养分，瑞士尊崇着商业长久之道的本质，不鼓励通过对个人的崇拜转化为产品销售力，而是推崇用最好最可靠的产品品质赢得市场，与客户建立长久简单的供求关系。 在这方面，瑞士给了我们一个非常重要的参照环境，让我们体会到下一代深度学习产品应该怎么做」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种基于目标的生存追问成为 Demiurge 的文化基石，使他们做所有决策非常简单，因为这是他们唯一的标准。这个标准不仅帮助他们找到了最合适的办公场所，吸引到了最适合专家和团队，还让他们在前行的路上可以极度理性的看待争议和否定，摒除一切干扰因素。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在评论马斯克的&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401181797&amp;amp;idx=1&amp;amp;sn=7e392d911b29b04b964a5aad7706eecf&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401181797&amp;amp;idx=1&amp;amp;sn=7e392d911b29b04b964a5aad7706eecf&amp;amp;scene=21#wechat_redirect"&gt;第一原理&lt;/a&gt;时，Tim Urban 用了一个非常简洁的描述：「 Things I want → Want 」。Idonae 有着同样的理性和坚定：「我们就一个目标，很简单，就是要到达哪儿，一路上没有建设性的争议和否定不能给我任何食物和魔法帮助我去接近这个目标，所以我们会自动过滤。在这个路程上，我们就是不断选择、凝聚能理解我们理念，并能提供不可替代价值的人，我们不会去在乎其他。批评和讽刺的声音只有在看到产品，或者通过产品解决他们的需求时才会发生改变。我们是创业团队，我们要生存，所以我们会动用所有的资源放在解决这个问题上，这个问题解决了，产品本身就是答案。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于目标的生存追问让 Demiurge 敢于去解决一个世界难题，吸引到领域内顶级的科学家为他们提供支持，创造了第三种游戏规则，还组建了在每个环节上都拥有 1-2 名顶尖研究者的强大团队，包括数学家、神经科学家、机器人学家等。同时也形成了他们自己一套独特的管理方式——像特种部队一样用时间来激发最大潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从科技史的发展来看，Bragi 认为科技的突破很难预测，因为之所以成为突破就因为与现有技术有着质的不同，而我们的预测又是基于对已知技术的理解， 所以预测本身是不可靠的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;面对公司目前在做的这样一项质变而非演变的技术，从内部管理来说，他们挑选出来的成员都是有着相同的生存本能——所有任务的期限，就是自己毫不妥协地完成任务所用的最短时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 说「就像是英国特种部队的野外徒步训练，教官不告诉特种兵任务的 deadline ，所以没有人知道 deadline 的具体时间，但都知道它的存在，所以每个人都会竭尽全力以最少的时间来完成任务，激发出最大的潜能。我们的团队成员都是这样，必须全力以赴，以最短的时间高质量的完成每一个任务。如果我们人为设置一些时间点，如果过短，就会迫使大家走一些错误的捷径，如果过长，反而会让人松懈自满。我们要一直处在寻求生存的状态下才能够最好地生存下去。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tim Urban 认为培养马斯克的思考方式有三个维度：1）对已知的事物保持谦逊；2）对可能实现的事物保持自信；2）对无关紧要的事物不存怯懦之心。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与之类似， 如果将 Demiurge 理念进行拆解的话，就是他们印在公司 T 恤上的 5 个短语：仁爱的理性（empathetic rationality）、谨慎的无畏（prudent fearlessness）、有批判精神的信仰（skeptical faithfulness）、谦逊的灵魂（humble souls）、勇敢的思想（daring minds）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 对此的解释是：「要心怀远大，但恰恰是因为非常理解目标的远大，所以才知道每一步所必要的谨小，既做到感情上的认同，又极其理性地去分析，因为靠一腔热血是走不远的；对目标极其坚定，但在前往目标的道路上时刻保持批判精神；有谦逊的灵魂才会接近客观地理解问题，理解一路上将要面对多少艰难险阻，而勇敢的思想是我们的生命之源，使我们能够自信认定地一步一步走下去。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 公司 T 恤上还有另外一个图案，是线虫、鱼、昆虫、狗和婴儿五种从低级到高级的生物，下面依次标注了各自的神经元数量。这些神经元凭借一种目前还不为我们所知晓的机制互相连接，让生物拥有在现实世界中生存下去的智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而 Demiurge 自身也是这样一个「神经元」，以实现智能为最终目标，同时以一种效率最高且目的性最强的方式做着每一次与外界的连接。生物神经元的计算模型 Demiurge 还在研究和测试，但 Demiurge 自身生存和成长的机制他们已经找到，正是这种机制将 Demiurge 的目标、研究、管理和合作等所有行为都完整地融合在了一起。使他们成为一个拥有智能的生命体更好地生存下去，并帮助他们创造出更多的「生命体」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 世纪伟大的物理学家埃尔温·薛定谔的著作 《What is Life》前言里有这样一段话：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「古往今来，从探索中发现普遍的真理是最有价值的。但是过去一百年中所积累的众多物理学领域的知识，其广度和深度，让我们对统一真理的探索面临如下挑战：一方面，我们已经开始掌握了能够将各个领域的知识融汇贯通的办法；另一方面，即便是对某一学科领域更加专业化的知识，如果想要彻底掌握它几乎又是不可能的事情。只有我们中的某些人，敢于冒着自己被看成是愚蠢之人的风险，去大胆地综合所有发现和理论，我们才有可能摆脱挑战，发现普遍真理。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于 Demiurge 乃至今天的人工智能领域来说，可能也是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;©本文由机器之心原创，转载请联系本公众号获得授权。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 04 Sep 2016 19:40:49 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | ELM超限学习机：填补罗森布拉特的神经网络梦想和冯·诺依曼对生物学习困惑之间的空白</title>
      <link>http://www.iwgc.cn/link/2555447</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;黄广斌&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文总结被神经网络前辈和著名经济学家 Halbert White 认为「Sexy」的超限学习机（Extreme Learning Machines, ELM）的「Sexy」之处和之所以被称为「超限学习机（ELM）」的原因。在超限学习机的理论框架下，机器（Machine, Devices, Sensors）和生物脑可以看成一致的，只是构造的基本材料和硬件不同而已。有机的「机器」（生物学习系统）也有千万种，并且还在一直自我演化。但我们坚信两者之间可以拥有一个共同的「基本粒子」级（或称为「基本单元」级）的学习结构和学习算法，那就是超限学习机（Extreme Learning Machines, ELM）。而这种超限学习机（ELM）的实现和硬件材料以及具体数据可以是无关的。ELM 理论指出众多种的生物神经元在学习中是不需要调整的和数据无关的。生物学习机制的秘密可能就在于其神经元的随机性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;作者介绍：黄广斌（Guang-Bin Huang）是新加坡南洋理工大学教授（终身）。在 2014 和 2015 年被 Thomson Reuters 评为「高引用研究者」（工程类，计算机科学类），以及「2014 年世界最有影响力的科学精英」和「2015 年世界最有影响力的科学精英」。他是新加坡总统科学奖被提名人（2016）。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;他主持的主要项目有&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：德国宝马集团和南洋理工大学未来汽车联合研究实验室人机交互，脑机交互以及汽车辅助驾驶项目，英国劳斯莱斯和南洋理工大学联合研究实验室海上自主导航决策辅助系统项目，新加坡科技工程和南洋理工大学先进机器人联合研究实验室场景识别和机器学习项目，台湾台达电子股份有限公司和南洋理工大学物联网联合研究实验室数据分析和视频项目。还担任过新加坡樟宜机场新加坡航空公司地面服务公司第五货运大厦的信息跟踪控制系统升级改造的总设计师和技术负责人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;神经网络和生物学习之间的空白&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 弗兰克&lt;span&gt;·&lt;/span&gt;罗森布拉特的神经网络梦想&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 1950 年代初期，生物学家弗兰克&lt;span&gt;·&lt;/span&gt;罗森布拉特（Frank Rosenblatt）提出了他称为感知器（Perceptron）的多层前馈网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来跨越 60 多年特别是从 1980 年代到现在用的大部分神经网络结构其实都是罗森布拉特神经网络感知器的一种，这些包括早期流行的支持向量机（SVM）和现在风靡产业界的卷积神经网络（CNN），也包括 CNN 的前身 Neocognition ，只是针对不同的实现后人提出了不同的学习算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特最初提出他的神经网络结构时并没有有效的学习算法，但是他梦想这种神经网络感知器可以看作是「计算机的一种胚胎」，一种最终能够帮助计算机实现「走、说、看、写、繁衍并有自我意识」的智能源泉。罗森布拉特的预测在 60 年后的今天被证明是正确的，这种神经网络技术还有可能是未来人工智能和机器学习的主要技术基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 马文&lt;span&gt;·&lt;/span&gt;明斯基和 1970 年代人工智能冬天&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特的预测在 60 年前是极其大胆和有远见的，在当时计算机犹如一个庞然大物的时代几乎没有几个人相信他的预测是对的和他的梦想是能实现的。也许伟大的思想之所以伟大就在于远远超前现有人们所能理解和所能想象的。包括人工智能之父、图灵奖获得者马文&lt;span&gt;·&lt;/span&gt;明斯基（Marvin Minsky）和神经网络之父 Bernard Widrow 都对罗森布拉特的预测表示怀疑。罗森布拉特提出的神经网络感知器严格意义上讲在提出之初还只是概念，正如许多伟大的想法在提出之初都会出现有些概念模糊不清的情况，大部分人有疑虑也就正常了。明斯基对罗森布拉特的神经网络感知器的否定直接导致了被后人称为「美丽错误」的发生在 1970 年代的「人工智能的冬天」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两年前在 Bernard 家吃着他夫人精心准备的旧金山螃蟹，边回顾着 60 年来的神经网络发展往事，受益匪浅也感慨万千。Bernard 在和我探讨超限学习机（Extreme Learning Machines, ELM）时提及他和明斯基以及罗森布拉特三人之间的往事时诚恳地承认在 1950 年代他对罗森布拉特的神经网络感知器也是不太认同，在他和罗森布拉特之间的争论中他是错了。不得不被前辈们敢于承认错误的勇气折服。&lt;span&gt;（提醒：学术争论无论激烈与否可以有助于找寻自然规律的真象，这和打着学术争论之名行人身攻击之实是有本质区别的。）&lt;/span&gt;Bernard 提及在 1971 年，也就在「人工智能的冬天」开始之初，罗森布拉特在他 43 岁生日那天在一个湖里划帆板时发生意外就再也没有回来，连尸身都没有找到，令人不禁辛酸和感叹。试想：罗森布拉特如果不是英年早逝（某种程度上讲是含冤而死），人工神经网络、人工智能和机器学习技术也许还会往前推进 10-20 年。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;有关 Bernard 和超限学习机的一段小插曲：Bernard 在超限学习机发表后 10 年左右提出了一个类似超限学习机的技术但却没有注意到早期有关超限学习机工作。本来这是一个小事，人们很难查看到所有有关资料，科研很能面面俱到。Bernard 却向我当面提出道歉，前辈们谦卑的人格再次让人折服。&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的困惑&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算机的硬件实现是要极其精致美妙的，但计算机的实现也是极其脆弱的，不能有任何瑕疵。任何硬件实现上的不完美都可能导致计算机不能正常运作。约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼（John von Neumann）在造出第一代计算机之后，做为计算机之父的他感到困惑不解的是：和计算机需要完美硬件连接组成所不同的是，为什么「一个看上去不完美的包含许多看似随机连接的（生物）神经网络却能够可靠地实现完美的学习功能」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特的梦想和冯&lt;span&gt;·&lt;/span&gt;诺依曼的困惑之间有着很大的空白地带和理论技术鸿沟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;超限学习机：&lt;span&gt;填补神经网络和生物学习之间的空白&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人脑可能是宇宙中最复杂的东西。人类在过去几百年对自然界和宇宙的认识在飞速发展，对生物学习特别是人脑的思维机制还知之甚少。罗森布拉特的人工神经网络感知器和冯&lt;span&gt;·&lt;/span&gt;诺依曼关于生物学习的困惑以及未解之谜看似关联性不大。其实在超限学习机的理论框架下，机器（Machine、Devices、Sensors）和生物脑可以看成一致的，只是构造的基本材料和硬件不同而已。一种由无机的硅等组成，一种由有机的碳水化合物蛋白质等组成。生物脑本质上也是一种「机器」。无机和有机的「机器」可以完全不一样，它们的结构和算法也千变万化。有机的「机器」（生物学习系统）也有千万种，并且还在一直自我演化。但我们坚信两者之间可以拥有一个共同的「基本粒子」级（或称为「基本单元」级）的学习结构和学习算法，那就是超限学习机。而这种超限学习机的实现和硬件材料和具体数据可以是无关的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 作为人工神经网络的超限学习机&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）「秒杀」学习速度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经网络在人工智能和机器学习中的重要作用最近几年又再次得到认可和追捧，大有人工智能和机器学习的实现必须依赖于人工神经网络之势。然而人工神经网络技术普遍面临着一些挑战，比如繁重而「痛苦」的人工干预、缓慢的学习速度和较弱的可扩展性。超限学习机的一个基本目的是要克服这些过去几十年来人工神经网络界面临的发展瓶颈，达到尽可能少的人工干预，高的测试准确度和实时快速本地化学习的能力，在许多应用中达到秒级，毫秒甚至微妙级或更快。&lt;span&gt;[图1]&lt;/span&gt; 相比其它通用的学习技术（比如深度学习），在有些应用中超限学习机可以快几千几万倍。比如在有些手写体识别，3D 图形应用，各国交通路牌识别等应用中，超限学习机与深度学习相比可进一步提高准确率, 并且大幅度降低训练时间（相比较深度学习基于 GPU 的 1-2 天训练时间，超限学习机在普通计算机上的训练时间缩短到几分钟或更少）。在许多医疗大数据应用上，超限学习机也比传统的学习方法在提高准确率的情况下将学习速度大幅提高几千倍。&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFRPH15pqYmrxiaeOKq7F7mm5uMDfbkfU7zgwBfiboa0uH9Qo5ZRCoeq7A/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, "Representational Learning with Extreme Learning Machine for Big Data," IEEE Intelligent Systems, vol. 28, no. 6, pp. 31-34, 2013.&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Z. Huang, Y. Yu, J. Gu, and H. Liu, "An Efficient Method for Traffic Sign Recognition Based on Extreme Learning Machine," (in press) IEEE Transactions on Cybernetics, 2016&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Z. Xie, K. Xu, W. Shan, L. Liu, Y. Xiong, and H. Huang, "Projective Feature Learning for 3D Shapes with Multi-View Depth Images," The 23rd Pacific Conference on Computer Graphics and Applications, Tsinghua University, China, October 7-9, 2015.&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）统一的神经网络结构和算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 年前当神经网络发展处于第一次复兴的巅峰，普天下都在忙于为神经网络训练「调参」和苦于寻找办法如何使流行的神经网络学习算法跳出「局部最小点」时，我们的疑问是：1）当普天下的研究人员都乐于和疲于「调参」时，神经网络的发展本身是不是也陷入了局部最小点？2）不同类型的网络「真的需要不同类型的学习算法吗」？3）是否存在一种通用的学习框架来处理不同类型的网络（单层前馈网络和多层网络）？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;不同单隐层前馈神经网络的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;许多种单隐层前馈神经网络在广泛使用中，包括前馈网络、径向基函数（RBF）网络、支持向量机（SVM）、多项式网络、傅里叶变换和小波网络等。这些之前都被认为是不同而且没有联系的学习或计算技术。超限学习机理论认为这些都有一样的网络结构，只是网络的隐层用的是不同的神经元而已。并提出在考虑 Universal Approximation Capability（有人翻译成「万能逼近」能力）和分类能力的前提下，只要隐层神经元是非线性阶段连续的，人们就不需要为不同的前馈神经网络设计不同的学习算法。作为 ELM 的一个特例（傅立叶序列作为隐层神经元），后来 Intel 和美国加州大学伯克利分校研究团队提出的 Random Kitchen Sink（RKS）以及 Google 团队提出的 FastFood 也在近几年有许多发展和实际成功应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, "Extreme learning machine: a new learning scheme of feedforward neural networks," Proceedings of international joint conference on neural networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, L. Chen and C.-K. Siew, "Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes," IEEE Transactions on Neural Networks. vol. 17, no. 4, pp. 879-892, 2006.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang and L. Chen. "Convex Incremental Extreme Learning Machine," Neurocomputing, vol. 70, pp. 3056-3062, 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;A. Rahimi and B. Recht, "Random features for large-scale kernel machines," Proceedings of the 2007 neural information processing systems (NIPS2007), 3–6 Dec 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Q. Le, T. Sarlós T, and A. Smola, "Fastfood approximating kernel expansions in loglinear time," Proceedings of the 30th international conference on machine learning, Atlanta, USA, p. 16–21, June 2013.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;单隐层学习和多隐层学习的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们真的需要迭代式地调整多层前馈神经网络的隐层节点吗？前馈神经网络真的要像六十年来一直被认为是个黑箱吗？传统的误差反向传播（BP）算法和支持向量机（SVM）将多层网络视为黑箱。与此不同的是，超限学习机将多层网络视为白箱，并且一层一层地进行训练。总体看，超限学习机将单隐层前馈和多隐层网络看成一个类似的统一体，用雷同的方法来处理单隐层前馈和多隐层网络。然而，与深度神经网络需要密集地调整其隐层节点不同，超限学习理论显示，隐层节点很重要，但（单隐层神经网络和多层网络的）隐层节点可以和数据无关，可以随机产生或从上一代传给下一代而不需要调整。学习可以无需通过迭代式地调整隐层节点来实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFfPdqyicSgico4BtbZ0aMibKPACeI9Za442pI76CZX6wbNayxicFW7Fic5jg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;J. Tang, C. Deng, and G.-B. Huang, "Extreme Learning Machine for Multilayer Perceptron" , IEEE Transactions on Neural Networks and Learning Systems, May 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong, "Local Receptive Fields Based Extreme Learning Machine," IEEE Computational Intelligence Magazine, vol. 10, no. 2, pp. 18-29, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, "Representational Learning with Extreme Learning Machine for Big Data," IEEE Intelligence Systems, vol. 28, no. 6, pp. 31-34, 2013.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;单隐层学习和多隐层学习与层次性学习的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多隐层学习（Multi-Hidden Layer Learning）和层次性学习（Hierarchical Learning）的概念不是完全一样。多隐层学习强调的是一个目标应用（比如图像分类）由一个包含多个隐层节点的网络实现。而超限学习机的层次性学习强调的是每个隐层实现一个功能，各个功能单元通过级联，并联，串联等组合形成一个学习能力复合的机器学习系统。&lt;span&gt;[图3]&lt;/span&gt; 层次性学习的一个特例可以是一个多隐层学习方法。在超限学习机的体系下，各个功能块可以采用和应用相关的超限学习机算法。另外，在超限学习机中，一个隐层节点可以是一个由多个神经元组成的超级隐节点单元。&lt;span&gt;[图4] &amp;nbsp;&lt;/span&gt;这种层次性学习可以最终提供比较理想的 End-to-End Learning 和 One-Shot Learning。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL2ybaFdXI11miagzDPk2U2uZj2XRNGfMQSL9haj0xb7Yx99gZWXe6odA/0?wx_fmt=png"/&gt;&lt;br/&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFKeDIicvSrrkYqbP4Wicp3Scg1QGpBVImicC7PZlKzJUeDKwKLuH3xSQSA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "What are Extreme Learning Machines? Filling the Gap between Frank Rosenblatt's Dream and John von Neumann's Puzzle," Cognitive Computation, vol. 7, pp. 263-278, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）基本学习单元的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像加减乘除四大基本运算操作是数学体系的基础，物理体系也是建立在几大基本定律上一样，基于生命体的生物学习（Biological Learning）体系其实是建基于至少六大基本学习单元操作之上：压缩（Compression）、特征学习（Feature Learning）、稀疏编码（Sparse coding）、聚类（Clustering）、回归拟合（Regression）和分类（Classification）。&lt;span&gt;[图5] &amp;nbsp;&lt;/span&gt;这六大基本学习单元操作可以由同样的超限学习机实现，隐层节点与数据无关，要调整的是从隐层节点到输出层的连接。&lt;span&gt;[图4] &amp;nbsp;[图6]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如支持向量机（SVM），随机投影（Random Projection，RP）以及主成份分析（Principal Component Analysis, PCA）看似不太相关，却在超限学习机理论和算法下可以有机的统一。2012 年发表在 IEEE Transactions on Cybernetics 上的文章证明了支持向量机是超限学习机的次优解。刚刚发表在 IEEE Transactions on Image Processing 文章指出随机投影和主成份分析其实可以看作是超限学习机的隐层神经元用线性函数时的的一个特例。可是超限学习机也可以用非线性的隐层神经元，所以就可以进行升维，降维，特征学习等功能。所以从特征学习角度看随机投影和主成份分析也是提供次优解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLiaBYibMn9JN68q3w0TURnoibUia4m8dFJle3CqKiaFCbKGFCvKicznKWOqicA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLhw0mfk07oib6pzsb0dQx07QgM5UhQ4k3yZsNxBcPrnGLX7wNE22p5Gg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, H. Zhou, X.Ding, and R. Zhang, "Extreme Learning Machine for Regression and MulticlassClassification", IEEE Transactions on Systems, Man, and Cybernetics – Part B:Cybernetics, vol. 42, no. 2, pp. 513-529, 2012.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;L. L. C. Kasun, Y. Yang, G.-B. Huang, and Z. Zhang, Fellow, "Dimension Reduction With Extreme Learning Machine", IEEE Transactions on Neural Networks, vol. 25, no.8, pp. 3906-3918, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4）普适学习和普适智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着物联网的深入发展，在不远的未来，大部分的设备将拥有智能与学习能力。我们相信，就如包括人类在内的生物社会一样，这些智能设备也将发展出一个互相交流的「智能体社会」（Internet of Intelligent Things）&lt;span&gt;图7&lt;/span&gt;。每个智能体都嵌入有学习功能并且能相互交流。因而我们有必要提出普适学习（Pervasive Learning）和普适智能（Pervasive Intelligence）的概念和目标。由于超限学习机的学习速度比深度学习快上万倍，它可以帮助我们实现智能体社会。超限学习机芯片可以集成到硬件中，并实现实时本地在线学习，从而实现普适学习（Pervasive Learning）和普适智能（Pervasive Intelligence）。这几年，关于超限学习机芯片的研究得到一些实质进展，主要集中在三个方面：多核加速芯片（现场可编程门阵列（FPGA）和专用集成电路（ASIC）），神经形态芯片以及以光技术实现 ELM。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLtaN8Ro9qBHvY5a4COQ6xvfXZqvkC7Inw5ZhP6ib4Bp3wXxaGatoPxCw/0?wx_fmt=png"/&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;图 7&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "Extreme learning Machines: Enabling Pervasive Learning and Pervasive Intelligence", Pushing Frontiers, vol. 8, pp. 22-23, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5）填补不同学习理论间的空白&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 60 年来传统的学习理论不同，超限学习机理论的一个重要性质是其通用学习能力（压缩、特征学习、聚类、回归、分类等）无需通过调整隐层节点来获得，例如隐层节点可以从前辈继承或随机生成。进一步来说，超限学习机理论也为传统神经网络提供了理论支持（包括局部感受域（Local Receptive Field）和池化策略（Pooling）），而做为局部感受域的一个特殊实现方法的卷积神经操作和池化策略正是深度学习得以成功的主要原因之一。在 ELM 理论和应用下，不同随机分布的随机隐层神经元的产生形成全联结的网络或部分联结的网络&lt;span&gt;（图8）&lt;/span&gt;。或如 ELM 早期理论（2007 年）指出不同的部分联结也可以形成局部稠密边缘稀疏的局部感受域或不同局部感受域的非线性组合（池化策略）&lt;span&gt;（图 9）&lt;/span&gt;。根据 ELM 理论，卷积神经网络只是一种局部感受域和池化策略实现，除了卷积神经操作，还有许多其它的局部感受域存在，如何实现还有待进一步研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL5ibS4gv0TShMEDcS75k3ib2qE0gvAGnmjuL47gug98LgSZmbwatIq4icg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;图 8&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLF92ZMuzgBehBefpgz0LTuMCyACT5dTiaQdArcialOTRuN44ia6dJVibZ0w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 9&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;岭回归（Ridge Regression Theory）、线性系统的稳定性、矩阵稳定性、Bartlett 神经网络泛化能力理论（Neural Network Generalization Performance Theory）、支持向量机最大边界理论（Maximal Margin Theory）等在超限学习机以前被认为是不同的理论。特别是 Bartlett 神经网络泛化能力理论在以前很少用于训练神经网络。超限学习机采用了 Bartlett 理论，从而保证其泛化能力。超限学习机的理论显示，这些之前的理论从机器学习角度看是有机一致的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang and L. Chen, "Convex Incremental Extreme Learning Machine," Neurocomputing, vol. 70, pp. 3056-3062, 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "An Insight into Extreme Learning Machine: Random Neurons, Random Features and Kernels", Cognitive Computation, vol. 6, pp. 376-390, 2014.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong, "Local Receptive Fields Based Extreme Learning Machine", IEEE Computational Intelligence Magazine, vol. 10, no. 2, pp. 18-29, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 作为生物学习的一个「基本粒子」级学习单元的超限学习机&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）生物学习机制的验证&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超限学习机理论显示，隐层节点很重要，但在很多应用中不需要调整（比如压缩感知、特征学习、聚类、回归和分类）。在理论上，这种神经元的激活函数几乎可以是任何非线性分段连续的，包括上百种人类无法知道其准确数学模型的人脑中的神经元。在超限学习机理论和技术提出之后的大概 10 年左右，越来越多的有关生物脑学习系统的研究成果直接或间接的支持了超限学习机理论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2013 年及之后发表在《自然》等期刊上文章报告了来自美国斯坦福大学，哈佛医学院，麻省理工学院和哥伦比亚大学等大学的研究人员发现在老鼠的嗅觉系统中神经元在学习过程中是随机产生的。这可能是超限学习机理论首次在生物系统中得到验证。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2015 年美国哥伦比亚大学和 IBM Watson 的研究人员进一步阐述生物学习系统中神经元的随机产生可以进一步帮助生物学习系统实现对特征学习（升维，降维等），并且明确指出这在工程实现比如超限学习机是被证明有效的。这些在生物脑中发现的神经元机制和超限学习机理论预测是一致的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2015 年美国乔治亚理工学院和华盛顿大学的一批研究人员通过人的行为学分析简直验证人脑中随机神经元机制可以帮助人拥有小样本学习能力。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2016 年发表在《自然•神经科学》上的文章说明了超限学习机理论进一步在猴子的脑中得到了直接验证。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: normal; text-align: justify;"&gt;&lt;span&gt;M. Rigotti, O. Barak, M. R. Warden, X.-J. Wang, N. D. Daw, E. X. Miller, S. Fusi, "The importance of mixed selectivity in complex cognitive tasks," Nature, vol.497, pp. 585-590, 2013&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;O. Barak, M. Rigotti, S. Fusi, "The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off," Journal of Neuroscience, vol. 33, no. 9, pp. 3844-3856, 2013&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;S. Fusi, E. K Miller, and M. Rigotti, "Why neurons mix: high dimensionality for higher cognition," Current Opinion in Neurobiology, vol. 37, pp. 66-74, 2015&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;R. I. Arriaga, et al.Visual Categorization with Random Projection, Neural Computation, vol. 27, 2015&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;J. Xie and C. Padoa-Schioppa, "Neuronal remapping and circuit persistence in economic decisions," Nature Neuroscience, vol. 19, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;E. L Rich and J. D Wallis, "What stays the same in orbitofrontal cortex," Nature Neuroscience, vol. 19, no. 6, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）解答约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的困惑&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在罗森布拉特的梦想中，他的神经网络感知器可以最终帮助实现电子计算机走路、说话、看东西、写作、繁衍自己并有自我意识，而作为计算机之父的冯&lt;span&gt;·&lt;/span&gt;诺依曼却不解为什么一个看似不完美生物神经网络系统却有完美的学习能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超限学习机理论的目标之一是打破机器学习和生物学习之间的壁垒。尽管动物的大脑在总体上来说是结构化及有序的，在其某些层或区域，其局部结构可看成「无序」的。从超限学习理论的角度看，网络的整个多层结构（人工神经网络或生物网络）是结构化且有序的，但它们在某一个神经元层或神经模块片中看起来「混乱、非组织结构化」。从局部来看，「硬连线」可以是全连接或部分连接。这种全局结构化而局部随机连接的看似「不完美」结构，却正好构成了基本的完美的学习能力，包括压缩感知、特征学习、稀疏编码、聚类、回归和分类等。这就解决了冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的谜惑。生物学习机制极其复杂，而我们相信「无需调节隐层节点的学习」是很多学习模块中的一种基本生物学习机制。虽然人脑中也许有几百种不同种类的生物神经元，他们的数学模型也不为人类所知，但是超限学习机理论指出一个基本的生物学习机制也许是生物神经元本身在学习中是不需要调整的，和应用是无关的。进一步说，随机隐层神经元节点和「随机连线」只是两种特定的实现「无需调节隐层节点的学习」的方法。IBM 团队最近也宣布他们研制出类生物神经元，他们实现的理论基础正是基于 ELM 理论最早所提出，倡导和支持的：生物神经元应该是随机的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;G.-B. Huang, What are Extreme Learning Machines? Filling the Gap between Frank Rosenblatt's Dream and John von Neumann's Puzzle, Cognitive Computation, vol. 7, pp. 263-278, 2015.&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;T. Tuma, A. Pantazi, M. L. Gallo, A. Sebastian, and E. Eleftheriou, "Stochastic phase-change neurons," &amp;nbsp;Nature Nanotechnology, vol. 11, August 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）展望&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们相信超限学习机理论和技术提供了一个架接跨越机器学习和生物学习基本「粒子」级的学习机制。也填补了罗森布拉特的梦想和冯&lt;span&gt;·&lt;/span&gt;诺依曼的困惑之间有着很大的空白地带和理论技术鸿沟。这也是实现普适学习和普适智能的必要条件。然而这些还很初步，套用个别神经网络界前辈对超限学习机的评论和期望：「好戏还没有开始」，也许更多的令人激动和感兴趣的东西还等着大家研究开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有兴趣的研究人员，可以申请新加坡南洋理工大学黄广斌教授研发团队在下列研究方向的博士生、博士后和访问学者位置：海上自主导航数据分析、智能芯片设计、多模数据分析、视频分析、目标识别和跟踪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;©本文由机器之心发布，转载请联系本公众号和作者获得授权，点击「阅读原文」关注黄广斌教授微博。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 04 Sep 2016 19:40:49 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 人类力量和机器相结合：human-bot交互的新范式</title>
      <link>http://www.iwgc.cn/link/2555448</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自TechCrunch&amp;amp;blog.kitt.ai&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;&lt;strong&gt;Xuchen Chao、&lt;/strong&gt;Daniel Li、Guoguo Chen、Kenji Sagae&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者授权机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Terrence L、赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文第一作者姚旭晨为 KITT.AI 创始人。KITT.AI 是一家专注于嵌入式系统热词检测和人机多轮对话系统的自然语义理解创业公司。总部位于西雅图，由艾伦人工智能研究所孵化，亚马逊、Founders' Co-op 和 Madrona Venture Group 为种子投资方。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;原文注：这是一篇从TechCrunch上转发的文章，由 Guoguo，Kenji，和&amp;nbsp;Daniel Li 共同完成，同时还补充了 Skype 上的一些信息。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聊天机器人为商品、服务和信息的无障碍获取提供了保障，但是创造出一个有效的 bots 是非常困难的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以带来一种全新的和用户之间无缝自然的互动方式，但从另一角度来讲，这也会使用户对产品的期望变得尤其高。 Bots 需要变得十分聪明，并且能提供比 APP 更多的便利—— APP 是一个针对当今移动设备非常有效的 UI 范例，并且一直在被精确的完善，这个过程已经超过了十年时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;好消息是，关于 bots 必须掌握人类语言或者替代 APP 这一个想法是错误的。Bots 将会以新的方式与消费者互动，这种方式就是把人类力量和机器相结合，使得结构化和非结构化的信息都可以被自然高效的进行交换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;交流速度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个简单但非常直观的测量交流有效性的方法，就是统计每单位时间内信息交换的数量。在这个框架下，文本互动（如 SMS 、聊天信息、邮件）和语音互动（如电话）在可被产生的信息数量和可被消耗的信息数量这两方面是不同的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLeIiaaQSDibbcZ29wtu12pCpPHj6KwOTaLJJ26aydQ7tgyKhnRUtH5GpA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正常来说，我们说话时可以每分钟产生 120-140 个单词，在书写时可以每分钟写或打字 40-70 个单词。当我们统计信息消耗速度时，阅读英语的速度达到每分钟 200 个单词，但是听力速度最多达到每分钟听语音产生的 120-140 个单词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SMS 和聊天应用已经通过自动纠错和新式键盘改进了文本产生速度，但是对人来说，文本产生速度仍然赶不上文本消耗（阅读）速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象一下，一个朋友能以超人的速度打字、画画、搜索信息和找 GIF 图，并且可以生产按钮、菜单和图片，以让你的输入速度更快。更好的情况是，你提升输入速度的同时也可以让你的小伙伴更加容易地理解你的意思，并且在必要时还保持着自然语言的灵活性和熟悉性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可能现在还没达到那个程度，但已经非常近了，尤其是拥有了那些在特定平台上被很好构造出的 bots 。下面是一些不同机器人平台的特征，他们正在逐渐将人机交流（human-bot communication）塑造成一种更加高效、稳定和自然的 UI 范例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;快速回复按钮&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;快速回复按钮是一个节省用户时间、防止未预料到的输入的简单方便的方法。它们对于人机交流（human-bot communication）来说是独一无二的，因为按钮对于创造出一个机器人来说是至关重要的，并且人类使用起来也非常简单。好处包括可以提升交流速度和增强 bot 的理解能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook、Telegram、Kik 和 Skype 的机器人都有快速回复按钮，只不过名字不太一样。有的机器人，比如 KiK 的 Sephora 机器人，将快速回复按钮作为交流的初级模式。Slack 仍然不具备快速回复按钮，但是具备带有辅助反应的消息按钮。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTvFib9WR05T5JeMdzZgUY611sDAsrNfe10Fg7Lbcj9icXeefEaWBslNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em&gt;&lt;span&gt;Telegram Custom Keyboard&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLXibb4PDPuHcMBESlRaWeRm2RKfNxh5ic1ME2tN3P9VdnT2CDA6WYzicrA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Facebook Messenger Quick Replies&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL4QU8J8VAEnbr2yt6ia5vJjjNNBhicIqxrPHPE84L79CC2x8TMFWkunXA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Kik Suggested Response Keyboard&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLDbd6bnmbysQNMwuGXO9dNxAQJmVbFiaSpxzIrK5dKqYXpPbVVvVYZUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;Skype buttons in a card&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回调按钮&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回调按钮和快速回复按钮很像，但是允许范围更广的潜在互动。当点击了回调按钮以后，它就会对一个登记过的网络连接产生一个 HTTP 访问，并触发一个事先定义好的动作。回调按钮是提供反馈的一个很好的方式，并且他们可以为 bot 后端提供进一步深入分析的机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLpq7eN5XicrHa1UP1P6EtwfZeoiaPMDVC9EHtTTnUUNbV0WE23zuVGjZw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Slack Message Buttons&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLNCBIxJ6skW26Zub0WdapIm8kUeI8WfRGzM4FFGvGw9CaVacsqRbFibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Messenger Postback Button&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLYy6IZJ4EMIRZ4dhIeadedSYeayJ6l40nLibxSA1HIbC8aScdxTcG6dQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Telegram Callback Buttons&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结构化信息共享&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可以被轻易解析的共享信息以编程的方式，将把在仅有语言的范例中的复杂结构化信息，转变成为多元范例中简单清晰的结构化信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如，分享一个像「3rd &amp;amp; Madison」这类的位置信息是模糊不清的，并且人类和机器解析起来会很慢。然而分享 GPS 坐标就可以快速的在地图上显示出来，并且很快的被 bots 理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL4QU8J8VAEnbr2yt6ia5vJjjNNBhicIqxrPHPE84L79CC2x8TMFWkunXA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL18MACSewrrASmtENYQCIJicomxsJ6azEibcgaxGbF4pZky9sYPR5y3VA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Telegram SendContact and SendLocation&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTV0PU2MAia99O9ibfWnaiac0pOiau0eEVMWxdGtOSsjrK0dVDIkMMp4QDw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Facebook Messenger location sharing&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;内嵌式 bots 在交谈过程中是一个非常好的快速获取、发送和分享信息的方式，同时不需要从当面界面或应用跳转到另外一个对话框或者应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLZywjeZIsCgcFfK5v6tTIL8qZCUTkkvDMXKSU7YjichicSKKNXJbSANFQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Telegram Inline Bot&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLoj1dTnUDwa35LA9Xp9MGp1PvdkzTT45ApnGFicH5L9vsLMDLsEMrs1A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Slack Bot Mention&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLNlkODJBMAJ09cdyJk9A6Vq4mUqgDvZEB8pR4UnfXDKA6UWsvfr8orQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;Skype Bot Mention in Groups&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不用多次触控和打开菜单去使用某个功能，bot 的一个 @ 键就可以允许完成我们一行互动。允许机器人和另一个机器人分享对话文本，将会大幅提高互动速度，因为使用者不再需要为了每个交谈而重新输入数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面的这张表格总结了四个流行的聊天和 bot 平台上增加的语言触摸功能。这些特征代表着一个多元交流范例的开端，这样的多元交流范例使我们可以和 bots 进行更加高校和有效的交流：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;快速回复按钮：节省用户时间并提升机器理解程度&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;回调按钮：提供动作回调和后端分析&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结构化信息共享：轻易分享机器可读信息&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;bot 提及：让机器人可以一直在线并被简单调用&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLmhooEibBNoib91iaZppcO484sQKh5hdibt9PQ7YWWRXX6IDJNVUib5WBSzA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;如果机器人不使用语言触摸多元交流模式，这里仍有几种方法可以使你利用一些按钮和hui'ti的 UI 技术去创造一个更好的 bot ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在建造系统时，整个环节以人为起始点，去定义最普通的交流模式和对该模式的期望&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;用清晰并定义完善的回复（如回复“是”来购买）或者开放式信息（如“你可以告诉我泰勒斯威夫特最新专辑什么时候发行呢”），来为双频道交流（快和慢）优化对话&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;即使没有本土整合，也要用回调功能。对于更加复杂的任务来说，应该让用户离开当前对话，使他们使用更适合用手操作的点击界面或触摸界面。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;考虑转移至一个更优化的新人机交互平台。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要让 bots 达到人类般的交流水平，人工智能和自然语言处理还有很长一段路要走。但无论如何，在那发生之前，人机交互的新方法将会利用人类和机器的力量创造出和我们语言一样自然的新的交互模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;©本文由机器之心获授权编译，转载请联系本公众号获得授权。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 04 Sep 2016 19:40:49 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 从分割到识别，全面解析Facebook开源的3款机器视觉工具（附论文）</title>
      <link>http://www.iwgc.cn/link/2544108</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 adeshpande3.github.io&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;Adit Deshpande&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：武竞、Cindy、李亚洲、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;8 月 26 日，机器之心发布文章&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718597&amp;amp;idx=1&amp;amp;sn=56aa4e5deff99620fe6ed42000903849&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718597&amp;amp;idx=1&amp;amp;sn=56aa4e5deff99620fe6ed42000903849&amp;amp;scene=21#wechat_redirect"&gt;《深度 | Facebook 的图像识别很强大，一次开源了三款机器视觉工具（附论文）》&lt;/a&gt;介绍 Facebook 在开源上的新动态。在这篇文章中，作者对此次开源的 3 款工具分别进行了深度解析。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;说到分析图象，Facebook 毫无疑问在深度卷积神经网络方面取得了很大成就。一周前，Facebook 人工智能研究团队发表了一个 blog，详细阐述了一些对象分割算法背后的计算机视觉技术。在这篇文章中，我们将会总结并解释 blog 中引用的三篇论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook 人工智能研究团队利用的主要渠道如下。图象被输入到 DeepMask 分割构架，被分割的部分通过 SharpMask 模型进行精炼，再通过 MultiPathNet 进行分类。让我们来看看每一个组成部分是怎么单独运行的吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFrCL8iaIFicvZq6e8iaBG4OnNBiaOIHKC6mzV8aubMNbibtRlOOulo9ibibiajw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;DeepMask&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;简介&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Pedro Pinheiro, Roman Collobert 与 Piotr Dollar 写了一篇标题为「学习分割候选对象（Learning to Segment Object Candidates）」的论文。作者通过一个模型着手处理对象分割的任务。该模型在给定的一个图像补丁的情况下，首先输出一个分割掩码，然后再输出该图像补丁在完整对象上居中的概率。那个过程被应用在整个图象上以便每一个对象都可以得到一个创造的掩码。而这整个过程仅通过一个 CNN 来完成，因为在该网络中，这两个组件都共享了许多层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;输入和输出&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们先可视化出我们想让这个模型做什么。给定一个输入图像后，我们希望这个网络可以为每一个对象都输出一组掩码或轮廓。我们可以认为每一个输入图象都包含一组补丁（原图的一小部分）。对于一个输入的补丁而言，输出是一个二进制掩码，它能勾勒出主要对象的形状并对输入补丁有多少可能包括一个对象进行评分（在-1 和 1 之间）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFdDzFzoXERuibiaA3FjZAz6lIfDDd2QmmFYvX2QHpmGtnDgfTKxNKTO2g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个训练样本都将要包含这 3 个要素（边注：带有 1 标注的样本需要在给定大小的范围内包含一个在该图像中大致居中的对象）。该模型在多个尺寸刻度和位置上将这个过程应用在图像上（这是一个我们之前讨论过的补丁集）。然后将结果汇总形成一个带有所有掩码的最终图像。现在，让我们看一看这个模型是这么架构起来的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFKPX8BOWK4gk995zlu4fltGOUL6ibR1Tr06FGjsCIeppktlQib34lwZIA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;网络架构&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个网络是为了 ImageNet（迁移学习。使用它）上的分类而预训练的。图象通过一个含有八个 3x3 的卷积图层和五个 2x2 maxpool 层的类似 VGG 的模型（不含充分连接的层）流入。根据输入图象的尺寸，你将会得到一个确定的输出量（在这个案例中是 512x14x14）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输入：3 x 高 x 宽&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输出：512 x 高/16 x 宽/16&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，模型分成了之前描述过的 2 个组件。一个主要处理分割，而另一个判定图象中是否有一个对象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Segmentation Head&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们采用输出量，将它穿过一个 network-in-network 层和一个 ReLU 层。然后，我们将会得到一个宽乘高（这个宽和高小于原图的高和宽）像素分类器的层，而这个层将决定已知像素是否是图象中心的对象的一部分（如果你有一个 28x28 大小的原图，那么将会少于 784 个分类器）。然后我们来看看这些分类器的输出，双线性增采样的输出符合完整的原分辨率，取得一个黑白的二进制掩码（1 代表「是」，0 代表「否」）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Objectness Head&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个网络组件决定了图象是否包含居中并且比例适中的对象。将 VGG 图层的输出穿过一个 2x2 的 maxpool（一个 dropout 单元）和两个充分相连接的网络层，我们就可以得到「objectness」的得分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;训练&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于损失函数是逻辑回归损失的总和（一个是 objectness head 的损失，一个是 segmentation head 中每个位置的损失），网络的两个组件是同时训练的。反向传播在 segmentation head 和 objectness head 之间交替进行。数据增加技术也被用来改善这个模型。这个模型采用随机梯度下降法在 Nvidia Tesla K40m GPU 上训练了将进 5 天。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;为什么这篇论文很酷&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个单独的卷积神经网络。我们不需要一个额外的图片目标提议步骤或一些复杂的训练渠道。很明确的是，这个模型提高了网络的效率、速度和灵活性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;SharpMask&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前面的作者（与 Tsung-Yi Lin 合作）也发表了一篇标题为「学习提炼对象分割块（Learning to Refine Object Segments）」的论文。正如标题所示，这篇论文的主旨是提炼 DeepMask 模型生成的分割掩码。DeepMask 的主要问题在于，它使用了一个简单的前馈网络，它在生成「粗糙对象掩码」时是很成功的，却并不能生成「精确到像素的分割」。原因是，这里运用了全图像大小的二元线性采样。这就导致了对象边界的粗糙和不精确对齐。为了解决这个问题，SharpMask 模型把网络中的浅层（early layers）的低级（low-level）特征与深层（layers deeper）的高级（high-level）的对象信息结合起来。这个模型通过首先对每一个输入部分生成一个粗糙掩码，然后将它传入网络中的不同提炼模型来实现这点。现在我们来介绍细节。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFOqWmMQPeibhzMICQ2o0Qz9bIk7EiaicTqJNpIVGx6cGMssibb9oz14FpYQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;网络的构架&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SharpMask 模型构架的动机源于高级（high-level）的对象信息对精确的分割掩码十分有用，我们需要一个自上而下的方法：第一步生成粗糙的边界，然后整合浅层（early layers）中重要的低级（low-level）信息。正如你从上图中看到的，原始输入最先传入 DeepMask 通道来得到粗略的分割。然后它通过一系列精炼模型，增采样进而还原图像的原始维度。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;精炼模型&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们深入挖掘下该精炼模块的特性。该模块的目标通过增采样将自下而上（你可以认为 DeepMas=bottom-up 和精炼模块=top-down）创造的特征图考虑在内的方式生成的掩码，对抗 DeepMask 通道中池化层的影响。看待它的一种数学方式是精炼模块 R 是一个生成增采样的掩码 M 的函数，掩码 M 是前面层中掩码的函数，也是特征图 F 的函数。使用的精炼模块的数量等于 DeepMask 通道中使用的池化层的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUF7Jz3qYa78fBaEUuszum9zu2sX8Lz5ullX77vkxdvJ6uS88WYErAoSA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，函数 R 中到底发生了什么？一个简单方法是只要连接（concatenate) M 和 F，因为它们有同样的宽和高。问题存在于这些每个组件的深度信道中。特征图中深度信道的数量要比掩码的深度信道数量大的多。因此，M 和 F 的连接将过于偏向 F。解决方案是通过应用一个 3×3 卷积层减少 F 深度信道的数量，然后再连接 M，通过另一个 3 ×3 卷积层，最终使用一个双线性增采样。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;训练&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepMask 使用的训练数据同样适用于 SharpMask。我们需要输入二进制掩码补丁和标签。DeepMask 层先被训练，然后在精炼模块开始训练的时候，权重会被冻结。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;为什么这篇论文很酷&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在引入一个新的、易于使用的模块的同时，该论文也能够建立在 DeepMask 的方法之上。作者们创造性的认识到通过只融合在 DeepMask 通道的较早层中可用的低层信息，就能获得更精确的分割。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFM7xaBia9Ot7ul9rf7GFjBcNmeWRzkFOLkpjH6dt7WX42uCPC3iaOBdSQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;MultiPathNet&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;简介&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepMask 用来生成粗糙的分割掩码，SharpMask 用来精炼这些轮廓。而 MultiPathNet 的职责是在这些掩码中进行对象识别或者分类。包括 Sergey Zagoruyko、Adam Lerer、Tsung-Yi Lin、Pedro Pinheiro、Sam Gross、Soumith Chintala、Piotr Dollar 在内的几个人发表了一篇名为「A MultiPath Network for Object Detection」的论文。这篇文章的目的，是通过专注处理尺度变化、高度遮挡、杂乱的图片，以及更高的精度定位，来提高对对象的检测技术。这个模型是以快速 R-CNN（Fast R-CNN）为起点的。总的来讲，这个模型应用了快速 R-CNN 以及 DeepMask 和 SharpMask 的对象检测技术。这篇论文的三个主要贡献之处是：忽略连接（skip connections）、黄斑区域（foveal regions）和一个积分损失函数（integral loss function）。在详细介绍这三个方法之前，我们先来看看这个网络的架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFe1lsia59cDztcic29Irv5d6trClP1XyCA2ic6HtvkBNufVbk0aibRFQzog/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;网络架构 / 黄斑区域&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用快速 R-CNN 时，我们首先将输入图像传入一个层与层之间没有完全连接的 VGG 网络。POI 池化（ROI pooling）被用于提取区域的特征（在 Ross Girshick 的论文中，ROI 池化是一个将图像的特征映射（mapping）到描述图像区域的固定空间的特征图（feature map）的方法）。对于每一个对象，我们接着创建四个不同的区域来从多个角度观察对象。这些区域就是我们在简介中介绍的「黄斑区域」。这些区域被传入完全连接的层中，得到的输出被连接起来，然后网络分成一个分类器和一个回归头文件。基于这个网络架构可以从多个尺度观察图像，并可以注意到目标对象周围的事物，这篇论文的作者推测这些黄斑区域可以帮助精确定位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;忽略连接&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于快速 R-CNN 的架构问题，一张 32x32 大小的输入图像会在最后的 VGG 卷积层迅速缩减到 2x2 的大小。ROI 池化会创建一个 7x7 的映射，但是我们仍然丢失了很多原始空间信息。为了解决这个问题，我们将特征从卷积 3、4、5 层就开始连接，然后把它们传入黄斑分类器（foveal classifier）。这篇论文指出，这些连接「给分类器提供了多种位置的特征信息」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;积分损失函数&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我并不想对这个概念深入介绍，在纸上演算数学公式无疑更简明易懂。这个概念的大致意思是，作者发明了一个损失函数，使得在预测有多个交除并（intersection-over-union, IoU）值的图像时有更好的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;为什么这篇论文很酷&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你对快速 R-CNN 感兴趣，那么你一定喜欢这个模型。它运用了 VGG 网络（VGG Net）和 ROI 池化（ROI pooling）的主要思想，同时还介绍了一种通过黄斑区域（foveal regions）、忽略连接（skip connections）和积分损失函数（integral loss function）得到更精确的定位和分类的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;点击「阅读原文」，下载论文↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 03 Sep 2016 16:54:06 +0800</pubDate>
    </item>
    <item>
      <title>机器之心 x MIT-CHIEF | 金融创新，Eximchain为中小进出口商提供高效可信的区块链平台</title>
      <link>http://www.iwgc.cn/link/2544109</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Yina Zhao&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;编辑：Rita Chen&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;麻省理工学院中国创新与创业论坛（简称 MIT-CHIEF) 是美东地区最大的创新创业平台，汇集了美国最尖端的人才和项目，融合了中国和美国的各项优势资源。在刚刚过去的七月里，十六支涵盖医疗健康，新能源，教育及金融等领域的创业团队和 MIT-CHIEF 一起，走访了北京，上海，深圳和成都四大城市和与其相关的创业合作基地，与当地的政府，企事业单位代表进行了卓有成效的合作与交流。机器之心有幸采访到了其中的十一支团队，这是该系列采访的第三篇。&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Eximchain 是这次中国行中为数不多的金融创新类的团队。Eximchain 由来自于麻省理工和布朗大学的团队组成，旨在为中小进出口商提供经济实用贸易融资产品。Eximchain 将中小企业与机构投资者对接，降低服务门槛，确保信用安全，利用区块链平台为出口商提供价格低廉的供应链金融产品，缩短收款时间，为进口商提供更长的账期。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;Synced: 刘希，你好！能否简单的和机器之心的读者介绍一下自己和你的团队？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希: &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;大家好，我是 Eximchain 的 CCO 刘希，在北京大学外国语学院读的本科。毕业后在瑞士银行工作六年半，北京 2 年半，香港 1 年，新加坡 2 年半，现在在 MIT 商学院。我们现在的团队虽然只有五个人，但大家在一起会讲十种不同国家的语言。背景很不同，来自欧洲，中东，美国和中国，非常多元。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFqvMwl3BxU6Cm8AOxbjo4tMzhkULVvvj0WqDZdpUuIAHo7p6DZ9fHTw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 你们的团队主要是为中小进出口商提供经济实用贸易融资产品的区块链平台。能不能分享一下当初组建团队创业的契机？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFIb45YFl5hkIakJhAfSCibAfd7icrqBBBeLdJLyL0LgPYHg6QRuVYm5jg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我的合伙人，CEO 是 MIT Sloan 商学院的师兄。他是土耳其人，家里本身是做进出口生意。我们都非常认可区块链在金融领域，特别是进出口贸易领域的趋势，目前还没有人能在中美贸易领域做到真正落地。正因为这是一件非常具有挑战的事情，我们希望成为世界上第一个能把这件事情在中美贸易领域落到实地的团队。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 既然你也说道这是一件非常具有挑战的事情，我相信在过程中也遇到了很多困难。你认为发展过程中最大的困难是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;挑战非常多。&lt;strong&gt;首先就是团队建设方面。&lt;/strong&gt;我当时可以选择一个中国的团队，这样在交流饮食上都比较容易。但是，我最终还是选了一个团队背景多样化的团队。因为在我看来，做跨国进出口贸易的事情，需要多样化的团队来事先。正因为如此，早期的沟通成本非常得多，我需要解释为什么中国的情况是这样。如果我们发展客户，在美国可以这样做，但是在中国不能这样做。而且外国人给的反馈比较直接，会用建议的方式提出来，中国人说话比较委婉。大家沟通的方式不同，一开始沟通会有磨合的阶段。&lt;strong&gt;第二点就是，我们是学校出来的团队，涉及到团队转换的问题，&lt;/strong&gt;以及新团队招聘和融合的过程，怎样组建团队适合跨国的背景，能让各种风格的人在我们团队发挥他的作用。在克服这些困难后，会有很大的成就感。现在我的合伙人常常调侃说自己是在土耳其用微信用的最好的土耳其人。经过沟通，在中国的一些事情上，他也更加放心的交给我做。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced: 现在团队只有四个人，会考虑继续扩张么？下一步的团队规划是怎样的？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们现在在硅谷，刚入选美国西海岸 Plug and Play 的 Fin Tech (北加州硅谷企业孵化器，参与初始孵化并投资了著名的 Paypal，Dropbox，Google 等），它每年从全球 1000 家互联网金融公司选出来 25 到 30 家。我们希望能在硅谷招到能跟我们团队一起工作三个月的技术人员，最近也刚招了一个新的技术顾问。我们觉得中国这边是很大的一个市场，所以接下来希望团队中能加强中美贸易中中国方面的背景，以后会招更多这样的人。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 这次你作为你们团队的代表参加了 MIT-CHIEF 的中国行活动。四个城市的活动参与下来，能不能和我们分享一下你的收获？&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我认为收获分为个人和专业两方面。如果没有这个平台，我完全不知道我该如何获得那么多关注。我们现在在谈的合作伙伴，客户还有投资人，都是我通过这一个活动积累下来的。我在中国行之后一直跟进活动中建立的联系。虽然行程每个城市只有两三天，但每个城市都留下了很多后续可以挖掘的东西。所以后来，我在深圳又多呆了一周，我目前人在北京，我下周还要再去上海，和嘉定区政府人员去联系当地的中小型企业，去看有没有需求。这次的中国行活动从各个方面来说，都有非常直接的推动。当时来这个活动是抱着试试看的心态，因为我们还处于很早期的阶段，没想到得到这么多关注。从个人的角度，我在华人的创业圈里和大家互相培养了感情，我回来后，收到了我们初创团队成员的介绍，比如帮我招人，帮我找律师，给我提供建议，我们每天互相都会保持个人的交流，这是个人非常大的收获。因为我们创业者都是希望把创业作为职业和人生目标来奋斗的人，不管现在我们做的事情有没有交集，但以后大家会在不同专业和个人领域有合作，所以我这是中国行也算是大家结识的一个起点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced：在这次的中国行里面，哪个城市给你的印象最深刻？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;刘希：每个城市偏重不同。作为初创公司，要在看似没有资源的情况下挖到资源。我从每个城市收获的角度都不同。在上海，有可能会建区块链园区，所以地方政府给我们绿灯，希望我们能过去，还会带我们见客户。在成都，我一开始觉得没什么突破，反而我在成都签了第一个中国行客户意向书，因为他们在中国有一个企业要转型，需要一个新的转变，中小企业需要专业的服务来帮助他们，尤其是在贸易融资这一块，在成都拿到了一个意向客户。在深圳和北京，现在有投资人的意向现在在谈。虽然每个城市的特色不一样，只要创业者在里面深挖，还是能找到其中能用的资源和在中国的落地点。这个是需要我们做一个选择的。一开始我们可能会选一个沿海城市作为我们在中国的办公室，因为沿海城市的贸易量还是最多的，政策支持方面也可能是比较多的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced: 结合这次中国行的经历，你们认为国内外的创业氛围有没有什么差异？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我觉得差异还是蛮多的。首先，从大环境来说，美国的环境没有中国好，现在在美国不是很好融资。大环境上，中国从资金和政策方面都还是比较支持初创型公司的。第二，在中国大家都会问壁垒是什么，大的机构会不会超过你。这里是更大的资本市场，寡头在中国会有更加垄断的地位，在中国大的平台会做很多事情。在美国，即使是很大的上市公司，即使是有延伸的业务，它也只在它的领域做这个业务，很少能看到像阿里巴巴这样纵向横向发展的公司。我觉得这个有好的地方，也有不好的地方。好的地方就是，它会迫使你去想为什么寡头公司没有办法去做，为什么我做的方法是很精的，迫使你去想这些事情。你会觉得有阴影在你头上，就是有一个大的机构虽然现在和你做的完全不一样，但是它有资本，有平台，有势力，可能有一天会很快超过你。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced: 你们是一个金融创新的团队，可能很多读者对于区块链这个概念并不是很熟悉。能否介绍一下你们的区块链技术具体是如何支持贸易融资平台的商业模型？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;区块链有几个本质：去中心，去信任，电子化，加密原则，有时间戳的，是协作式的电子合作平台。概括来说就是由共识能触发的活动（consensus-driven）。国际贸易在信用担保这块其实就是一个出口商把交传单给银行，给银行验证后，几方经过验证的模式，出口商才能付款。这个模式从国际贸易一开始起，就是非常纸质的繁重工作。也是由于纸质的限制，全球每年大概有 3500 亿美金的贸易融资需求没法得到满足。我们觉得一个亟待解决的问题，于是我们先着手解决其中的一部分。现在双方有条件的达成合约，或者根据某种条件，某一方才会获得付款或者获得担保，只要是这样的工作我们都可以用区块链将其电子化。因为区块链可以做智能合同（smart contract），可以在你设定的条件下，通过电子的方式自动实现交易。具体来说，我们开始第一步就建立一个电子平台，为中小型企业实现进出口信用担保的服务。现在我们能实现将纸质的平台和手工达成一致的方式，将其电子化自动化，将交易记录存储在区块链上。我们第一步并不是用区块链来驱动交易，只是先做电子的合同成交，然后和平台上的担保机构也进行电子化的沟通。所有的结果通过电子加密的方式存储在区块链上，这是我们第一步想做的事情。这样一来，我们能帮出口商节省现在货款时间的 90%，帮进口商能节省贸易融资费用的 20%-50%，帮平台机构投资者实现每年 3%-9% 的风险收益套利。我们希望把进出口商还有机构投资者在我们的平台上实现这些功能，能节省时间和金钱；在未来 3-5 年，等国际市场对国际货币的接受程度更高之后，我们才会在我们的平台上实现用电子货币的交易。在未来十年之内，实现金流，物流，信息流三流的统一。在这个链上，每个人都贡献一部分信息，你可以得到一个人的完整生态系统，这是一个愿景。我们第一步先是帮客户解决实实在在的问题。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUF7QjQfybxl2L4OIshj2JHt20yFWMAzjJ7B8ojVcpPfUPIaXg3GlawTA/0?wx_fmt=jpeg"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced：怎样实现的节省最多 50% 的交易费用和 90% 的交易时间？能不能结合案例和我们说明一下？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;打个比方，中国北京有一个公司 A 想卖玩具车给美国人，但他们之间互相不信任。为了保证公司 A 的利益，目前国际上的做法是用信用证的方法，就是银行承担进口商的信用风险，给公司 A 开一张信用证。只要公司 A 把玩具车运出去，装船出海后就会有一个装船单。银行确认装船单与合同中的描述一样后，就会打钱给公司 A。之后，银行再与美国的进口商进行结算。信用证有不可撤销的信用证，这种信用证一旦双方成交了，出口商一定会获得货款的，是不可撤销的。出口商还可以拿信用证在本地来做信用打包贷款，用来生产玩具车。进口商可以在货抵之后，请求银行给予延期付款。银行在开信用证的时候，银行不需要垫资的，只是给出口商一个保证，几个月后货装了船我会给你付款。由于银行的人工成本很高，需要手工进行核查，银行会收取货值 1%-2% 的费用。从国际平均标准来讲，出口商是在装船后 20 天才能获得付款，这就是供应链的金融成本，出口商需要在本地找其他的钱来弥补这期间的空缺。而且现在只有国际一线银行才能提供这样的服务。公司 A 会找中国的一家银行去要钱，中国这家银行会找美国的另一家银行要钱，中国的银行也担心美国的银行倒闭没法要到钱。这样传统的体系，加上手工纸质的方式，造成现在系统门槛高，费用贵，流程复杂，时间慢。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 你们为中小进出口商提供经济适用的贸易融资产品，并且主要关注与机构投资者的对接。为什么没有选择这两年特别火爆的区块链结合 P2P 融资平台概念，而选择了机构投资者平台？相对 P2P，你们在机构投资者这个领域的优势是什么？使用区块链技术是如何增加你们在这个领域的优势的？&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;首先，之所以做中小型企业贸易融资，是因为国企没有这个需求的，国企可以不花钱去找银行开信用担保产品。而中小企业没有这个途径，而且中小企业在中国的信用数据基本没有。我们希望通过区块链，在我们平台能建立起来可信的中小企业的交易记录和信用数据库。我们目前做的是中国出口美国进口，目前中国中小企业风险我们还不可控，所以我们在做美国进口这块儿一开始会和美国对冲基金或银行合作。如果我们的信息是存在一个本地服务区域上，这些数据对于这些美国大型银行来说不具有可信性；如果是存储在公共的区块链上，他们会信任这些数据。所以，我们要解决的就是信用问题。之所以不用 P2P，而是要做进出口是因为世界上还没有一家能将跨国的区块链平台落到实地的。现在中国，用区块链做本土供应链的，已经有二三十家了。但是我们团队能在中国和美国同时推动这件事情，这就是我们的优势。我们想做发展中国家出口，发达国家进口的贸易融资，保证发展中国家企业的利益。另外，由于中型企业货值每年的流水有上亿人民币的，他们更希望用到信用担保，从这一点来看，P2P 的资金并不能满足我们的需求。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFFjdJy9OZjtIMf5ticzT7NkticCiaGaibzFX4jDDtYFWey1AzjXVHxr3Fog/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced: 一般生成信用记录都需要有一定的交易历史才可以达成，在产品初期，入驻的中小企业往往交易并不频繁，对于这种情况，你们是如何保证信用记录的有效性的？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这就是为什么我们做中国出口，美国进口。因为在存在风险的情况下, 信用担保更需要进口商的信用记录，并不需要中国中小企业的信用数据，信用担保保护的是出口商的风险。银行承担的是进口商的风险，因为进口商跑了，就不会付钱给银行了，而这方面美国进口商的信用记录是很多的。我们希望通过我们平台，积累足够多的中小出口商的交易记录。试想平台建立起来后，比如十年数据的积累，之后很多国内的企业也会作为进口商进口国外的原材料进行加工，而在平台上积累的这些数据可以作为他们获得信用产品或者贷款的信用记录。区块链会把金流，信息流，物流都会放到一起，我们之后会做成智能合同（smart contract）。比如在集装箱里放一个感应器，一路可以追踪货物的走向，跟踪货物的湿度，密度等等。一旦感应器发现货物有异常情况，这个货款就可能并不会付给进口商。而这些是传统的国际贸易系统无法解决的，但将来很可能通过区域链来解决。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced:对进出口贸易来说，可能有不同的政府，供应商，企事业机构的合作和协调。你们要构建一个通过区块链管理进出口商的身份和交易的平台，协助各方面整更加高效地进行国际进出口交易的全流程，在全流程中，你认为哪个环节会是最难协调的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;在美国得到大企业的支持，在中国得到政府的支持，都是比较重要的。跨国贸易涉及到两个国家的问题，如果能有本土的支持，是最好的。举个例子，overstock.com 这个网站就是做尾单。这个网站物美价廉，质量高, 因为收的东西都是出口的产品，质量比较好。在美国黄金时段都有这家公司的广告。比如中国的一家出口商多生产了 1000 个纽扣，overstock.com 可能会把这个尾单收了。这个网站大概和中国 1000 家供应商有联系，每年的交易额大概是 80 亿美金。他的 CEO 找我们希望他的平台能用我们的服务。他是我们谈的第一个比较好的意向客户。而且是一下子就给我们拉来了 1000 个客户。这也坚定了我们确认这个事情是有需求的，是可以做成的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 感谢你和我们分享了这么多区块链方面的经验和实践案例。最后，能否和我们分享一下 Eximchain 接下来五年的发展规划？&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;希望今年年底有客户在我们的平台上有完整的交易，真正落实地服务到我们现在手头的客户。未来 3-5 年，我们希望能在平台上积累起来中国中小进出口企业的数据。我们希望成为世界上第一家把中美贸易、进出口融资落到实处的公司。我们希望能持续服务客户，使大家想到信用数据都会来我们这个平台。希望服务更多的客户，产生规模效应，让越来越多的机构投资者对我们感兴趣。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFcmHFpGvq9ZRvoRaPPJzzfSAet46n6qoMM8oEGftEhgvKLIfophorCQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced: 好的，谢谢！采访最后是我们 Synced Talk 的固定快问快答环节。每一期我们都有固定的主题，这一期我们的主题是大数据。那么这里有三个问题，请你根据直观感受作答。第一，你认为，大数据对于现代社会最有用的助力在哪个方面？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我个人认为，大数据在医疗健康方面会实现更大社会价值。比如在美国 FDA 批新药，临床测试需要非常多的数据。目前新药的测试从数据采集到数据分析的方面有非常多不合理的地方。而大数据可以把这个过程系统化，产生非常大的社会价值。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 大数据时代, 如何保护自己的个人信息安全？你觉得哪些人能拥有获取数据的权利？&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个我觉得很难保证。全球每个人的信息安全可能都会受到威胁，虽然 Google 保证内部有控制个人信息安全的系统。就拿我熟悉的领域来说，银行就很不安全，银行系统有很多漏洞，很难把每个漏洞都补上。银行也常常受到黑客攻击。现在来看就是让银行之间联合起来，共享黑客攻击信息，防止同一黑客手段反复入侵多家银行。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced:你认为当代社会对于大数据这个概念是否夸大其词？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘希：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;大数据目前是比较火的。这个现象很正常，我也理解。比如一件事情做的好不好，很难去衡量。但至少，大数据可以让我们用数据说话，这点是大数据客观的地方。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced : 谢谢！也希望你们在中国的推广和落地一切顺利！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Eximchain (&lt;span&gt;www.eximchain.com&lt;/span&gt;) 由来自于麻省理工大学和布朗大学的团队组成，旨在为进出口商提供经济实用的信用担保融资产品和国际认可的贸易信用记录。我们的平台能够帮助出口商降低供应链金融成本，为进口商提供更长账期。我们使用现代技术将现有银行纸质和手工的流程进行电子化、自动化，并通过区块链管理进出口商的身份和交易，构建更加高效的国际进出口贸易平台。&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;我们现在在硅谷 Plug And Play Fintech Accelerator（每年从全球 1000 多家互联网金融公司选取 25-30 家）期待你的加入！我们常年招募区块链领域的技术人才和对中美供应链金融领域的商务人才。如果你感兴趣加入我们或者想感受和硅谷创业公司的合作的体验，请将简历发邮件到 &lt;span&gt;hope@eximchain.com&lt;/span&gt; 注明全职／兼职＋技术／商务。期待与你们一起成长！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;麻省理工学院中国创新与创业论坛 (MIT-CHIEF)2016 Community 继续招募优质创业团队入驻！「2106 MIT-CHIEF Conetst 暨商业企划书大赛」正式向团队开放，详情请登录 www.mitchief.org&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 03 Sep 2016 16:54:06 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 计算语言学思想碰撞的浪潮：ACL 2016</title>
      <link>http://www.iwgc.cn/link/2544110</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;转自微软研究院&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：黄丹青、闫昭&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;小编按&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;计算机语言学和自然语言处理最顶尖的会议之一ACL 2016于八月初在德国举行。微软有20多位研究员参加了ACL 2016，是本次大会上一个最为重要的研究团队之一。本次微软研究院共为与会者带来了1个Tutorial，2个workshop，以及17篇被录用的论文。想知道这次ACL大会上都有哪些不容错过的闪光？微软亚洲研究院实习生黄丹青和闫昭给你带来了这次大会分享。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者介绍：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;黄丹青&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，是微软亚洲研究院实习生，本科毕业于中山大学，目前是中山大学和微软亚洲研究院联合培养博士生。她的研究兴趣为 knowledge computing 和 question answering。&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;闫昭&lt;/strong&gt;&lt;span&gt;，是来自北京航空航天大学的同学，今年博三，在微软亚洲研究院NLC组已经实习了两年多了，他的研究兴趣是 question answering 和 dialogue system。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;备受关注的 ACL&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ACL 会议是计算机语言学和自然语言处理最顶尖的会议之一，每年都吸引了许多学者投稿及参与。今年 8 月初 ACL 2016 在德国柏林洪堡大学举办。柏林是一座具有深厚文化底蕴以及历史沧桑感的城市，洪堡大学更是柏林最古老的大学，先后出过 29 名诺贝尔奖获得者。其校训是校友马克思的名言：「从来哲学家都在解释这个世界，而问题在于改变这个世界&lt;span&gt;」&lt;/span&gt;。在这个知名学者倍出的大学里，尤其学校创始者 Wilhelmvon Humboldt，他是位出色的语言学学者，ACL 2016 可谓是带来了又一波计算机语言学思想碰撞的浪潮。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqwpgtRw0uxtl9yErZibpuDrm2IPHygnTONUgPTVBNfND8tLk5lMq9hrg/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;会议上所用的 conference book&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此次会议共收到 1290 篇投稿，接收 328 篇，其中长文 231 篇，短文 97 篇，总体接收率 25%，与往年差不多。超过1600 多人注册参加此次会议。微软、谷歌、亚马逊等IT巨头都参与了企业展示。从中可以看到，ACL 受到了越来越多学术界与工业界的关注。从接收的论文来看，研究领域十分的多样化，使用的模型更新速度也很快，毫无疑问，其中深度学习（deep learning）相关的占据了半壁江山。整个会议日程安排得很好，美中不足的是由于论文数量多，一天安排了多达 7 个 session 同时进行，感兴趣的几个报告都赶在同一个时间段了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;精彩纷呈的ACL&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;会议举办了两场特邀讲座，分别是来自加州大学的 Amber Boydstun 以及爱丁堡大学的 Mark Steedman。Amber Boydstun 主要的研究方向是政治学而并非语言学，但她引进了语言学中Tone和Frames等理论分析文本研究媒体和政治之间的联系。她结合了心理学、新闻学以及计算机科学等多个学科的理论，这可能也是大会想要传递的一个信息：鼓励大家对语言学理论以及其他学科的运用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外一场讲座则是由著名的 Combinatory Categorial Grammar (CCG)的发明人之一，英国爱丁堡大学的 Mark Steedman 教授带来的「On Distributional Semantics&lt;span&gt;」&lt;/span&gt;，场面相当火爆。他回顾了目前基于词汇的表面意思或者隐含意思这两大类构造分布式语义的方法，以及在 QA，机器翻译等的一些应用，整个介绍十分完整，同时他指出，语义任务中要解决的最难问题不是逻辑运算，而是在同一语义下能表达的语言是多变的，「如何定义内容( content )并使之能够支持逻辑运算和常识推理」是关键。进而，教授讲述了他们在语义逻辑表达概率化方面的一些相关工作。(讲义链接 http://homepages.inf.ed.ac.uk/steedman/papers/semantics/acl16a.pdf)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqVay4hCZJLxvLIxJPDtC8q3ia2icVOz7gPSpkL18bTPHzUko95OWV15Sg/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;海报展示（一）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqq8cXvRJaMTeEnECCSszuN73MNDgaj1hBIicRYNeibd3AVnwicfPjm0bUQ/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;海报展示（二）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，组委会还根据不同的话题，安排了总共 44 场报告。其中，至少被安排了两场报告的主题包括，Parsing(4 场)，Word Vector(3 场)，Machine Translation(3 场)，Word Meaning(2 场)，Question Answering (2 场)，Semantic Parsing(2 场)。 本次大会所有的报告均在洪堡大学的校园内的阶梯教室进行，其中 Audimax 和 Kinosaal 是两个可以容纳听众最多的两个。Word Vectors,Semantic Parsing, 以及 Question Answering 的报告均被安排在了这两个大厅中进行。我们多多少少可以从组委会对于报告的安排，一窥当前自然语言处理领域的热点话题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;会议的重磅新闻之一就是终身成就奖的颁发，得奖者是斯坦福大学的教授Joan Bresnan。谈起她，可能最多人提起的应该是她在70年代定义了一种语法框架，LFG（lexical functional grammar），强调结构对语言的重要性，提出了句法结构、参数结构、形态结构等，成为了许多机器翻译工具的理论基础。她的得奖感言主题是：Linguistics:The Garden and the Bush。这个题目十分形象地体现了她的工作: 在真实场景下语言并不像花园那样可以单纯用理论去刻画，而是像灌木丛那样，因此她把统计的方法引入到语言学研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;会议挑选了十篇 outstanding 论文，deep learning 相关的占了绝大部分。而今年大会的最佳论文奖并非任何一篇关于深度学习的文章，而是一篇讨论符号语义性的文章——来自加州大学伯克利分校E. Dario Gutierrez 的工作 Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression。该论文主要研究词形式和词义之间是否有联系的问题（比如一些url结尾的单词curl，furl 等都与「卷曲」的意思相关）。论文的贡献在于，从全局更好的找到从词形式到词义的映射关系，通过实验分析对之前不同工作得出的不同结论做了一个解释和统一。作者用很简单直观的统计学习模型解决十分纯粹的 morphology 形态学问题，加上对这个任务的一个分析总结，逻辑十分清晰，读起来十分舒服。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大会的最佳学生论文奖授予了剑桥大学 Steve Young 组的 Pei-Hao Su。他们的论文 On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems，提出了一个在线学习框架，可以显著减少在语音对话系统中使用强化学习所需的数据标注量，并减弱用户反馈中数据噪音对于对话系统策略学习的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqst6LjzvbxFZkrjMicibR9hGykJib4tkwBdia9QrVLLdwM1Eyn9V6PZzo9w/0?"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;微软在ACL&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软有20多位研究员参加了 ACL 2016，是本次大会上一个最为重要的研究团队之一。微软研究院的首席研究员Bill Dolan第一时间在他的博客上发布了题为「Microsoft NLP researchers convergeat ACL 2016, edging ever closer to human-like conversational experiences」的博客 (原文请见： https://www.microsoft.com/en-us/research/microsoft-nlp-researchers-converge-acl-2016-edging-ever-closer-human-like-conversational-experiences )，博文介绍了微软研究院本次大会的与会情况。根据 Bill 的统计，本次微软研究院共为与会者带来了 1 个Tutorial，2 个workshop，以及 17 篇被录用的论文。下面我从中选了一些我们亲身参加了的部分，为大家详细介绍一下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中的一场Tutorial (Understanding Short Texts)是由微软亚洲研究院的王仲远以及前微软亚洲研究院研究员王海勋共同讲解的。仲远通过数据说明了短文本分析在大量互联网应用中的重要性，分析了短文本理解中的一些特点与挑战，并介绍了多种基于知识库的显示表达模型及应用(Knowledge Based Explicit Representation Models)。 而海勋则从隐式表达模型的角度对短文本分析进行了讲解，并做了最后的总结。(讲义链接：http://www.wangzhongyuan.com/tutorial/ACL2016/Understanding-Short-Texts/)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqxqLwdwIOLAkdJLZwXkA5wk9ASX9XgfJiaziaz4TCW2DRZaf4rq8JPhvg/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;来自微软亚洲研究院的王仲远及前研究员王海勋正在共同讲解tutorial&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;ACL论文是如何写成的&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄丹青：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqia0LI1lkhXhdwm8D11ZVHSGaFlhPwvH8xkbKrwd25KAQzn4SISw7N1g/0?"/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们团队的工作是《How Well Do Computers Solve Math Word Problems? Large Scale Dataset Construction and Evaluation》。在计算机自动解数学题这个任务上，之前的工作都是基于一些小规模而且多样性不足的数学题集，我们认为这样得出来的结论可能不太有代表性。因此我们使用半自动的方法搜集标注了多达 1 万 8 千道小学数学题，并在此基础上对现有系统重新评估。从目前来看计算机的自动解题能力还是十分薄弱，接下来我们会专注于如何提高这种计算机的这种能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从定义任务，提出想法，到一步步完成论文的过程中，微软亚洲研究院的导师都给我了十分有用的帮助和指导。除了技术支持，他们更多的是教会了我如何更全面地思考分析问题，做每一步之前都要思考背后的 motivation，如何严谨地论证每一个细节。我十分佩服他们对事情的洞察力以及对全局的把握能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这次收获很多，除了对自己相关工作有更多交流见解之外，还了解了其他领域最前线的发展方向，发现能借鉴的有很多。参加 ACL 让我感觉到，大家都怀着对学术的热情，踏实地专注于自己的研究，通过众人之力一步一步推动着计算机更好的发展。这让我能更沉下心来继续我往后的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;闫昭：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqx3wSjfiayny101FCct9Uiaj8Jr2aVPjlgRibUqV2OzY41OU4lpXbs9BXw/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本次 ACL 中，我们的文章 DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents 讲述了一种基于检索与排序直接从非结构化文档中选取句子作为聊天机器人回复的方法。以往的方法无论是基于检索的还是基于生成的，都会依赖大量的对话句对作为训练数据。而在给定领域的情况下，大量的对话语料是比较难以获得的，但普通的文本就容易获取的多。我们的方法现在已经运用于新一代的微软小冰跨平台商业解决方案之中，助力小冰的自主知识学习技能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;非常庆幸我参与了 DocChat 这个项目，参与了一个解决实际产品问题的全过程。从对问题的分解，模型的构建，到实验的验证，数据的分析，最后到论文的撰写，参与整个过程让我受益非凡。非常感谢我的 mentor 段楠研究员，以及 NLC 组所有的研究员和同学，感谢他们对我的帮助以及所营造的科研学习氛围。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近些年，自然语言处理在很多方面取得了非常快的发展，也衍生出了很多新的科学问题。参加这次大会，让我近距离的接触了这些最前沿的成果，让我有机会与NLP领域的同仁交流与学习。在我个人比较关心问答和对话系统领域，我听到了很多精彩的报告，这些带给了我许多关于未来研究方向的思考，更是激励我做好眼下研究工作的动力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNMd9mibSWEA07cn1iaXAicibwrqwAkafiaX3QyWq2C4qbVs4fRTia5OODuW9pZxJicgvonBCuVdEdGxJJicrw/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文转自微软研究院，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 03 Sep 2016 16:54:06 +0800</pubDate>
    </item>
  </channel>
</rss>
