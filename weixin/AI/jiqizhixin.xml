<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>专访 | 谷歌神经网络翻译系统发布后，我们和Google Brain的工程师聊了聊</title>
      <link>http://www.iwgc.cn/link/2899977</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;记者：老红&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt; 参与：吴攀、张俊、李亚洲、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;9 月 27 日，谷歌在 arXiv.org 上发表论文《Google`s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》介绍谷歌的神经网络翻译系统（GNMT）后，「机器之心」第一时间进行了详细的解读和报道，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719470&amp;amp;idx=1&amp;amp;sn=3368dea980517ea7d7942967da2740dc&amp;amp;chksm=871b0090b06c89863620be4e75c757940d03d8a43cd3c1d9a8309b6594c1bccd769cab193177&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719470&amp;amp;idx=1&amp;amp;sn=3368dea980517ea7d7942967da2740dc&amp;amp;chksm=871b0090b06c89863620be4e75c757940d03d8a43cd3c1d9a8309b6594c1bccd769cab193177&amp;amp;scene=21#wechat_redirect"&gt;重磅 | 谷歌翻译整合神经网络：机器翻译实现颠覆性突破（附论文）&lt;/a&gt;。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最新报道发出后的第二天，「机器之心」受邀来到谷歌中国和来自 Google Brain 的软件工程师陈智峰聊了聊人机翻译、GNMT 和谷歌的技术创新等问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下为采访对话，「机器之心」略有删改：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：神经网络翻译系统（NMT）将整个输入的句子视作翻译的基本单元，相比于之前基于短语的翻译系统，除了所需的工程设计更少这个优点外，句子意思理解的精确度有多大的提升？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们过去的方法有很多翻译出来给人看，会发现有很多错误。机器会给这些翻译结果打个分，而我们新的系统作出的翻译所得的分会很高。我们在翻译结果的正确率在一些分数上会提高大概 0.5 分到 1 分，这是非常巨大的进步。比如，刚才我的同事讨论在微信上最近有些人开始测试「小偷偷偷偷东西」这个句子，相比过去的模型，这个翻译会非常非常正确。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：GNMT 有利用外部对准模型（External Alignment Model）对罕见词进行处理吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们这个模型是没有 External Alignment Model 的，其他一些地方需要使用到外部对准模型来帮助神经网络模型来达到同样的效果，我们这个模型是不需要外面帮助的，整个训练和整个模型就是端对端的模型，它的迅速速度非常简单，你对照着中文句子，对照着英文句子就可以告诉我们，当中几乎没有任何其他的帮助就能够学习到这样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：那在罕见词的处理上是用了什么方法呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们有两种方法，你可以把中文一个句子分成一个词一个词，比如说「我们今天见面」，你可以把它分成三个词，我们、今天、见面，你也可以把它分成六个字，传统的方法还有很多其他的类似系统里面大多数都是把它分成词来进行建模和训练的。英文也有同样的规律，比如说这个英文里是两个词，我们现在用的方法是把中文词全部打成字，然后把英文单词全部打成像词根一样的部分，比如说英文词语言、图像它的前缀都是一样的，所以说我们把这些前缀作为一个单元进行翻译，而不是像过去那样三个词分别翻译。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们都知道人在阅读时有一种能力是可以忽略文字排序错乱的问题，机器面临这种情况如何像人类一样高效处理这些内容？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;不受顺序的影响我觉得是一个程度，比如说你随便打出一个中文的十个字的句子，你任意打乱的话很有可能是错误表达的，但如果你随机调整两个的话是可以跳过一些错误拼写出来的字。现在的模型也能做到，因为整个模型是统计模型，就是你看到前面几个字，然后猜下一个字是什么，有些字的可能性大一些，有些可能性低一点，也可能它会说下一个字就应该是一个空格，这样的话它就会跳过去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们做过一些测试。中文里经常会有一些标点符号，比如前面是双引号，写了句子以后会发现最后忘了加引号，你可以很清楚地看到这个模型会意识到这个地方你是想写一个双引号的，但你没有写。这些现在都能在一定程度上避免这些错误。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：你觉得 GNMT 在技术上的新突破以及未来的发展是否会完全取代人工翻译？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我觉得完全替换，或者说在任何情况下替换人的翻译还是有一定难度的。现在的机器翻译都是基于已经出现过的语言现象，但是整个人类不停地在发展，比如说网络上经常出现不同的新的语言现象，比如说有些短语、有些常用语，也是不停地在变化的。所以说机器是很难发明新的规则来表达你现在的意义，最终还是要靠人来创造出新的表达方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就是机器从人那边学习到怎么表达这个意思更加贴近时代，比如说同样一个英文词一百万，在英国和美国 20 年前是不一样的。还有对于同样的单词，如果是英国长大的人搬到美国，他会自己调整适应度。也就是说英语环境变化了，它的意思发生了微妙的变化，而你要让一个机器翻译的系统能够捕捉到这样细微的变化还是很难的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：在结构化比较高的文章中，比如论文、科技文献上的处理是不是会更接近一些？可以取代一些？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;陈智峰：对，那些我觉得可能，比如说翻译一些医学的文章，就是这个领域非常非常固定的，有些很成文的规则表达的东西，我觉得将来会非常依赖于机器翻译来处理各个语言之间的信息交流，帮助会很大，而且精确度也会很快提高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：从你们一些产品的实验中，就是从更大的范围来看，您觉得有哪些领域是现在特别适合机器翻译来做，或者机器翻译的水平和人的水平最接近的？哪些领域是目前不太适合机器翻译来做的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;比如说在时事新闻方面，因为很多时事新闻写作都是有套路的，比如说美国总统今天怎么样，中国国家主席怎么样，这些模式比较固定的情况机器翻译就能够做得比较好，而且读新闻的人不太注重时事新闻的写作文笔，更注重的是信息的传达，所以说在一些修辞方面或者情感的传达方面要求比较弱。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器翻译就能够很快地帮助你获得信息，这是主要机器翻译目前对人类的帮助。目前来讲我觉得在人与人之间的自然沟通上，机器翻译还是有很大的工作需要做，才能达到真正能够让你感觉到跟你说话的人是一个真的人，而不是一个机器，这还需要很多年的努力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：在科幻小说《银河系漫游指南》里，有一个种叫「巴别鱼」的生物能实时翻译任何语言，你觉得 Google 实现这样的水平还需要多久？（也就是说在更高层次上与自然语言处理上，实现两种语言对话的实时翻译，预计这种情景能多久实现？中间有着什么样的技术难题？）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;陈智峰：很难预测，但我觉得从目前来看如果是要达到信息交互的过程还是很有可能的，但是要达到让你感觉是跟你家人说话的亲切感还是有很大的距离的，尤其是如果你要实时地和另一个人交互。我觉得现在要做到实时翻译还是有一些距离的，尤其是实时的语言翻译还是有一些距离的，但是三年五年可能会有一些突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：除了在机器翻译上，你们现在在其他语言和产品（例如 Allo）上这项技术应用的进展如何？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;GNMT 是非常针对机器翻译这个产品做的，但是它的顶层研究，就是模型本身是非常广的，可以使用到很多领域里的，每一个产品都在这个基础模型上做一些开发的。举例来讲大家都有 iPhone，但是每个公司做的每个项目上的 APP 不一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：现在 seq2seq+attention 的模型已经在 NMT 及其他众多 NLP 任务上取得了非常好的效果，本次发布的 paper 中提到用了更多层的网络得到了更好的效果，请问是否还可以不断地增加网络层数来提升效果？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;现在所谓你的层数增加，大家会普遍认为你的模型的能力就相对增强，但是在现有的技术条件下盲目地增加深度的话也有缺点。你的层次增加了，在应用时候的速度会变慢，因为它的计算量会增加，所以在现实当中都是有不同的考虑的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：seq2seq+attention 的模型在效果方面是否达到了上限，从而需要更新的模型来解决问题？如果有的话，Google 最近在研究什么新的模型？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我觉得在机器翻译这个问题上，就是在基本的模型上还有很多扩展的余地，就是说你可以把这个模型变得更大，程序增加，它的模型架构是基本上一样的，但是在这方面你可以进一步推展。当然，这个领域变化很快，每年都会有不同的细分结构、模型结构出来，我们会不停地取长补短，就是有没有可能更快地提供更好的翻译服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：我有注意到现在移动版和网页版的 Google Translate 的汉英翻译已经在 100% 使用 GNMT 机器翻译了，为什么会率先在汉英 翻译上去应用呢？现在是有什么技术难点或者考虑吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;有两个基本考虑，第一个考虑是在所有的 Google Translate 领域中，中文翻英文和英文翻中文的确是很大的一部分。用户很多，这也是在很多翻译任务当中相对较难的一部分，因为这是两个非常不同的语言，所以从传统上来讲这是一个比较难的。另外一点，在整个项目开发过程中有很多中国同事参与，有很多能懂中文也能懂英文的人在第一个系统的时候可以帮助调试，这会有很大帮助，所以这是主要的考虑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：不同语言的语料规模差别很大，英文中的语料非常多，但中文语料就显得非常少。请问，能够将 NMT 的研究成果应用在不同语言语料构建上，从而提升其他语言 NLP 研究水平？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;其实中文语料也是很多的，在我们的数据库里中文语料是英文语料的一半，但是这两个语言的语料库中我们掌握的语料是非常非常巨大的，世界上有很多其他语言没有足够的数据，所以这也是一个研究难点，就是怎么通过其他的语言来帮助翻译一些小语种，也是我们正在努力的方向。因为 Google 不光是要服务英文、中文，它的目的是让世界上所有的国家、所有的人都能够获得同样的服务，所以说我们非常致力于全世界有 100 多种语言，我们希望这 100 多种语言我们都能够做到很快翻译。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：刚才提到 Google Brain 和 Translate 这次的合作，我想知道在 GNMT 这次的技术研发包括产品的应用上，两个团队之间的分工是怎么样的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们主要是在初期建模的工作上做的工作比较多，在怎么使用最新的那些 GPU上，我们早期工作做的比较多。在 Google Translate 里面，他们负责很多怎么获得数据的工作，有些数据的有关情况需不需要调试，还有最后怎么把这个模型应用到产品里去，他们也做了巨大的工作，这是主要的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实 Google 很多技术开发都是这个模式的，不同的团队之间都会有很强的合作，因为 Google Brain 更多的是一个注重于神经网络、人工智能方面研发的团队，而 Google Translate 主要负责的是翻译这个产品，当然他们也有研究团队。所以各个团队不同，他们花了很多的时间，那么多年，就是需要采集数据，比如说我们训练的时候可以完全用他们已有的训练数据，每个团队的目标不一样，但是合作非常流畅，没有任何问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：你们的新论文描述了让 NMT 在非常大型的数据集上工作的许多挑战，你觉得当前最大的技术难点在哪里？在翻译速度和准确度的提高上你们又做了哪些创新？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个项目在三年前，也就是 2013、2014 年的时候 Google Brain 就想做了，当时从硬件和软件上都无法支持训练这个模型，在过去两三年中 Google Brain 开发了 TensorFlow，使得训练类似的模型可以充分利用分布式计算，利用很多很多不同的硬件类型。另外，如果你这两三年没有一些专门的硬件加速器的话也是很难在短时间内完成这个训练。而过去两三年 Google 在机器学习、在人工智能方面的巨大投入，使得类似的操作才变得可行。当然，我们也做了很多，对于具体的训练在我们现有的硬件资源上做了很多优化，这也是一方面的努力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您说这个短时间是短到什么程度？是多久训练一次？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;一般来说我们训练一个语言现在需要五天到六天的时间，差不多需要将近一百个 GPU 的加速器才能做完，差不多一星期才能处理一个方向的语言模型。但是 Google 有大概一万个语言的模型需要训练，当然我们有巨大的资源投入，也在不停地改进算法，所以说都在努力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：这次我们这个系统昨天晚上的新闻发出来以后，很多人拿一些有趣的句子去测试，我想了解一下你们内部也有很多中国人，有这种有趣的句子测试它吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;上市之前我们每个同事都绞尽脑汁想出一些办法考考我们这个系统，大家都觉得不太容易考倒它，所以最后决定我们可以拿出来给用户用一下。当然，肯定不是一百分，但是我们还是相对满意的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 Google 以前有那么多年积累下来的难题，用户在使用Google翻译时，可能当时觉得翻译得不好，他们就会有记录，然后发给我们。我们会把那些东西放进新的系统看看这次做得好不好，比如说把中国的一些歌词拿出来放进去看看翻译出来的结果会不会比原来好，其实那种测试主要是防止它说出一些不太好的话。我们都做了很多测试，所以我们觉得还是有信心的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：我昨天晚上做了一个测试，我输入中文「我要下班」，结果翻译成「I want to work」。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;陈智峰：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;哈哈，这个我们也有收到反馈并修复了这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 30 Sep 2016 11:39:19 +0800</pubDate>
    </item>
    <item>
      <title>深度 | Kaggle创始人Quora问答：深度学习会淘汰其他的机器学习方法吗？</title>
      <link>http://www.iwgc.cn/link/2899978</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Quora&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Leonardo Luke、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Kaggle 联合创始人兼 CTO Anthony Goldbloom 在 Quora 上公开回答问题，&lt;em style="color: rgb(136, 136, 136); text-align: justify; white-space: pre-wrap;"&gt;&lt;span&gt;内容涉及了 数据科学、Kaggle 入门方法等&lt;/span&gt;&lt;/em&gt;。机器之心整理了他回答的所有问题。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.未来 5 年，数据科学将会发生什么变化？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个问题上，我不打算谈太多数据前沿如何发展，而更多关注于数据科学逐渐成为主流变得无处不在。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在预测数据科学未来五年的发展时，回想其过去五年的进展大有裨益。2010 年 Kaggle 成立时，「数据科学」一词还不常见。我们社群的同僚们用其他的词汇来描述自己的工作，比如高级分析、统计学、机器学习、生物信息学、计量经济学或者其他和数据以及统计技术相关的专业之一。很多公司也用职能来称呼其负责数据相关工作的部门：市场分析、风险控制、包销、化学信息学等等。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2011 年 O`Reilly 的 Strata 大会让数据科学一词红了起来。会议聚集了一千五百名「数据科学家」，也让有着不同职称的人们有了根据他们的技能来称呼自己的方式。会议也让高级管理人员明白，不同部门的数据人员实际上有着同样的技能组合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果说 O`Reilly 的 Strata 大会是棒球里面的第一局（注：棒球共 9 局），我认为现在我们正在进入第二局。现在很多的公司将他们的数据科学家整合成单个数据科学部门。最有效率的公司架构就是数据科学部门将其数据科学家分配给业务部门（市场、风险等等）。这种架构十分有效，因为数据科学部门会学习如何吸引和招募数据科学团队，同时也允许数据科学家和具体问题语境中的人一起工作。Airbnb 就是有效利用这种架构的公司之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着公司不断挖掘现存数据科学团队的价值，这些团队也会继续发展。最终，我认为中央的数据科学团队会消失，每个业务部门会有自己专门的大型数据科学团队。如果数据科学成为了公司中主要的决策工具，那么它就获得了成功——当需要作出决定时，管理层的第一反应是问「数据科学怎么说？」&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从另外一个角度回答这个问题的话，我认为在下一个十年，数据科学领域的规模会比软件工程更大。如果我们把使用 R 或者 Pythong 数据工具的人定义为数据科学家，目前大约有 150 至 300 万数据科学家（根据 Kaggle 的用户量 65 万 和 Jupyter 计划用户数约 300 万估算，目前全球软件工程师约有 2000 万）。同时，目前全球有约 800 万 SAS 用户和 1.2 亿 Excel 用户。我认为 SAS 会慢慢衰落，重度依赖 SAS 和 Excel 的工作都会转向 R 和 Python。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.初学者如何在 Kaggle 上入门？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kaggle 对于喜欢边学边做（而不是通过读书或者看讲座）的人来说是一个非常好的入门方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于喜欢从非常具体问题开始的人，我建议从四个「入门」竞赛（compeition）开始。我们最简单的竞赛是根据性别、舱位等级来预测谁能在泰坦尼克号事故中幸存。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你的电脑上还没有 Python 或者 R 的运行环境，我们为你准备了 Kernels 工具，这是一个在线的脚本编辑器，可以让你在不安装 R 或者 Python 的情况下运行代码（并且已经连接好了数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我建议从「fork」（程序员词汇，意为复制）别人的 Kernel 并进行编辑入手，而不是自己从头开始。如果你想从 Python 开始（我推荐），我建议用 Omar El Gabry 的 Kernel，这个 Kernel 是一个优秀的端到端的工作流程，从探索数据开始，到基本的机器学习模型结束。如果你喜欢 R，我推荐 Megan Risdal 的 Kernel。如果你还没准备好从 Python 或者 R 开始，我们准备了一个简单的 Excel 教程 (https://www.kaggle.com/c/titanic/details/getting-started-with-excel)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你喜欢自由探索，或者就是不喜欢竞赛，我建议看一看我们的开源数据集 。这些数据集和竞赛没有关系，但通过共享代码和论坛讨论，他们依然对学习有所帮助。你可以从美国婴儿名字开始，这是一个简单、有趣的数据集，记录了过去一百多年以来美国婴儿起名的趋势变化。再说一遍，我建议从 fork 别人的 Kernel 开始，相比从空白屏幕上的一个光标开始，这样学习不那么困难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.哪些工具可以让数据科学家更有效率？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们认为目前数据科学工具差不多和 15 年前软件工程的工具一样。现在做数据科学工作比 5 至 10 年之后要痛苦得多。目前，数据科学工作流程的共享和协作非常痛苦（甚至让其他人的分析在你的机器上运行都需要一些技巧），将机器学习模型用于生产也是一项挑战。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，我们正在开发一个叫做 Kaggle Kernels 的环境，致力于让数据科学工作流的共享、协作更加便捷（最适合的类比是给数据科学家的 Github）。有人用了 Kaggle Kernels 进行分析之后，你就可以 fork 他的分析（复制代码，Docker 容器以及数据连接）。这样你就可以立刻运行他们的分析，并可以迭代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，Kaggle Kernels 只对 Kaggle 竞赛和 Kaggle 上分享的开源数据集开放。明年年中，我们将会有商业化的产品，数据科学团队可以使用其进行团队内协作并分享结果。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;至于将模型投入生产，是大型的云服务提供商正在专注的领域（比如微软的 AzureML、亚马逊的 Amazon Machine Learning）。对于他们现存的计算和数据存储业务来说，这是顺其自然的扩展。目前还没有大型云服务商成功，但我预测在未来几个月至几年的时间我们会看到他们的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，数据科学家需要将专用于工作流上不同细分领域的工具拼凑起来。Git 是控制代码版本的最佳选择。类似 RStudio 和 Sublime Text 这样的 IDE 可以让编程效率更高。呈现结果的工具有 Jupyter Notebooks 和 Shiny。Kaggle 之前使用 Make 来作为编制工具。我也听说一些公司使用 PMML 部署简单的模型，取得了成功。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.深度学习会淘汰其他的机器学习方法吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我不这么认为。我认为在一些场合中深度神经网络是最佳选择，而在其他的地方则需要其他的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，我们发现深度神经网络在一些无结构的数据竞赛中能够获胜（比如图像、视频、脑电图）。对图像和视频来说，卷积神经网络较好；对于脑电图数据和其他包含序列的数据来说，循环神经网络较好。对于结构化的数据问题，我们发现巧妙的特征工程结合梯度提升机器（特别是整合 XGBoost）获胜最多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于自然语言处理问题，情况会复杂一些：有些时候需要循环神经网络，有些时候需要结合 XGBoost 的信息检索方法。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度神经网络的支持者说在数据集「足够大」时，深度神经网络就会超越其他方法。在 Kaggle 的竞赛中我们还没有发现能够佐证这一说法的证据。可能是因为我们的竞赛还没有「足够大」的数据。然而，即便足够大的数据会让深度神经网络显出优势，很多应用还是只有小、中规模的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为相关方，我们在大部分的竞赛结束之后都会在我们的博客上对获胜者进行采访，问他们采用了什么方法。我们把这看作是记录有监督的机器学习问题中的优秀方法的「活日志」。我们也把所有获胜者的采访作为数据集发布在了 Kaggle 上，你可以使用 Kaggle Kernel 来发现机器学习的发展趋势，看出在哪些场合深度学习占优，哪些场合深度学习处于劣势（比如这个简单的 Kernel 就说明深度学习在在 Kaggle 竞赛中呈现上涨趋势）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5.Kaggle 如何盈利？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前我们有两个主要的现金流来源于主持的竞赛和我们提供给的工作招聘版块。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有三种比赛：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 特色比赛：公司可以利用这种比赛提升自己在数据科学社群的知名度，接触新的方法或者解决疑难杂症&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 招聘比赛：公司利用这种比赛吸引并招揽之前通过其他方式招募不到的人才&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 研究比赛：研究人员用把这种比赛作为与数据科学家协作的另一种方式。这对于非常具体且具有挑战性的问题大有裨益。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在未来，Kaggle 计划增加额外的服务。我们打算让 Kaggle Kernels 成为免费增值服务，付费后，公司可以将其用为数据科学团队的协作环境。在更远的未来，还有更多的有意义 d 附加服务（包括数据科学家咨询服务的市场）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;6.招聘者在职位申请上看到 Kaggle 竞赛会怎么想？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司用不同的方式使用 Kaggle 进行招聘。Facebook 和沃尔玛单独举办竞赛来招聘人才：他们会对竞赛中表现出色的人进行面试。Google DeepMind 这类的公司会关注我们的竞赛，并主动接洽表现出色的人才。我们了解到有些公司要求在工作申请中放上 Kaggle 资料链接。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们听说招聘方喜欢在简历中有 Kaggle 结果的数据科学家，因为这样显得应聘者对数据科学有一定的热情（即并不只是为了工资），也让招聘方了解到应聘者的能力如何。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也了解到招聘方希望看到应聘者在竞赛之外的其他信息。就这一点，我们最近上线了开源数据平台，并对 Kaggle 资料进行了改版，以突显个人在优秀 Kernel 以及我们论坛上的贡献（在竞赛表现之外）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;7.Kaggle 目前的重点是什么？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kaggle 目前的重点是从数据科学家业余项目的站点，变为数据科学家常用工作站点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了完成这一点，我们正在着重发展 Kaggle Kernel，让其成为数据科学家共享和协作工作的地方。目前，数据科学家可以使用 Kaggle Kernel 来共享代码、竞赛结果和开源数据集。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到明年年中，我们计划把 Kernel 向小型团队开放，让他们在团队内部使用。小型团队将可以使用 Kaggle Kernel 来 fork 同事的工作，或者从来自社群公开分享的庞大资源中选择并 fork。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;8.数据科学家最佳的协作方式是什么？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前还没有好的解决方案。正如上面说到的一样，现在就连把一个人的分析放到另外一台机器上运行都是一个挑战（需要一样的数据，一样的语言版本，一样的库，有时候库的版本也需要一致）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也是我们在 Kaggle Kernel 上积极解决的问题。Kernel 整合了 Git（代码版本控制）、Docker（运行环境版本控制）以及数据连接。Kernel 可以用于竞赛协作和 Kaggle 上的开源数据集。目前 Kernel 还不对小团队开放。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，小团队的高效协作方式之一是自己把这些技术整合起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;9.Kaggle 竞赛和数据科学家的工作有多相似？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kaggle 竞赛涵盖了很多数据科学家的工作内容。缺少的两块内容是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 把一个业务问题具体化为一个数据科学问题（包括提取数据，进行结构化，以解决业务问题）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 将模型投入产品&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kaggle 竞赛间接帮助训练了数据科学家对问题进行结构化。竞赛让我们的社区了解到了大量的、精巧的结构问题。所以即便社区并没有直接参与，他们也会看到我们如何对不同问题进行结构化，并且将这些结构应用到解决实际的业务问题中。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在也有开源数据平台了，这样一来，社区可以更早地接触到问题。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前我们的社区很少有模型产品化的接触。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 30 Sep 2016 11:39:19 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 详解谷歌神经网络图像压缩技术：如何高质量地将图像压缩得更小</title>
      <link>http://www.iwgc.cn/link/2899979</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Nick Johnston 、David Minnen&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌 Research Blog 今日发布文章解读了其不久前在论文&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718466&amp;amp;idx=3&amp;amp;sn=ffeb503724abb0934c5c60e1202b12c0&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718466&amp;amp;idx=3&amp;amp;sn=ffeb503724abb0934c5c60e1202b12c0&amp;amp;scene=21#wechat_redirect"&gt;《Full Resolution Image Compression with Recurrent Neural Networks》&lt;/a&gt;中报告的神经网络图像压缩技术。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据压缩已经在互联网上无处不在。你在网上看的视频、图片，听的音乐，甚至现在正在阅读的这篇文章的背后都有使用到数据压缩技术。压缩技术让我们可以更快更高效地分享内容。如果没有数据压缩，获取你需要的信息的时间和带宽成本将会高得吓死个人！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在论文《Full Resolution Image Compression with Recurrent Neural Networks》中，我们在我们之前的使用神经网络的数据压缩的研究上进行了延展，探索了机器学习是否能够像在图像识别和文本总结上一样提供更好的图像压缩结果。此外，我们也通过 TensorFlow 发布了我们的压缩模型，这样你也可以使用我们的网络实验压缩你自己的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;发布地址：https://github.com/tensorflow/models/tree/master/compression&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们介绍了一种新架构——该架构使用了门控循环单元（Gated Recurrent Unit，一种允许单元保存激活和处理序列的 RNN）的一种新变体「残差门控循环单元（Residual GRU: Residual Gated Recurrent Unit）」。我们的 Residual GRU 将已有的 GRU 和论文《Deep Residual Learning for Image Recognition》中介绍的残差连接（residual connections）结合了起来，从而在给定的压缩率下实现了显著的图像质量增益。我们没有和今天所用的许多压缩方法一样使用 DCT 来生成新的位表示（bit representation），而是训练了两组神经网络——一组用于创造图像的编码（编码器（encoder）），另一组用于从这些编码中创造图像（解码器（decoder））。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的系统可以迭代式地细化原始图像的重建，其中的编码器和解码器都使用了 Residual GRU 层，这使得更多的信息可以从一次迭代传递到下一次迭代。每一次迭代都会给编码增加更多比特，从而可以实现更高质量的重建。从概念上讲，该网络的工作方式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 初始残差 R[0] 对应于原始图像 I ：R[0]=I&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 第一次迭代，设 i=1&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 第 [i] 次迭代以 R[i-1] 作为输入，并运行编码器和二值化器（binarizer）将该图像压缩成 B[i]&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 第 [i] 次迭代在 B[i] 上运行解码器以生成一个重建出的图像 P[i]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 计算出第 [i] 次迭代的残差：R[i] = I - P[i]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6. 设 i=i+1，然后转到第 3 步（直到达到所需的迭代次数）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;残差图像（residual image）表示了当前版本的压缩图像与原始图像之间的差异。然后该图像被用作该网络的输入，以便为压缩图像的下一个版本移除压缩误差。现在压缩图像被表示成了 B[1] 到 B[N] 的级联。对于更大的 N 值，解码器可在减少误差和生成更高质量的原始图像的重建上获得更多的信息。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了了解其工作方式，可以看一下下面这个图像压缩网络的前两次迭代的例子，如下图所示。我们从一张灯塔的图像开始。在该网络的第一次前向通过中，原始图像作为输入（R[0]=I）。P[1] 是重建的图像。原始图像和编码图像之间的差异是残差 R[1]，其表示了压缩过程中的误差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBl1ncctWPibxyMYem9piaQUNALribwwNrsuic4u6N8jUkrpm7ZyTAZPkYjnw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;左图：原始图像 I=R[0]；中图：重建的图像 P[1]；右图：残差 R[1]，其表示压缩中所产生的误差&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该网络的第二次前向通过中，R[1] 作为该网络的输入（见下图），然后创造出一张更高质量的图像 P[2]。所以该系统是如何从残差 R[1] 中创造出一张如此高质量的图像（下面的中图 P[2]）的呢？因为该模型使用了带有记忆的循环节点（recurrent nodes with memory），所以该网络会保存每一次迭代的信息以便在下一次迭代中使用。它在第 [1] 次迭代中从原始图像上学到的东西被用在了对 R[1] 的处理上，从而可以从 B[2] 中生成更好的 P[2]。最后，通过从原始图像中减去 P[2]，该网络生成了一个新的残差 R[2]（右图）。这一次的残差更小，因为原始图像和重建的图像之间的差异更小了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBllDk1bnTgvB0WpXO3oqLV2WPicAxsokQOvawP0eCUwJRibJPpjHH5RticA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;该网络的第二次前向通过。左图：用作输入的 R[1]；中图：更高质量的重建 P[2]；右图：原始图像减去 P[2] 所得到的更小的残差 R[2]&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在更进一步的每次迭代中，该网络都会获得更多有关压缩中所引入的误差的信息（这些信息可在残差图像中获取）。如果其可以使用这些信息来预测残差（即使只有一点点），那也能得到更好的重建。我们的模型可以使用额外的比特，直到达到收益递减的程度——此时该网络的表达力（representational power）就耗尽了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了验证文件大小和质量差异，我们使用了一种日本犬种 Vash 的一张照片生成了两张压缩图像：一张 JPEG，一张 Residual GRU。这两张图像的目标都是达到 0.9 MS-SSIM 的感知相似度（perceptual similarity）——该感知质量标准如果达到 1.0 则表示完全等同。我们训练好的模型所生成的图像的文件大小比 JPEG 小 25%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBlpRhlMus1xqL3xeFibOwibXahRk4ibibI4fl8OKg4nfLPvGgMW87265MINQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;em&gt;&lt;span&gt;左图：原始图片（1419 KB PNG）~1.0 MS-SSIM；中图：JPEG（33 KB）~0.9 MS-SSIM；右图：Residual GRU（24 KB）~0.9 MS-SSIM。图像质量差不多的情况下，图像大小减小了 25%。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;观察一下图中狗的鼻子和嘴巴，我们发现我们的方法在图像中不会产生红色块和噪声。这是由于 JPEG 产生的块效应（blocking artifacts），然而我们的压缩网络一次性地在整个图像上起作用。我们的模型会在细节上有所权衡，图像中的纹理会丢失，但在消除块效应上，该系统显示出了极大的潜力。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBl1ibiaHF8E8RbUQ4DmcObjYBLUp5xnUaClckLhhdTClfjnubYd6MJia3ibw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;左图：原始图片；中图：JPEG；右图：Residual GRU&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然如今的编解码器表现很好，但我们的研究表明使用神经网络压缩图片能得到更高的质量和更小的文件大小。想要了解关于该研究的更多内容，可以阅读下面的论文：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：Full Resolution Image Compression with Recurrent Neural Networks&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBlytLqtJcJyIudRV0cbvwa2eE81IMFjxynU7qwQtiaaTODSwbgIRE2C2g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本论文提出了一套基于神经网络的全分辨率有损图像压缩方法。这些我们所描述的每一种架构都可以在实施过程中提供可变的压缩率，而不需要对网络进行再训练（retraining）：每个网络只需要训练一次。我们所有的架构都由一个基于循环神经网络（RNN：recurrent neural network）的编码器和解码器、一个 binarizer 和一个用于熵编码（entropy coding）的神经网络构成。我们对 RNN 的类型（LSTM、关联 LSTM（associative LSTM）进行了比较，并引入了一种新的 GRU 和 ResNet 的混合结构。我们还研究了 one-shot 与附加重建架构（additive reconstruction architectures）的对比，并引入了一种新的扩展过的附加框架。对比之前的研究成果，我们的成果显示出了 4.3%-8.8% AUC（率失真曲线下的区域）提升，具体数字取决于所用的感知标准（perceptual metric）。就我们所知，在 Kodak 数据集图像的率失真曲线（rate-distortion curve）的大部分比特率的图像压缩上，这是第一个表现优于 JPEG 的神经网络架构，不管有没有熵编码的辅助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBlytT5xhqc4nfmECDRKLMxO1aHBtRqDkr0WIds4FUKRLyTmkDSKDbDeQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：我们共享的 RNN 架构的单次迭代&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 30 Sep 2016 11:39:19 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 超限学习机提出者黄广斌教授：生物脑与物理世界的相似性指出了机器学习的另一个方向</title>
      <link>http://www.iwgc.cn/link/2899980</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心中文首发&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;编译：Terrence、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;黄广斌教授最近被南洋理工大学授予了全职终身教授的头衔（编者注：在新加坡南洋理工大学每年只有1-2%的教师提升为正教授）。在最近的英文采访中（点击阅读原文查看英文原文），他愉快地分享了自己的背景，以及目前在新加坡南洋理工大学电子电气工程学院的一些经验和感受。采访中，黄教授提到三个主要机器学习趋势：1）机器学习和生物学习存在收敛趋势，也许几十年后有明显作用，我们已经感觉到脚步声；2）生物脑和物理世界存在惊人的相似性；3）机器学习和达尔文的进化论存在一致性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去十五年在南洋理工大学的研究和教学生活是我最享受的一段时光，我非常珍惜和同事间的亲密友谊，过去这些年他们一直鼓励并支持着我。我们组成过许多临时的研究团队，我们共同的研究兴趣将我们联系在了一起。我非常敬佩身边的每一位同事和合作伙伴，他们的求知欲和探索欲深深地扎根在研究当中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也特别感激南洋理工大学和南洋理工大学电子电气工程学院的管理层给予我们的在研究课题与兴趣方面的自由。这种灵活性对于成功的研究至关重要。伟大的研究思路从来不会在压力与紧迫的时限下诞生，它们产生于真正的兴趣。人工智能与机器学习（尤其是人工神经网络）领域近来非常的成功，也变得非常的流行。然而在 15 年前，很多人都怀疑神经网络是否真的有用。一些研究人员也开始撤离神经网络的研究领域。就好像整个世界都将精力花费在调整神经网络的参数上，从而导致缺乏明智的机器学习研究进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我强烈地感觉到，一些具有挑战性的研究问题可能让神经网络研究社区陷入困境。这一点给了我很大的驱动力，让我想要在这些领域进行研究，因为我坚信我能够利用自己的跨学科研究和工程背景为这些研究做出自己的贡献。就这样，我决定离开制造业，怀着强烈的使命感和承诺加入南洋理工大学，感觉也许可以帮助机器学习社区尤其是神经网络研究社区从研究局限中走出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一直以来和同事及研究伙伴们在不同的场合讨论不同的研究课题都是令人享受的。这些谈论将会在我的记忆中永存留鲜。有时我们会聚集在 NTU 的餐厅或者躲在 NTU 电子电气工程学院的某些角落里继续讨论，燃烧某些余热未尽的想法，这些时刻我永远不会忘记。我们一起努力研究这些想法并将它们应用于现实，这些想法及其成果在过去几年最终吸引了数以千计的研究人员采纳使用。它们或许在未来会成为关键的机器学习技术，闪亮耀眼——准确的说，是非常绚丽。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;黄教授着重谈及他早期的研究历程中难忘的时刻：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2001 年加入 NTU 之后，我花费大部分的时间研究和琢磨两个问题：&lt;strong&gt;1）为什么大多数学习算法的学习效率很低？；2）为什么人脑比电脑更加智能和高效？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;起初并没有明确的答案，所以我开始认为从这个方向开展研究有些令人绝望。之后为了不至于让自己对研究感到太压抑，我开始沉浸阅读文学作品中，这段时间虽然短暂但重振了我失落的精神。我开始阅读经典文献，从头到尾把四大名著中的《三国演义》读了好几遍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中有一段奇特的经历。在 2002 年的某个午夜，我第七次阅读这本名著。脑中满是书中独特鲜活的人物性格，我突然意识到，过去的数百万年产生了数万亿人类和动物，每一个人或动物都有不同的大脑，却很难在这么多的大脑中安装用于不同的应用和任务的不同学习算法。我想之后可能会出现一些通用的学习算法，可以安装在这数万亿个不同的大脑中，用于不同的应用和任务，而大脑中的这些算法都应该和数据和应用相互独立无关的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我马上给我的学生打电话，让他来实验室，就在当天凌晨我们测试了我们的 data-independent 算法。然后我最终将该算法命名为超限学习机（Extreme Learning Machines，ELM）。在那时，我们意识到我们可能已经发现了生物学学习机制的一些秘密，生物学习可以不依靠调整神经元来实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随机神经元和随机「连线」可能是实现「不依靠调整隐藏神经元来学习」这一学习机制的两种特殊方式。神经元也可以从它们的祖先那里遗传。整体来说，整个活跃的大脑都是结构化的、有序的，但它们在某一特定的层或者大脑神经元薄片上或许是「随机的」、「非结构化」的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;令我们感到惊讶的是，大脑作为宇宙中最复杂的物体之一，竟然和虽然整体结构化但是局部满是随机布朗运动的物理世界如此相似，二者之间竟然有如此强的相似性！生物学习正是因为其全局的结构化构造与局部的随机性的共存才能如此地完美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经元与具体的训练数据无关。这一学习理论后来在 2013 年和 2015 年分别在老鼠和猴子的大脑中被发现。这一发现可能彻底颠覆了传统生物学习对于神经元调整可能是学习关键的认知。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种新的学习理念与传统的神经网络学习原理完全不同。我们也希望与一些神经网络领域的前辈和先驱们讨论我们的想法，但几乎没有人愿意在最开始相信我们。在最开始从理论上证明它们也是一件难事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于研究人员来说研究生活有时候很寂寞。但最终在 2005 年，我们觉得我们是正确的了，证明了我们理论中假设的正确性（在很多次错误的证明之后）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，理论上的证明在 2006 年正式公开发表。我感到比较自豪的是，我们在加入 NTU 的 5 年内的努力最终奠定了今后研究的基础。&lt;strong&gt;我们最终得到的学习技术比传统的学习技术快了将近一万倍，为实时的学习、认知和推理指明了方向。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;令人惊讶的是，尽管早期的一些机器学习领域的前辈们几乎不相信超限学习机的理论和技术，但是当我与大多数生物学家和神经学家讨论时发现，他们对于超限学习机的理论和技术是很容易接受的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习的研究人员通常遵循传统的机器学习和神经网络理论，认为学习智能只能基于精细的调整来实现。许多在一开始不相信超限学习机有效性的机器学习界的前辈们现在也开始转到超限学习机的研究领域中来了！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自那以后，生物学家、神经科学家和人工神经网络专家展开了非常多有趣的讨论！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超限学习机（ELM）成功和持续发展大都要归功于我的博士生们。和他们在一起的那些非正式的讨论启发了我们很多重要的想法。我过去常常请他们周末到家里，下厨给他们做点我拿手的菜，并且通过在周末打牌等不拘一格的形式来讨论一些至关重要的研究课题。有一次我们发现，正流行的机器学习技术——支持向量机（Support Vector Machines）——实际上提供了次优解。接下来我的研究人员和我花费了 6 个月的时间写了一篇关于支持向量机的论文——这篇论文成为了自那年开始发表的20多万IEEE 出版物中被引用得最多的一篇论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管我们一直专注于研究是希望能得到丰硕的成果，但现实并不总是尽如人意。&lt;strong&gt;我们相信超限学习机理论可以帮助解释达尔文主义的某些方面。&lt;/strong&gt;因此在 2014-2015 年我们决定来调查这一思想的分支。&lt;strong&gt;我认为：从祖先的神经元遗传到某些随机性（比如随机神经元和神经元之间的随机「连线」）在每一代都会给系统（甚至是那些生物体的系统）的进化和自然选择带来帮助。&lt;/strong&gt;然而，经过两年多的努力，由于缺乏可靠的大数据，我们暂时中止了这方面的工作——但是我们会在将来的某个时候在开始这方面的研究的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄教授也给出了他对于他当前所参与的电子电气工程相关项目的一些评价：「我以前在制造行业工作过，并喜欢它带给我的现实生活的那些实际操作的经验。话虽如此，我在 NTU 电子电气工程学院的研究经历也一样充实。我在这里的时候，我也一直主持负责一些由 BMW、Rolls Royce、Delta Electronics 和 ST Engineering 资助的和工业应用相关的研发项目。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;黄教授也对他全新的全职教授的职位充满了期待：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我很期待和我大学里的同事建立更密切的合作，并和其他世界顶级学府和公司的合作者们一起共事。总之，我相信，我们可以发现一些有效的解决方案，从而克服机器学习应用方面的一些具有挑战性的的瓶颈。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从长远来看，看到机器学习和生物学习不断靠近是令人兴奋的。&lt;strong&gt;超限学习机（ELM）可能会是弥补机器学习和生物学习间隔的基本「学习粒子」之一。&lt;/strong&gt;我希望我们能做出一些重要的贡献，来填补在机器学习、脑科学、智能材料以及智能传感器交叉研究的协同性方面的空白。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习在很多的实现成果中仍然是费力的和昂贵的。我们的目标是帮助研究界和工业领域走出机器学习研究的局限区域。在不久的将来，我们希望有一些有趣的机器学习解决方案能够：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1）用更小的数据来处理复杂的应用；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2）能够普适学习（pervasive learning）和普适智能（pervasive intelligence）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;帮助机器学习从工程学走向科学是十分有趣的。我相信「机器学习科学」最终将发挥重要作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也希望能成立一个公司并能稳步向前发展。我相信这家公司不仅会帮助将技术转换为实际的产品，同时也会给整个合作伙伴都带来益处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;黄教授在 NTU 电子电气工程学院最喜欢的地方：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我最喜欢的地方包括员工休息室，以及校园内其它拥有舒适的沙发桌椅的角落，因为在那里我可以与同事和学生们关于研究想法开展头脑风暴——也可以自由地谈论我们对于研究和未来计划的梦想。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;闲暇时间的兴趣爱好：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我喜欢看电影，经常在家里和电影院看一些有趣的影片。新加坡的裕华园自然之美吸引我常去进行锻炼。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我有机会时也会经常和其他研究人员一起去爬山。在我们登山的过程中集思广益头脑风暴总是会令人享受并富有成效。当你呼吸着新鲜的山区空气的时候，你会为这些创造力感到惊叹。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;给学生和其他教师的一些建议：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;致我所有亲爱的 NTU 电子电气工程学院的教职工：我发现，除了教学和研究，我们的友谊是让我们作为教育工作者更加和谐、创新、高效和愉快的原因。NTU 电子电气工程学院一直是温暖的大家庭。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我只想说，我很珍惜我拥有的友谊，并期待和大家建立更紧密的合作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我亲爱的 NTU 电子电气工程学院的学生们：尤其是在这个机器学习和大数据的时代，有很多很多极佳的黄金机会正在向你们招手！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心首发，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 30 Sep 2016 11:39:19 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 硅谷安全教父加盟滴滴：AI将成未来信息安全关键</title>
      <link>http://www.iwgc.cn/link/2899981</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9 月 29 日，滴滴出行今天宣布，硅谷知名信息安全科学家、AssureSec 联合创始人弓峰敏和卜峥加入滴滴。弓峰敏将担任滴滴信息安全战略副总裁和滴滴研究院副院长，卜峥将担任滴滴信息安全副总裁，全面负责滴滴信息安全的运营。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是滴滴在信息安全领域相关内容的首次重大披露。目前，全球信息安全市场正面临新挑战，同时也充满机遇。在两位重量级安全大佬加盟之后，滴滴很可能将尝试用人工智能和机器学习技术去发展信息安全技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;安全威胁日益严峻&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为 Palo Alto Networks 联合创始人及多家知名信息安全公司高管，弓峰敏指出，过去几年，信息安全威胁的发展很快。相对于安全防御者，黑客似乎总是可以更快地利用网络和移动技术去发动攻击，找到绕开防御的种种规避手段。另一方面，黑客工具的制造者、僵尸网络的运营者之间已形成了「高效」的信息分享产业链。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一系列事件证明了信息安全形势的严峻。例如，在 9 月 19 日的国家网络安全宣传周上，百度安全发布了《百度安全打击网络黑产白皮书》。白皮书显示，2016 年上半年，涉嫌泄露或窃取用户信息的事件超过 10.6 亿次，其中用户信息泄露超过 5.4 亿条，用户隐私窃取超过 6.3 亿次。与此同时，网络黑产的规模不断「壮大」。上半年，网络黑产从业者已达 56 万人，市场规模超过 1482 亿元，从业者人均收入 26.5 万元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在美国，信息安全事故同样频发。2014 年，索尼影业遭遇了大规模信息安全攻击，损失至少达 1500 万美元。2013 年底，美国零售巨头塔吉特在数据泄露事件中丢失了 1. 8 亿用户的信用卡和其他个人信息。今年 9 月，雅虎也曝出了安全事故，5 亿帐号的信息被黑客窃取，引发了业内哗然。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这样的情况下，企业 IT 团队应对信息安全威胁的能力依然有限。以往，企业信息安全防护着眼于孤立的网络连接、数据存储和计算资源，通常只在安全威胁出现后才会被动地去响应。随着云计算和移动设备的普及，企业 IT 环境正变得日趋复杂。而物联网的发展，家电、汽车和工业设备纷纷接入网络则进一步导致了可能被黑客突破的「攻击面」不断扩大。换句话说，传统信息安全策略在新环境中很可能顾此失彼，难以实现整体式防御。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;策略：以业务为中心&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;弓峰敏指出，信息安全行业以往强调入侵防御，对攻击的应对策略是「拒敌于国门之外」。但实际上，近期出现了越来越多传统方法难以检测的高级安全威胁。同时，日趋多样化的安全威胁往往有着不同意图：一部分会对企业业务产生重大威胁，而另一部分则是黑客或业余爱好者的恶作剧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBlvxDMNSMWNtia6XQf2HMMdQ6FZuSejKttdJS8Z0pM2Ud8xicicrtlcbic5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;滴滴信息安全战略副总裁弓峰敏&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以往，大多信息安全产品只关注安全问题的某个阶段或某个侧面，例如软件是否存在漏洞。然而，这些漏洞并不一定会给业务造成不利后果，例如数据丢失，交易信息泄露。企业并不需要去处理所有安全漏洞。如果继续沿用传统的信息安全策略，那么效果通常不佳，安全防御只能被动地跟随黑客的步伐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于这样的局面，弓峰敏认为，安全防御重点应当转向以业务为中心，以不间断、大规模的监测为基础，并利用大数据和人工智能技术去判断是否有威胁和异常的出现。简而言之，这就是分布式的安全检测配合中心化的威胁数据分析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;安全的未来：人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统上，企业信息安全团队需要在沙箱中运行恶意软件，人工分析，进而得出潜在威胁的属性，并在此基础上制定防御规则，将规则应用于安全网关或其他网管设备。这样的流程耗时耗力，且分析能力有限。在这种架构下，安全网关等设备对企业的信息安全能力至关重要。一旦安全网关被攻破，企业内部网络将门户洞开。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此不同，弓峰敏和卜峥团队的技术摆脱了中心化的安全网关。这一技术基于软件和虚拟配置，在终端设备中部署分布式「探针」，从而充分利用终端设备去收集潜在威胁信号，在威胁刚刚出现时捕捉其中的蛛丝马迹。与此同时，系统利用机器学习和人工智能技术，通过云计算平台沙箱对终端设备收集到的海量数据进行自动化分析和学习，不断寻找恶意软件和非恶意软件所表现出的不同模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用机器学习算法去取代传统的「if-else」逻辑带来了很强的通用性，能将数千输入信号考虑在内。与此同时，利用持续输入的数据，机器学习算法能以流程化方法不断建立新模型，并随恶意软件的变化而主动调整，增强检测能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这样做带来了两方面优势。一方面，系统对信息安全风险的监测将不再是孤立的，而是有能力全面了解各方面环境因素。因此，无论是底层硬件还是业务逻辑，各种异常都可以被检出。另一方面，这将成为基于云计算的一体化产品，并具备极强的自主运行能力。企业 IT 团队将无需去维护碎片化工具，减少所投入的人力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;滴滴助力信息安全研究&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，弓峰敏和卜峥团队加入滴滴正是由于，滴滴提供了团队迫切需要的大数据集。弓峰敏指出，其团队的技术要求与业务数据密切交互，而利用滴滴的框架和资源，团队能更方便地去展开技术研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9ibicGpNCBjqeDIoXuzlnTBlKl51kmoQmHJrFu7ACr2aicF8ADqcTPHRaeicSZ7Ec1R6VCYogz4ZCkpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;滴滴信息安全副总裁卜峥&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一方面，滴滴也带来了丰富的安全场景。这既包括基本的安全攻防问题，也包括如何应对网络欺诈和犯罪，预防用户信息泄露，交易出现风险。通过滴滴的业务实践，以及滴滴自身对云计算、大数据和人工智能等前沿技术的开发，研发团队将获得第一手研究素材，使持续发展的新技术在第一时间得到尝试和应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;信息安全市场仍在快速发展。根据市场研究公司 Gartner 的数据，2016 年全球信息安全产品和服务支出将达到 816 亿美元，同比增长 7.9%。这一市场正受到传统企业 IT 巨头的密切关注。例如，甲骨文上月宣布收购云计算信息安全公司 Palerra，而思科和赛门铁克近期也均在这一领域进行过收购。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在国内，移动互联网的发展也给信息安全带来了挑战和机遇。弓峰敏认为，作为具有代表性的移动互联网公司，滴滴在这笔收购后有望实现突破性的成功应用案例，进而给整个中国互联网安全市场产生积极影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心发布&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 30 Sep 2016 11:39:19 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 谷歌发布YouTube-8M：单个GPU一天就能完成训练的最大视频数据集（附论文）</title>
      <link>http://www.iwgc.cn/link/2884651</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sudheendra Vijayanarasimhan、Paul Natsev&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习和机器感知领域近来的许多突破都可归功于大型有标注数据集的可用性，例如 ImageNet，其包含了分成了数千个类别的数百万张有标签的图像。它们的可用性显著加速了图像理解领域的研究，例如检测和分类静态图像中的物体&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视频分析能为检测和识别物体、理解人类行为和与世界的交互提供更多的信息。改善视频理解能带来更好的视频搜索和发现，这类似于图像理解帮助重新想象照片中的经历的方式。但是，这一领域进一步发展的一个关键瓶颈是缺乏与图像数据集同等规模和多样性的真实世界视频数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，我们很高兴宣布发布 YouTube-8M——该数据集包含了 800 万个 YouTube 视频 URL（代表着 500,000 小时的视频）以及它们的视频层面的标签（video-level labels），这些标签来自一个多样化的包含 4800 个知识图谱实体（Knowledge Graph entity）的集合。相比于之前已有的视频数据集，这个数据集的规模和多样性都实现了显著的增长。比如说，我们所知的之前最大的视频数据集 Sports-1M 包含了大约 100 万段 YouTube 视频和 500 个体育领域的分类——YouTube-8M 在视频数量和分类数量上都差不多比它高一个数量级。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOxjcCG3FssrxrxmrcfQEXmOtgfPhs58iarib1HIVUGCVBrAmu81ibsq5Fw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了构建一个这样大规模的有标签视频数据集，我们需要解决两个关键难题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;用人工标注的话，视频标注比图像标注所需的时间远远更多；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;视频的处理和存储的计算成本非常高。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了克服难题 1，我们使用了 YouTube 及其视频标注系统（video annotation system）。该系统可为所有公开的 YouTube 视频确定相关的知识图谱主题。尽管这些标注是机器生成的，但它们整合了来自数百万用户的强大的用户参与信号（user engagement signals）以及视频元数据和内容分析。由此，这些标注的质量是足够高的，可用于视频理解研究和制定标准的目的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了确保该有标注视频数据集的稳定性和质量，我们仅使用了包含超过 1000 条评论的公开视频，而且我们创建了一个多样化的实体词汇集（vocabulary of entities）——这些实体都是视觉上可见的，而且出现的频率也足够高。该词汇集的创建结合了频率分析、自动过滤、人类评估者验证该实体是视觉上可见的、以及分组成 24 个顶层的垂直类别（更多详情参见我们的技术报告）。下图描述了其数据浏览器（dataset browser：https://research.google.com/youtube8m/explore.html）和在顶层垂直类别的视频分布，同时也说明了该数据集的规模和多样性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOibn9vIZV3wDPqvld1KaAkiciaRBulnsicLAfErxylSbZEBy2bglUmUaejw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;数据浏览器允许浏览和搜索整个知识图谱实体词汇集，它们被分成了包含了对应视频的 24 个顶层的垂直类别。这张截图描述了一个标注了实体「Guitar」的数据集视频的子集。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOMdLwD3lJQR76NN9OBvbImHpyd7FU3ibdibUAVlv0qooZiaiaqXW8wKLMhA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;该顶层垂直类别的视频分布说明了该数据集的规模和多样性，同时也反映了流行的 YouTube 视频的自然分布。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解决难题 2，我们必须克服研究者处理这些视频时所面临的存储和计算资源的瓶颈。在 YouTube-8M 的规模上进行视频理解通常应该需要 PB 级存储以及相当于 CPU 工作几十年的处理能力，为了让计算资源有限的研究者和学生也能用上这个数据集，我们对视频进行了预处理并使用了在 ImageNet 上训练的公开可用的 Inception-V3 图像标注模型（一种目前最佳的深度学习模型）提取出了帧层面的特征（frame-level features）。这些特征是按每秒 1 帧的时间分辨率从 19 亿个视频帧中提取的，然后它们还被进一步压缩到了可装入单个商品级硬盘的大小（少于 1.5 TB）。这使得我们可以在单个 GPU 上只用低于一天的时间就能全规模地下载该数据集并完成基准的 TensorFlow 模型的训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们相信该数据集能极大地加速在视频理解上的研究，因为它能让研究者和学生无需使用大数据和大机器就能进行之前前所未有的规模的研究。我们希望这个数据集将能激励在视频建模架构和表征学习上的激动人心的新研究，尤其是能有效处理噪声或不完整标签的方法、迁移学习（transfer learning）和领域适应（domain adaptation）方面的研究。事实上，我们的实验表明：在该数据集上对模型进行预训练并在其它外部数据集上进行应用/微调，可以在这些外部数据集（如 ActivityNet、Sports-1M）上实现当前最佳的表现。关于我们使用该数据集进行的所有实验以及我们构建它的更多细节，请参阅我们的技术报告论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是对该技术报告论文的摘要翻译。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：YouTube-8M：一个大型视频分类基准（YouTube-8M: A Large-Scale Video Classification Benchmark）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOzhWdgf03HVRONpHovvVekY5ibibvuUVwibDorjLfHCgPNsduvJ1u2sJSg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;计算机视觉领域近来的许多进步都可归功于大型数据集。机器学习的开源软件包和不再昂贵的商品级硬件大幅降低了探索新方法的进入壁垒。我们可以在几天时间内就在数百万个样本上完成模型的训练。但是大型数据集（如 ImageNet）都是为图像理解存在的，还没有规模能与之媲美的视频分类数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇论文中，我们介绍 YouTube-8M——这是目前最大的多标签视频分类数据集，包含了约 800 万段视频（约 50 万小时），这些视频用一个包含了 4800 个视觉实体（visual entity）的词汇集进行了标注。为了获取这些视频和它们的（多）标签，我们使用了一个 YouTube 视频标注系统（video annotation system），该系统可以给其中的视频标注上主要的主题。尽管这些标签是机器生成的，但它们的准确度非常高并且是衍生自各种基于人类的信号，其中包括元数据（metadata）、查询点击信号，所以可以说它们是基于内容的标注方法的一个非常好的目标。我们使用了自动和人工兼用的调制（curation）策略对视频标签（知识图谱实体）进行了过滤，其中包括询问人类评估者标签是否可以通过视觉识别。然后我们以每秒一帧的速度对每个视频进行了解码，然后使用了一个在 ImageNet 上预训练过的 Deep CNN 来提取刚好在分类层之前的隐藏表征（hidden representation）。最后，我们对帧特征（frame features）进行了压缩，使帧层面和视频层面的标签都可供下载。该数据集包含了超过 19 亿个视频帧和 800 万段视频的帧层面的特征，所以它是最大的公开的多标签视频数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在该数据集上训练了多种（中等的）分类模型，并使用了流行的评估标准对它们进行了评估，然后将它们报告作为了基准。尽管这个数据集很大，但我们的一些使用了公开公用的 TensorFlow 框架的模型在单台机器上只用不到一天的时间就训练到了收敛（convergence）的程度。我们计划发布用于训练基本 TensorFlow 模型和用于计算标准的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的实验表明：在大型数据集上的预训练可以泛化到其它数据集上，比如 Sports-1M 和 ActivityNet。我们在 ActivityNet 上实现了当前最佳的表现，将 mAP 从 53.8% 提升到了 77.6%。我们希望 YouTube-8M 的前所未有的规模和多样性可以带来在视频理解和表征学习上的进步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 29 Sep 2016 11:19:43 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 机器之心代表读者对话Yoshua Bengio：没有可与深度学习竞争的人工智能技术（附演讲）</title>
      <link>http://www.iwgc.cn/link/2884653</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心整理&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;演讲：Yoshua Bengio&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;特别感谢：石涛 （帮忙法语转英语的朋友）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;现场报道：王双栋、&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;Chain Zhang&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;报道准备：赵巍、玉喜、李勇、Yanchen&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;编译：杜雪、吴攀&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在 2016 年 9 月 21 日，蒙特利尔举办的 cdlm 2016 大会上，Yoshua Bengio、Yann LeCun、Joëlle Pineau 三人发表了经常的主题演讲。机器之心在北美的小伙伴对该会议进行了跟踪报道，并有机会对 Yoshua Bengio 和 Yann LeCun 进行问题采访。此篇文章是 Yoshua Bengio 演讲以及机器之心对其采访问题的整理。因大会全程法文（PPT 也是），所以对 PPT 就不做过多展示。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;mpvoice frameborder="0" class="res_iframe js_editor_audio audio_iframe" src="/cgi-bin/readtemplate?t=tmpl/audio_tmpl&amp;amp;name=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83%E9%87%87%E8%AE%BF%20YoshuaBengio&amp;amp;play_length=03:23" name="%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83%E9%87%87%E8%AE%BF%20YoshuaBengio" play_length="203000" voice_encode_fileid="MzA3MzI4MjgzM18yNjUwNzE5NDkz"&gt;&lt;/mpvoice&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;采访内容：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：有什么未来可以与深度学习竞争的人工智能技术吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Yoshua Bengio&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：没有。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：你强调了神经科学所带来的灵感，你认为新的深度学习研究者应该接受一些怎样的神经科学训练？反向传播真的是一种近似生物学习的过程吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Yoshua Bengio：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我写了一些关于反向传播和神经科学的论文，我认为弥合对神经科学的理解与机器学习比如反向传播之间的差距非常重要，因为大脑必须使用一些非常强大的东西。我们现在还有很多无法回答的问题。这是非常有趣的探索，也是非常基础的科学研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：你有什么深度学习方面的书推荐吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Yoshua Bengio&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：他们需要大量阅读论文和专业书籍，比如我和Ian Goodfellow合作的那本。他们还需要练习，真正的投入进入，可以使用现有的packages，做一些试验，用数据来训练自己的网络，调整相应的参数，并熟悉算法的使用过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：数据科学比赛有什么作用？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Yoshua Bengio&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：参加比赛是很好的学习方式，可以挑战自我。你可以阅读论文，论文中会提到很多数据集，你可以拿来与自己的比较，这是一个好方法，重复论文中的试验也是一个好方法，虽然很难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yoshua Bengio 大会演讲内容：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOQyp9HSzGF5ibubhKf3USUPzJF8AdQroZia9z0vxE9RZzZlsCJiaNKARdQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我将在这里谈一谈发生在人工智能领域的这场革命。它是什么？然后我们会谈一谈深度学习。我会从便于理解的内容入手，也就是自动驾驶汽车，它还未实现，但得益于视觉研究的深入，它已有相当可观的进展。这种视觉理解之所以成为可能，乃是得益于深度学习和认知网络的发展，我们稍后会谈到它们... 也许你们中的一些人已经开始与你的手机聊天了？随着我们将与这种变形了的计算机进行交互，这种事会开始占据我们越来越多的空间。过去几个月里，也许你听到过一项引起了人们广泛讨论的突破成果：一台使用了深度学习和强化学习的计算机击败了世界围棋（一种复杂的中国棋盘游戏）冠军，在这之前人们普遍认为计算机要想成功击败世界冠军可能需要几年甚至几十年，而它现在就做到了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicQUibx1s2Llj1oosE69QCicO19ibOwRziaNvIOXTsfFwuq8lw9nkR9ZgNFRz0H9dbnghG3g2V2SCpX8g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么到底发生了什么？的确人工智能是循序渐进的耐心工作的成果，而且它总是站在巨人的肩膀上，并且这些进步在某种程度上促成了转折点——我们可以在新服务中利用这些成果来生产新东西，进行经济转型以及改变社会。正如人们所写的那样，我们正在经历另一场工业革命，它并不是简单地增加人类的机械力；计算机将增加人类的认知能力和智力。我谈到了深度学习，因为这些变化和突破在很大程度上正是由于深度学习的进步。所以它是什么呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它始于十年前，这得感谢加拿大高级网络研究所（Canadian Institute of Advanced Network Research/ICRA），它资助了这项在那段时间被认为过时而不受欢迎的研究，几个傻瓜仍然相信这些受大脑概念所启发的想法，它们想做更进一步的研究。经过了数十年的人工神经网络研究之后，这项研究进入到了深度神经网络领域，这是过去几年所发生的事情的一点点起源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以在你们讲人工智能之前，我会告诉你们我真正感兴趣的是去理解我们所能实现和理解的数学原理和信息技术，这能帮助我们解释「智能（intelligence）」。这是广义上的智能，它可以是人类和动物，当然它可以用来构建智能机器。这也多亏了这个事实：我们可以使用能够一步一步发展的机器进行实验。那么智能是什么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这不是我们在社区中达成的几件共识，但我们足以一致同意的是：智能与智能行为相联系并为一个系统/代理人采取好的决策和行为，我们需要知识。自 50 年代初开始的那个有关基本智能的基本问题是：计算机如何能够获得使它们行动起来更加智能的知识？上文所描述的研究已经运用符号的经典基本智能方法进行了几十年，这个著名的专家系统并没有真正地解决问题，因为它试图将我们所了解的知识直接提供给计算机；但遗憾的是我们知道但我们无法向机器解释，我们不能为一台计算机设计一个程序来做这样的事，因为有许多知识是凭直觉获知的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;真正效果惊人的解决方法是去教计算机我们自己获取知识的方式，通过观察、案例分析、模仿人类、结合数据，最终我们拥有越多的数据，计算机就可以使用越多的信息来了解世界的某一方面——世界可以用数据进行阐释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此这门探讨生物系统或计算机如何能够从样本中学习的科学是一门有关学习的科学，尤其是机器学习。所以近几十年中所孕育的这个突破却主要在过去的 3、4 年间才显露出来，深度学习是一种特殊的机器学习方法，它显然滋养于机器学习领域所取得的大量研究成果，而且其中有着更多的重要思想。也就是说，比我所坚持的两个思想多多了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中一个是表征（representation）的思想。这是说计算机不仅要去学习一个任务，而且要学习如何表现那些表征了声音、词、句子的信息。第二个思想是我们不仅要学习表征，还要学习多个层次的表征，而且我们必须要将这些表征的不同层次理解为抽象的层次。为什么这很有趣呢，因为一个人能够让计算机建立越多的抽象层次，它就能更好地了解世界，从而更好地生成情景，而这才是关键。然后计算机就会在编辑和理解单词、机器翻译、自然语言理解、机器人方面出现巨大改进，我今天也会给出一些例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近我们开始结合表征学习的思想，它使得计算机能将来自不同资源的表征放在一起，比如图像和短语的表征。所以这里我们看到一个例子是说计算机完全做到了图像中的对象识别，计算机可以识别出图像中出现了哪些物体及其位置。左边是一个计算机要处理的简单例子，阅读它并回答一个相关背景的问题。而底部则说明了我们不久前在实验室所做的一些事情，我们在这个实验室将这些能力结合了起来，在那里计算机会看到一张图片（例如左侧一张公园中的女人的图片）后将会生成一句法语「一个女人开始在公园里掷飞盘」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOltk0MlYZ4AtuxKMpFiaL1GEpZclnxCj7BoiafGZ7dUVicWoyf85fZtKDQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此计算机不仅能解释图像，还用自然语言生成单词序列，这里用的是英文。就在几年前，人们还认为这种事还太过遥远，而现在它并不像我们想象中的那么困难。我们仍然还远远不能解决这个问题，但你将会看到这个方向上的迅速进步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我将回到表征的概念上，我们做了一些事情来试着了解计算机在这些表征中所发现的东西，试着去表达单个单词。我们可以将每个单词联想成一个所谓的「向量（vector）」，它可以看做二维的一系列实数，也是一个有关计算机学习过的单词的近似，这里看不到它们，它们太小了。但我们可以放大，然后我们可以看到那些有着相似含义的单词将在表征空间中彼此靠近。不仅如此，我们还有绝对吸引人的发现：我们可以用这种表征来做类比推理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOXDy7B6ZWZ5qt4msetTZcbRyiaINicEcMCHbksegEyc00rUkJGORw7Vrw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以如果我们有了包含一系列数字的向量，空间中对应于一个单词「巴黎」的点和对应于「法国」的向量表征，然后我们将两者之间的差别当做另一个向量，这个向量会告诉我们从「巴黎」到「法国」的方向。同样的方向可以应用于表征罗马到意大利。同样如果是单词「国王」和「王后」之间的表征差异，我们会得到了一些非常接近于「男人」和「女人」之间差异的东西。最后计算机能够看到对词「男人」来说词「女人」所意味的就是对于单词「王后」来说单词「国王」所意味着的东西。这是我们没有教但是计算机在无监督的情况下自己发现的一些事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后我会以人们可以理解的一个例子来尝试去解释这种表征思想。如果我们看一张图片，一张数字化文字页面上的图片，在这张图片中有某些层次的表征，例如作为第一个非常粗糙的层次来说是像素，然后我们会在这张图片中识别到稍高的一个层次是边缘，它定义了物体的边缘。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicODsxoWxUFXlHtxGicOiaBpjrk7RJiasic2ZibdlicnsUibO7jicSrARKfMToKAw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后你可以想象一个更高的表征层次，其中计算机可以识别到线条所匹配的部分，例如字符，而再高一层次就是线条能够结合在一起形成整体特征，然后这些特征会结合成为单词，而这就到了我们还未能掌握的地方，即一个更高层次的抽象——单词的含义。所以像我们之前所展示的那样，你看到了一个尝试去捕捉词含义的词表征，而毕竟它还不足以完成一些有趣的任务，比如你想让它捕捉到隐藏单词的含义（潜在意思），或者短语、文档背后隐藏的想法，甚至是消息的核心内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以我向你展示所有这些内容，是因为在这种情况下会有若干层次的表征，词层面的表征比像素层面更为抽象，而如果我们必须作出决策并要求计算机回答一个有关该文件的问题，如果它工作在词层面的话会更容易，同样的单词可以用许许多多可能的图像来说明。我们能够以非常不同的方式来选择字符，并且我们想让计算机分离出有关字符如何被书写的细节，比如这里的「S」是什么，让它了解单词的含义等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此这种表征和抽象的思想是非常重要的。我这里举的例子包括了我们所理解的表征，从而我们能够迫使计算机去遵循这些表征，但事实上在我们的研究领域，我们感兴趣的是让计算机自己去发现不同层次的表征，从第一个层次到词的含义这个层次，这些表征对我们来说很自然，但我们并不需要让计算机真的以我们的方式去学习，即使这些层次之间的关系并不明显，而机器学习被用于识别字母，当你达到更高的抽象层次时，事实上我们并不知道怎样去表征语义，而这就是为什么要拥有一个完全自动化的方法来发现这些抽象层次会变得如此重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以我将以两个不那么偏技术性但会得到更多共鸣的想法作为结束：我们当下在人工智能领域所目睹的围绕这些进展的社会问题。首先，正如许多强大的科学和技术，我们对它们放任自流的风险是什么？为了不让一些人利用这一权力从而使得财富集中在少数人手中，我认为许多研究人员会希望让这些进步首先去造福尽可能多的人。我们如何安排来让事情如此进展呢？这里有一个特殊情况，即这些技术在未来几年很有潜力，例如自动驾驶汽车对就业市场有着显着的影响，这意味着一些人可能会失去工作并被机器取代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicQUibx1s2Llj1oosE69QCicORlicDbsN4ZxFQwic1n7MGribKGbicqP8AlsvYPaEozsKSceW1nsNWAEIaA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们如何确保它从人道角度出发并造福于每个人，而不让部分群众悬而待决。然后更一般地说，还有许多普通人尤其是公民应该思考的道德问题。例如数据的隐私问题，还有一些问题不仅仅……比如我想要保留我的数据并且保证无人可以访问它，但是如果我们将自己的数据分享到健康领域呢，比如它可以造福于每一个人。这有点像是疫苗，如果每个人都接种疫苗，它将能够造福于每一个人。所以我们要如何管理这其中的个人和集体的利益？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，正如 Valerie 所说，我所谈论的是试图将研究方向引向推动人工智能的社会型积极应用上来。显然健康、环境、教育、创造性服务将服务于世界上的大部分群体，不一定是为了即时的商业利润。有一些人提出关于人工智能长期风险的道德问题。我是那些不太害怕它的人之一。我也认为考虑这种可能性很重要，不能不经过思考就行动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于所有这些原因，拥有某种对话是很重要的，我希望今天可以实现它。人们可以从中参与讨论、了解问题，代表集体意志的政治家们也是对话的一部分。所以我邀请你们学习更多东西，以任何方式参与进来，无论是贡献于技术的发展，我希望公司可以遵循这些，或者是简单地作为一个公民。现在，目前在蒙特利尔有一个机会，我们有一个人工智能领域的批判性团体，特别是我领导的小组和 McGill 大学的机器学习小组。我们在该领域一共有 150 位研究人员，在任何情况下这都是世界上一个非常独特的大学水准的集结。我们刚刚收到的一项拨款，它会帮助我们发展这一科学，而且会有政府委员会来确保技术的可转化性，使它在蒙特利尔创造财富，所以这是一个挑战，如果我们能够在蒙特利尔创造一个硅谷，我们如何将一个批判性团体转型成私人企业，创建一个围绕所有这些的生态系统呢？我认为最重要的因素是吸引和留住世界上最好的人才，无论他们是来自大学还是私人领域。谢谢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心整理，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 29 Sep 2016 11:19:43 +0800</pubDate>
    </item>
    <item>
      <title>学界 | DeepMind最新论文：在线 Segment to Segment 神经传导</title>
      <link>http://www.iwgc.cn/link/2884654</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ArXiv&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lei Yu, Jan Buys, Phil Blunsom&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOEfNQoJhMgMYO8qTgOaHPY7U3kHIYfv3yrA8VFOakpInPp2YVHic7V5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在这里介绍一种在线神经序列到序列模型（online neural sequence to sequence model），其可以从其读取到的输入的编码和解码片段（segment）中交替地学习。通过独立地追踪编码和解码的表征，我们的算法可以在训练过程中实现潜在分割（latent segmentation）的准确的多项式边缘化（polynomial marginalization），而在解码过程中，我们使用了波束搜索（beam search）来寻找最好的对准路径（alignment path）以及预测出的输出序列。我们的模型解决了 vanilla 编码器-解码器的瓶颈，其在产生任何输出前必须在其固定长度的隐藏状态中读取和记忆整个输入序列。这不同于之前的注意模型（attentive model）：我们的模型并不将注意权重（attention weights）看作是一个确定函数的输出，而是将注意权重分配给一个序列隐变量（sequential latent variable），其可以被边缘化而且允许在线生成。在抽象句子总结和形态曲折（morphological inflection）上的实验表明我们的模型在基准编码器-解码器（baseline encoder-decoders）之上实现了显著的性能提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 29 Sep 2016 11:19:43 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 谷歌、微软等五大巨头历史性走到一起，建立合作组织应对人工智能的社会挑战</title>
      <link>http://www.iwgc.cn/link/2884655</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Venture Beat&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;span&gt;亚马逊、DeepMind/谷歌、 Facebook、IBM 和微软今天宣布成立一家非营利组织，致力于推进公众对人工智能技术的理解，针对当前该领域的挑战和机遇执行可行方案。学界、非营利组织以及政策和伦理专家也将被邀请进入该组织委员会，比如艾伦人工智能研究所。该人工智能合作关系的目标是处理人工智能技术的挑战和机遇，惠及社会和人类。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicQUibx1s2Llj1oosE69QCicOQyc7CytQlR1x0ufw1l0EolaMslqPxJ0kkIYTL6dAb912hMBOrouWhg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能这一科技概念总能让人想到计算机成为世界霸主并营造出科技反面乌托邦的画面，或者机器人要全面接管人类工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是人工智能一直都在那：是它驱动了你的利用语音激活的数码个人助手和搜索引擎，引导你汽车上的自动功能，帮你翻译外文文本，它能从你上传到社交媒体上的照片中找出你朋友的照片，并过滤掉垃圾邮件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是最近几年人工智能在实际中的应用缺少了一个关键的元素：一个行业伦理标准，或者这一领域成长的最佳实践指导。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，该行业的巨头公司们开始合作来填补这个空白，联合成立了 Partnership on Artificial Intelligence to Benefit People and Society，一项为了造福人类和社会的 人工智能合作伙伴关系，囊括了亚马逊、Facebook、谷歌、微软和 IBM，目前苹果也在商讨加入进来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「关于这个问题，我们已经讨论多年了，大多都是非正式的，」IBM 的认知计算副总裁 Guruduth Banavar 说道。「最终们终于有机会正式讨论了这个问题（谈话）。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个团队的目标是创建业界领先的第一个财团，同时将学界和非营利组织的研究者拉进来，主导相关工作，从本质上确保人工智能的可信性：将研究推向与伦理相关的、安全可靠的技术上，这些技术可以带来更多的帮助而不是伤害，也会帮助减少人们对人工智能的恐惧和误解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我计划展开讨论，计划推广，也计划潜在地赞助一些深入具体问题的研究项目，」Banavar 说，「但是首先，这是一个跨行业的开放讨论的平台」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在某种程度上，人工智能在过去几年中发展太快，已经在不知不觉中潜进很多人的生活中。人工智能科学家长期一来一直都在预测人工智能浪潮，但是时机是一个处于移动中的目标。现在，机器在翻译和讯息发送上击败了人类。各家语音激活助手的竞争不断激烈：苹果的 Siri、亚马逊的 Alex、微软的 Cortana。IBM 的 Watson 超级计算机正在开出医疗处方，帮助医生治疗癌症。谷歌的 DeepMind 在复杂的围棋游戏中击败了人类。各类算法不断走向顶尖。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「各家公司不得不在已有的情况下，从基本上做出自己的最佳实践。」亚利桑那州立大学计算机科学教授，人工智能发展协会（Association for the Advancement of Artificial Intelligence）主席 Subbarao Kambhampati 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些研究者们在一起讨论，咨询和协商；做人工智能的各家公司已经形成了伦理管理会和委员会。但是行业协调还没有退出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这段时间，人工智能技术已经进入我们生活中的很多决策中，并且还在深入。」Kambhampati 说。「大多数情况下，事情进展顺利，但是有时也不会那么顺利。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像那次微软做了一个 Twitter 聊天机器人，它学习了对话，但很脱离了原有轨道，变得具有攻击性。还有那次特斯拉的自动驾驶汽车没有在明亮的天空下识别出一辆白色的拖拉机，导致了驾驶员的死亡。还有谷歌的广告算法要面对各种种族歧视和性别歧视的指控。这类事故还是会出现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「当你在一堆数据上训练一个学算法时，它会在这些数据中发现一个模式。显然，人工智能领域中的每个人都已经知道并理解这个事情，」Kambhampati 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「但事实是，这件事的影响会在无意在形成刻板印象和歧视，这已经成为一个日益凸显的问题，」因为这些技术确实正在日常生活中做出非常重要的决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kambhampati，他领导的科学社会 AAAI 也会参与到这个人工智能的伙伴关系中，他希望这个新建的团队能够重点专注于当前和短期的实际问题，而不是伦理学家经常探讨的遥远的世界末日场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 的 Banavar 说大家对这个团队的期望是它能长期坚持下去，但它的目标确都是短期的。他期望这个人工智能伙伴关系能创建一个教育论坛，有线上也有线下，能为人工智能聚集各方资源。该团队有一个具体的计划，有望在未来几周内推广开来，随后还有一个事件来启动该项合作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，他承认该团队的工作无法停止任何对人工智能的滥用，它的实际目的是产生影响。但是 Banavar 希望这个团队的工作能在世界范围内办起教育课程，激发新一代的人工智能研究者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 29 Sep 2016 11:19:43 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 谷歌翻译整合神经网络：机器翻译实现颠覆性突破（附论文）</title>
      <link>http://www.iwgc.cn/link/2869136</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Quoc V. Le、Mike Schuster&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;昨日，谷歌在 ArXiv.org 上发表论文《Google`s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》介绍谷歌的神经机器翻译系统（GNMT），当日机器之心就对该论文进行了摘要翻译并推荐到网站（www.jiqizhixin.com）上。今日，谷歌 Research Blog 发布文章对该研究进行了介绍，还宣布将 GNMT 投入到了非常困难的汉语-英语语言对的翻译生产中，引起了业内的极大的关注。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;十年前，我们发布了 Google Translate（谷歌翻译），这项服务背后的核心算法是基于短语的机器翻译（PBMT:Phrase-Based Machine Translation）。自那时起，机器智能的快速发展已经给我们的语音识别和图像识别能力带来了巨大的提升，但改进机器翻译仍然是一个高难度的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，我们宣布发布谷歌神经机器翻译（GNMT：Google Neural Machine Translation）系统，该系统使用了当前最先进的训练技术，能够实现到目前为止机器翻译质量的最大提升。我们的全部研究结果详情请参阅我们的论文《Google`s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》（见文末）[1]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几年之前，我们开始使用循环神经网络（RNN：Recurrent Neural Networks）来直接学习一个输入序列（如一种语言的一个句子）到一个输出序列（另一种语言的同一个句子）的映射 [2]。其中基于短语的机器学习（PBMT）将输入句子分解成词和短语，然后很大程度上对它们进行独立地翻译，而神经机器翻译（NMT）则将整个输入句子视作翻译的基本单元。这种方法的优点是：相比于之前的基于短语的翻译系统，这种方法所需的工程设计更少。当其首次被提出时，NMT 在中等规模的公共基准数据集上就达到了可与基于短语的翻译系统媲美的准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自那以后，研究者已经提出了很多改进 NMT 的技术，其中包括模拟外部对准模型（external alignment model）来处理罕见词 [3]，使用注意（attention）来对准输入词和输出词 [4] 以及将词分解成更小的单元以应对罕见词 [5,6]。尽管有这些进步，但 NMT 的速度和准确度还没能达到成为 Google Translate 这样的生产系统的要求。我们的新论文 [1] 描述了我们怎样克服了让 NMT 在非常大型的数据集上工作的许多挑战，以及我们如何打造了一个在速度和准确度上都已经足够能为谷歌的用户和服务带来更好的翻译的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYYnia3jZRmI5bMIIluWHibScueHom3bXNCS0rORlZiaEoRuzsvkGVYc2TA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;来自对比评估的数据，其中人类评估者对给定源句子的翻译质量进行比较评分。得分范围是 0 到 6，其中 0 表示「完全没有意义的翻译」，6 表示「完美的翻译」。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面的可视化图展示了 GNMT 将一个汉语句子翻译成英语句子的过程。首先，该网络将该汉语句子的词编码成一个向量列表，其中每个向量都表征了到目前为止所有被读取到的词的含义（「编码器（Encoder）」）。一旦读取完整个句子，解码器就开始工作——一次生成英语句子的一个词（「解码器（Decoder）」。为了在每一步都生成翻译正确的词，解码器重点注意了与生成英语词最相关的编码的汉语向量的权重分布（「注意（Attention）」，蓝色链接的透明度表示解码器对一个被编码的词的注意程度）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYKGC1GMI9FibcKU7hc2dia5qJzKVUg9B1FULZKcIs3wvxjCOsk7o57QfQ/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用人类评估的并排比较作为一项标准，GNMT 系统得出的翻译相比于之前的基于短语的生产系统实现了极大的提升。在双语人类评估者的帮助下，我们在来自维基百科和新闻网站的样本句子上测定发现：GNMT 在多个主要语言对的翻译中将翻译误差降低了 55%-85% 以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYoD35dMSt4n4vib0HZUT5k5xxh1ohicNClItTUycqMiaW7lahOJFBYzVMQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们的系统产出一个翻译案例，其输入句子采样自一个新闻网站。这个地址（https://drive.google.com/file/d/0B4-Ig7UAZe3BSUYweVo3eVhNY3c/view?usp=sharing）可以看到更多随机采样自新闻网站和书籍的输入句子翻译样本。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天除了发布这份研究论文之外，我们还宣布将 GNMT 投入到了一个非常困难的语言对（汉语-英语）的翻译的生产中。现在，移动版和网页版的 Google Translate 的汉英翻译已经在 100% 使用 GNMT 机器翻译了——每天大约 1800 万条翻译。GNMT 的生产部署是使用我们公开开放的机器学习工具套件 TensorFlow 和我们的张量处理单元（TPU：Tensor Processing Units），它们为部署这些强大的 GNMT 模型提供了足够的计算算力，同时也满足了 Google Translate 产品的严格的延迟要求。汉语到英语的翻译是 Google Translate 所支持的超过 10000 种语言对中的一种，在未来几个月，我们还将继续将我们的 GNMT 扩展到远远更多的语言对上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器翻译还远未得到完全解决。GNMT 仍然会做出一些人类翻译者永远不出做出的重大错误，例如漏词和错误翻译专有名词或罕见术语，以及将句子单独进行翻译而不考虑其段落或页面的上下文。为了给我们的用户带来更好的服务，我们还有更多的工作要做。但是，GNMT 代表着一个重大的里程碑。我们希望与过去几年在这个研究方向上有所贡献的许多研究者和工程师一起庆祝它——不管是来自谷歌还是更广泛的社区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Google Brain 团队和 Google Translate 团队都参与了该项目。Nikhil Thorat 和 Big Picture 也帮助了该项目的可视化工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;论文：Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYT2r1HkDqWO8ianIib4y3rsKrCDj3oq8IicQMlqe2AkxEibuSfNp4D582gA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：神经机器翻译（NMT: Neural Machine Translation）是一种用于自动翻译的端到端的学习方法，该方法有望克服传统的基于短语的翻译系统的缺点。不幸的是，众所周知 NMT 系统的训练和翻译推理的计算成本非常高。另外，大多数 NMT 系统都难以应对罕见词。这些问题阻碍了 NMT 在实际部署和服务中的应用，因为在实际应用中，准确度和速度都很关键。我们在本成果中提出了 GNMT——谷歌的神经机器翻译（Google's Neural Machine Translation）系统来试图解决许多这些问题。我们的模型由带有 8 个编码器和 8 个解码器的深度 LSTM 网络组成，其使用了注意（attention）和残差连接（residual connections）。为了提升并行性从而降低训练时间，我们的注意机制将解码器的底层连接到了编码器的顶层。为了加速最终的翻译速度，我们在推理计算过程中使用了低精度运算。为了改善对罕见词的处理，我们将词分成常见子词（sub-word）单元（词的组件）的一个有限集合，该集合既是输入也是输出。这种方法能提供「字符（character）」-delimited models 的灵活性和「词（word）」-delimited models 的有效性之间的平衡、能自然地处理罕见词的翻译、并能最终提升系统的整体准确度。我们的波束搜索技术（beam search technique）使用了一个长度规范化（length-normalization）过程，并使用了一个覆盖度惩罚（coverage penalty），其可以激励很可能能覆盖源句子中所有的词的输出句子的生成。在 WMT' 14 英语-法语和英语-德语基准上，GNMT 实现了可与当前最佳结果媲美的结果。通过在一个单独的简单句子集合的人类对比评估中，它相比于谷歌已经投入生产的基于短语的系统的翻译误差平均降低了 60%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[1] Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean. Technical Report, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[2] Sequence to Sequence Learning with Neural Networks, Ilya Sutskever, Oriol Vinyals, Quoc V. Le. Advances in Neural Information Processing Systems, 2014.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[3] Addressing the rare word problem in neural machine translation, Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[4] Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. International Conference on Learning Representations, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[5] Japanese and Korean voice search, Mike Schuster, and Kaisuke Nakajima. IEEE International Conference on Acoustics, Speech and Signal Processing, 2012.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[6] Neural Machine Translation of Rare Words with Subword Units, Rico Sennrich, Barry Haddow, Alexandra Birch. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 28 Sep 2016 11:01:38 +0800</pubDate>
    </item>
  </channel>
</rss>
