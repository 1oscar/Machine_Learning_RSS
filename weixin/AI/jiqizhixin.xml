<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>独家专访 | KBP2016 冠军背后，科大讯飞 NLP 实力几何？</title>
      <link>http://www.iwgc.cn/link/3605764</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KBP2016 是由 NIST（National Institute of Standards and Technology，美国国家标准与技术研究院）指导、美国国防部协办的赛事，主要任务为从自然书写的非结构化文本中抽取实体，以及实体之间的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美国当地时间 2016 年 11 月 15 日，NIST 揭晓 KBP2016 EDL 大赛结果。其中，科大讯飞包揽了本届 EDL 比赛的冠亚军。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器之心第一时间采访了科大讯飞研究院研究员刘丹，从 KBP2016 比赛情况、KBP 任务难点、以及讯飞的 NLP 方向进展展开话题。以下为采访实录。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;机器之心：能请您介绍一下 KBP 这个任务的情况吗？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：本次我们参加的是 KBP 国际公开评测任务，比赛由 NIST 资助，从 2009 年起举办至今。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KBP（Knowledge base population）任务的主要目标是知识库扩展和填充，研究的主要内容是传统的结构化知识库如 Freebase，目前它的构建绝大多数都要依靠人的编辑工作。知识库中描述的信息是物理世界的命名实体和实体之间关系的抽取，如「克林顿和希拉里之间是夫妻关系」、「克林顿毕业于耶鲁法学院」这样一个个实体的关系。但人工编辑有两个问题，一是工作量较大，再就是可能出现错误和时效性的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w0kxNobxfVbJU2V6u2lvBXFAPcopmY7SuicW2HY5Eet9ElW1fIhLmNGUw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;KBP 任务框架，资料来自科大讯飞&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很久以来大家都在思考，人可以通过阅读新闻和书本这样的文本语料获得相关知识，机器可不可以？KBP 公开任务的研究目标，是让机器可以自动从自然书写的非结构化文本中抽取实体，以及实体之间的关系。我们今年参加的 EDL（Entity Discovery and Linking）命名实体的发现和连接任务所做的事情，是从自然语言的文本中抽取命名实体，标注它们的类型及实体与已有知识库之间的对应关系。从 2015 年开始，这个任务采用了中文、英文、西班牙文三个语种，需要找到三个语种的文本语料中的实体，并连接在一起。中文的「克林顿」要与英文的「Clinton」、西班牙文的「Clinton」连接到 Freebase 的同一个实体上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：参赛方都有哪些公司、学校或者企业？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：学校居多，也有一些公司的研究机构。今年的 KBP 比赛 EDL 任务中，参加的学校有卡耐基梅隆、UIUC、伦斯勒理工，还有很多其他的学校。以企业名义参加的有 IBM，国内的机构有国防科技大、北京邮电大学和浙江大学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：讯飞在 KBP 任务的多个指标中获得了第一，这些指标指哪些？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：命名实体连接这个项目分为很多子项目，一个是将三个语种放在一同统计，需要将不同语言的相同实体连接在一起；一个是将三个语种各自统计的指标，统计指标包括命名实体发现的正确率和命名实体连接的正确率。最终我们在整个比赛任务中的绝大多数指标都是第一，其中三个语种总体的指标是最高的，与其他参赛系统相比有比较显著的优势。单独的三个语种指标中，命名实体发现的部分都是第一。连接这部分中文我们是最高的，英文和西班牙文是第二。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：命名实体发现和命名实体连接发现的难点在哪里？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：命名实体发现中部分是 NLP 传统的命名实体标注任务，但 KBP 任务同其有两个区别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个是传统命名实体标注不允许有嵌套关系，如提到「中国科学技术大学」时，「中国科学技术大学」就是一个命名实体；KBP 则需要在文本中抽取更多细节的关系，不仅要将「中国科学技术大学」的「中科大」标注出来，同时「中国」也要标注出来，命名实体有一定的嵌套关系。除此之外，名词性的实体如「中国科大」、「科大讯飞」、「机器之心」都是一个个独立的名字（专有名词），这样的名字更容易标注；名词性的普通名词短语如「中国人」、「美国人」、「中国的公司」，这样的名词性实体需要与一般的名词短语区分开。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外 KBP 任务需要标注的是有意义的名词性实体，如「XX 作为一个中国人」，这里的「中国人」是有明确指代的，所以需要标注；泛指性的如「中国人可以发怒了」，这里的「中国人」是不允许标注的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KPB 任务更符合人对于事物类别的区分判断，有的内容是无法从语言语法的角度区分的，这让命名实体发现具有了比较大的难度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;命名实体连接发现最关键的部分是消歧的问题，普通的文本大多数时候不会提全名，如科大讯飞大家不会说「科大讯飞股份有限公司」，而会说「讯飞」。因为人们会使用缩写、昵称、绰号以及上下文指代的内容，使得命名实体连接时的消歧会非常难做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一个比较有意思的事情是，去年 KBP 比赛时杰布•布什正在竞选美国总统，新闻中会出现很多「布什一家」，其中以杰布•布什居多。但是因为训练语料中老布什和小布什出现的频率更高，所以在去年的比赛中，大多数参与的系统都会将杰布•布什连接到乔治•布什上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，在语料中的昵称缩写也很容易连接错误，或错标为非实体。除了这些，偏谈论性的语调中会出现「希拉里这个女人……」。这里的「这个女人」在实体发现中被标为实体会很难；其次把「这个女人」连接到希拉里是另外一个比较有难度的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：这个任务更偏向于 NLP 中的语义理解方向吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：应该算语义理解。与之相似的是最近几年发展的「抽象语义表示」，希望将文本中的句子抽象出和语言无关的实体，以及实体关系、动作之间的图像表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：KBP 任务的评判标准是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;刘丹：评判标准是正确率与召回率两者兼顾的，采用的是 NLP 中常用的 f-score。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w00WvlvCy0sf7WYbu9Ke8MicSPo3mibkbJtt5tUg1aJRFibqoic6XibDdKiaiaQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果语料中出现了一百个实体，系统标出了 105 个，其中 80 个是正确的实体，另外 25 个是系统错标的，正确率就是标出的正确实体数除以系统标出数（80/105=0.7619）；一共有 100 个实体，召回了 80 个，召回率就是召回数比总数（80/100=0.8）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果正确率和召回率只考虑一部分，往往可以做到很高，比如系统尽可能只找绝对有把握的（名词实体），或者文本中出现了 100 个实体，只找到 1 个实体并且是正确的，那么正确率总是 100%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：因为 KBP 任务包含中文、英文、西班牙文，那么多语言之间连接部分的难点在那里？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：如果按照传统 NLP 做法，会根据每个语言精细定义比较复杂的规则，系统的调节也会倾向于抽取非常大的规模特征。做传统 NLP 中国人做中文是最好的，英文还能做得来，但是西班牙文是没办法做的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在解决 KBP 任务是采用的是基于深度学习框架下发展的技术，特点是用比较复杂的神经网络和端到端的学习，尽可能多的靠数据驱动并尽量减少人工定义的规则和特征。我们参加本次比赛的模型结构很有自己的特色，在三个语种中使用的是同一个系统，取得的结果都不错。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然西班牙文我们完全看不懂，但在西班牙文命名实体发现的部分，最终结果是我们做得最好，领先第二名不少。命名实体连接部分我们是第二，比第一差了 1.9 分（讯飞成绩为 63.5，最优为 65.4）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w0UwM3Mng3y6ktxTpDZuKoHog7EUFSWTXyI39pLKNmrQjEl42u13Y0ww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;KBP 任务可提交参赛系统可提交两次，该图表为第一次提交时的指标结果。其中，1 号系统为讯飞与 USTC 实验室联合提交的系统。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w0PWyt5LJddmm80wuua5mfLiboLavjRtdpfLRSlxypW5icBUVf8DE0UsUA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;该图表为第二次提交的指标结果。其中，2 号系统为讯飞与约克大学联合实验室共同提交的系统。两个系统共同囊括了 EDL 任务的冠亚军。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：西班牙语比较难做的原因是因为训练语料较少吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有两方面原因。一个是传统 NLP 依赖的规则资源在西班牙语方面比较少，我们中文做的比较多，至少对于人名判断有姓氏列表，中国所有的省份也有列表，想去做规则总是做得出来。西班牙文的类似列表做得少，相关资源也很少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外研究者不是母语使用者（Native Speaker），想要进一步调试系统都没有办法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：比赛使用的输入语料是随机语料吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;是官方提供的。今年比赛和此前相比还有一点不同，前面几年的语料非常少，每个语种只有 500 篇。今年规则改为一共提供 90003 篇，每个语种平均 30001 篇。需要将 90003 篇中的实体都找出，并连接起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们在比赛中主要用到的技术有哪些？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;主要还是深度学习。讯飞在基于深度学习解决 NLP 问题方面已经做了很多年，在深度学习火起来但在 NLP 没有做出太多成果时，我们就已经有了思考和尝试。差不多在两年前，与我们合作的加拿大约克大学江辉教授提出基于神经网络的阅读机器（Neural Reading Machine）。对于阅读机器（Reading Machine）来说，先是将自然语言文本当作一个时序的单词序列，针对时序序列考虑各种建模方式，包括传统的卷积网络、循环神经网络，以及江辉教授提出的一种名为 FOFE 的特殊网络结构。这次比赛我们在这些基础上，用了最近两年比较流行的注意力模型（Attention）来做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w0ekWsNicEeYVwkbgjAFVlHYk8I5SmoySzsFibyjapPicLenzSA0DxMjRibA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;约克大学·讯飞联合实验室成立于 2015 年，专注神经计算与深度学习，图为实验室成立合影&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那么这些技术已经应用到我们的产品中了吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：应该说 KBP 的最终目标是知识图谱的扩展，知识图谱对于目前的讯飞来说并没有太多用处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但其技术有非常大应用。首先是基于神经网络端到端的学习方案，在类似的 NLP 问题上都能发挥作用。例如我们在教育方面的自动阅卷、书面作文和口头作文的评分批改、试卷难度预测，我们用了各种各样的技术，但网络结构的总体思想是类似、相关的。大体上是将时序的文本序列进行某种基于神经网络获得的抽象表示，在这种抽象表示上面定义结构，来描述所要抽取的结构信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然说我们不做知识图谱，但 KBP 的研究是要在知识图谱上找到命名实体的对应连接，这一点对于讯飞的核心业务语音对话系统是非常重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前所有的对话系统都是功能引导式的对话，比如让语音助手订票、查餐馆之类。在业务范围外的百科知识和与用户闲聊的时候，往往只能利用人工规则和补资源的方式兜底，比如问对话系统「姚明身高有多少」、「奥巴马的妻子是谁」，多数情况下都表现不佳。基于刚才提到的命名实体抽取连接的相关技术，我们可以对于问题进行简单分析，将问题与维基百科、Freebase 知识库连接起来，从结构化的知识库中找到对应的答案，这是相对直接的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：除此之外，在 NLP 方面讯飞还有哪些成果？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：首先是 NLP 中非常重大的部分——机器翻译，我们目前在机器翻译方面做得还不错，2014 年获得国际评测任务 IWSLT 的第一名，IWSLT 的特点是口语化演讲。去年 NIST 组织的 OpenMT（Open Machine Translation Evaluation）比赛，我们同样获得了第一名。目前我们在中-英语互译、中-维吾尔语互译、中-藏语互译做的还不错。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器翻译之外，讯飞在教育方面做的比较多。我本人 4 年前一直在做教育相关的业务，包括给定主题自由表述的口语开放题型、自由书面作文的评分和批改。它们不仅牵扯到语音识别、手写识别技术，在识别正确的基础上，要在偏口语、噪音干扰的情况下，将整个考生的表述脉络理清，找出其中的病句、搭配不当。对中文作文的评分要难一些，因为中国人基本不会有语法错误，要给出前后语义搭配的连贯性等方面的评价和修改建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：在您看来，NLP 的下一步发展需要解决哪些问题？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;刘丹&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：目前来看大家研究的比较多的是语义理解。最近一两年 Google、Facebook，当然我们也做了一些阅读理解的问题。还有文本产生，就是让机器自己去写东西。目前在文本产生这部分，机器能产生语法没有错误、比较顺滑的句子，但产生出的段落看上去没有什么意义。机器能够产生文本，但没做到「创造」。这方面 Google 也做了 DeepArtist，面临的也是同样的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更偏实用的是虽然对话系统大家都在做、能做到「可用」，但和真人还是有显著差距，包括我们的语音助手和友商的产品都是这样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从技术上看，目前我认为值得深入的是两部分：一个是无监督学习，自然语言有大规模的无标注数据，但针对任务的标注如对话系统的数据是非常有限的。怎样做到使用无监督数据和少量有监督数据将问题做到大规模标注数据效果，是目前我们比较感兴趣的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外从神经科学角度看，还有对人记忆的仿生。目前的神经网络，包括号称有记忆的循环神经网络，所描述的记忆还是短时记忆，只能理解人说的一句话的内容。人的智慧随着年龄不断增长，核心在于人的记忆。只有人有记忆，在看到新事物的时候，才能通过唤醒记忆的方式找到类似的解决方案和创新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;记忆机制在机器学习尤其是自然语言领域都是近两年大家非常关注的课题，Google、Facebook 都做了非常不错的工作，Google 也在前两个月发了一篇可微分的神经计算机，记忆机制会是后续比较重要的东西。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 22 Nov 2016 10:38:12 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 谷歌新成立蒙特利尔AI研究室，向Yoshua Bengio等人赞助337万美元</title>
      <link>http://www.iwgc.cn/link/3605765</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自VB、Google Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌昨天宣布其在蒙特利尔设立了新的人工智能研究单位，并向蒙特利尔大学学习算法学院（MILA）的 Yoshua Bengio 等八名学者提供了 337 万美元研究资金。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 是深度学习界的著名学者，在此之前已经收获了来自谷歌和其他公司，包括 IBM、三星和英特尔的资金支持，但谷歌显然是其中最为重要的角色。「这笔资助是迄今为止来自私人企业最丰厚的一笔。」Bengio 表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一动向，让我们想起了 3 年前，谷歌收购 Geoffrey Hinton 与两名学生共同创建的创业公司 DNNresearch Inc。当时，谷歌宣布会向 Hinton 教授及其研究组捐赠 60 万元﹐用以支持日后的神经系统网络研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但在蒙特利尔的这次投资并不意味着 Bengio 与谷歌彻底联系在了一起，这位深度学习巨头仍然希望保持相对独立。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这就是我的态度，」Bengio 说道，「目前的进展符合我的价值观，我不想在大公司里获得数百万年薪，我现在很好，我在大学里的薪水很高。我现在更关心自己是否能对科学，人类做出更多贡献。我希望能够培养更多新一代学者。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为大型科技公司，谷歌正在人工智能领域军备竞赛中不断寻求提升自身的机会。它正在增加与 MILA 的联系，与蒙特利尔大学和麦吉尔大学开展更广泛地合作。&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720553&amp;amp;idx=1&amp;amp;sn=c88e48aab2d789d744ac1629ffec9a8a&amp;amp;chksm=871b0d57b06c844121d6fdf6fda546996e87c1cc395bef55fde20cb910b2fe1bbcdf8d1ce6c9&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720553&amp;amp;idx=1&amp;amp;sn=c88e48aab2d789d744ac1629ffec9a8a&amp;amp;chksm=871b0d57b06c844121d6fdf6fda546996e87c1cc395bef55fde20cb910b2fe1bbcdf8d1ce6c9&amp;amp;scene=21#wechat_redirect"&gt;不久之前谷歌拉拢到了斯坦福大学的李飞飞&lt;/a&gt;，确信无疑的是加入谷歌后李飞飞将仍继续在斯坦福的工作（Wired Update 的消息）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;增加与学界的互动，这正是目前所有开发深度学习应用的企业所做的。在人工智能领域里 Facebook 建立了 FAIR，谷歌拥有 Google Brain，其母公司 Alphabet 购买了英国的 DeepMind，百度也在硅谷建立了自己的人工智能实验室，微软研究院也在积极扩展这一领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然目前正值人工智能研究的爆发阶段，各大公司都在积极招揽人才，但对于 Yoshua Bengio，这位正在指导 25 名研究生与 5 名博士的蒙特利尔大学教授而言，留在学界才是最好的选择。有趣的是，他的兄弟 Samy Bengio 正在 Google Research 工作，为谷歌图片搜索等应用的开发提供了支持。所以之后的发展是否像 Bengio 坚持的那样还难以明确。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌位于蒙特利尔的新研究部门将由 Hugo Larochelle 领导，他最近刚刚离开 Twitter 的 Cortex 深度学习部门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其他 MILA 资金获得者包括 Christopher Pal，Doina Precup，Joelle Pineau，Simon Lacoste-Julien 和 Laurent Charlin，以及两名经常与 Bengio 合作的学者 Pascal Vincent 和 Aaron Courville。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 22 Nov 2016 10:38:12 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 关于人工智能未来发展的十三点预测</title>
      <link>http://www.iwgc.cn/link/3605766</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自KDNuggets&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Francesco Corea&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：沈泽江、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;曾经，人工智能被人们视作未来科技。但如今，人们想要看到超越人工智能的未来。&lt;span&gt;在如今物联网、机器人、纳米科技及机器学习逐渐发展和崛起的背景下，&lt;/span&gt;本文试图解读人们对人工智能在未来五年内发展的看法。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、概要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很显然，在过去几年间，人工智能给许多领域造成了非常大影响。不过，人们现在考虑的是，人工智能在未来五年内会在哪些领域发展。笔者认为，有必要（撰写一篇文章）描述如今我们如今看到的一些发展趋势，并对关于机器学习领域未来的发展做出一些预测。如下提出的列表并不一定穷举了所有的可能，读者也无需奉之为圭臬。但它们源自于在考虑人工智能对我们世界影响时，笔者认为有用的一些观点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、关于人工智能的十三点预测&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.人工智能工作时需要的数据量会变得更少&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。诸如 Vicarious 或 Geometric Intelligence 这样的公司，正在努力减少训练神经网络所需要的数据集的大小。训练人工智能使用的数据量如今被视为其发展的主要障碍，同时也是其最主要的竞争优势。同时，使用概率归纳模型（probabilistic induction, Lake 等人提出, 2015）能够解决这个在人工智能发展上的主要问题。某种不那么需要大量数据的算法，最终将会以丰富地方式学习、吸收并使用这个概念，无论是在行动上、想象上还是在探索中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.新的学习模型是关键要素&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。一种名为转移学习（Transfer Learning）的技术能允许标准的强化学习（Reinforcement Learning）系统基于之前获取的知识进行构建——而这是人类能轻松完成的。它隶属于增量学习（Incremental Learning）技术。而 MetaMind 则在研究多任务学习（Multitask Learning）问题。在其中，同一个神经网络被用来解决不同类型的问题，且当该神经网络能够在一类问题上表现更好时，那么它也能在另一些问题上表现更好。MetaMind 的下一步发展，是引入动态神经网络（Dynamic Memory Network）的概念，它能够回答特定问题，并能够推断一系列话语间的逻辑联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.人工智能会消除人类（认知）偏差，并能让我们变的更像「人造」的&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。人类的天性，将会因为人工智能而改变。Simon（1995）表示，人类不会作出完全理性的选择，因为（做出）最优化选择代价高昂，还因为人脑计算能力有限（Lo, 2004）。人们常常做的是寻求满意解，即挑选出至少是能使自己满意的选择。在生活中引入人工智能，或许会结束这样的情况。当（装备了人工智能）人类不再受计算能力约束后，这终将会一劳永逸地回答，是认知偏差真实存在并且是人类本能，还是这些行为只是在有限信息环境下或限制性情况下进行决策的捷径。Lo（2004）认为，人类（做决策时）的满意点，是在一系列的进化尝试和自然选择的过程中形成的。在其中，个体基于过去数据和经验进行预测并做出选择。他们根据接受的正／负反馈进行学习，并能够启发式的快速解决相关问题。但是，一旦环境改变，适应过程则有些延迟和缓慢，而且一些老的习惯并不能适应新的改变——这就造成了行为偏差。人工智能则会缩减这些延迟时间到 0，虚拟化的消除任何行为偏差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，基于经验随时间进行学习，人工智能成为新的变革工具：我们通常不评估所有的备选决策，因为我们不能想到所有决策（知识空间有限）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.人工智能会被愚弄&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。如今的人工智能远非完美，同时也有很多人正专注于研究如何欺骗人工智能设备。最近一个被叫做对抗样例（Adeversarial Examples; Papernot 等人, 2016; Kurakin 等人, 2016）算法被研发出来，它是首个能够误导计算机视觉的方法。智能图像识别软件会被经过微妙处理的图像所愚弄，该软件会对这些图像进行错误地分类。但有趣的是，这种方法却不会欺骗人类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5.人工智能的发展伴随着风险&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。主流的声音认为，人工智能正越来越成为人类潜在的灾难。当一个超级人工智能系统（ASI, Artificial Super Intelligence）被造出的时候，也许它的智慧远超过人类，甚至它能够想到并做到我们今天不能预测的事情。尽管如此，我们认为，在这些可怕的于人类存亡相关的威胁之外，还存在着不少和人工智能相关的风险。我们对于超级人工智能会做什么、怎么做，这背后隐藏的风险实际上都无法理解，无论它们会对人们造成正面的还是负面的影响。再之，在从狭义人工智能（Narrow Artifical Intelligence）向强人工智能乃至超级人工智能转换的过程中，会产生一个内在的责任风险——谁会对可能出现的错误或者故障负责？更进一步，在究竟谁能主导人工智能、人工智能的能力应该被如何使用的方面，也同样存在着风险。在这种情况下，我们确实觉得，人工智能应当作为一个工具（或是面向所有人的公众服务）被使用，并预留一定程度的决策权给人类以帮助该系统处理罕见的意外情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;6.真正的通用性人工智能很可能是一种集体智能（Collective Intelligence）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。强人工智很有可能不会是一个具有强大决策功能的单一终端，而是一种集体智能。群体智能（Swarm or Collective Intelligence, Rosenberg, 2015;2016）可以被视作「一群大脑的大脑」。到目前为止，我们仅让个体提供输入值，然后我们以一种「平均情绪」的智能方式整合这些事后输入。Rosenberg 称，现存的实现人类集体智能的方法，甚至都不允许用户之间互相影响。它们通常的处理方式，是只允许影响非同步出现——这会导致群体性偏差。另一方面，人工智能则会解决这样的联通缺陷，并且创建一个与其他物种非常相像的统一的集体智慧。自然中较好的例子来自于蜜蜂，它们进行决策的方式和人类神经运作的方式非常相像。它们都是用了大量的可执行单元，它们同步运行，能够整合噪声、权衡替代方案，并最后能够形成特定的决策。Rosenberg 认为，这个决策经过在分布的可执行单元和子群上的实时闭环竞争而最终形成。每一个子群都支持一个不同的选择，而最后共识的达成不是经过经过类似「平均情绪」的方法由大众决定，而是以一种「足够激励量」（Sufficient Quorum of Excitation, Rosenberg, 2015）的方式确定的。对于替代方案的抑制机制，由其他子群产生，能够避免整体系统达到一个局部优化决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;7.人工智能会带来无法预期的社会政治影响&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。人工智能首先带来的社会经济方面的影响，是失业问题。尽管从一方面来说这是一个非常现实的问题（当然也在很多方面带来了机会），我们认为也应当从其他不同的方面来看这个问题。第一，工作机会是被完全地摧毁了，而是会变得不同。因为数据将能被个人而非企业直接获取和分析，因而许多服务会逐步消失。并且，人工智能会使得知识分布趋于分散化。我们认为在这场革命中更应该关切的，是它带来的双重后果。首先，使用更聪明的（人工智能）系统后，在特定的领域内，越来越多人将丧失它们的专业性。这预示着，人工智能软件需要被设计整合一套双重反馈系统，能够整合人类和机器的处理方法。我们的第二点担忧和之前提到的第一个风险相关，我们担心人类将沦为「机器技术员」。因为大家都认为人工智能更擅长于解决问题，觉得它们很更可靠（，所以我们会更多地部署人工智能系统）。这种恶性循环将会让我们变得更没有创造力、失去独创力、更不聪明，并会以指数地增加人机的差异。我们正体验着这样的系统，或是当我们使用它时我们会更聪明，或是当我们不使用它时我们会觉得糟糕。我们希望人工智能更倾向于变为前者，而不是带来新的「智能手机效应」——我们会完全地依赖于它。最后，这个世界正变得越来越对机器人友好，人类在其中也扮演着连接机器人的角色而与之对立。机器人（在社会中）正逐步起到主导作用，它们对人类的影响相比人类对他们的影响也越来越大，这也许会让人类最终变为（社会系统中的那个）「故障」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在地缘政治方面，我们则认为人工智能会对全球化造成巨大的影响。有这样的可能，由人工智能系统控制机器人运行的被优化的工厂，其厂址最终会重新回到发达国家。因为（到那时）在新兴国家建厂，会失去那些传统的低成本的理由。我们不清楚，这是会平衡国家之间的差异，还是会增大发达国家和发展中国家间已经存在的差异。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;8.真正的人工智能应该开始问「为什么」&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。到目前，大多机器学习系统都能够在模式识别及辅助决策方面做的很好；并且因为大部分程序都被硬编码了，所以它们仍能够被理解。尽管我们已经能让人工智能阐明「是什么」和「如何做」，这已经是一个不错的成就，但人工智能仍未能够理解事物背后的「为什么」。因此，我们需要设计一个通用算法，它能够从物理上及精神上建立关于世界本质的模型（Lake 等人, 2016）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;9.人工智能正在推进隐私保护问题和数据泄漏预防问题&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。人工智能将隐私问题提升到了一个新的等级。新的隐私保护方法应当被发明及采用，它们应当比简单的安全多方计算法（SMPC）复杂得多，也应该比同态加密法（homomorphic encryption）高效迅速。最近的研究表明，差分隐私（Differential Privacy）法能够解决大部分我们日常遇到的隐私问题。不过已经有不少公司走得更远，如 Post-Quantum 公司——这是一家基于量子计算的网络安全创业公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;10.人工智能正在改变物联网（设备）。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;人工智能（的发展）允许物联网设备能被以完全分布式的架构进行设计，在其中每一个节点都能够进行自己的预算（也就所谓的边界计算）。在传统的中心化模型中，有一个被称作是服务器／客户端模型的问题。其中的每一台设备都连接到云端服务器，并由云端服务器识别、验证，这导致了非常昂贵的设备费用。但基于分布式方法设计的物联网网络或是传统的点对点（Peer-to-Peer, P2P）架构，则能够解决这个问题、降低费用，并能够避免因一个节点实效而造成整个系统损坏的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;11.机器人学正变为主流。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;笔者认为，人工智能的发展会受到机器人学发展的制约。同时，这两个关联的领域会以相同的速度发展，以最终得到一个适当的强人工智能或超级人工智能。如下图所示，在我们的研究乃至我们的集体意识中，我们不会视那种没有「物理实体」的人工智能为强人工智能或超级人工智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F10mgJvMfv961naQTnCDnMkia1sI0QncMia7ahV9zUXumVFzn7wX14AIw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;关于机器人学及人工智能相关领域的研究趋势（由 CBInsights Trends tool 制作）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外还有一些证据能证明这个趋势：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最近激增的机器人相关专利申请数量。据 IFI 称，中国（近期）已经有超过 3000 项目申请，在美国、欧洲、日本、韩国等地的数量大致相同。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如下图所示的最近机器人相关基金的价格走向。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fsic8sFQrXpLib4ibuhauoJpPRF0rYJVVOx4ib8Tfh7zGyJ5D86bUhK3n8w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器人 STOX 基金从 2013 年至 2016 年的价格走向&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;12.人工智能在发展中也许会面临阻碍。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;在实现强人工智能的过程中面临的最大阻碍，不是算法的选择或数据的使用（至少不只是），而是一个结构上的问题。硬件性能、（设备间）物理的联系（如互联网）及设备的耗能，是创建足够快人工智能的瓶颈。这也是我相信存在着如 Google Fiber 这样部门的原因，也是为什么量子计算正变得越来越相关的原因。量子计算允许我们以超高的速度进行运算（根据物理规则它会瞬间完成），而这在传统电脑上会耗费非常长的时间。它依靠量子力学的性质，基于传统计算机用二进制描述问题的想法。因此，据 Frank Chen（在 Andreessen Horowitz 的合伙人）称，晶体管、半导体及电子导体都将被量子比特所取代。量子比特由向量表示，这也意味着其运算律会不同于的传统的布尔代数规则。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种对传统计算法和量子计算法区别的通俗比较，基于电话本问题（Phonebook Problem）。在电话本中搜寻号码，传统的方式是一条接着一条地搜索以最终找到匹配的号码。但基本的量子搜索算法（也被叫做 Grover's 算法）则依靠所谓的「量子叠加态」。它能一次性分析所有的元素并确定最可能的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建造量子计算也许会是科学界革命性的突破，但 Chen 表示现在建造它是非常困难的。亟待解决问题包括：建造计算机的超导材料需要的高温，极短的贯通时间（Coherence Time）——这是量子计算机实际进行计算的时间窗口，单次计算所需的时间，以及正误答案之间的能量差过小难于被探测到。所有这些问题缩小了（量子计算机的）市场空间，并且只有小部分公司能够涉足量子计算领域：科技界的巨擘如 IBM 和 Intel 已经对其研究多年；创新公司如 D-Wave System（2013 年被谷歌收购）、Rigetti Computing、QxBranch、1Qbit、Post-Quantum、ID Quantique、Eagle Power Technologies、Qubitekk、QC Ware、Nano-Meta Technonoliges；还有奠定量子计算基础的 Cambridge Quantum Computing 有限公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;13.生物机器人和纳米科技将是未来人工智能的应用方向&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。我们正见证着在人工智能和纳米机器人交叉领域，一些列令人震惊的发展。研究人员正致力于创造完全完全智能的装置，同时也在研究相关的结合体。他们甚至尝试研发出生物导线（一种由细菌制造的导线）及器官芯片（由人细胞制作的、人器官中起功能部分的微型模型，能够复制器官的部分功能；在该领域，Emulate 是最领先的公司）。生物机器人方面的研究同时也考验着着材料性能的极限。最近一种「软」机器人被制造出来，他只有软的构建。BAS Systems 公司也在推进计算的发展，正尝试研发一种「化学计算机」（Chemputer），一种能够使用先进化学过程以「生长」复杂电子系统的装置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 22 Nov 2016 10:38:12 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 微软动真格，聚集顶尖量子物理学家打造拓扑量子样机</title>
      <link>http://www.iwgc.cn/link/3605767</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自纽约时报&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：约翰·马尔科夫&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今年&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719922&amp;amp;idx=2&amp;amp;sn=62de6104fdc8b834c1f3c95ff1418b31&amp;amp;chksm=871b02ccb06c8bdaedb8d5430d3972a379c4e4dfff391683f4784c9b416403a6f0071fd000f9&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719922&amp;amp;idx=2&amp;amp;sn=62de6104fdc8b834c1f3c95ff1418b31&amp;amp;chksm=871b02ccb06c8bdaedb8d5430d3972a379c4e4dfff391683f4784c9b416403a6f0071fd000f9&amp;amp;scene=21#wechat_redirect"&gt;十月自然杂志采访了微软研究的量子结构和计算小组&lt;/a&gt;，介绍了微软在拓扑量子计算研究上的成果。微软研究拓扑量子计算已经有十多年了。但是市面上关于其研究的具体内容并不多见。近日，微软决定着手打造量子计算工程样机，十多年的研究成果终将付诸实践。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软在量子计算领域投入了大量的资金和工程，意图打造一台秒杀当下数字计算机的机器。眼下技术世界的前景比较乐观，有望造出科幻作品中曾出现的那种量子计算机、超级计算设备。一旦这种机器运转起来，药物设计和人工智能这样的领域都会受其影响，而且还会为现代物理学的基础提供更好的理解。微软决心从纯研究转向昂贵的应用研究，反映出包括谷歌、IBM 在内的全球科技公司的竞争态势，这些技术巨头都在大量投入突破性技术的研发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在量子物理学的奇异世界里，微软选择了一条不同的道路，将自已与其他竞争对手区分开来。微软选择研究「编织」粒子，也被称为任意子「anyons」，据物理学家描述，这种粒子存在于两个维度。微软将利用其利用亚原子粒子的独特物理性质来建立超级计算机的构建块。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;领先的研究者承认打造有用的量子机器仍然有障碍，这些障碍来自于基础物理学以及开发探索量子比特特性的软件，而当下的数字系统的计算能力无法跨越这些障碍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不同与可以任意开关的传统晶体管，可以用 0 或 1 表示，量子比特存在于这二者的叠加状态中，或者同时拥有这两种状态。如果量子比特被置于一种与其他量子比特「纠缠」在一起的状态——物理上分离，但是行为上深深地交织在一起。一台量子计算机很可能有数百数千个量子比特构成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软 2005 年起开始这方面的研究，建立了 Station Q，由数学家 Michael Freedman 领导。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在微软相信很快就能设计出基础的量子比特构建块，已经着手打造完整的计算机，指导该项目的资深工程经理 Todd Holmdahl 说到。在过去几年中，他已经领导了多个微软项目，包括 Xbox 视频游戏机器和即将发布的 HoloLens 增强现实系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibZpzxicBpPVYbHDXI2dN7w0O2FutPtWfq2RvlIXbQCSbUMeWC9ygZDgNrdg4YaGehYINZsokpiaLRg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Todd Holmdahl &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Holmdahl 先生说道，「一旦我们弄出了第一个量子比特，后面的路线会非常清晰，我们就能轻松造出数千个量子比特。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而物理学家和计算机科学家仍在争论到底能不能造出一个真正有用的量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多种其他的研究项目正在尝试用不同的材料和设计打造量子比特。微软的方法被称为拓扑量子计算，这种方法基于今年的诺贝尔物理学奖成果，存在于两个维度的物质形式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Holmdahl 的项目囊括了一票知名物理学家，包括代尔夫特工业大学的 Leo Kouwenhoven，哥本哈根大学的物理学家 Charles M. Marcus，悉尼大学的 David Reilly 和苏黎世联邦理工学院的 Matthias Troyer。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些物理学家都将成为在 Artificial Intelligence and Research Group 老大沈向洋的带领下成为微软的员工，他们说微软之所以决定做拓扑计算机是因为过去两年内科学的巨大进展让他们相信这件公司会造出更稳定的量子比特。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「魔法是结合半导体和超导体，」Marcus 博士说。研究人员最近在制作量子比特的控制上取得了重大突破。最具竞争力的方法是将量子计算机冷却到接近绝对 0 度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到目前为止，几乎还没有出现能比数字计算机更快解决问题的有效算法。早期有个知名的 Short 算法，用于因素数字，当时非常有希望打败数字计算机中的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那将可能出现振动世界的结果，因为现代电子商务是建立在密码系统上的，用传统的数字计算机破解不了。其他的方法或许可以更快地搜索数据库或者执行机器学习算法，目前正用于解决计算机视觉和语音识别的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而更直接的影响是是，这些工具或许可以推动物理学的基本理解，这是物理学家 Richard P. Feynman 在 1982 年推测量子计算机时生出的想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在 Kouwenhoven 博士看来，「我对量子计算的理想应用是一台可以解决量子物理问题的机器」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 22 Nov 2016 10:38:12 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 伯克利教授Stuart Russell：人工智能基础概念与34个误区</title>
      <link>http://www.iwgc.cn/link/3592114</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自UC Berkely&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Russell 是加州大学伯克利分校人工智能系统中心创始人兼计算机科学专业教授，同时还是人工智能领域里「标准教科书」《人工智能：一种现代方法》作者（谷歌研究主管 Peter Norvig 也是该书作者）。在这篇文章中，他以 Q&amp;amp;A 的方式讲解了人工智能的未来以及常见的误解。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 什么是人工智能？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;是对让计算机展现出智慧的方法的研究。计算机在获得正确方向后可以高效工作，在这里，正确的方向意味着最有可能实现目标的方向，用术语来说就是最大化效果预期。人工智能需要处理的任务包括学习、推理、规划、感知、语言识别和机器人控制等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「它是一个特定技术」。例如在二十世纪八十年代到九十年代，人们经常会看到新闻报道中人工智能与基于规则的专家系统被混为一谈。现在，人工智能经常会与多层卷积神经网络混淆。这有点像把物理和蒸汽机的概念搞混了。人工智能探究如何在机器中创造智能意识，它不是在研究中产生的任何一个特定的技术。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「这是一个特定类别的技术方法」。例如，经常有人用符号化或逻辑化的方法将人工智能与「其他方法」相互比较，如神经网络和遗传编程。人工智能不是一种方法，它是一个课题。所有这些方法都是在对人工智能进行研究的产物。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「这是一小群研究者的方向」。这个误解与前几个错误有关。一些作者使用「计算智能」指代几个特定的研究者群体，如研究神经网络，模糊逻辑和遗传算法的研究者。这是非常片面的，因为这种分类让人工智能的研究陷入孤立的境地，让研究成果不能得到广泛的讨论。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「人工智能只是算法」。严格说来不算是误解，人工智能的确包含算法（也可粗略定义为程序），它也包含计算机中其他的应用。当然，人工智能系统需要处理的任务相比传统算法任务（比如排序、算平方根）复杂得多。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 人工智能将如何造福人类？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;文明的一切都是人类智慧的产物。在未来，人工智能会将会扩展人类的智力，这就像起重机让我们能够举起几百吨的重物，飞机让我们很快飞到地球的另一端，电话让我们在任何角落实时交流一样。如果人工智能被适当地设计，它可以创造更多价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「人工智能没有人性」。在很多反乌托邦幻想中，人工智能会被用来控制大部分人类，无论是通过监视，机器人执法，法律判决甚至控制经济。这都是未来可能出现的情况，但首先它不会被大多数人接受。人们往往忽视人工智能可以让人类接触更多的知识，消除人与人之间的语言隔阂，解决无意义和重复的繁重任务。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「人工智能将造成不平等」。毫无疑问，自动化程度的提升将使财富集中到越来越少的人手里。但是现在，如何使用人工智能的选择权在我们手里。例如，人工智能可以促进协作，让生产者与客户有更多交流，它可以让个人和小组织在全球化的经济环境下独立运作，摆脱对于特定大公司订单的依赖。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 什么是机器学习？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是人工智能的一个分支，探索如何让计算机通过经验学习提高性能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「机器学习是一个新的领域，它已经代替了人工智能的地位」。这种误解是最近机器学习热潮产生的副作用，大量学生在之前没有接触过人工智能的情况下学习了机器学习课程。机器学习一直是人工智能的核心话题：阿兰·图灵在二十世纪五十年代的论文中已经认为学习是通向人工智能最可行的途径。这一观点似乎是正确的，人工智能最突出的早期成果，Arthur Samuel 的跳棋程序就是使用机器学习构建的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「机器不能学习，它们只能做程序员告诉它的事情」。这显然是错的，程序员能够告诉机器如何学习。Samuel 是一个优秀的跳棋玩家，但他的程序很快就通过学习超过了他。近年来，机器学习的很多应用都需要大量数据来进行训练。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4. 什么是神经网络？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络是受生物神经元启发构建的计算系统。神经网络由许多独立的单元组成，每个单元接收来自上一层单元的输入，并将输出发送到下个单元（「单元」不一定是单独的物理存在；它们可以被认为是计算机程序的不同组成部分）。单元的输出通常通过取输入的加权和并通过某种简单的非线性转型，神经网络的关键特性是基于经验修改与单元之间的链接比较相关权重。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「神经网络是一种新型计算机」。在实践中，几乎所有的神经网络都运行在普通的计算机架构上。一些公司正在设计专用机器，它们有时会被称作是「神经计算机」，可以有效地运行神经网络，但目前为止，这类机器无法提供足够的优势，值得花费大量时间去开发。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「神经网络像大脑一样工作」。事实上，生物神经元的工作方式比神经网络复杂得多，自然界存在很多种不同的神经元，神经元的连接可以随时间进行改变，大脑中也存在其他的机制，可以影响动物的行为。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5. 什么是深度学习？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习是一种特定形式的机器学习，训练多层神经网络。深度学习近年来非常流行，引领了图像识别和语音识别等领域的突破性进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「深度学习是一个新领域，已经代替了机器学习的地位」。事实上，深度学习在神经网络研究者中间已经被讨论了超过二十年。最近深度学习的发展是由相对较小的算法改进以及大数据集模型和计算机硬件发展驱动的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;6. 什么是强人工智能和弱人工智能？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「强人工智能」和「弱人工智能」概念是由 John Searle 最先提出的，是他对人工智能研究方向的两个假设。弱人工智能假设机器可以通过编程展现出人类智能的水平。强人工智能则假设机器出现意识，或者说机器思考和认知的方式可以用以前形容人类的方式来形容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「强人工智能是人类智力级别通用人工智能研究的方向」。这个解释具有代表性，但这不是强/弱人工智能概念被提出时的本来意义。同样，「弱人工智能」被认为是针对特定领域，执行特定任务的人工智能研究，如语音识别和推荐系统（也称工具 AI）。虽然没有人具有最终解释权，但这种语义的转换可能会造成不必要的混乱。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;7. 什么是 AGI，ASI 和超级智能？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AGI 代表的是通用人工智能，这个术语意在强调建立通用目的智能系统的雄心目标，其应用的宽度至少能覆盖人类能解决任务。ASI 指的是人工超级智能：远远超越人类智能的人工智能。更具体地说，一个超级智能系统高质量决策能力要比人类强，它能考虑更多的信息和进一步深入未来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「主流的人工智能研究者并不关心通用人工智能。」像语音识别这种细分领域的某些研究者主要关心的是其所在领域的具体目标，其他一些研究者比较关心找到现有技术的商业应用。在我的影像里，如学习、推理、和计划等细分领域的大多数人工智能研究者认为他们目前的研究工作有助于解决通用人工智能的子问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「人类的智能是一种通用智能」。这种观点常被认为是显而易见，不值得讨论，但它却几乎回避了关于 AGI 的所有讨论。持有这种观点的人通常会认为通用智能就是人类能做到所有任务的能力。然而当然不存在人工不能做的人类工作，所以人类能做已经存在的人类工作也没什么好惊讶的。难的是怎么定义那种完全独立于以人类为中心的价值观和偏见的宽度。所以我们只能说人类智能是某种程度上的通用智能，人类能做人类能做的所有事情。另一种更有意义的说法是人类能做很多事情，但目前为止这个问题 还没有确切的答案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;8. 什么是摩尔定律？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「摩尔定律」指的是多个相关的观察和预测能影响电路性能和密度。现代理解的「摩尔定律」是每一秒的操作次数以及每一美元所能买到的电脑性能，将每隔 N 个月翻一倍以上，N 大约是 18，这一表述有些背离「摩尔定律」最初的定义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「摩尔定律是物理定律」。事实上，摩尔定律只是一种关于技术进步的经验观察。没有什么规定摩尔定律会持续下去，当然它也不可能无限持续下去。时钟速度的增加已经达到了顶峰，目前价格/性能上的提升也来自于单个芯片上内核（处理单元）数量的上升。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;9. 摩尔定律能让我们预测出超级人工智能的到来吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不能。人工智能系统不能做的事情很多，比如理解复杂的自然语言文本；加速意味着在很多情况下得到的错误答案的速度也越快。超级智能需要在主要的概念突破。这些很难预测，即便我们有了速度更快的机器也没啥用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「让机器更强大的意思是提升它们的智能」。这是人工智能的未来的讨论中的一个常见主题，这个主题似乎建立在一个混乱的概念上，我们使用「强大」来描述人类智力，但是在描述计算机时用的「强大」的含义更加简单，就是每秒操作的次数。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;10. 什么是机器 IQ？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;没有机器 IQ 这种说法。某种程度上一个人在多个任务上的多种智慧能力是高度相关的，人类可以说有 IQ，但是研究者们对任意单一维度上的 IQ 定义有争议。另一方面，任意给定的机器的各种能力之间都是不相关的：一台机器能打败世界象棋冠军，并不意味着它能玩的好别的棋类游戏。能赢得猜谜比赛的机器也无法回答「你叫什么名字？」这样简单的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;根据摩尔定律，机器 IQ 会不断上升&lt;span&gt;」&lt;/span&gt;。既然根本不存在什么机器 IQ，它也就不可能增长；摩尔定律描述的仅仅是原始的计算吞吐量，与是有存在执行任意特定任务的算法没有关系。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;11. 什么是智能爆炸？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「智能爆炸」这个术语是 I.J.Good 于 1965 年在其文章「Speculations Concerning the First Ultraintelligent Machine」中创造的。它指的是足够智能的机器能重复设计它自己的硬件和软件来创造出一个更加智能的机器的可能性，这个过程会一直重复下去，直到「人的智能被远远的甩在后面」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;一旦机器达到人类水平的智能，智能爆炸就在所难免&lt;span&gt;」&lt;/span&gt;。反过来：虽然逻辑上是可行的，但是让 N 代的机器设计出 N+1 代的机器太难了。同样的道理，我们造的机器可能在一些重要的方面成为超过人类，但是在其他方面可能会落后于人类。在解决贫困、治疗癌症等重要问题上，机器的能力肯定会比人类强，而且不需要在人工智能研究上有大突破就能实现。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;12. 人工智能系统何时才能超过人类智力？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个难以回答的问题。因为首先它假定这件事必然发生，事实上它具有选择性：假如人类选择不去发展这样的人工智能，这件事就不太可能发生。第二，「超过」假定智力是线性的，而这不是真实情况，机器在某些任务的处理上比人类更快，而在更多放面则很糟糕。第三，如果我们认为「通用的」智能是有用的，我们就可以开发这样的机器，但目前我们不知道它是不是有用的。宽泛地说，实现这样的人工智能还需要很多技术突破，而这些都是难以预测的，大多数科学家认为这件事会在本世纪内发生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「它永远不会发生」。对技术突破进行预测是很难的。1933 年 9 月 11 日，Rutherford，也许是那个时代最著名的核物理学家，在英国科学促进年会上向人们宣布：「任何想从原子变形过程中获取能源的努力都是徒劳的。」（他在各种场合发表过许多类似言论，大意都是表达使用原子能是不可能的）结果第二天早上，Leo Szilard 发现了中子诱导链式反应，并很快对核反应堆申请了专利。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;13. 人工智能系统现在能做什么？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能的应用范围已经比几年前大很多了。从围棋、纸牌、简单的问答、从新闻中抓取信息、组合复杂的对象、翻译文字、识别语音、识别图像中的概念、到在「普通」交通条件下驾驶汽车，不一而足。在很多情况下，人工智能在你不知道的情况下发挥着作用，如检测信用卡欺诈，评估信用，甚至在复杂的电子商务拍卖中投标。搜索引擎中的部分功能也是人工智能的简单形式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「像『下棋』这样的任务对机器来说和对人类来说是一样的」。这是一个错误的假设：机器「掌握」一项技能的程度超过了人类。人类通过阅读和理解学会游戏规则，通过观看棋局和下棋来提高水平。但典型的下棋程序没有这样的能力——将下棋规则编程，让机器算法直接给出所有可能的下一步。机器无法「知道」人类所谓的规则（目前新兴的强化学习方式改变了这一点）。DeepMind 的人工智能系统可以学会很多种游戏，它不知道自己在学习什么，看起来也不太可能学会这些游戏的规则。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「机器执行任务的方式和人类一样」。我们不知道人类思考问题的机制，但这种机制与人工智能系统处理任务的方式看起来大不相同。例如，下棋程序通过考虑当前棋局状态和下一步可能的序列比较结果考虑下一步，而人类经常是先发现可能获得的优势，然后继续考虑如何找到一系列方式来实现它。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「如果机器可以做到任务 X，那么它就可以做类似的所有任务了」。参见有关机器 IQ 的问题，机器目前还不能形成通用化的智能，它们的功能通常局限于某一领域。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;14. 人工智能会对社会造成什么样的影响？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在可预见的未来中，人工智能的各种应用将会改变社会形式。自动驾驶汽车现在已经在路上进行测试，至少有一家公司承诺将在 2016 年内交货（考虑到目前遇到的困难，其他公司的态度则更为谨慎）随着计算机视觉和机械腿设计的进化，机器人非结构化环境正在变得更为实用——可能的应用范围包括农业和服务领域（特别是对于老人和残疾人而言）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，随着机器能够理解人类语言，搜索引擎和手机上的「个人助理」将会改变现有的人机交互方式，它们可以回答问题，整合信息，提供建议，并促进交流。人工智能还可能会对科学领域（如系统生物学）产生重大影响，这些学科中信息的复杂性和数量一直令人望而却步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「机器人正在接管一切」。参见《人工智能的智力何时才能超过人类》，人工智能中的绝大多数进步是基于任务处理的改进。当然，从长远来看，维持人类的控制很重要。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;15. 人工智能与机器人的发展会取代大量人类的工作吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些研究（比如 Frey 和 Osborne 在 2013 年的调查）表明在未来美国将近一半的工作在自动化面前会变得很脆弱。其他作者，比如 Bryjolfsson 和麦肯锡在 2011 年的工作表明这一变化已经开始了：2008 年经济萧条之后就业率的缓慢恢复，生产率与停滞不前的工资之间的差异化增加了自动化的进程。随着人工智能与机器人的持续发展，更多的工作将受到影响看起来不可避免。大量的失业并不是必然的，但这可能会造成经济结构的巨大转变，需要想出组织工作与酬劳的新思路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「机器人的工作越多意味着人类工作越少」。工作不是零和（zero-sum）的：由一对机器人协助的工人可能更具工作效率，也因此需要更多这样的工人。没有机器人的帮助，一些领域的工作由人类完成可能不具备经济效益，或者一些工作单独的人或机器无法完成。同样，就像涂刷匠的刷子与滚筒：如果使用针尖大小的刷子一点一点的涂刷，我们就雇不起涂刷匠来涂刷一整间屋子了。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;16. 什么是无人机，自动武器，杀人机器人？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无人机是由人远程控制的飞行器；有些无人机可以携带武器（通常是导弹），这些武器的释放也是由人远程控制的。自动武器是可以自主选择和吸引攻击对象的装置。目前这类装置包括韩国非军事化区里的自动瞄准机枪和一些不同类型的船载反导弹系统。目前在技术上可以实现将无人飞机的控制员替换成完全自动的计算机系统，以达到致命自主武器系统的要求。致命自主武器系统是日内瓦会议裁减军备议题的讨论主题。杀人机器人是对具有轮动能力和行走能力的武器的统称，包括：船，飞行器以及人工智能的昆虫飞行器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;完全自主武器的出现还需要 20-30 年的研发&lt;span&gt;」&lt;/span&gt;。得出这个预估时间的依据无从知晓，但是 20-30 年的时间跨度有点夸大所需的研发时间长度。目前自主武器的研发已经在全世界内大范围的开展，英国国防部已经宣称，对于一些简单对抗如海上战役，完全自动武器现在已经可以实施。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;17. 我们需要担心杀人机器人胡作非为或接管世界吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果部署了自动化武器，它们也会有士兵那样的难题：有时难以分别朋友与敌人、平民与敌军。而且可能会有军事事故造成平民伤亡，或者机器人受到干扰与网络攻击。也因为后者，一些军事专家预测自动化武器可能需要封闭操作系统，没有电子通讯。如果系统行为不准确的话，这样做能防止有人凌驾于自动化控制器之上。但在可预见的未来，自动化武器可能会变得很常见，在有限的任务中被使用。但在全局规模上，它们很难自己编程出计划。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们可以按下「关闭」按钮。「关闭」按钮会使得自动化武器在网络攻击面前变得很脆弱。这样的通信频道在战争中也是如此。此外，通用智能系统会被赋予一项任务，防止自己的「关闭」按钮被按下。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;18. 人工智能的「存在风险」是什么？它是真的吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于人工智能风险的早期警告曾是非常模糊的。I.J.Good 对于人工智能的可行性提出了自己的观点：「只要机器能够聪明到告诉我们如何保持对它的控制。」人们普遍意识到，在我们的星球上如果存在一个超级智能实体，可能会出现恐慌；但另一方面，我们也都清楚更加聪明的机器会更加有用，而且更加聪明不一定意味着邪恶。事实上，论据很简单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;假设超智能系统被设计成实现由人类设计者指定的某一目标，并假设这一目标不完全符合人类的价值观，人工智能形成的价值观（如果有）是非常难以确定的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;任何充分有能力的智能系统将倾向于确保其自身的持续存在并且获取物理和计算资源——不是为了他们自己的目的，而是为了更好地执行人类为它设定的任务。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们问题的本质是你所要求的不是你所得到的。Norbert Wiener 是自动化和控制理论的先驱者，他在 1960 年写道：「如果我们使用——为达到某些目的——一些机器来代替我们做某些工作，我们最好能够清楚它们的确在按我们的想法工作。」Marvin Minsky 举了让机器计算 pi 这个例子，Nick Bostrom 则举了回形针的例子。对于人类而言，这些目标是根据人类视角提出的，这意味着计算机服务器或回形针覆盖整个银河系不是好的解决方案。一个具有能力的决策者——特别是能够通过互联网连接全球每块屏幕的智能——可能会对人类产生不可逆转的影响。幸运的是，这个问题相对比较明确，所以现在就可以开始解决。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;超智能机器将变得自发地产生意识、本能地变得邪恶或伤害人类。科幻小说作者通常假定上面这些一个或多个问题来设定机器与人类的对立面，这样的假设完全是不必要的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们人类发展人工智能系统，那么为什么我们要制造出来毁灭自己呢？有一些人类工智能「捍卫者」常常争辩道因为人类建立了人工智能系统，那么完全没有理由来支持这样的假设，即我们是在制造一个旨在毁灭人类的机器。这个没有抓住辩论要点，即哪个是邪恶意图，在设计者这一边还是代中间者这一边，这是存在存亡威胁的先决条件，这个问题也就是错误设定了对象。这将永远不会发生。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;19. 为什么人们会突然对人工智能如此担心？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 2014 年开始，媒体就定期地报道如 Stephen Hawking、 Elon Musk、 Steve Wozniak and Bill Gates 那样名人的对人工智能的担忧。这些报道通常引用那些最绝望话语并省略实质担心的深层原因，通常就像「什么是人工智能现存风险」那样的问题。在许多情况下，担忧就是在阅读 Nick Bostrom 的书籍超智能（*Superintelligence*）之后产生的。另外一些当下关心这个问题的潮流也是因为人工智能的发展正在加速。这种加速可能是很多因素的集合，包括逐步完善的理论基础，它连接了很多的人工智能领域成为一个统一的整体。还有学术实验室能产出达到能够应用并解决现实世界的实际问题在人工智能方向商业投资的急剧增加也作为。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果人们是担心超人工智能就在某个角落，那么基本上人工智能研究者很少认为超智能机器就在我们周围某个角落。这并不暗示着我们应该等着，直到这个问题变得很严重！如果我们发现直径 10 英里的小行星将于 50 年后撞向地球，我们难道能够不消灭它并声称「我们会在五年的时候去关注它」？&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;20. 人工智能在接下来的几十年里会取得怎样的进步？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个领域好像并不要求人类级的通用人工智能能够达到成熟，而制造一些可信赖的高质量的产品也许在下个十年内有能实现。这就包括了语音识别、从简单的实际材料中提炼信息、对物体和行为的视觉识别、日常事物的机器人操作和自动驾驶。努力提升质量和扩展文本与视频的理解系统能制造更强劲的家用机器人，产生更为广泛有用的机器人，它能展示常识知识系统，一起学习并在遍历所有形式后表现得更好。还存在获取和组织科学知识的专业系统，它能管理复杂假说并可能对分子生物学、系统生物学和制药方面产生重大的影响。我们也许也会看到它在社会科学和政策制定有相同的影响，特别是在给它关于人类活动巨量的机器可读性数据之后，并如果机器是很可靠有用的，那么人们同样也需要机器去理解人类价值。公共和私人知识源，也就是知道和推理真实世界的系统，它不仅仅是数据的仓库，它会成为社会的组成部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;21. 什么是「价值定位（value alignment）」？它有什么要紧的？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;价值定位（Value alignment）就是校准人机关系具体目标价值的任务，所以机器最优选择大概来说就是无论做什么都是最大化人类的幸福感。如果没有价值定位，那么超脱人类掌控的超智能机器的出现就是不可忽视的风险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;我们所有需要的就是阿西莫夫定律（Asimov's laws）&lt;span&gt;」&lt;/span&gt;。阿西莫夫定律本质上就是一些条款：它们给人类创造出各种故事情节提供灵感，但是基本对约束机器人没有什么有用的信息，因为它没有更多具体的细节。它们的基本结构为一组规则而不是效用函数，这是很有问题的：它们的词典式结构（例如任何对人类的伤害是比所有机器人的损害还要严格重要地多）意味着没有给不确定性或权衡留下空间。也许机器人只为了拍死一只在以后可能叮咬人类的蚊子会跳出悬崖毁灭了自己。另外，它也许会锁上人类汽车的门，因为坐车会提高人类受伤的可能性。最后，基于最大化人类效用的方法，对于第三条法则是没有必要的（机器人自我保护），因为机器人不保证自身的存在是不能为人类效用做出贡献的，还会令其拥有者十分失望。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;22. 对于存在主义风险（existential risk），人工智能社区做了什么？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;许多关于人工智能的存在主义风险的讨论都是处于人工智能社区主流之外的，它们是从人工智能研究最初到最主要的反动力。在 2008 年的时候，AAAI（美国人工智能学会）就举行了个座谈会来讨论这个问题。座谈会中期报告就指出了存在的一些长期问题，并降低了一些人工智能对人类社会风险的想法。最近，在 2015 年 1 月 Puerto Rico 由 Future of Life Institute 主办的会议上，参会者和随后参加者共六千多人共同签署了一份公开信，强烈呼吁应该有关注这些风险问题的研究和提出一个更加详细的研究议程。随后，Elon Musk 为支持这方面的研究而拿出了 1000 万美元。另外，Eric Horvitz 已经建立个期望追踪风险问题并在需要时给出政策建议的长期研究。最后还有 AAAI 也已经建立了一个关注人工智能影响和伦理问题（Impact of AI and Ethical Issues）的常务委员会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;规约或控制研究是不可能的&lt;span&gt;」&lt;/span&gt;。有些人辩称没办法避免消极后果，因为研究进展是无法停止和规约的。实际上这种声称本身就是错误的：在 1975 年关于基因重组的阿西洛马会议（Asilomar Conference）就成功地发起自愿活动中止了设计制造人类遗传性基因修饰，并一直持续成为了国际准则。另外，如果实现人类级的人工智能研究未加抑制（这个是很可能出现的），那么在方法上开始谨慎地研究确保人工智能系统在我们掌控下是十分重要的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;23. 我能提供什么帮助吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你是一个人工智能研究者（或对这方面感兴趣的经济学家、伦理学家、政治学者、未来主义者和律师），从 2015 年波多黎各会议（Puerto Rico conference）在研究议程中就已经兴起了一个主题，即在主要的人工智能会议上会举行相应的研讨会，比如说 AAAI Fall 和 Spring Symposium series 等等。FHI、CSER、 FLI 和 MIRI 网站都有更多的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;常见误解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;「&lt;/span&gt;完成这些是没什么困难的&lt;span&gt;」。&lt;/span&gt;我们不管做什么都无法改变未来，这些事都终将发生。也没有什么能离真相更近一点的，我们不能预测未来，因为我们正在创造未来，这是我们集体的选择。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 21 Nov 2016 14:24:11 +0800</pubDate>
    </item>
    <item>
      <title>视频 | Yann LeCun CMU 演讲：人工智能的下一个前沿——无监督学习</title>
      <link>http://www.iwgc.cn/link/3592115</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CMU&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;上个月 Yann LeCun 在 CMU 做了一场讲座，讲解了人工智能中的下一个发展前沿：无监督学习。几天前，CMU 官方放出了此次讲座的视频，共计 1 小时 15 分钟。机器之心对此视频内容进行了摘要概述，希望对读者了解无监督学习有所帮助。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FNoIsxEETia9zH4TRXHwLsPTb2BR1m0RcgMiab11LviaXzticf9rE3icvcmg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在观看视频前，小编先为大家出一道题。这张图包含了如今人工智能领域的众多大牛：Hinton、Michael Jordan、LeCun 都在内，你能发现他们吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?width=500&amp;amp;height=375&amp;amp;auto=0&amp;amp;vid=y0348sofhrw" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;视频摘要：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去几年，人工智能领域的快速发展得益于深度学习和神经网络的发展，再加上大型数据集和高速 GPU 的可用性。我们如今已经有了能与人类相媲美的图像识别系统，它们将变革自动驾驶交通和医疗图像理解在内的众多领域。但目前，大部分系统都是使用监督学习，机器在人工标注的输入上进行训练。接下来几年的一项挑战是让机器从原始的、无标记数据中学习，比如视频或文本。这也就是无监督学习。如今的人工智能系统不能处理人类或动物通过观察世界而获得的「常识」。人工智能领域内的一些人将无监督学习视为通往机器拥有常识道路上的关键。在此视频中，Yann LeCun 回顾了无监督学习方法，讲解了深度学习的一些基础概念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;PPT 目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;神经网络简述&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;记忆增强网络（微分计算）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工智能的障碍&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;智能系统的架构&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;学习预测世界的 Forward Models&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对抗训练&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;视频预测&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、神经网络简述&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FJKht7KTfhTfloYonE3vORNO8NfNSFOP6a7RbibibP1jHQyUHIUd0xotg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经科学激发了早期在人工智能与机器学习中的研究&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FCxJhjGf1tW5UzDL7mg4gGbAT0ibyuu8wdnLlvc2iaicTIicoDCBib2tbRwQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;监督学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FWoYxJYbvlxDYkicGugF89b2csPQvMMiajYoVQTTAKSGKKYKZnQ8YJ4ZA/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层神经网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F5GC79zMcms714dYsN850aVzWtclEkUvQ8PIYHWKPW8OEiaSr80nQibvw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过反向传播计算梯度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FddqkicNgj0oBzXMt0qwbibLNpCYMWBdIHXaku7vmM25icmu1YPvm9wnLg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积神经网络架构&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FEfibRKMGSUg0p3rkE6XRoSMw43ftp6H8a39ZBasfIXa5FVe4fHQBMnA/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度卷积网络进行物体识别&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FWLA1vm7icr8raVpxyFlib8tS8kMF5icjqRZKpyxspBxHZicviaPCibcvew5A/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习=学习层级表征&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FskJiavIbQuichWesQVekuJfibYudbqlHl7ENocMZWzasPfdaUSicERSicYg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视觉皮层中的层级架构&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FHxrkJGgqeluTT4hDwxcwIgEETN92lHVf5DLiaxLwwsU8WiansboGiaHPA/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;监督卷积网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fnic9Elib1aiaCkUVzCLyMUGF5jOhJSBuEAOtMfVeTR6KBNRicOnvL0mTww/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;汽车中的卷积网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FAByIdY1UlmdfibV4CAC1AXBBQ26fHicEf2brmprBxcVB1XZ8oD0iaZmibQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718597&amp;amp;idx=1&amp;amp;sn=56aa4e5deff99620fe6ed42000903849&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718597&amp;amp;idx=1&amp;amp;sn=56aa4e5deff99620fe6ed42000903849&amp;amp;scene=21#wechat_redirect" style="font-size: 14px; text-decoration: none;"&gt;&lt;span&gt;DeepMask：ConvNet Locates 和识别物体&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、记忆增强网络（微分计算）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F5RkMAybJSnXC94K2crFJcEzPicMGYtGU41wQTsuCQHCF3QBew54nx5Q/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FF8STZ7aQOF7ZGJgQVnCVY3JQgH0ibNGTKjOQ0Uo8jB6DlaoeLnuDdhw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用记忆模块增强神经网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FRcHsY4CvgLw8icQxMAMGPWDfbs0iczXwibSeEiaOjOictMxRwO3m3DCLetw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实体循环神经网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、人工智能的障碍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FHGHvtZ3yMv0tQBD4vy3CyvmfFrnTy9l2N4muRHvpYLpkJwEsnKPVDg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F2MSkbsQ9Yz78DjfcpSgPwFEYc09rfGgGic64MewxMtBX6ZDAqJxvR1Q/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能进展中的障碍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FW8uU19BReWzltawia3pmf06NjUiaKlibCicGVibr0Xmib30qcEv7RpFCbiaVg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器需要预测多少信息？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四：智能系统的架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F7md737vdofAT9XGTJXbqacykYicjFl6thiclibBdJQOF4m4Jg3k8icVB1w/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FnxbdNHEadMKsEyXaXD39xKG3sr2708UzP0zjkRJY8zFogMpicRHCWCQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能系统：学习代理+不便的目标&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F1qDIK5Xqy3pVy7G1EerkevHveyMS5ib7BqwU0PXmKQlbuT1I4icY8zqQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能系统：预测+规划=推理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、学习预测世界的 Forward Models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F7Cj9858VicmL4YKYRia6BiaicGSzVrm6Mw2M2zv6zUNMAqg3jNBDnDribFw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FN5ia9kcV61lLxdVt3a8hr1Cc9OGXOjEQSyC63GCcRZdcErlQicicMOWkA/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fhou9LNhscqN0HibkHaibt923OrWicPrQUOFWicjUULMWA2VXJugxbbwtZQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于能量的无监督学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fpleq7rkpawOibHcbgtXTDY9fWcVTj0DAs1MxcicAib5fyyyalC8aKJbCQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立能量函数的7个策略&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;六：对抗训练&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fvvr2ic88ibEVibShyrUQKlOY28hITiaDlRBpAgJ6krMBASA8fyHax16Vzg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FhmYv8ujOv6AyicUicOJCVjjhibk5TrEz9SW5KJDEAQ25PHMWyVDGmfBcQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F0VpM4Qib7wo7Dxe1biaLgUISjc3HcUv6yGTSMErHGVo52J4FeibHH6GYg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FVxqP76xbJCsEtEvX4xRPvTC6HukXGPapHAI2l11jfkW8DuV7wiakaTw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 ImageNet 128×128 像素图像上基于能量的 GAN 训练&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;七：视频预测&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FlTPr2jugXILbLalTHfJq5HddqVNkferve8dSRJ2dxqxicjJoJZwNO3Q/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FH0XmNTT0HBsnccoo0yZIbycs5WzzKry2pcl9v5FomoMsRflTyloIKw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;进行视频预测的多尺度卷积&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FbN9SEFgyFVJklKVFMJ9cqK28pkbrJxGhtSW2oRsdOtR4KELT00sKicQ/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测式无监督学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视频中在演讲完之后，LeCun 还与 CMU 的学生进行了长时间的互动，感兴趣的同学可以观看视频。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 21 Nov 2016 14:24:11 +0800</pubDate>
    </item>
    <item>
      <title>技术 | 深度解读最流行的优化算法：梯度下降</title>
      <link>http://www.iwgc.cn/link/3592116</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自sebastianruder&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：沈泽江&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;梯度下降法，是当今最流行的优化（optimization）算法，亦是至今最常用的优化神经网络的方法。本文旨在让你对不同的优化梯度下降法的算法有一个直观认识，以帮助你使用这些算法。我们首先会考察梯度下降法的各种变体，然后会简要地总结在训练（神经网络或是机器学习算法）的过程中可能遇到的挑战。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目录：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度下降的各种变体&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;批量梯度下降（Batch gradient descent）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;随机梯度下降（Stochastic gradient descent）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;小批量梯度下降（Mini-batch gradient descent）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;面临的挑战&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度下降的优化算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Momentum法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Nesterov加速梯度法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Adagrad法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Adadelta法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RMSprop法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;适应性动量估计法（Adam）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;几种算法的可视化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该选择哪种优化器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对SGD进行平行或分布式运算&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hogwild!&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Downpour SGD&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;容忍延迟的SGD算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;弹性平均梯度下降法（Elastic Averaging SGD）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;优化SGD的其他手段&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;重排（Shuffling ）和递进学习（Curriculum Learning）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;批量标准化（Batch normalization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;早停（Early Stopping）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;梯度噪声（Gradient noise）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度下降法，是当今最流行的优化（optimization）算法，亦是至今最常用的优化神经网络的方法。与此同时，最新的深度学习程序库都包含了各种优化梯度下降的算法（可以参见如 lasagne、caffe 及 Kera 等程序库的说明文档）。但它们的算法则不被公开，都作为黑箱优化器被使用，这也就是为什么它们的优势和劣势往往难以被实际地解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文旨在让你对不同的优化梯度下降法的算法有一个直观认识，以帮助你使用这些算法。我们首先会考察梯度下降法的各种变体，然后会简要地总结在训练（神经网络或是机器学习算法）的过程中可能遇到的挑战。接着，我们将会讨论一些最常见的优化算法，研究它们的解决这些挑战的动机及推导出更新规律（update rules）的过程。我们还会简要探讨一下，在平行计算或是分布式处理情况下优化梯度下降法的算法和架构。最后，我们会考虑一下其他有助于优化梯度下降法的策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度下降法的核心，是最小化目标函数 J(θ)，其中θ是模型的参数，θ∈Rd。它的方法是，在每次迭代中，对每个变量，按照目标函数在该变量梯度的相反方向，更新对应的参数值。其中，学习率η决定了函数到达（局部）最小值的迭代次数。换句话说，我们在目标函数的超平面上，沿着斜率下降的方向前进，直到我们遇到了超平面构成的「谷底」。如果你不熟悉梯度下降法的话，你可以在这里找到一个很好的关于优化神经网络的介绍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;梯度下降法变体&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文讨论了三种梯度下降法的变体——它们的不同之处在于，一次性使用多少数据来计算目标函数的梯度。对于不同的数据量，我们需要在参数更新准确性和参数更新花费时间两方面做出权衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;批量梯度下降法（Batch Gradient Descent）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Vanilla 梯度下降法（译者注：Vanilla 是早期机器学习算法相关的名词，也是如今一个机器学习 python 程序库的名字，在该处指的是后者，参见：https://github.com/vinhkhuc/VanillaML），也就是大家所熟知的批量梯度下降法，在整个数据集上（求出罚函数 J(θ 并）对每个参数 θ 求目标函数 J(θ) 的偏导数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FY3Shw8qiaw7cnaZjZgibXUSibsWE6yMmUibbVgqa9K7sneHdZ0I5OFEAibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该方法中，每次更新我们都需要在整个数据集上求出所有的偏导数。因此批量梯度下降法的速度会比较慢，甚至对于较大的、内存无法容纳的数据集，该方法都无法被使用。同时，梯度下降法不能以「在线」的形式更新我们的模型，也就是不能再运行中加入新的样本进行运算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;批量梯度下降法的实现代码，如下所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;code class=" language-python" style=" font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; line-height: 1.5; ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range&lt;span&gt;(&lt;/span&gt;nb_epochs&lt;span&gt;):&lt;/span&gt;
 &amp;nbsp;params_grad &lt;span&gt;=&lt;/span&gt; evaluate_gradient&lt;span&gt;(&lt;/span&gt;loss_function&lt;span&gt;,&lt;/span&gt; data&lt;span&gt;,&lt;/span&gt; params&lt;span&gt;)&lt;/span&gt;
 &amp;nbsp;params &lt;span&gt;=&lt;/span&gt; params &lt;span&gt;-&lt;/span&gt; learning_rate &lt;span&gt;*&lt;/span&gt; params_grad&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于给定的迭代次数，我们首先基于输入的罚函数 loss_function 对输入的参数向量 params 计算梯度向量 params_grad。注意，最新的深度学习程序库中，提供了自动求导的功能，能够高效、快速地求给定函数对于特定参数的导数。如果你希望自己写代码求出梯度值，那么「梯度检查」会是一个不错的注意。（你可以参考这里，了解关于如何检查梯度的相关建议。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，我们对参数减去梯度值乘学习率的值，也就是在反梯度方向，更新我们参数。当目标函数 J(θ) 是一凸函数时，则批量梯度下降法必然会在全局最小值处收敛；否则，目标函数则可能会局部极小值处收敛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;随机梯度下降法（Stochastic Gradient Descent）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比批量梯度下降法，随机梯度下降法的每次更新，是对数据集中的一个样本（x，y）求出罚函数，然后对其求相应的偏导数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fjv3r6W8AicUHibnaDPJwCFl5VzaIoFbqYtmCkn4rSLaDawrtHxPEsG0A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为批量梯度下降法在每次更新前，会对相似的样本求算梯度值，因而它在较大的数据集上的计算会有些冗余（redundant）。而随机梯度下降法通过每次更新仅对一个样本求梯度，去除了这种冗余的情况。因而，它的运行速度被大大加快，同时也能够「在线」学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随机梯度下降法更新值的方差很大，在频繁的更新之下，它的目标函数有着如下图所示的剧烈波动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FQic5YJX97yics5iaK2DuMeyiat9HjUfcfWKnrHqvqCpBfbdCOGBptrDvAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;—image—SGD 函数波动，来源：Wikipedia&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比批量梯度下降法的收敛会使目标函数落入一个局部极小值，SGD 收敛过程中的波动，会帮助目标函数跳入另一个可能的更小的极小值。另一方面，这最终会让收敛到特定最小值的过程复杂化，因为该方法可能持续的波动而不停止。但是，当我们慢慢降低学习率的时候，SGD 表现出了与批量梯度下降法相似的收敛过程，也就是说，对非凸函数和凸函数，必然会分别收敛到它们的极小值和最小值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比批量梯度下降法的代码，在如下的代码中，我们仅仅加入了一个循环，用以遍历所有的训练样本并求出相应的梯度值。注意，如这里所说，在每次迭代中，我们会打乱训练数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;code class=" language-python" style=" font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; line-height: 1.5; ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range&lt;span&gt;(&lt;/span&gt;nb_epochs&lt;span&gt;):&lt;/span&gt;
 &amp;nbsp;np&lt;span&gt;.&lt;/span&gt;random&lt;span&gt;.&lt;/span&gt;shuffle&lt;span&gt;(&lt;/span&gt;data&lt;span&gt;)&lt;/span&gt;
 &amp;nbsp;&lt;span&gt;for&lt;/span&gt; example &lt;span&gt;in&lt;/span&gt; data&lt;span&gt;:&lt;/span&gt;
 &amp;nbsp; &amp;nbsp;params_grad &lt;span&gt;=&lt;/span&gt; evaluate_gradient&lt;span&gt;(&lt;/span&gt;loss_function&lt;span&gt;,&lt;/span&gt; example&lt;span&gt;,&lt;/span&gt; params&lt;span&gt;)&lt;/span&gt;
 &amp;nbsp; &amp;nbsp;params &lt;span&gt;=&lt;/span&gt; params &lt;span&gt;-&lt;/span&gt; learning_rate &lt;span&gt;*&lt;/span&gt; params_grad&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;小批量梯度下降法（Mini-Batch Gradient Descent）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;小批量梯度下降法集合了上述两种方法的优势，在每次更新中，对 n 个样本构成的一批数据，计算罚函数 J(θ)，并对相应的参数求导：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fohh6D6A1dibYskXr8ibtKuUo09XBPMXn9HuXY5ULS5bOf2EysVpfmG1g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种方法，(a) 降低了更新参数的方差（variance），使得收敛过程更为稳定；(b) 能够利用最新的深度学习程序库中高度优化的矩阵运算器，能够高效地求出每小批数据的梯度。通常一小批数据含有的样本数量在 50 至 256 之间，但对于不同的用途也会有所变化。小批量梯度下降法，通常是我们训练神经网络的首选算法。同时，有时候我们也会使用随机梯度下降法，来称呼小批量梯度下降法（译者注：在下文中，我们就用 SGD 代替随机梯度下降法）。注意：在下文对于随机梯度法优化的介绍中，为方便起见，我们会省略式子中的参数 x(i:i+n),y(i:i+n)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下的代码所示，我们不再对每个样本进行循环，而是对每批带有 50 个样本的小批数据进行循环：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;code class=" language-python" style=" font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; line-height: 1.5; ; ; ; ; ; ; ; ; ; ; ; ; "&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range&lt;span&gt;(&lt;/span&gt;nb_epochs&lt;span&gt;):&lt;/span&gt;
 &amp;nbsp;np&lt;span&gt;.&lt;/span&gt;random&lt;span&gt;.&lt;/span&gt;shuffle&lt;span&gt;(&lt;/span&gt;data&lt;span&gt;)&lt;/span&gt;
 &amp;nbsp;&lt;span&gt;for&lt;/span&gt; batch &lt;span&gt;in&lt;/span&gt; get_batches&lt;span&gt;(&lt;/span&gt;data&lt;span&gt;,&lt;/span&gt; batch_size&lt;span&gt;=&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;&lt;span&gt;):&lt;/span&gt;
 &amp;nbsp; &amp;nbsp;params_grad &lt;span&gt;=&lt;/span&gt; evaluate_gradient&lt;span&gt;(&lt;/span&gt;loss_function&lt;span&gt;,&lt;/span&gt; batch&lt;span&gt;,&lt;/span&gt; params&lt;span&gt;)&lt;/span&gt;
 &amp;nbsp; &amp;nbsp;params &lt;span&gt;=&lt;/span&gt; params &lt;span&gt;-&lt;/span&gt; learning_rate &lt;span&gt;*&lt;/span&gt; params_grad&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;面临的挑战&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于 Vanilla 小批量梯度下降法并不能保证良好地收敛，这给我们留下了如下待解决的挑战：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;选择适当的学习率是一个难题。太小的学习率会导致较慢的收敛速度，而太大的学习率则会阻碍收敛，并会引起罚函数在最小值处震荡，甚至有可能导致结果发散；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们可以设置一个关于学习率地列表，通过如退火的方法，在学习过程中调整学习率——按照一个预先定义的列表、或是当每次迭代中目标函数的变化小于一定阈值时来降低学习率。但这些列表或阈值，需要根据数据集地特性，被提前定义。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;此外，我们对所有的参数都采用了相同的学习率。但如果我们的数据比较稀疏，同时特征有着不同的出现频率，那么我们不希望以相同的学习率来更新这些变量，我们希望对较少出现的特征有更大的学习率。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在对神经网络最优化非凸的罚函数时，另一个通常面临的挑战，是如何避免目标函数被困在无数的局部最小值中，以导致的未完全优化的情况。Dauphin 及其他人 [19] 认为，这个困难并不来自于局部最小值，而是来自于「鞍点」，也就是在一个方向上斜率是正的、在一个方向上斜率是负的点。这些鞍点通常由一些函数值相同的面环绕，它们在各个方向的梯度值都为 0，所以 SGD 很难从这些鞍点中脱开。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;梯度下降的优化算法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在如下的讨论中，我们将会列举一些应对上述问题的算法，它们被广泛应用于深度学习社区。同时，我们不会讨论那些不能应用于高维数据集的方法，例如牛顿法等针对二阶问题的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;动量法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SGD 很难在陡谷——一种在一个方向的弯曲程度远大于其他方向的表面弯曲情况——中找到正确更新方向。而这种陡谷，经常在局部极值中出现。在这种情况下，如图 2 所示，SGD 在陡谷的周围震荡，向局部极值处缓慢地前进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3Fgj4nFAhmBe5GwJYxdlhwQEHdd1k8bPpMxleh5wkAZo9leVnpntIBRQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动量法 [2]，如图 3 所示，则帮助 SGD 在相关方向加速前进，并减少它的震荡。他通过修改公式中，在原有项前增加一个折损系数γ，来实现这样的功能：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FIfZcxibLVG01xq25K1h04A8Y2KLMCXAgRVPYnr033mve5R2KbiaAkp4Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：在其他的一些算法实现中，公式中的符号也许有所不同。动量项 γ 往往被设置为 0.9 或为其他差不多的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从本质上说，动量法，就仿佛我们从高坡上推下一个球，小球在向下滚动的过程中积累了动量，在途中他变得越来越快（直到它达到了峰值速度，如果有空气阻力的话，γ&amp;lt;1）。在我们的算法中，相同的事情发生在我们的参数更新上：动量项在梯度指向方向相同的方向逐渐增大，对梯度指向改变的方向逐渐减小。由此，我们得到了更快的收敛速度以及减弱的震荡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Nesterov 加速梯度法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但当一个小球从山谷上滚下的时候，盲目的沿着斜率方向前行，其效果并不令人满意。我们需要有一个更「聪明」的小球，它能够知道它再往哪里前行，并在知道斜率再度上升的时候减速。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nesterov 加速梯度法（NAG）是一种能给予梯度项上述「预测」功能的方法。我们知道，我们使用动量项γvt-1 来「移动」参数项θ。通过计算θ-γvt-1，我们能够得到一个下次参数位置的近似值——也就是能告诉我们参数大致会变为多少。那么，通过基于未来参数的近似值而非当前的参数值计算相得应罚函数 J(θ-γvt-1) 并求偏导数，我们能让优化器高效地「前进」并收敛：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F3ExjXIYfgHI39lQ1eqoEHKia0jL9aUZs30zekCslkD3P3qxXLd9vodw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该情况下，我们依然设定动量系数γ 在 0.9 左右。如下图 4 所示，动量法首先计算当前的梯度值（小蓝色向量），然后在更新的积累向量（大蓝色向量）方向前进一大步。但 NAG 法则首先（试探性地）在之前积累的梯度方向（棕色向量）前进一大步，再根据当前地情况修正，以得到最终的前进方向（绿色向量）。这种基于预测的更新方法，使我们避免过快地前进，并提高了算法地响应能力（responsiveness），大大改进了 RNN 在一些任务上的表现 [8]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FIn5Py92O1T7QGE52bYibURplrMOIibCjfGibfXzdGXqlI9N0xXa2XEXIw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;--image4: Nesterov Update 法，来源：G. Hinton's lecture 6c--&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考这里，以查看 Ilya Sutskever 在它博士论文中，对 NAG 机理的更为详尽的解释 [9]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们现在能根据我们罚函数的梯度值来调整我们的更新，并能相应地加速 SGD，我们也希望能够对罚函数中的每个参数调整我们的更新值，基于它们的重要性以进行或大或小的更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adagrad 法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adagrad[3] 是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。Dean 等人 [14] 发现，Adagrad 法大大提升了 SGD 的鲁棒性，并在谷歌使用它训练大规模的神经网络，其诸多功能包括识别 Youtube 视频中的猫。此外，Pennington 等人 [5] 使用它训练 GloVe 单词向量映射（Word Embedding），在其中不频繁出现的词语需要比频繁出现的更大的更新值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这之前，我们对于所有的参数使用相同的学习率进行更新。但 Adagrad 则不然，对不同的训练迭代次数 t，adagrad 对每个参数都有一个不同的学习率。我们首先考察 adagrad 每个参数的的更新过程，然后我们再使之向量化。为简洁起见，我们记在迭代次数 t 下，对参数θi 求目标函数梯度的结果为 gt,i：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FzmdChz7rmz0aQKiaj8c5MUkcW4iaNMndYxk874X4J83XfOCVgO8N9MCw/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么普通 SGD 的更新规则为：&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FPr2dLchbeibCiaSB8TdRHocx0R2dibtg2HBKDZqZ3PuibstjoOH2UtAaoQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而 adagrad 将学习率η进行了修正，对迭代次数 t，基于每个参数之前计算的梯度值，将每个参数的学习率η按如下方式修正：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FOTnL5YKb855xeue5ghyRSkzwEuQz6RCibEDia1CP6jj7tEHujovgicxrQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 是一个对角阵，其中对角线上的元素是从一开始到 时刻目标函数对于参数 梯度的平方和。是一个平滑项，以避免分母为 0 的情况，它的数量级通常在。有趣的是，如果不开方的话，这个算法的表现会变得很糟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 在其对角线上，含有过去目标函数对于参数 梯度的平方和，我们可以利用一个元素对元素的向量乘法，将我们的表达式向量化：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F8ZjrDWiciaOdx4NT3D92EgSgjvKrBOOZnu0VS4SQV4dQV8jvUmjcJXeA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adagrad 主要优势之一，是它不需要对每个学习率手工地调节。而大多数算法，只是简单地使用一个相同地默认值如 0.1，来避免这样地情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adagrad 地主要劣势，是他在分母上的项中积累了平方梯度和。因为每次加入的项总是一个正值，所以累积的和将会随着训练过程而增大。因而，这会导致学习率不断缩小，并最终变为一个无限小值——此时，这个算法已经不能从数据中学到额外的信息。而下面的算法，则旨在解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adadelta 法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adadelta 法 [6] 是 Adagrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，Adadelta 法仅计算在一个大小为 的时间区间内梯度值的累积和。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但该方法并不会存储之前 个梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：它被定义为关于过去梯度值的衰减均值（decade average），当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FLRfdrkPHMDkQXT5gRkx8KkmC31ibc0U8xUpm0B5rRyEU7mmxxP6C9ug/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与动量项的设定类似，我们设定 为以 0.9 左右的值。为明确起见，我们将我们的 SGD 更新规则写为关于参数更新向量 的形式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FFiataS8tuVIMBlfbsTt5CnVzz0vaVd3EvZWxvUWPMyuAYJgo2F40b1w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由此，我们刚刚在 Adagrad 法中推导的的参数更新规则的向量表示，变为如下形式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FygCF0qc0WGLC9wXXGibCTzpLop78BNU685QTnrBCfSJErsE5tEDAYwA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在将其中的对角矩阵 用上述定义的基于过去梯度平方和的衰减均值 替换：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FB0JA2TsibQrxsTaPnMqSnmvzNLorkgXWkYiaZgVNzIezesicLfGcYlrlA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为分母表达式的形式与梯度值的方均根（root mean squared,RMS）形式类似，因而我们使用相应的简写来替换：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FRoYhWOPlXic4FMxFOc9LEo9JqN3eEbft0yf7eKia5tRkW4EkkQyoEZnw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者还注意到，在该更新中（在 SGD、动量法或者 Adagrad 也类似）的单位并不一致，也就是说，更新值的量纲与参数值的假设量纲并不一致。为改进这个问题，他们定义了另外一种指数衰减的衰减均值，他是基于参数更新的平方而非梯度的平方来定义的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FGrkh5fZR95UvJxDWVicwe9HibhJwthmfkolficnyE3AqfOKFvFK3KEDbA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，对该问题的方均根为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FpryZzWAvsUZsVfia3dLSaJwXnBD9enIIFHeGXyiaSt7ZkzkqEKxy7Ixg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 值未知，所以我们使用 时刻的方均根来近似。将前述规则中的学习率 替换为，我们最终得到了 Adadelta 法的更新规则：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F63EQXZgttGOYDeXyiaXpUbItv89ib86ibNia93UFiaiasSrFO9hqXX6UlrTg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;借助 Adadelta 法，我们甚至不需要预设一个默认学习率，因为它已经从我们的更新规则中被删除了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RMSprop 法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RMSprop 法和 Adadelta 法几乎同时被发展出来。他们 解决 Adagrad 激进的学习率缩减问题。实际上，RMSprop 和我们推导出的 Adadelta 法第一个更规则相同：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FzwydLk7Hpiab7Dhq8hibzb7QwBP5URysD18QO2gPARWBWAaRkIqqgBxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RMSprop 也将学习率除以了一个指数衰减的衰减均值。Hinton 建议设定 为 0.9，对 而言，0.001 是一个较好的默认值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adam&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;适应性动量估计法（Adam）[15] 是另一种能对不同参数计算适应性学习率的方法。除了存储类似 Adadelta 法或 RMSprop 中指数衰减的过去梯度平方均值 外，Adam 法也存储像动量法中的指数衰减的过去梯度值均值 ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FKm4QoT3f16NXK4ciceKBlY0ib3SYc92ricC0rxLU1IP6lVP8MpmpKSfMw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;和 分别是梯度的一阶矩（均值）和二阶矩（表示不确定度的方差），这也就是该方法名字的来源。因为当 和 一开始被初始化为 0 向量时，Adam 的作者观察到，该方法会有趋向 0 的偏差，尤其是在最初的几步或是在衰减率很小（即 和 接近 1）的情况下。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们使用偏差纠正系数，来修正一阶矩和二阶矩的偏差：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FAB8MeBXp3WvJwj89CfJMXbRpFURuxtojku8csx98TDdKiaWBunaAyww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们使用这些来更新参数，更新规则很我们在 Adadelta 和 RMSprop 法中看到的一样，服从 Adam 的更新规则：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FCaRO3Yt8zUd2TUccKoL10yh3WhnIpOXc0vO2qZk9wMNibeeR5Q3SVibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者认为参数的默认值应设为：&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FvDpI83nP2F92ibr5KticHRKczjibQLwxBn3hXMQ5Y6w2CIZs8zdEaI0ng/0?wx_fmt=png"/&gt;&lt;br/&gt;。他们的经验表明，Adam 在实践中表现很好，和其他适应性学习算法相比也比较不错。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;算法可视化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如下的两个动画（图像版权：Alec Radford）给了我们关于特定优化算法在优化过程中行为的直观感受。你可以参见这里，以获取 Karpathy 对相同图像的一些描述，及另关于一些相关算法的细致讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在图 5 中，我们可以看到，在罚函数的等高线图中，优化器的位置随时间的变化情况。注意到，Adagrad、 Adadelta 及 RMSprop 法几乎立刻就找到了正确前进方向并以相似的速度很快收敛。而动量法和 NAG 法，则找错了方向，如图所示，让小球沿着梯度下降的方向前进。但 NAG 法能够很快改正它的方向向最小指出前进，因为他能够往前看并对前面的情况做出响应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图 6 展现了各算法在鞍点附近的表现。如上面所说，这对对于 SGD 法、动量法及 NAG 法制造了一个难题。他们很难打破」对称性「带来的壁垒，尽管最后两者设法逃脱了鞍点。而 Adagrad 法、RMSprop 法及 Adadelta 法都能快速的沿着负斜率的方向前进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FL7lSCHlvWIeCQyATDgl4Jaz6zJYAY2QqSSq6wlpicTw1zd5BUVc1Crg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如我们所见，适应性学习率方法，也就是 Adagrad 法、Adadelta 法 、RMSprop 法及 Adam 法最适合处理上述情况，并有最好的收敛效果&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;如何选择优化器？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么，我们该如何选择优化器呢？如果你的输入数据较为稀疏（sparse），那么使用适应性学习率类型的算法会有助于你得到好的结果。此外，使用该方法的另一好处是，你在不调参、直接使用默认值的情况下，就能得到最好的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总的来说，RMSprop 法是一种基于 Adagrad 法的拓展，他从根本上解决学习率骤缩的问题。Adadelta 法于 RMSprop 法大致相同，除了前者使用了。而 Adam 法，则基于 RMSprop 法添加了偏差修正项和动量项。在我们地讨论范围中，RMSprop、Adadelta 及 Adam 法都是非常相似地算法，在相似地情况下都能做的很好。Kingma 及其他人 [15] 展示了他们的偏差修正项帮助 Adam 法，在最优化过程快要结束、梯度变得越发稀疏的时候，表现略微优于 RMSprop 法。总的来说，Adam 也许是总体来说最好的选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，很多最新的论文，都直接使用了（不带动量项的）Vanilla SGD 法，配合一个简单的学习率（退火）列表。如论文所示，这些 SGD 最终都能帮助他们找到一个最小值，但会花费远多于上述方法的时间。并且这些方法非常依赖于鲁棒的初始化值及退火列表。因此，如果你非常在你的模型能快速收敛，或是你需要训练一个深度或复杂模型，你可能需要选择上述的适应性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;对 SGD 进行平行计算或分布式计算&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现如今，大规模数据集随处可见、小型计算机集群也易于获得。因而，使用分布式方法进一步加速 SGD 是一个惯常的选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SGD 它本事是序列化的：通过一步一步的迭代，我们最终求到了最小值。运行它能够得到不错的收敛结果，但是特别是对于大规模的数据集，它的运行速度很慢。相比而言，异步 SGD 的运行速度相对较快，但在不同的工作机之间的关于非完全优化的沟通可能会导致较差的收敛结果。此外，我们能够对 SGD 进行平行运算而不需要一个计算机集群。下文讨论了相关的算法或架构，它们或关于平行计算或者对其进行了分布式优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Hogwild!&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Niu 等人提出了一种叫做 Hogwild! 的更新规则，它允许在平行 GPU 上进行 SGD 更新。处理器。这仅能在输入数据集是稀疏的时起效，在每次更新过程中仅会修正一部分的参数值。他们展示了，在这种情况下，这个更新规则达到了最优化的收敛速度，因为处理器不太会覆盖有用的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Downpour SGD&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Downpour SGD 是一个异步的 SGD 法变体，它被 Dean 等人 [4] 用在了谷歌的 DistBelief 架构中（它是 TensorFlow 的前身）。他对训练集地子集同步地运行模型的多个副本。这些模型将它们的更新值发送到参数服务器，服务器被分为了许多台主机。每一台主机都负责存储和上载模型的一部分参数。但是，副本之间却没有相互的通信——例如，共享权重值或者更新值——其参数面临着发散的风险，会阻止收敛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;容忍延迟的 SGD 算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;McMahan 和 Streeter [12] 改良了 AdaGrad 法使之能够用于平行运算的场景。通过实现延迟容忍的算法，它不仅能能够适应于过去的梯度，还能够适应于更新的延迟。在实践中，它的表现很好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow[13] 是谷歌最近开源的一个实现和部署大规模机器学习模型的架构。它基于他们之前对于使用 DistBelief 的经验，并已在内部被部署在一系列的移动设备及大规模的分布式系统上进行计算。为了分布式执行，一个计算图被分为了许多子图给不同的设备，设备之间的通信使用了发送和接受节点对。2016 年 4 月 13 日更新：一个分布式 TensorFlow 的版本已经被发布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;弹性平均梯度下降法（Elastic Averaging SGD）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;张等人 [14] 提出了弹性平均梯度下降法（EASGD），他使不同工作机之间不同的 SGD 以一个「弹性力」连接，也就是一个储存于参数服务器的中心变量。这允许局部变量比中心变量更大地波动，理论上允许了对参数空间更多的探索。他们的经验表明，提高的探索能力有助于在寻找新的局部极值中提升（优化器的）表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;优化 SGD 的其他手段&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们将讨论一些其他手段，他们可以与前述的方法搭配使用，并能进一步提升 SGD 的效果。你可以参考 [22]，以了解一些其他常用策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;重排法（Shuffling）和递进学习（Curriculum Learning）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体而言，我们希望避免训练样本以某种特定顺序传入到我们的学习模型中，因为这会向我们的算法引入偏差。因此，在每次迭代后，对训练数据集中的样本进行重排（shuffling），会是一个不错的注意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一方面，在某些情况下，我们会需要解决难度逐步提升的问题。那么，按照一定的顺序遍历训练样本，会有助于改进学习效果及加快收敛速度。这种构建特定遍历顺序的方法，叫做递进学习（Curriculum Learning）[16]。*这个词目前没有标准翻译，我根据表意和意义翻译成这个。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Zaremba 和 Sutskever [17] 仅使用了递进学习法训练 LSTMs 来学习简单的项目，但结果表明，递进学习法使用的混合策略的表现好于朴素策略——后者不断地重排数据，反而增加了学习过程的难度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;批量标准化（Batch Normalization）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们通常设置我们参数初值的均值和方差分别为 0 和单位值，以帮助模型进行学习。随着学习过程的进行，每个参数被不同程度地更新，相应地，参数的正则化特征也随之失去了。因此，随着训练网络的越来越深，训练的速度会越来越慢，变化值也会被放大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;批量标准化 [18] 对每小批数据都重新进行标准化，并也会在操作中逆传播（back-propgate）变化量。在模型中加入批量标准化后，我们能使用更高的学习率且不要那么在意初始化参数。此外，批量正则化还可以看作是一种正则化手段，能够减少（甚至去除）留出法的使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;早停（Early Stopping）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;诚如 Geoff Hinton 所言：「Early stopping (is) beautiful free lunch（早停是美妙的免费午餐，又简单效果又好）」（NIPS 2015 Tutorial Sildes, Slide 63）。在训练过程中，你应该时刻关注模型在验证集上的误差情况，并且在改误差没有明显改进的时候停止训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度噪声（Gradient Noise）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Neelakentan 等人 [21] 在每次梯度的更新中，向其中加入一个服从合高斯分布 N(0,σ^2) 的噪声值：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FYT6H8dH0J0rkCS6MeibYbIicqPckYoOWD4Enx7APg1EE8JINysfvbH8A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;并按照如下的方式修正方差：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3FAr17OWWQ17aibiacxFxem2YGVuUgyunF4xognv0KvVvmwXnpWDz7MOaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们指出，这种方式能够提升神经网络在不良初始化前提下的鲁棒性，并能帮助训练特别是深层、复杂的神经网络。他们发现，加入噪声项之后，模型更有可能发现并跳出在深度网络中频繁出现的局部最小值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本文中，我们首先分析了梯度下降法的三个变体，在其中小批量梯度下降法最受欢迎。接着，我们研究了常用的优化 SGD 的算法，包括：动量法、Nesterov accelerated gradient 法、Adagrad 法、Adadelta 法、RMSprop 法、Adam 法及其他优化异步 SGD 的算法。最终，我们讨论了另外一些改进 SGD 的策略，包括样本重排法（shuffling）、递进学习（curriculum learning）、批量标准化（Batch Normali·zation）及早停（early stopping）等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我希望本文能增进读者关于这些优化算法的认识，能对这些算法的行为与动机有一个了解。也许我遗漏了一些常用的优化 SGD 的算法，或是你有一些自己使用 SGD 训练的技巧。如果有的话，请在下方留言区留言让我知道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原文连接查看参考文献：http://sebastianruder.com/optimizing-gradient-descent/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 21 Nov 2016 14:24:11 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 为什么自然界是我们理解人工智能的最优导师？</title>
      <link>http://www.iwgc.cn/link/3592118</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自TechCrunch&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：朱思颖&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对生物体而言，进化是一个多代累积的基因改变过程，在每一代的进化过程中会有基因的剔除和基因的增加。在每一次的基因改变后，只有那些拥有适宜于生存环境基因的变异生物能够存活，而那些拥有不适宜生存环境基因的变异生物则无情的被环境淘汰。这个过程就是一次自然选择的过程。在自然选择中，生物的适应能力固然重要，但能恰到好处的拥有适宜于当前环境的特征才是关键，就像在洪水爆发的时候，能够用鳃呼吸的鱼才可以生存。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比而言，工程设计则是一个严谨规划的过程，尽力确保过程中每一步达到预计目标。然而，随着人工智能的出现，机器学习算法的迭代具有类似生物进化的功效，使得生物进化和工程设计过程的融合成为可能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体细看自然进化的过程和机器学习的过程，我们可以把机器学习所需的数据（data）及其规格化处理类比为生物进化过程中的「环境」，把机器学习过程类比为「自然选择」。机器学习在训练的时候分为监督式学习、非监督式学习、增强学习、聚类、决策树以及深度学习的其他方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在自然进化过程中，虽然不同的生物在遇到相同的生存难题时会进化出各自不同的特征，但最终它们将进化出类似的特征来解决其所遇到的生存难题。鲨鱼和海豚从不同的原始生物种类进化而来，却具备相似的伤口愈合机制。在人工智能领域，我们同样能看到与此类似的现象。例如：K-均值聚类算法常被用来处理图像分割问题，通过对原始无标签的输入数据（通常是图像）进行聚类直至相似特征的数据被合理的聚分至各族群内。如果你把这个问题交给 10 个机器学习工程师，并且是处理同样输入数据集，很可能他们 每个人使用的算法都不相同，但并不妨碍最终的聚类结果。从这个维度来比较自然选择和机器学习过程，两者何其相似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8gsdO8GAUKvdwVZiaUXzb3F5CvOSC45mPWAXcgjIrW4S94W8CFtg58EEvqGDP3ESPDxJOweCPaLdw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么，这与商业有何相关呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为机器学习技术已经有了商业化的应用，目前机器学习在商业化应用上遇到的难题是如何安全稳妥并富有效率的运用机器学习技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回顾科技的发展历史，大自然给了工程师们很多启发。这里，我将给出一些在商业上运用进化理论来理解人工智能潜在影响的范例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;趋异进化：人工智能下的趋异进化，是指在这个过程中很难将同一个数据集来处理数据集类型相似的问题。就如：你用 ImageNet 数据集来处理一个目标识别的问题，最后的识别结果非常好，但这并不能够保证你在处理视频识别和面部识别时依旧可以有非常好的识别结果。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;趋同进化：人工智能的趋同进化是指一些看似不同类型的数据集处理过程，其实是同一类问题。例如：Google 借助搜索关键词来优化检索时的拼写检查功能。Google 通过跟踪用户的检索词，当你检索词的拼写和大部分人有差异时，将会出现检索词推荐，这个优化过程很人性化。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;捕食者和被捕食者或者寄生和宿主共同进化：在人工智能里，如果两个人工智能算法一起迭代，会出现很多意想不到的结果。网络安全公司（如 Cylance 和 Bromium）正在开发如何运用机器学习算法来实现不间断的系统训练，从而可以第一时间识别新的网络安全隐患。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，只有少量的 AI 公司在帮助我们更高效的工作（X.ai 可以帮助我们规划繁忙的工作生活，Diffbot 能帮助我们更智能的管理网站等等），但这些应用还只是处于起步阶段，能够成熟到用户可以方便使用的程度，仍需极大的提升。或者说这也是它们的「进化」过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI 领域还有待开垦，而生物界自然选择的过程为我们提供了一个很好的框架来理解机器学习的进化发展，并为之到来做好准备。与此同时，公司的领导层需要着重考虑如何借助 AI 来提升公司业务，并且招募相关的人才来研发出具有创新性的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 21 Nov 2016 14:24:11 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 为何最强人工智能比不上婴儿大脑？</title>
      <link>http://www.iwgc.cn/link/3579156</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自IBTimes&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：王宇欣、候韵楚&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器可以理解语音、识别面部和安全驾驶汽车。这让人们十分讶异于近期的技术方面的进步。但是，如果人工智能领域想要实现革命性的跨越，从而建造出类人式的机器，它首先将要掌握婴儿的学习方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「在相对最近的人工智能中，人们从想直接设计一个可以完成成人做的事情的系统转变成一种认识——即如果想要有一个灵活和强大的系统来完成成人做的事情，这个系统需要能够学习婴儿和孩子做事情的方式。」加州大学伯克利分校的发展心理学家 Alison Gopnik 说，「如果你将现在计算机可以完成的事情与 10 年前可以完成的事情相比较，它们已经取得了很大的进步，但是如果你将这些事情与一个 4 岁儿童可以做的事相比较，仍然有相当大的差距。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;婴儿和孩子使用一种和科学家用来构建科学理论的相同的方法来构建关于他们的周围的世界的理论。他们以一种系统的和实验性的努力来探索和测试他们周围的环境以及环境中的人，这对于学习至关重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Gopnik 最近和一组研究人员一起研究揭示了 15 个月大的孩子相比年龄更大的孩子是如何使用统计数据来更好地学习因果关系的。婴幼儿也许是更好的学习者，因为他们的大脑更加灵活或者「可塑性」更强 ；他们较少地被背景知识所影响，这也让他们有着更加开放的头脑。大脑并非是不变的，而是随着每一次学习的经验而改变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过将发展心理学家和计算学家的专业知识相结合，人们可以揭示出世界上最好的学习型大脑是如何工作的，并且将其计算能力转化到机器的身上。最近，人工智能需要大量的数据来提取模式和结论，但那些对周围世界有相对较少数据的婴儿使用的是一种被称为贝叶斯学习（Bayesian learning）的统计评估方法（参阅机器之心文章《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401831226&amp;amp;idx=1&amp;amp;sn=daa0f6faa0e13a9e2c857d24d7784318&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401831226&amp;amp;idx=1&amp;amp;sn=daa0f6faa0e13a9e2c857d24d7784318&amp;amp;scene=21#wechat_redirect"&gt;深度 | 大脑认知机制是贝叶斯式的吗？&lt;/a&gt;》）。也就是说，这种理解并非是基于一个结果的已知频率（婴儿所没有的信息），而是基于当前的知识推断出的事情发生的可能性，其随着新接收到的信息而连续调整。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「令人震惊的是，婴儿可以只看到一次或听到一个新单词的时候，他们就已经对这个新词的可能意思和可能的使用方法等有了一个很好的认识了；」Gopnik 说。「所以这些贝叶斯方法很好地解释了在没有充足数据的情况下，这些孩子为什么如此擅长于学习。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;婴儿们使用概率模型通过组合概率和可能性（probabilities and possibilities）来得出结论，从而创造出各种假设。随着大脑的成熟，它变得更加专业化以便执行复杂的功能，因此也变得不那么灵活，越来越难以随着时间而改变。年长的学习者发展出了有偏见的观点，因为他们更多地了解世界并且加强某些神经连接，这阻碍了他们基于很少的信息来形成具有创新性的假设和抽象理论的能力。这种能力使得 5 岁以下的婴儿和儿童茁壮成长。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这种权衡关系就是，你知道的越多，你就越难以考虑新的可能性，」Gopnik 说。「你知道的越多，你就越依赖于你知道的东西，而对新的东西则不能保持一个开放的态度。从进化的角度来看，婴儿的整体情况就是他们不知道那么多，所以他们可以更好地学习新的东西。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在婴儿刚出生的几年，每一秒都有 700 个新神经连接生成，这是让一个灵活的大脑处理快速积累的来自环境和社交的信息所必需的部分。比起在成年时期重新组合大脑回路，生命早期的可塑性使得从零建立大脑的架构更加容易。贝叶斯学习已经被证明是儿童发展中的一个强大工具，计算机科学家正在使用该模型设计智能学习机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;麻省理工学院大脑和认知科学系的教授、计算认识科学家 Joshua Tenenbaum 说：「贝叶斯算法正在试图捕捉婴儿的学习模式，」他正在与 Gopnik 合作进一步研究其计算机和心理学的混合领域。「当这些孩子进入了真实的世界时，就已经有准备好的基本的构建模块来让他们理解一些最复杂的概念。然后，他们有学习机制——即以这些最初的构建模块来尝试从稀疏数据推理，并创造因果理论。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人类的大脑，不管处在哪一个发展阶段，都是被设计通过一系列的感觉系统，包括视觉、听觉、嗅觉、味觉、触觉、空间取向和平衡从而进入物理世界。当一个人只有有限的数据时，大脑就会填补空白，这是一种被称为「退化（degeneracy）」的神经结构现象。尽管婴儿的大脑缺乏一个或多个感知，但是他们还是尤其擅长处理信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tenenbaum 说：「为了理解世界，孩子们会像科学家一样学习，这包括形成理论、进行试验、玩耍并且看看到他们可有所发现的东西，积极思考什么是正确的方法来测试他们的理论或者应对一些他们没有想到的东西，并试图找出什么是错，什么是对。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;采取孩子的措施&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tenenbaum 和来自纽约大学和多伦多大学的研究人员团队合作设计了一种能够以更有效和更复杂的方式捕获新知识的人工智能软件。在 2015 年 12 月，他们的研究论文《Human-level concept learning through probabilistic program induction》指出用于创建计算机的机器学习算法接近我们所处理信息的方式；该论文已发表在 Science 杂志上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;新的人工智能程序可以在看到一个样本之后就像人类一样准确地识别手写字符。使用贝叶斯程式学习框架，软件能够为每个至少看到一次的手写字符生成一个独特的指令。但是，当机器面临一个不熟悉的特性的时候，这种算法的独特功能就发挥了作用。它从数据搜索转换到寻找匹配，使用概率程序并通过组合已经见过的字符的部分和子部分来创建一个新的字符以此检测其假设——即当婴儿面对他们从未见过的角色和对象时，他们如何从有限的数据中学习到丰富的概念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，软件仍然无法通过形成原始假设自主学习方式模仿孩子学习的方式。当研究人员能够设计具有原始假设和真实的目标的软件时（例如产生识别字符的愿望而非遵循研究者的指令），人工智能系统的潜力将会有里程碑式的转变。没有自我驱动的目标，人工智能系统就限制了他们自主运作的潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tenenbaum 说：「使用越来越多的数据进行的持续性学习是任何人工智能系统都想要做到的，但自主学习却是棘手的，因为总会有人来操控整件事情，数据的数量与类型也由他们给出。婴儿是自主选择的，但是要让人工智能系统能够更自主地构建自己学习过程仍旧是一个众所周知的挑战。目前的人工智能系统并没有建立任何目标，应此它们也无法为自己的学习负责。当一个机器人按指示拿起一个盒子时，看着它们做着和人类一样的事情是非常令人欣喜的，然而它们并不会拥有像孩子那样复杂的思维水平。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tenenbaum 和他的同事采用了在神经元的虚拟网络上建模的深度学习算法。它建造了一个非常初步模仿人脑的工作方式。当机器处理一个对象时，它搜索其巨大的数据库来获取与机器匹配的像素以进行识别。而人类依赖于更高形式的认知功能来解释对象的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们正在试图编写像大脑的软件一样的计算机程序，这通常被称之为思维。思维是程序且运行于大脑这个硬件上，我们就是试图在对准软件层面。神经网络在人工智能中就像计算机程序的软件层面一样。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2013 年，美国国家科学基金会拨款 2500 万美元资助了麻省理工学院一项为期五年的项目，用于建立脑、思维和机器中心。为了解大脑如何执行复杂计算，不同领域的科学家和工程师共同合作，希望构建更类似于人类智能的智能机器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tenenbaum 说：「近期我们才建立出一个能够做到这一点的数学与计算机模型，我们将需要更多的资源、人才、公司、技术和公司的利益以及更快的计算机。我们可能需要等待或依靠其他工程进展，然后才能赶上即使是非常幼小孩子的智力。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;构建第一个婴儿大脑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;新西兰的奥克兰大学生物工程研究所正在试图通过一个动画制作的可互动的婴儿来弥合大脑和机器之间的差距。Mark Sagar 是该研究所动画技术实验室的导演和创始人，其动画作品《阿凡达》和《金刚》获多项奥斯卡奖。他在实验室和一个叫做 BabyX 的 3D 电脑屏幕上的金发碧眼宝宝玩躲猫猫，这个 BabyX 是一个能够学习、思考并可以产生面部表情，能够自己做出反应的实时系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过在麻省理工学院建立身体部位的医学模拟，Sagar 开始了他的职业生涯。在那里他致力于实现数字面孔，并使用这些技能开发 BabyX。动画人工智能能够模仿他的面部表情、朗读简单的字、识别对象和播放经典的视频游戏 Pong，这使它每天都变得更聪明。BabyX 不仅是 Sagar 的「大脑宝宝」，也是在他的女儿 Francesca 在不同年龄阶段的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了构建 BabyX，Sagar 在他的女儿 6 个月、12 个月、18 个月和 24 个月时进行了扫描，并将数据上传到了系统中。他选择通过动画技术来复制他女儿的行为、面部表情和声音，作为人工智能初生的隐喻。Sagar 亲切地将 BabyX 称为「她」，并解释她如何使用光纤电缆：由她的模拟神经活动所驱动，如同脊髓连接到大脑。与之前的系统不同，由于 BabyX 是一个具有人工智能的交互式化身，故它具有学习和保留信息的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们不以大多数人所想的方式开发人工智能，」Sagar 说，「在神经科学和认知科学中存在许多有争议的理论，现有知识可能仅代表冰山一角。而最困难但也最深刻有趣的部分是：生物学启发的方法如何从不同规模过程的相互作用中出现更高的认知水平。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Sagar 和他的团队测试了 BabyX 与人类的互动。BabyX 能够处理人类的情绪、理解他们的行动背后的意义、并根据她过去与 Sagar 的互动中所学的东西做出回应。BabyX 的屏幕之后是一个大脑的实时模拟，使它能够提示面部模拟眨眼和观众报以微笑。Sagar 认为脸部是发展是有效交互式人工智能的关键，因为它是大脑的反映，并揭示了有意识思维的内在运作。例如，一个简单的微笑是脑内连接的复杂、交织系统运行的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「BabyX 通过使用者的行为和宝宝的行为之间的关联来学习，」Sagar 说道，「在一种学习形式中，咿呀声会使 BabyX 探索她的运动空间、移动她的脸或手臂。如果使用者的响应类似，则表示 BabyX 的动作神经元开始通过称之为 Hebbian 学习的过程与响应使用者动作的神经元相联系，共同发挥作用的神经元会聚在一起。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在重复过程之后，新的神经连接开始在 BabyX 的模拟大脑中创建一个映射，将其动作与使用者的动作相匹配，为更高级的模仿打基础。人类大脑的工作方式大同小异——即通过完成一个动作，大脑形成新的连接并通过重复这个动作而加强。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，这个模拟的婴儿通过她大脑处理的环境信息来做出自己的反应。本质上，BabyX 通过不断改进代码进行学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BabyX 的学习能力是基于生物学似乎可信的学习模型中，这种算法模拟和翻译人类大脑如何处理信息和释放大脑中的化学反应，例如多巴胺或催产素水平。当她不明白一个单词或动作时，BabyX 显示困惑的表情，但当她正确地读一个单词时，她会快乐地笑起来，并释放更高水平的「快乐激素」多巴胺。每个算法都控制神经系统从而令她能够模仿、建立反馈系统还有通过互动和演示学习新的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我想探索如何将基于生物学的行为、情绪和认知的计算模型集成到动画中，特别是面部，」Sagar 说道，「面部表情是人类经验许多方面的纽带。这对探索学习和心理发展的基础，甚至可能对我们未来与更复杂、自主技术的相互作用和使用都至关重要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于面部是沟通的一个首要手段，Sagar 希望他的实验可以为未来的健康和教育应用奠定基础，例如旨在与自闭症或其它社交障碍疾病儿童患者进行互动的方案。一个可以感受到人的情绪、处理并了解他们的感受的系统是驱动人工智能研究的目标，这就像我们人生中最初的光阴一样——建立一个可以自己思考的大脑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 20 Nov 2016 11:17:41 +0800</pubDate>
    </item>
    <item>
      <title>技术 | 斯坦福大学副教授Reza Zadeh：神经网络越深就越难优化</title>
      <link>http://www.iwgc.cn/link/3579157</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自O'Reilly&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：武竞、吴攀、蒋思源&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文作者 Reza Zadeh 是斯坦福大学副教授及 Matroid 公司创始人兼 CEO。他的研究工作主要涉及机器学习、分布式计算和离散应用数学。他同时也是微软和 Databricks 的技术顾问委员会的成员。对于这篇文章，他总结说：「神经网络越深，往往就越难优化。」&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9YibFeykBDgdvOUNVxn0L44aVibqcbWobPqyhOicLFRpYpibh6S1n3ChhiawRFDUcfaJS9Qn8JpO4sIUg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Rastrigin 函数&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;优化是非常困难的一类问题，而这正是深度学习的核心。优化问题是如此困难，以至于在神经网络引入几十年后，深度神经网络的优化问题仍阻碍着它们的推广，并导致了其 20 世纪 90 年代到 21 世纪初的衰落。自那以后，我们解决了这个问题。在这篇文章中，我会探讨优化神经网络的「困难度（hardness）」，并发掘其背后的理论。简而言之：网络越深，优化问题就越难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最简单的神经网络是单节点感知器，其优化问题是凸优化的。凸优化问题的好处是其所有的局部最小值也是全局最小值。现在有各种各样的优化算法来处理凸优化问题，且每隔几年就有更好用于凸优化理论的多项式时间算法（polynomial-time algorithms）出现。运用凸优化很容易得到单个神经元的优化权重（见下图）。从单个神经元开始，让我们看看会发生什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9YibFeykBDgdvOUNVxn0L44OXV1TB7eFu5IDrdDhWEC84ojcs3eyfJS9jQIHvvpfesLJZiaaOibpVsw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1。左：凸函数。右：非凸函数。沿着函数表面，凸函数比非凸函数更容易找到表面的最小值。（来源： Reza Zadeh）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一步是保持单层网络下添加多个神经元。对于单层、n 节点的感知机神经网络，如果存在一组边权重使得网络可以正确地分类给定的训练集，则这样的权重可以通过线性规划用 n 的多项式时间找到，这也是凸优化的特殊例子。所以下个问题是：对于更深的多层神经网络，我们是否可以类似地使用这种多项式时间方法？不幸的是，我们无法保证。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;能够有效解决两层或多层的广义神经网络的优化问题并不容易，这些算法将会涉及计算机科学中的一些最棘手的开放性问题。因此，要想机器学习研究人员找到可靠的深度网络最佳优化算法可能性十分渺茫。因为该问题是 NP-hard（非确定性多项式困难 non-deterministic polynomial hard）的，也就意味着如果可以在多项式时间的计算复杂度中解决它，也将解决数十年来悬而未决的几千个开放性问题。事实上，1988 年 J.Stephen Judd 阐述的以下问题就是 NP-hard：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;给定一个广义神经网络和一组训练集样本，是否存在一组网络边权重（edge weight），使网络能够为所有的训练样本产生正确的输出结果？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Judd 还表明，即使只需要神经网络正确输出三分之二的训练样本，但还是 NP-hard 的，这意味着即使在最坏的情况下，训练一个不精确的神经网络在本质上也是困难的。1993 年，Blum 和 Rivest 证明了更坏的消息：即使训练一个只有两层和三个节点的简单神经网络也是 NP-hard！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;理论上，对比深度学习与机器学习中的许多更简单的模型（例如支持向量机和逻辑回归），这些模型可以在数学上保证优化能在多项式时间中执行。对于这些更简单的模型，我们能够保证优化算法在多项式时间内就会找到最佳模型。但是，对于深度神经网络的优化算法，并没有这样的保证。根据你的设置来看，你不知道你训练的深度神经网络是否是你可以找到的最好的模型。所以你也并不知道如果继续训练是否能得到更好的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;幸运的是，实践中我们可以非常有效地解决这些「困难」结果：运用典型梯度下降（gradient descent）优化方法可以给出足够好的局部最小值，让我们在许多问题上取得了巨大进步，例如图像识别、语音识别和机器翻译。我们只是忽略困难的部分，在时间允许下尽可能多地运用梯度下降迭代。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;似乎最优化问题的传统理论结果是很笨拙的，我们很大程度上可以通过工程和数学技巧、启发式方法、增加更多的机器或使用新的硬件（如 GPU）来解决它们。有意思的是，仍有很多人研究为什么典型的优化算法如此有用，当然除了那些困难的部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习流行的原因远远不止是解决了优化问题。深度学习在许多机器学习任务中获得领先，它的网络的架构、训练的数据量、损失函数和正则化都起着关键作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 20 Nov 2016 11:17:41 +0800</pubDate>
    </item>
  </channel>
</rss>
