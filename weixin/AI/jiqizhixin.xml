<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 世界首个光子神经网络诞生：比传统方法快1960倍</title>
      <link>http://www.iwgc.cn/link/3568914</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;日前，来自普林斯顿大学的 Alexander N. Tait 等科学家在 arXiv 上发表了一篇题为《神经形态硅光子学（Neuromorphic Silicon Photonics）》的论文，介绍了世界上首个「光子神经网络（Photonic Neural Network）」。点击阅读原文可下载此论文。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以机器学习神经网络为代表的人工智能技术正在改变我们生活的许多方面，对支撑这些技术所需的数据处理能力的需求也越来越强。而针对神经网络的结构开发出神经形态芯片（neuromorphic chip）有望能大幅提升神经网络的性能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;日前，来自普林斯顿大学的 Alexander N. Tait 等科学家在 arXiv 上发表了一篇题为《神经形态硅光子学（Neuromorphic Silicon Photonics）》的论文，介绍了世界上首个「光子神经网络（Photonic Neural Network）」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和我们目前主流的基于电子的处理方式相比，光子的方式的速度可以达到更快、也能实现更高的带宽。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据 MIT Technology Review 的报道介绍，开发光子神经网络这个难题的核心是生产一种每个节点都有相同的响应特征的装置以用作神经元。这些节点采用了微型的环形波导（tiny circular waveguides）的形式，这些波导被蚀刻在硅衬底中，光可以在其中循环。当释放这个光并调制在阈值处工作的一个激光的输出时，该环境中输入光的一点微小的改变就会给该激光的输出带来巨大的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中的关键在于，该系统中的每个节点都工作在一定的光波长长——这是一种被称为波分复用（wave division multiplexing）的技术。在来自各个节点的光被送入该激光之前会被总功率检测求和。然后该激光输出会被反馈回节点以创造出一个带有非线性特征的反馈回路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么这种非线性能在多大程度上模拟神经行为呢。研究表明其输出在数学上等效于一种被称为连续时间循环神经网络（continuous-time recurrent neural network）的输出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些研究者为进行概念验证开发了一个 49 节点的硅光子神经网络——实验表明在一个实验性的差分系统仿真任务中比传统方法快了 1960 倍！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，验证性的实验不一定能够适用于实际的应用场景，但毫无疑问，这一研究为基于光子的神经网络的发展提供了重要的推动力。在带宽和速度需求日渐高涨的今天，这有望能为我们提供一种我们所需的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：神经形态硅光子学（Neuromorphic Silicon Photonics）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2oHGXtMueBWJRd0aIL6Ql79jc7sOibaWCAdGicsvZ0z8eTrG1lo7OZNMA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们报告首次观察到了集成模拟光子网络（integrated analog photonic network），其中的连接（connection）是通过 microring weight banks 和首次被用作光子神经元（photonic neurons）的电光调制器（electro-optic modulators）配置而成。这种硅光子电路和连续神经模型之间的数学同构（mathematical isomorphism）通过动态分叉分析（dynamical bifurcation analysis）而得到了证明。已有的神经工程工具可以利用这种同构性来适应硅光子信息处理系统。我们使用一种「神经编译器（neural compiler）」编程了一个的 49 节点的硅光子神经网络，它模拟了传统的神经网络，并且预计其表现在一个实验性的差分系统仿真任务中超过了传统方法 1960 倍。利用了硅光子平台的光子神经网络可以接入用于无线电、控制和科学计算的超快信息处理环境。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib23nicicxz6UtbDfjEmKthoNVBPYaFFkI2WlnexOxFmppQCic9FSI7zMjhA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：带有用作神经元的调制器的 STAR broadcast-and-weight network。MRR: microring resonator, BPD: balanced photodiode, LD: laser diode, MZM: 马赫-曾德尔调制器 (Mach-Zehnder modulator), WDM: 波分复用器（wavelength-division multiplexer）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2IqJRPVMib08b6ToIWLew9uSicvngjcbR4UTCiaia0DvRRTNiaMqHolcd4JA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：带有 2 个 MZM 神经元和一个外部输入的实验配置，阵列波导光栅（AWG）中进行波长复用，并耦合到一个片上 broadcastand-weight network。这个 2×2 的循环网络是由 MRR 权重配置的，w11，w12 等等。神经元状态由低通滤波跨阻抗放大器（low-pass filtered transimpedance amplifiers）的电压 s1 和 s2 表示。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 19 Nov 2016 11:34:50 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 逐层剖析，谷歌机器翻译突破背后的神经网络架构是怎样的？</title>
      <link>http://www.iwgc.cn/link/3568915</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SMERITY&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌神经机器翻译（GNMT）论文《Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》描述了一种将深度学习融入产品的有趣方法。该论文和架构是不标准的，在很多情况下偏离学术论文中的架构。通过典型但计算密集型的调整，谷歌这个系统的重点是保证系统的实用性而并非追求顶尖结果。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了理解 GNMT 中使用的模型，我们将从传统的编码器-解码器机器翻译模型开始，持续变化该模型直到匹配 GNMT 中的模型。看起来，GNMT 进化的东西是改进准确率，同时维持实际产品化时的训练与预测速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V1：编码器-解码器&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;编码器-解码器架构开始了近期的神经机器翻译趋势，出现于数年前。如同名字中的含义，该架构包含两个组件：一个解码器和一个编码器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个词级的编码器-解码器机器翻译系统，如同下面所描述的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;采用一个循环神经网络，通常是 LSTM 来编码用语言 A（英语）写出的语句。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该 RNN 吐出一个隐态（hidden state），我们称之为 S。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该隐态有希望表征前面编码出的语句的所有内容。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;然后 S 被应用到解码器，一个单词一个单词的生成 B 语言（德语）句子。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2wCIHNabCic1Hr144r4PAuSDMLNMHGgWz12GtibYdgF1jTvHtuniauHYSw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;编码器-解码器展示了基于神经的机器翻译可能提供的潜力。即使有了如今复杂的神经机器翻译架构，大部分还是根据解码器-编码器架构分解出的产物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该架构有两个主要的缺点，都和长度有关。第一个，像人类一样，该架构记忆有限。LSTM 最后的隐态，也就是 S，要死记硬背需要翻译的句子的全部内容。S 通常只有数百个 unit（读取：浮点数），你越是尝试挤入固定维数向量，该神经网络也被迫有更多损失。可以将神经网络的这个过程看作是一种有损压缩，有时候这是很有用的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二，根据经验法则，神经网络越深，越难以训练。对循环神经网络而言，序列越长，神经网络随着时间维度越深。这造成了梯度的消失，也就是随着反向传输，循环神经网络学到的目标的梯度信号会消失。即使 RNN 是专门用来帮助防止梯度消失的，比如 LSTM，这仍然是个根本性的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2tARonBxE8fnibMhvcrk1L6wIBReEbpBmXTnpp9S6MmvNAaQgbnTiaKyQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图：来自 Bahdanau 等人的论文 "Neural Machine Translation by Jointly Learning to Align and Translate" (2014)，展示了在翻译得分（评分形式为 BLEU 得分）随着语句变长后的影响。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然它在短句上有效，但当句子变得更长时就开始失效了。记忆问题的一种解决方案是增加 LSTM 隐态的大小。不幸的是，如果我们这么做训练速度也变得不够现实了。随着增加 LSTM 的隐藏大小，参数的数量也次方数的增加。你会耗尽 GPU 的存储或训练时间过长。这种方法也无法解决梯度消失问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V2：基于注意力的编码解码器&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们如何解决上面那些问题？或许会尝试一个人类天生会使用的技能——反复关注源句的相关部分。如果你在翻译一条长句子，你可能会回头看一看源句，确定你捕捉到了所有细节信息。我们能让神经网络也做到这些。我们可以通过存储和指定 LSTM 之前的输出来增加神经网络的存储，同时不用改变 LSTM 的操作。此外如果我们知道对句子的某个特定部分感兴趣，那么注意力机制就会做一个「shortcut」，因此我们就能提供出一个无需遍历大量时间步的监督信号，而遍历时间步会导致梯度消失。这就类似于一个人在读完指环王所有的书后可能会回答的一个问题。如果我想问一个关于这本书开头的特定问题，比如夏尔袋底洞（Bag End），这个人就知道在这部系列小说的第一本中的哪一一页能找到答案。系列小说的长度不会影响你查找答案的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2OppsZdcu4PCf4hvVtJic9KjWgud9ZSQJlFPNPsDYWDOtFTUjYAzUCPQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简单来说（由于注意力机制已经很好地覆盖到其他地方）这个想法就是，一旦你有了从存储的编码器获取的 LSTM 输出，你就可以通过问询它们是如何与解码器端的电流计算相关的来查询每一条输出。编码器的每一条输出后面都会获得一个相关性得分，我们可以将这个相关性得分转换成一个概率分布，再通过 softmax 激活来归一。然后我们可以提取一个语境向量（context vector），这是一个编码器输出的加权求和，其结果取决于我们认为它们是如何相关的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意力机制的一个缺点是我们现在必须为每一个解码器的输出执行所有经过编码的源句的计算。虽然这有利于句子之间的翻译，但可能给长输出带来问题。在计算术语中，如果你的源句与 NN 的长度相当，同时你的目标句与 MM 的长度相当，我们就可以将编码器-解码器架构中的 O(M)O(M) 的解码器带入注意力架构中的 O(MN) O(MN) 中。虽然不是最好的方法，但至少在这个任务上，注意力机制优点远远大于缺点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注：你或许会注意到编码器和解码器（之前架构中的 S）之间的直接连接已经消失了。虽然很多标准的编码器-解码器结构维持了这个直接连接，但是 GNMT 架构会消除这个连接。GNMT 架构会以信息能从编码器端转移到解码器端的方式来形成注意机制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2P6q80G9EvAqFBYWib6LtQiahiaJMWoPMelvyIJbeW9HDXR5nfoklV93IQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图：Bahdanau 等人的论文「Neural Machine Translation by Jointly Learning to Align and Translate"(2014) 中的图像。显示了使用注意力对翻译成绩的影响（以 BLEU 得分的形式）。RNN 搜索是带有注意力的架构，后面的数字指的是训练样本的长度。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V3：双向编码器层（BI-DIRECTIONAL ENCODER LAYER）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然注意力机制允许根据解码上下文检索句子的不同部分，但还是有一个问题。注意机制基本上会问这个编码器的存储输出「你是不是跟这个相关？」并用答案来决定提取什么样的信息。如果编码器输出自己没有充分的语境，它们就不太可能给出一个好答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加上后面的单词的相关信息，例如编码器输出由左右两边的词决定，显然是有益的。如果有一个完整的句子可用，人类几乎肯定会用完整的语境来决定一个词的意思和语境。那为什么我们要强迫计算机不去使用所有的信息呢？而且这对它们真的是一个障碍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2UpIe20Ect8iaDA9qaKdkM4eP05DMwCichHV0AXehMrhVH9LDgr3o0dew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加上这个双向性的最简单的方法是运行两个 RNN，一个在句子中前进，一个在句子中后退。然后，对每一个词，我们连接（concatenate）或者添加所产生的向量输出，从两边产生一个带有语境的向量。在你翻译一种有不同排序（例如从左向右）的语言时（例如无论用另外一种语言翻译这个语言，还是把这个语言翻译成另一种语言），双向的编码器就会变得更重要。GNMT 架构连接（concatenate）它们，潜在的优势是向前和向后的 RNN 的结果只有一半大小。由于双向层最终会成为 GNMT 中的一个瓶颈，同时 RNN 中的参数数量也会几何增长，所以这不是一个不重要的节省。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V4：「为深度学习增加深度」&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于许多神经机器翻译的架构而言，增加深度是精确模型中的关键组成部分。GNMT 架构也通过添加大量的层来增加模型的精度——研究者们使用了 8 层编码器与 8 层解码器的 16 层结构，令目前大多数最好的机器翻译系统望尘莫及。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;在编码器中，谷歌的模型具有一个双向的 RNN 层，随后是七个单向的 RNN 层。解码器则是八个单向 RNN 层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在大多数文章的翻译中，全部层都是双向的可以增加准确性。具有全部双向层的模型可以获得相同或更好的结果，谷歌并没有这么做，我们将在下一部分解释 GNMT 这么做的理由。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2U8XMnUWnDhaYs8NsfB5c14ng9wHe1CFWuOPiaMIwwicQ8SWBD36d1sow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环神经网络在训练中有很多不确定性。事实上，这种深度的标准神经网络需要进行大量的训练，和其他类型的系统不同，它相对简单，不会出现许多时间步。我们会在未来的尝试解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V5：并行&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GNMT 构架最主要的动力就是它的实用性。这个令其在和标准编码器（解码器架构 decoder architectures）相比较的时候显得有些局限和奇怪。为了将像 GNMT 那样的系统转化为产品，并行化就是必要的了。它不仅仅训练地更快、允许更多的试验，同时也让产品部署得更快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在看到的这幅图表不仅仅是机器翻译模型的构架，它更是个相关图（dependency graph）。在一个结点开始计算，所有接近你的结点都需要是已经计算过了的。即使你有无限的计算量，但仍然需要符合相关图表流。同样的，你想最小化相关性就可能相比于相似层级需要更多的计算量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2Zq2n8g1YS2zRezpttcKwicXnlpB45tHouic19zTb9xbRrWb2STLnwUVw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注：无遮盖结点（Unshaded nodes）还没有完成的时候，白遮盖节点（nodes shaded white）已经完成了它的计算。蓝遮盖层（Layers shaded blue）也处于计算的过程或已经完成了计算。这也就是为什么只使用了单一双向的循环神经网络（RNN）。双向层是两个循环神经网络组成的，一个从左至右另一个从右至左。这意味着计算第一个输出需要等待从右边到你现在位置共 N 个计算量（N 是序列的长度）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果层级是双向的，全部这一层就会在任何后面的相关性层开始计算前完成计算。但是如果使用的是单向层级，我们的计算就能有够多的灵活性与并行性。在上面的例子中，仅仅关注编码部分，最前面的四层网络全都是同时计算结点。这是上层神经网络不依赖于所有下层网络节点唯一可能性，只要是直属下层的。如果是双向神经网络，那就不可能了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib25rcl2a2yTxzYYpmbh10kXFWibQzcmB6YnQzTqhgKfcUQkAwiaada0bwg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;并行（解码器侧）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：由于使用解码器侧的所有输出的注意机制，解码器侧必须等待，直到所有编码器完成处理。但在编码器完成时，解码器能以和编码器侧相同的并行方式执行任务。通常，注意机制将使用解码器的最顶部输出来查询注意机制，但 GNMT 架构使用最低的解码器层来查询注意机制以最小化依赖图，并且允许最大并行化。在上面的例子中，如果我们使用最高的解码器层作为注意（attention），我们将无法开始计算解码器的第二输出，因为解码器的第一个输出仍在进行中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;旁注（强制教育和训练对生产）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：在训练期间，我们已经知道英语句子需要翻译成什么。这允许我们具有比在预测时更高的并行性水平。由于我们已经有了所有正确的词（在右下角的「已知」翻译），我们可以强迫系统使用它。强制教育是你给神经网络下一个词的正确答案，即使它实际上已经预测出错误的词。这种方式是有效的，训练将继续迫使神经网络输出正确的词，系统最终会输出正确的结果。这种方式允许你在计算第一个输出字的过程中「作弊」并计算第二个输出字。在原有情况下，我们需要等待系统逐词输出，而且不能加入「已知」的翻译。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;旁注（多 GPU）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这种并行化只有在多 GPU 情况下才有效率。在 GNMT 论文中，他们的图示实际上是根据 GPU 的数量来标记每个图层的。对于编码器和解码器而言，谷歌的方法使用了八个 GPU——每层一个。它对单 GPU 有没有意义？通常，在计算给定图层时，GPU 应该达到高利用率（假设你能够设置合理的批量大小，网络大小和序列长度）。这项工作有关重新排序的依赖性，重新排序允许立即进行更多的计算，允许更好地利用更多的设备。在 GPU 情况下，并行计算并不会提高效率，而在多 GPU 的情况下，这种重新排序能够显著加快速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;V6：残差是新的热点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;八层神经网络至少对循环神经网络已经是很深了，从一般经验法则来看，除了一些异常情况或特定结构外，更深的神经网络是更难以训练的。当然这有许多原因，最直观的就是有更深的梯度需要计算路径，它消失或爆炸（vanishing or exploding）的风险就越高。幸运的是，现在有很多很有潜力的方式来解决这一问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个解决梯度消失（vanishing gradients）的方法就是使用残差网络（residual networks），著名的卷积神经网络（CNN）就是训练数百层深度残差网络组成神经网络。&lt;/span&gt;&lt;span&gt;想法是相当简单的，通过默认一层神经网络计算一个恒等函数（identity function）。这是很有道理的，如果你在一层上面做得挺好，就不会期望二或三层运行得差。最坏的情况也是第二层和第三层只会不加修改地「复制」第一层输出。所以每层只需要学习一个恒等函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不幸的是，学习这样的恒等函数（identity function）好像对多数神经网络不算太麻烦。更糟的是后面的的层级作为监督信号会打乱前面层级的训练，也就是它打算要去方向是持续改变的。同样地，第一层在有很多层级在它下面的情况下也就根本不会训练地很好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2JT19TfibiaUhDufRv6GnlrQcf0OjRLarcWbnN04Gxglnicw7RUlbVB9ow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;为了解决这个问题，我们侧重构建这些层级在表达恒等函数时不同权重的表现。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2OibLor06lub0wZ4yqs0RYlCRe6dqvM8icNj4HefxuUYRBFeqjasDR0UA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;集众家所长的谷歌神经机器翻译&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GNMT 文章中所描述的架构建立在之前算法的迭代之上。尽管十分复杂，它仍然遵循编码器-解码器的流程。它看起来可能让人望而生畏，但每个变化都是由一个简单的想法驱动的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2sQZECAlicnfk3WeBwkoXqiaGBFiakC6G3VPABoRj7ov2axIbMRo1Lya7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌神经网络翻译的架构非常有趣，它在算法上没有太多创新，真正精妙的地方在于架构设计。如果把它比作一条船，它的船型是如此的完美，能快速穿过充斥波浪的水域而不受一点阻力。以上我们已经讨论了执行各种任务的架构——有关翻译和自然语言生成，它们完全可以应用到其他大计算量的密集型任务中。我们希望很快就会见到这些方式的更多应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文是谷歌新论文的一小部分介绍，没有详细讨论 BLEU 的细节，词汇级粒度（granularity）如何改进词级别的翻译效果，BLEU 的优缺点，在快速部署中量化模型，在各种优化算法中选择以获得更好收敛，以及在数据集过大情况下不要使用 dropout 等话题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 19 Nov 2016 11:34:50 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 体验百度无人车，系统性人工智能技术让自动驾驶越来越近</title>
      <link>http://www.iwgc.cn/link/3568916</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?width=500&amp;amp;height=375&amp;amp;auto=0&amp;amp;vid=h0346g9f437" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三届世界互联网大会在乌镇召开，人工智能成为热门话题，而真正能让大众切身体验到人工智能便是已经从「测试」走向「试乘」的无人车，百度无人车邀请了多位嘉宾进行了体验，李彦宏竟然也发朋友圈「吐槽」自己不是第一批体验的，同时这也标志着桐乡市子夜路智能汽车和智慧交通示范区内开始测试和试运营，这成为百度继 2013 年启动无人车项目、2015 年底完成多种路段测试、今年 9 月和 10 月分别获得美国加州自动驾驶汽车道路测试许可证和完成加州首次公共道路测试，无人车项目的有一个重大进展。&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiaTEbFPUtng1O7EGAP4zteYuTYO9Sz4zeNb4EuwJfQgxcOcjSIacvia4g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是国内首次第四级别的自动驾驶汽车全程无干预的在全开放城市道路上行驶，投入乌镇运营无人车 15 辆，3 天内超过 200 位乘客规模化试乘，应付了多时段的复杂气象条件。更加重要的是，这是支持 5 款车型的跨平台无人驾驶技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年底，在百度宣布正式成立自动驾驶事业部且表示「计划三年实现自动驾驶汽车的商用化，五年实现量产。」时，很多人会持观望态度。也有人在谷歌和特斯拉的自动驾驶汽车出现事故（后者为辅助驾驶）后去质疑自动驾驶的发展方向。但就像斯坦福「人工智能百年研究」的首份报告中所提到的那样：就像现在还不明确的一点是，自动驾驶汽车需要发展到何种程度才能引起大众的广泛接受。解决这个问题一方面是需要技术的持续进步和产品的不断成熟；另一方面是需要大众和自动驾驶汽车在生活中进行持续和良性的互动，感知到每一次自动驾驶技术的进步给自己生活带来的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiaeSbV0icgrN6FeTbUWo0ddHCqT8NicEDtNiaR6vnYojicUfnvE8Dm6VZfjA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而我们通过一次次的真实体验，逐渐感知到，梦想中的完全自动驾驶和决策精确的汽车以及随之而来的更加智能的城市已经离我们越来越近。波士顿咨询做了一项调查，58% 的调查对象表示他们愿意乘坐自动驾驶汽车。例如，印度和中国消费者的意愿很高：分别有 85% 和 75% 的调查对象表示准备乘坐自动驾驶汽车。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;城市是自动驾驶汽车最有可能给人们的生活、工作和出行带来更好的彻底变革的场景。因为城市既是我们最大的，也是我们增长最快的人口中心。现在全世界已有一半的人类——35 亿人——生活在城市里；而到 2030 年，全世界将有三分之二的人是城市居民。城市占据了 60%——80% 的能源消耗和全世界 70% 的温室气体排放。这也是此次桐乡市子夜路智能汽车和智慧交通示范区内开始运营的意义，它的重要性要远远超过一条供自动驾驶汽车测试的道路，因为这是一个智慧系统。对于我们生活的城市来说，我们可以预计到的好处包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）更为安全的驾驶和更少的事故&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，人工智能使机器在很多任务的决策速度上都超过了人类，在驾驶决策，尤其是紧急情况下的应变能力更是如此。驾驶员遇到突发情况，他从视觉感知、到肢体决策再到汽车的执行大约需要 1.2 秒，而自动驾驶汽车仅需要 0.2 秒。其次，驾驶员的安全视距一般在50米左右，而自动驾驶汽车安装有多种中远距雷达、摄像头等传感器，能实现200米以上的超视距扫描观测。第三，人类驾驶员会受疲劳、酒驾、醉驾、情绪等问题影响，而计算机系统却不存在这样的问题。因此，更好的「视力」、更快的「驾驶决策」和更加稳定的「驾驶表现」会使自动驾驶汽车成为比人类更加出色的驾驶员，从而降低事故发生率，甚至是提高我们的平均寿命。据麦肯锡咨询预测，自动驾驶可将制动距离及反应时间的大幅降低，车辆碰撞事故发生率减少九成。而根据波士顿咨询的数据，自动驾驶汽车的广泛应用能够让美国每年减少约3万起公路死亡事故。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「让一个有驾照的司机代替电脑驾驶，实际上增加了事故的可能性，因为人类没有那么可靠。」谷歌自动驾驶汽车项目总监 Chris Urmson 曾表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）资源的利用更加高效和环保&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先从车辆来说，目前的车辆利用率较低，而无人车将通过全局优化、共享经济、实时调度、智能派单、动态定价等方式来提高汽车使用率。其次，麻省理工学院MIT研究发现，如果采用自动驾驶汽车，路口通行效率可以提升一倍；IBM研究发现，30%的城市交通流量因找车位而产生。自动驾驶会减轻对道路和停车场需求的压力。BCG 的研究表明，SDV 和「自动驾驶出租车（robo-taxi）」（尤其是共享自动驾驶出租车）在市区的广泛使用可以让城市街道上的汽车数量下降 60%，尾气排放下降 80% 或更多，这会带来公共资源的更好利用和更好的环境。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）提高生活质量&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这出现表现在出行成本的下降和时间利用效率的提升。据百度自动驾驶事业部总经理王劲介绍，未来，80%的汽车将是共享的，百度经过建模测算得出：如果居民采用完全自动驾驶模式的电动无人驾驶出租车方式出行，其成本仅为当前有人驾驶出租车出行成本的44%。甚至，如果该电动无人驾驶出租车采用分时共享方式运营的话，其出行成本将进一步降到目前出租车方式的1/3。美国一个通勤者的单程平均驾驶时间是 25 分钟，中国大城的通勤时间更长，当有了自动驾驶技术之后，人们可以在通勤中有更多的时间来工作和休闲。同时，自动驾驶汽车还可以为我们「跑腿」去处理一些任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，自动驾驶将大幅提升我们的生活效率，并且会创造出一种全新的智能城市形态。带来的影响不可估量，因此也成为众多科技巨头纷纷加入进来的原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 20 世纪 80 年代卡耐基梅隆大学的 Navlab 计划，到谷歌自动驾驶项目，再到如今所有相关公司的强势布局，众多参与者都走在追求这个终极目标的路上，每个参与者都会基于自己的优势规划发展路径。各个领域的参与者从不同角度向自动驾驶这个目标进发，特斯拉上个月刚刚发布了完全自动驾驶硬件，包括 8 个环绕摄像机、12 个升级版的超声波传感器、具有增强功能的前视雷达、基于 GPU 的更强大的车载计算系统和特斯拉自己开发的神经网络；创业公司 Drive.ai 另辟蹊径选择了与自动驾驶汽车与周围环境（主要是人类）进行通信的发展方向；英伟达则将端到端学习应用到了自动驾驶中；牛津大学和Udacity刚刚开放了各自的自动驾驶数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以谷歌和特斯拉为例，两家公司都使用了多种相似的传感器、地图技术以及汽车的「学习」软件技术，但各自有一些不同的考量。但就像人类一样，无人驾驶汽车也需要一双「眼睛」去「看」到路上的人、车、物，以安全为前提做出决策。而在这项技术方面，谷歌和特斯拉有不同的考量。 谷歌激光测距系统 LIDAR，通过向目标发射探测信号(激光束)，然后将接收到的从目标反射回来的信号(目标回波)与发射信号进行比较，来计算目标的相关信息，比如距离、方位、高度、速度、姿态、甚至形状等参数。由于该技术可高度精确地计算汽车与周遭环境的位置关系，因此激光测距系统 LIDAR 是目前无人驾驶技术最优的选择之一。而特斯拉则完全不买 LIDAR 的帐，而是采用高速摄像头让汽车「看见」周遭的一切。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在数据方面，特斯拉在去年 10 月通过软件升级增加了辅助驾驶功能，这个功能在研发时使用了特斯拉车主过去 18 个月积累的 7.8 亿英里行驶数据。在该功能上线后的短短六个月内就积累了 4,700 万英里数据，远远超过谷歌历时 6 年积累的 150 万英里，而近期特斯拉的这个数据已经增加到 1 亿英里。特斯拉在收集数据上有着垄断性的巨大优势，所以能够利用现有深度学习做自动驾驶，在与大多数同行竞争中已然遥遥领先。但特斯拉并没有满足这一状态，Elon Musk 同时通过成立 Open AI 在本质上寻求能够实现第四级别自动驾驶的下一代的深度学习算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在通往自动驾驶之路上，各家科技公司各显神通，百度无人车的体验让我们意识到自动驾驶已经离我们的生活越来越近，而现在我们需要探寻的就是谁将先到达那里。自动驾驶涉及的技术非常复杂，现在下结论还为时过早，但唯一明确的一点是，自动驾驶是一项系统工程，需要各种技术相结合。BCG报告中指出，传感器和传感器整合技术在自动化驾驶中至关重要。但除此之外，自动驾驶还需要有精确到10厘米以内的高清地图，满足其对周围环境进行预判。传感器和地图的结合可以保证数据的连贯性，准确定位、导航，不止这些，高清地图可以对传感器进行交叉检查，帮助自动驾驶车辆对周围环境进行实际的测试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度无人车的这次试乘不仅可以让我们更加自信的畅想自动驾驶给我们带来的未来，同时也更好的让我们感知到系统性自动驾驶技术的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiaVGYj2rmQNKQNyrVfB9fFLH1b6WkiaGSg2gTj4nC9bDhqHSvqJibfibMmw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「百度汽车大脑」是百度自动驾驶的核心，包括高精度地图、定位、感知、智能决策与控制四大模块。底层为高精度地图、中间层为感知/定位，最高层为智能决策与控制。目前汽车大脑已经可为汽车提供高精度地图、高精定位、智能感知、智能控制的自动驾驶整体解决方案。其中，百度自主采集和制作的高精度地图记录完整的三维道路信息，能在厘米级精度实现车辆定位，相比于GPS定位精度提升了两个数量级。百度无人驾驶车依托国际领先的交通场景物体识别技术和环境感知技术，实现高精度车辆探测识别、跟踪、距离和速度估计、路面分割、车道线检测，为自动驾驶的智能决策提供依据。百度无人驾驶使用了64线激光雷达、毫米波雷达、视频等感应器。GPS定位系统等，随时采集车辆周边数据，精确识别路面交通线、红绿灯、各种交通标识，可准确接收车辆的定位信息。在国际通用的KITTI测试车辆检测项目中，百度的车辆识别准确率达到89.32%。在计算能力方面，百度无人车还拥有CPU+GPU+FPGA的异构车载计算平台，计算能力比去年提升8倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiaLgn9ZhnKD4g7ibnPUDoanOLld0icMeibjD7K0CDI48FmCa0dfI9gomyhQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然完全自动驾驶在技术上是一个极其困难的问题，我们需要在这方面持续加速，但 自动驾驶汽车的出现还会带来社会、法律和监管上的问题，这就需要决策者、规划者、企业和作为世界各地城市一份子的普通居民等城市市民都将要参与进来，让大家感知和接受它慢慢渗透进我们生活的完成过程。人工智能为大众所知需要一个载体，令人惊艳的 AlphaGo 取得了伟大成就，但是除了我们见证了技术之外没有从本质上改变我们的生活。从目前看来，自动驾驶就是人工智能的最佳呈现方式之一，自动驾驶不能仅仅提留在高校和科技公司的实验室里，也不能一直在工程师进行封闭的测试，而是应该通过一次次公开体验让大众感知到自动驾驶技术的逐渐进步。只有这样，我们才能拥有 BCG 在报告《Revolution in the Driver’s Seat: The Road to Autonomous Vehicles》中「问题不再是 SDV（自动驾驶汽车） 是否会上路，而是会何时上路。」那种状态和自信。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心原创文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 19 Nov 2016 11:34:50 +0800</pubDate>
    </item>
    <item>
      <title>演讲|首席研究员童欣：从交互到智能的网络图形</title>
      <link>http://www.iwgc.cn/link/3568917</link>
      <description>&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguic6E2KicGL1jsTEuYncmBwoZMIeo2kh583rErYY0v1YNL8h7s9l4asQAA/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;11月14日，微软亚洲研究院院友会成立，下午举行了“让世界充满AI：人工智能研讨会”，新老院友同台分享来自各自领域的洞见。以下是第一篇，来自微软亚洲研究院网络图形组首席研究员童欣。有关院友会报道请戳：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=2649439445&amp;amp;idx=1&amp;amp;sn=8ddc5f54a313909ce0bbbf126a2a61ac&amp;amp;chksm=82c0d551b5b75c4716e733a2687a4bafaf737728be2a2de5a9b40ca966c195e297aaa0bf3e15&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=2649439445&amp;amp;idx=1&amp;amp;sn=8ddc5f54a313909ce0bbbf126a2a61ac&amp;amp;chksm=82c0d551b5b75c4716e733a2687a4bafaf737728be2a2de5a9b40ca966c195e297aaa0bf3e15&amp;amp;scene=21#wechat_redirect"&gt;这里是你们永远的家——写在微软亚洲研究院院友会成立日。&lt;/a&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;主持人马歆&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：各位院友好。我现在的身份是微软亚洲研究院院友会常务副秘书长。正式开始今天下午让世界充满&lt;span&gt;AI&lt;/span&gt;：人工智能研讨会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面请我的同事童欣，他是&lt;span&gt;1999&lt;/span&gt;年毕业直接加入微软亚洲研究院，目前担任微软亚洲研究院网络图形组首席研究员。他主要研究方向为计算机图形学和计算机视觉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;童欣&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：谢谢马歆的介绍，&lt;/span&gt;&lt;span&gt;谢谢各位院友。几天前我得到通知要在这里做一个报告，我非常焦虑和紧张。上次这么紧张还是第一次在&lt;span&gt;SIGGRAPH&lt;/span&gt;报告论文的时候。我想了很久，决定了这个题目，&lt;span&gt;“&lt;/span&gt;网络图形：从交互到智能&lt;span&gt;”&lt;/span&gt;，我想把过去几年来的一些想法作一个思想汇报，请各位院友指正、批评、提出建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事情要从&lt;span&gt;15&lt;/span&gt;年前说起，&lt;span&gt;2001&lt;/span&gt;年的时候，&lt;span&gt;Har&lt;/span&gt;&lt;/span&gt;&lt;span&gt;ry（沈向洋）和百&lt;/span&gt;&lt;span&gt;宁（郭百宁）决定要成立一个新的图形组，那么就需要有一个很酷的组名，于是他们决定叫做&lt;span&gt;“&lt;/span&gt;互联网图形组&lt;span&gt;”&lt;/span&gt;。名字起得很好，问题也马上来了：基本上每个见到我们的人都问什么是&lt;span&gt;Internet&amp;nbsp; Graphics&lt;/span&gt;。为了回&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T12:07" style="box-sizing: border-box;"&gt;答&lt;/ins&gt;这个问题，在&lt;span&gt;2001&lt;/span&gt;年的时候我们集中全组的力量做了第一个项目，&lt;span&gt;Game&amp;nbsp; Download &amp;amp; Play&lt;/span&gt;，这项目我们想把游戏图形的数据、几何、纹理做一些压缩，那么通过互联网下载的时候，大家就不用等那么长的下载时间了，很快把一部分数据下载到本地之后，大家就可以开始玩游戏了。这项目可以说非常成功。这之后我们顺利地开始做&lt;span&gt;SIGGRAPH……&lt;/span&gt;转眼到了&lt;span&gt;2010&lt;/span&gt;年，百宁把接力棒交给我，让我慢慢开始负责整个图形组，那么我要怎样激励大家、我们组里应该有什么样的愿景。我也开始思考这些问题，重新在问自己到底什么是互联网图形？&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:40" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicmZWuZFo1cXxwQAP65S6z8W97QGSbg0kWjpnib29HZUNOzoynNVsdjfw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们看看周围，可以看到很多成功的例子。互联网加文字，有网络文学、微博，维基百科。加图片就有美图秀秀、&lt;span&gt;Instgram&lt;/span&gt;等等。互联网加视频也很好，有&lt;span&gt;Youtube&lt;/span&gt;、爱奇艺等很多国内网站，还有网络直播，还有了网红。回头再看看&lt;span&gt;Graphics&lt;/span&gt;，却好像什么都没发生，就这样过了十年，那么到底出了什么问题呢？&lt;span&gt;——&lt;/span&gt;有传言说，如果你站在风口，就算你是一头猪也能飞起来。可是我这么瘦的一个人，站了这么久，怎么还没飞起来，这到底出了什么问题？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我做了一些粗浅的研究，认真想了一想。我发现，飞起来这件事，不是什么都可以，要满足两个条件：第一，要&lt;span&gt;Everyone&lt;/span&gt;，就是内容最好是每一个人都能产生、都能创造，那么有了网络大家就可以互相交流，你的内容就会有海量增长。第二，要&lt;span&gt;Everywhere&lt;/span&gt;，随着移动平台的发展，如果你这个内容的产生和消费能互联到每一个平台上，让大家在任何地方都能生产消费，这时候你就真的飞起来了。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:42" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:42" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicrAaFXnKGROAQVGcb1oKY8ACDqhmAOtNzENicn7ZKgEXGiamzpOOeOB5g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们看看图形学到底是个什么状况？答案很悲惨：在&lt;span&gt;Everyone&lt;/span&gt;方面，三维内容的生产，对普通用户而言还是非常难的任务。最左边大家可以看到传统的造型动画软件，界面很复杂，即使是艺术家也需要好几年的学习才能做好一个模型。另一方面，虽然我们有一些设备帮助大家来做三维内容的捕捉，比如三维扫描仪、&lt;/span&gt;&lt;span&gt;光穹、动&lt;/span&gt;&lt;span&gt;捕等等，但这些设备都非常昂贵，每个要几百万，还需要专门的场地和专业的操作，普通用户享受不到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:40" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicicxnyTZmMbBRW6Tdhk2xUTAlsZsY1nkxCQ8OPdgqlSVwd0EC2d5pFMg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicwFiaiaGnEAtQDwug7w7gWbv7kmOziasqpbibsHibt81DXhY8FsfUcgouUjw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们再看看&lt;span&gt;Everywhere&lt;/span&gt;，发展了这么多年，所有三维图形的内容都是通过一个二维的屏幕来传递给大家的&lt;span&gt;——&lt;/span&gt;某种意义上来讲，我们的内容和&lt;span&gt;2D&lt;/span&gt;的视频就没有太大的区别。我们的交互就不用提了，我们还得通过鼠标、键盘或者&lt;span&gt;gamepad&lt;/span&gt;进行交互，这些交互&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T12:08" style="box-sizing: border-box;"&gt;跟&lt;/ins&gt;我们在真实三维世界中所做的交互是非常不同的。由于这些限制，大家就会发现，&lt;/span&gt;&lt;span&gt;到现在为止，图形的生产和消费基本和互联网无关，基本的方式还是少数的艺术家，他们组织在一起，经过艰苦的奋斗，做了一些游戏、电影，然后把东西通过市场分发给成千上万的消费者进行消费。一切还是停留在传统的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:43" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicHNgZjL9AjXT0zLFXEicpxB7ic2zpd1LgbJnkqEomNOSMLFGQyWeaFPRg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于这样的想法，我们就提出了我们互联网图形组的愿景，这就是，我们希望做一些图形学的工具和系统，能帮助每个人很方便地产生、观看和分享一些三维内容。同时，我们希望能在自然世界和虚拟世界间提供更自然的界面和交互的方式，另外我们还想在可视的和不可视的抽象信息之间提供一些自然的界面，把抽象的信息变成可视的展现出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguic9qv289w6ara2jjp6LERESIwA0htjBWISO8uAIYjevpf3Nk0Eicp41gw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去五年我们为了这一愿景做了很多不同方面的研究，慢慢意识到也许基于智能或者数据的方法是个很好的解决方案。原因有下面几个：第一，我们已经有了一些昂贵的设备，这些设备帮助我们捕捉了大量高质量的数据。第二，我们也有了比较便宜的设备，这些设备可以为我们的系统提供一个初始的输入，不用从零开始了。最后，是一些关于机器学习方面的技术进展可以让我们把这些技术用到图形学的问题里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicMkuFNHJuGVoGr11JWZVqj2XGttYg0FN6Dnty6Dc6kwIdAJyicyCyW6A/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么也许一个比较好的解决方案是通过低价普及的设备，比如普通相机和深度相机，加上智能的算法，再有些时候需要一些简单的用户输入，来方便&lt;/span&gt;&lt;span&gt;地产&lt;/span&gt;&lt;span&gt;生三维的内容。关于智能算法，我们希望它能做两件事，一是希望能够利用到所有三维数据的本征特性，用这些帮助我们产生内容&lt;span&gt;; &lt;/span&gt;二是可以用机器学习来进行端到端的学习，在输入和输出之间直接建立一些联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我用我们组研究的一个研究课题三维物体的数字化来进一步说明举例。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T12:09" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三维物体数字化的目标是希望将一个真实世界的三维物体，完美地传递扫描进一个虚拟世界。为做到这一点，我们不仅仅要捕捉三维物体的几何形状，还要重现它的材质信息。注意&lt;/span&gt;&lt;span&gt;，有了几何信息虽然可以知&lt;/span&gt;&lt;span&gt;道物体形状，却不知道这个物体是什么，只有有了物体材质表面反射属性以后，我们才能在三维世界中真正栩栩如生地体现出来，大家就会&lt;span&gt;的&lt;/span&gt;清楚知道这是真实世界的一个啤酒瓶，上面&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T12:10" style="box-sizing: border-box;"&gt;有&lt;/ins&gt;一个纸标签，标签上有烫金字&lt;span&gt;……&lt;/span&gt;我想我不需要再说明这样一个工具对&lt;span&gt;VR/AR&lt;/span&gt;内容的产生、或者对虚拟购物等应用是多么重要。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:44" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicY7Kh5Cat9FdLN9PhhL4dtcYy8kf7hcQgd635IyKIuu3NRd2XHx61Kw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们看看现在的解决方案是什么。基本上我们可以&lt;/span&gt;&lt;span&gt;发现这&lt;/span&gt;&lt;span&gt;流水线还是非常长的，首先用设备扫描三维几何形状，但是扫描得到的这些几何形状&lt;/span&gt;&lt;span&gt;在&lt;/span&gt;&lt;span&gt;大部分情况下非常糟糕，需要大量人工交互工作来去除噪&lt;/span&gt;&lt;span&gt;声、平&lt;/span&gt;&lt;span&gt;滑三维模型。材质捕捉就更麻烦了，我们需要把物体挪到专用的捕捉室，放在专用的设备上，捕捉物体在各种光照、各种视点下的外观，有了这些才能采集出真正的物体形状和材质。大家可以发现这样一个基本的任务还是有很多障碍，首先去噪方面需要很多手工交互工作，其次材质捕捉设备很昂贵，另外这个流水线很长，需要分开的步骤去先捕捉几何，再用另外的设备捕捉材质。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicPVLCtAmxjgyn8qa9z5pDE7lnQ947zhXicgVhIibck2w3d6wNq8J1ZGsg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们看看我们用一些智能的算法能帮我们做什么事情：第一个要介绍给大家的是我们去年研发出来的一个数据驱动的模型去噪算法。这里要做的是希望有个自动的算法，帮我们除去扫描模型上的噪音，同时保留模型上面所有的几何细节，并且算法对不同设备扫描出来的模型都能很好的处理。我们的算法通过收集带噪声的扫描模型和对应的基本没有噪声的高质量模型，先去学习训练这些几何之间的对&lt;/span&gt;&lt;span&gt;应关系。基于这个对应关系，我们就可以将一个带有噪声的扫描模型直接对应生成它的没有噪声的&lt;/span&gt;&lt;span&gt;模型，从而实现去噪的效果。这是我们组的刘洋研究员带领实习生完成的工作&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicyybeOia462a8607uTlpACFJICSOAooXR7fFktITncxHEibgodCO0tCwA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们这个算法在训练好了以后，用户在用的时候是全自动的。更了不起的是，我们的算法在我们所有的测试模型上去噪效果都超过了所有目前已有的模型去噪算法。同时我们的算法还比所有已知算法都要快。我们很快会把我们的算法源代码和数据公布在网上，希望其他研究人员都可以在基础上继续研究，同时很多用户也可以直接使用我们的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面我们来看一些实验结果。左边是输入一个扫描模型，有很多的噪声，右边是&lt;span&gt;Ground&amp;nbsp; Truth&lt;/span&gt;，右边第二个是我们算法得到的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicoSt5A0MP1E9tMoxgKzQ00mjqLo0JtyTlOKw0LmBj8icNsbiatl83AmPw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是另一个例子，扫描模型的噪音非常大，以前的算法只能除掉一些噪音，或者会抹去很多模型上的集合细节。我们的算法可以比较好地去掉模型上的噪声，同时比较好地保留它的几何细节。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:45" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguic9GDbU97ytx4uXvwkxuWQhyUZA2CzhygdbXiaAaZ9KEQIMCM6fTOaeSw/0?wx_fmt=jpeg"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们再看看材质捕捉方面，刚才我们说材质捕捉设备很昂贵，捕捉过程很麻烦。有什么更好的做法来做呢？我们在两年前做了世界上第一个不需要任何特殊设备和光照，只从自然未知光照下拍摄的物体视频出发进行材质捕捉的算法。这是我们团队的董悦研究员带领实习生完成的工作。输入就是大家看到的左边的视频序列，右边是输出的材质捕捉的结果，最后我们把它放在一个新的光照下，物体可以栩栩如生地再现出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicOc1UWZmVONlrZNnxrR4vLqMS2BysUWDKmfa9GQZlIkf7SVoEYF9gvQ/0?wx_fmt=jpeg"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个算法的关键是我们要从视频中同时估计物体的光照和材质属性。我们发现自然环境中的光照和材质本身具有不同的属性，可以用这些属性很巧妙地从观察的数据最终把二者分分离开来。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里显示了我们算法所恢复的物体的材质效果，不论是啤酒瓶上印刷的标签，还是光滑的瓷器，还是带有铁锈的金属，我们的算法都能自动地从一些视频序列中把高质量的材质重构出来。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:47" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguic3pV31p6rwyaMbQZKU3BHexOoGI88x3WNGTBInNIibWGdomMXibRXvKVQ/0?wx_fmt=jpeg"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了这些工作，上面的流水线变得简单自动了很多，但还是要经过两步。有没有可能一步就把所有事情搞定？去年我们在这方面做了一些研究，做了世界上第一个从视频中同时恢复物体的几何形状和表面材质的算法。这个方法只是用了视频而不再需要任何的深度相机捕捉的数据。同样，我们的算法不需要知道光照信息。左边是我们算法输入的视频，右边是捕捉的物体和材质在新的光照环境下绘制的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicEVM2ibp5dfcWpLX0B4nrF2wvXR8a6D2tmVibsBfa6FUXALU9NcVKAgZg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是我们捕捉到的几何和材质和真实照片的对比，你可以看到所有的几何细节、表面反光和材质属性都被很好的重建出来了。在不同的光照下看，所有物体都像真实物体一样得到真实再现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicuB46Y6YSKmEicEESNiciaRQX6xX25ib3Diaiajicf483uuDSpN1FRwVE4ykDA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于这一结果，我们把做的结果放到&lt;span&gt;HoloLens&lt;/span&gt;，并和我们周围的真实光照结合在一起，可以生成非常真实的效果。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:48" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguic5en9lbHKmyjFfThZICGmI2qKLRMTXIZ7LKtzVauX3NMSK7nZq6lZ2w/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;刚才我们以物体的数字化为例说明了如何采用智能的算法帮助我们简化建模过程，方便普通用户捕捉三维内容。总结一下，在过去几年中我们在智能算法方面做了很多努力，我们逐渐认识到&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;智能算法也许是能够实现普通用户产生三维内容的一个最终解决方案。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我也想分享一下我在这个过程中所得到的经验或者教训：我总结为三个&lt;span&gt;D&lt;/span&gt;。首先是&lt;span&gt;Open-minded&lt;/span&gt;。我们要积极地学习借鉴&lt;/span&gt;&lt;span&gt;其他&lt;/span&gt;&lt;span&gt;领域的方法算法，比如现在我们也在学习和深度学习相关的东西。第二是&lt;span&gt;Concentrated&lt;/span&gt;。第一条就像吸星大法，把别人的东西都吸过来了，但还不够，还要易筋经，把东西化成自己的，要知道自己拿到这个工具是要解决自己的问题的，聚焦于自己的问题，把那些东西为你所用。最后是&lt;span&gt;End&amp;nbsp; to&amp;nbsp; end&lt;/span&gt;，我们并不想发了一篇论文&lt;/span&gt;&lt;span&gt;然后&lt;/span&gt;&lt;span&gt;研究就结束了，论文更多的是一个交流表达的手段，关键是把问题真正给解决掉，最后给用户提供一个真正的端到端的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;展望未来，可以说我们才刚刚起步，前面还有很长的路要走。这也许是个坏消息，但对我来说这其实也是好消息。因为这意味着前面还有很多不确定性、很多挑战。作为一个研究人员来说，这些困难、挑战也正是我们最终的乐趣所在，虽千万人，吾往矣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谢谢大家。&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:49" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;ins cite="mailto:Ershan%20Wang" datetime="2016-11-17T13:49" style="box-sizing: border-box;"&gt;&lt;/ins&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMgrGtHlAqSolGAFJOjlguicNtT72ssZY0Oia4bwibricTicHeSITZoFHwyYoznSIg1ZBXY0IPD3Ysasug/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;你也许还想看：&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="max-width: 100%; width: 527.778px; line-height: 28.4444px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=2649439247&amp;amp;idx=1&amp;amp;sn=49e2b25ceab322c938049a082a3e6e1c&amp;amp;chksm=82c0d58bb5b75c9d6bca10b8b3f647498dd8de3f238cca6012a5c249bf8528387ba30ce85aff&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=2649439247&amp;amp;idx=1&amp;amp;sn=49e2b25ceab322c938049a082a3e6e1c&amp;amp;chksm=82c0d58bb5b75c9d6bca10b8b3f647498dd8de3f238cca6012a5c249bf8528387ba30ce85aff&amp;amp;scene=21#wechat_redirect"&gt;&lt;span&gt;对话|首席研究员童欣：从长远看，AR的应用范围远比VR广泛&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=401773681&amp;amp;idx=2&amp;amp;sn=5d4a33fae7b5dc90b425c646ef77dfbb&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=401773681&amp;amp;idx=2&amp;amp;sn=5d4a33fae7b5dc90b425c646ef77dfbb&amp;amp;scene=21#wechat_redirect" style="font-size: 15px; text-decoration: none;"&gt;&lt;span&gt;【将电影变成现实】用HoloLens玩虚拟传送&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=400607098&amp;amp;idx=1&amp;amp;sn=933c7328221cfec90e358314be8602e3&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=400607098&amp;amp;idx=1&amp;amp;sn=933c7328221cfec90e358314be8602e3&amp;amp;scene=21#wechat_redirect" style="font-size: 15px; text-decoration: none;"&gt;&lt;span&gt;刷新神经网络新深度：ImageNet计算机视觉挑战赛微软中国研究员夺冠&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;&lt;/pre&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="margin-bottom: 14px; padding-bottom: 11px; max-width: 100%; border: none; line-height: normal; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;感谢你关注“微软研究院AI头条”，我们期待你的留言和投稿，共建交流平台。来稿请寄：msraai@microsoft.com。&lt;/span&gt;&lt;br/&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNOiadOWXokcRPb57xdGSTVyicYWXylxhcJ1WZhXVOBwuWvLMoWcWzm9OxKc6PaiaBk1nIf1tfGQAgjSQ/640?"/&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;微软小冰&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;进驻微软研究院微信啦！快去主页和她聊聊天吧。&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNNibWT4MnMcgUQJEZLVyb5ZFPDia4l7FaIVk2q7lSlRBLibUdkydGuaStIS5NSPso2ek1NmMdAHGtakg/640?wx_fmt=jpeg"/&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 19 Nov 2016 11:34:50 +0800</pubDate>
    </item>
    <item>
      <title>一周论文 | TTIC在QA任务上的研究进展</title>
      <link>http://www.iwgc.cn/link/3568918</link>
      <description>&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;strong&gt;引言&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;PaperWeekly已经介绍过不少Question Answering的相关工作。主要有DeepMind Attentive Reader，FAIR Memory Networks，Danqi’s Stanford Reader, Attention Sum Reader, Gated Attention Sum Reader, Attention Over Attention Reader, etc. 这些模型关联性很大，或多或少存在相似之处。本文给大家介绍一下Toyota Technological Institute at Chicago (TTIC)在Question Answering方面的相关工作，共有3篇paper：&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;1、Who did What: A Large-Scale Person-Centered Cloze Dataset, 2016&lt;br/&gt;2、Broad Context Language Modeling as Reading Comprehension, 2016&lt;br/&gt;3、Emergent Logical Structure in Vector Representations of Neural Readers, 2016&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;strong&gt;Who did What: A Large-Scale Person-Centered Cloze Dataset&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;EMNLP 2016&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;文章构建了一个新的Question Answering dataset，”Who did What”。&lt;/p&gt;&lt;p&gt;sample instance如下图所示。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMvKibKh36a2NNib7nOZelL6SJLm9gdSwARmDDxoT9zh0OlZSwHq8P1t5A/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;问题的句子总是挖掉了一些named entities，然后给出在文中出现过的别的named entities作为选项。这一个dataset的难度要高于之前的CNN/DM dataset，可以作为创建新模型的参考数据集。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;构建此数据集的方法与CNN/DM不同，问题并不是context passge的一个summary。问题与context均来自Gigaword Corpus，他们是两篇非常相关的文章。&lt;/p&gt;&lt;p&gt;具体来说，我们先找到一篇文章，作为question文章。然后提取出文中第一句话的named entities，删除其中的一个named entity作为将要被预测的答案。然后利用这一句question sentence，我们可以利用一些Information Retrieval系统从Gigaword Corpus找到一篇相关的文章作为passage。这篇文章与question文章不同，但是包含着与question sentence非常类似的信息。&lt;/p&gt;&lt;p&gt;有了passage之后，我们再从passage中找出named entities作为candidate answers。&lt;/p&gt;&lt;p&gt;为了使任务难度更大，我们用一些简单的baseline (First person in passage, etc) 将一些很容易做出的问题删掉，只留下比较困难的instances。这样构建的数据比CNN/DM会困难不少。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;相信作者创建的新数据集会给Machine comprehension带来一些新的问题与挑战，是很有价值的资源。文章采用的baseline suppresion方法可以用比较小的代价加大问题的难度，值得参考。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;strong&gt;Broad Context Language Modeling as Reading Comprehension&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;arXiv&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;不久前发布的&lt;a target="_blank" rel="external" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;LAMBADA dataset&lt;/a&gt;中，作者尝试的各种baseline models都给出了比较差的结果。&lt;/p&gt;&lt;p&gt;每一个LAMBADA instance如下图所示。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMmkOuheREhVedFhMtZTvibBkibY90UvlLPeH4swDIxZEEeHTWR2s48Uow/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在观察了LAMBADA dataset之后，我们认为可以利用Reading comprehension models来提升准确率，而不必使用传统的language model。&lt;/p&gt;&lt;p&gt;由于state of the art reading comprehension models需要给出candidate answers，然后从中选出一个作为预测的答案，我们就将所有在context中出现过的单词都作为一个candidate answer。&lt;/p&gt;&lt;p&gt;LAMBADA给出的训练集是一些小说的文本。为了使训练集与测试集的数据类型保持一致，我们构建了一个biased training set。具体的做法是，我们将training set划分成4-5句话的context，然后保证target word在context passage中出现，只保留这样的训练数据。我们在新构建的training set上训练各种attention based models,得到了比原作者好得多的测试结果。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMLnWdq0hSAagXkPvoU4tsCcuukP2NZfkQE6RrPaJWumc8oxEqMVdeFw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;这篇文章中，作者利用了简单的方法和模型将LAMBADA dataset的准确率从7.3%提高到45.4%，非常简单有效。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;strong&gt;Emergent Logical Structure in Vector Representations of Neural Readers&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="作者" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="文章来源" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;ICLR 2017 Submission&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="问题" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;最近提出的各种各样的attention based reader models,本文作者做了一个比较全面的总结和分析，并且通过数学分析和实验展示了模型之间的相关性。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="模型" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;本文作者认为，当前的attention based models可以分为两类，aggregation readers(包括attentive readers和stanford readers)以及explicit reference readers(包括attention sum reader和gated attention sum reader)。&lt;/p&gt;&lt;p&gt;这两种reader可以用如下的公式联系在一起。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMQFhTTx6uXEUEK5eqhqezwSe3y3Uib8xjURURTJ2GWoV2wbkIKWMqIibg/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;要满足上述等式，只需要满足下面的公式。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMLVdCyxMmRicdGmw6JsGmbJHcErIjgke95z2EkOrW3iaHWmhqe0t1huuw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;也就是说，只有正确答案所在的hidden vector和question vector得到的inner product才能给出不为零的常数。以下实验结论支持了这一假设。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogMhkSVQydMibUiate8BcAVeNIgI06iawmIPvr1AB1T8w3Al1eUswaGF4Z5g/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;由于CNN/DM在训练和测试中经过了anonymization，作者认为此inner product其实可以分为两部分，一部分与anonymized token ID有关，另一部分与ID无关。与ID相关的那一部分在inner product应该直接给出0的答案。如下述公式所示。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnA1Oia1MHKMzFs1Fy1NnogME0UEcbYD2TyMgTYDgnibGrweUccspkXrjeCsCCmia7QmzwMvbDCEOkwg/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;本文的另一部分工作是在attention readers上加入一些linguistic features提升各个数据集的准确读，这里不仔细描述。&lt;/p&gt;&lt;h2 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="简评" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;本文是对于各个attetion based neural reader models很好的总结，它很好地连接了各个不同的model，说明了为何看似不同的model能够给出非常类似的结果。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;a title="总结" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;/a&gt;&lt;span&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;问答系统是一类大的问题，也是目前NLP应用的研究热点之一。本文作者介绍了TTIC在QA研究中的一些成果，其中第二篇是本文作者近期的paper。感谢来自芝加哥大学的@Zewei Chu童鞋辛勤的劳动。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于PaperWeekly&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的学术组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkSMlEJFo90NY8rCXr3mJMBibduVHxMKSHzQtVkHz8kNwpjCKBiccGuqLE0WpPuAbdtEs6cTF5iabpAQ/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a target="_blank" rel="external" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;http://weibo.com/u/2678093863&lt;/a&gt;&amp;nbsp;）&lt;br/&gt;微信交流群：微信+ zhangjun168305（请备注：加群交流或参与写papernote）&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;</description>
      <pubDate>Sat, 19 Nov 2016 11:34:50 +0800</pubDate>
    </item>
    <item>
      <title>突破 | DeepMind为强化学习引入无监督辅助任务，人工智能的Atari游戏水平达到人类的9倍</title>
      <link>http://www.iwgc.cn/link/3552550</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自DeepMind Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Max Jaderberg、Volodymyr Mnih、Wojciech Marian Czarnecki&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、吴攀、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;几个小时前，DeepMind 在其官方博客发表文章介绍他们在强化学习上的最新研究进展。他们通过为代理在训练过程中增加两项额外的任务来增强标准的深度强化学习方法，结果显示代理实现了更好的表现。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepMind 的主要任务是开拓人工智能的新疆界，开发可以自主解决任何复杂问题的新系统。我们的强化学习代理已在 Atari 2600 游戏和围棋中实现了突破。但这些系统需要大量数据进行长时间训练，我们一直致力于提高我们的通用学习算法，改变这一情况。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在最近的论文《使用无监督辅助任务的强化学习》中提出了一种可以大大提高人工智能代理学习速度和系统性能的方法。通过为代理在训练过程中增加两项额外的任务来增强标准的深度强化学习方法，我们的代理实现了更好的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是我们的代理在 Labyrinth 迷宫任务中的可视化展示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiavib0ic2ia7icRDZOoYutYx1XPmcm6TiaxjfAzbyMdQgyp6OVJ5urEdmmG6Q/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个任务包括让代理学习如何控制屏幕中的像素，这需要代理学习它的行为会如何影响它将要看到的事物，而不仅仅是预测。计算机学习的过程类似于人类婴儿通过移动和观察手的运动来学习如何控制手。通过学习如何改变屏幕的不同部分，我们的代理学习了视觉输入的特性，从而学会如何在游戏中打出高分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在第二个任务中，代理通过训练从近期战况中预测出动作的即刻得分。为了得到更好的结果，我们将有得分和无得分的历史数据等比例地输入系统。通过更多地学习有得分的数据，代理可以更快地学会预测回报的视觉特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结合这些辅助任务，以及我们之前发表的 A3C 论文《Human-level control through deep reinforcement learning》中的成果，我们提出了 UNREAL（无监督强化和辅助学习/UNsupervised REinforcement and Auxiliary Learning）代理。我们在一套 57 个 Atari 游戏合集和拥有 13 个级别的 3D 迷宫游戏 Labyrinth 中测试了这一新系统。在所有游戏中 UNREAL 代理被用同样的方式训练，系统只接收屏幕图像的信息，试图在游戏中获得最多的得分和奖励。在不同游戏中，得分的方式各不相同，从玩 3D 迷宫到《Space Invaders》——同样的 UNREAL 算法学会了所有这些游戏，得分几乎与人类玩家持平，有些甚至超过了人类。下面的视频中可以看到我们的部分结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?width=500&amp;amp;height=375&amp;amp;auto=0&amp;amp;vid=s03471hfjfw" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;UNREAL 代理玩 Labyrinth&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Labyrinth 中，通过使用辅助任务的结果——控制屏幕中的像素点预测何时奖励会出现——意味着 UNREAL 的速度比我们过去最好的 A3C 代理快超过十倍，而且得分好很多。我们的新系统在这些 Labyrinth 关卡中有 87% 的关卡可以达到专业人类玩家的表现，其中一些关卡的表现更是超过人类。在 Atari 游戏中，代理目前的游戏水平已是人类玩家的 9 倍。我们希望这些成果在不久的将来可以让人工智能系统应用到更加复杂的环境中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：使用无监督辅助任务的强化学习（Reinforcement Learning with Unsupervised Auxiliary Tasks）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibtpcdWV11ysdQEicbIfwqaiaKxmbtkZbrsHQUiaHQAYHcp8C1X5kRPUcbmWvPGwlmdiajI2wBGicDicuCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度强化学习代理已经通过直接最大化累积奖励而实现了当前最佳的表现。但是，环境包含了远远更多类型的可能的训练信号。在这篇论文中，我们介绍一种通过强化学习也能同时最大化许多其它伪奖励函数（pseudo-reward functions）的代理。所有这些任务都共享了一个共同的表征，就像无监督学习一样，这种表征可以继续在有外部奖励（extrinsic rewards）存在的情况下发展。我们还引入了一种全新的机制以将这种表征的重心放到外部奖励上，从而让学习可以快速适应该实际任务中最相关的方面。在 Atari 游戏上，我们的代理的表现显著超越了之前的最佳表现，平均达到了人类专家表现的 880%；并且在一个有挑战性的第一人称三维 Labyrinth 任务合集中实现了平均 10 倍的学习加速和平均 87% 的人类专家在 Labyrinth 上的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 18 Nov 2016 09:34:47 +0800</pubDate>
    </item>
    <item>
      <title>开源| 让老司机告别快进，Miles Deep使用CNN截取色情视频的关键部分</title>
      <link>http://www.iwgc.cn/link/3552551</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Github&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在「非法」查看色情视频时，你是否也为其「漫长」的铺垫和前奏感到懊恼呢？前几天，GitHub 用户 ryanjay0 开源了一个可以用来识别色情视频中特定类型的场景的人工智能项目 Miles Deep。该算法可以将你想看的类型的片段从完整视频中截取出来并生成一个集合了这些片段的新视频，让你可以不再为那些多余的片段烦恼。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Miles Deep 使用了一个带有残差连接（residual connections）的深度卷积神经网络（DCNN），可以基于性行为（sexual act）将一段色情视频的每一秒分类成 6 种类别，其准确度高达 95%。然后它可以使用这种分类来自动编辑该视频。它可以移除所有不包含性接触的场景，或者编辑去掉一种特定的性行为。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Miles Deep 和雅虎最近发布的 NSFW 模型（见机器之心报道《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=1&amp;amp;sn=e99d95427ce67c32323be5310b767b7c&amp;amp;chksm=871b0157b06c88412d9b933805f5ca3a30df745a8ac147f35204e545c250f42e68c57312cbec&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=1&amp;amp;sn=e99d95427ce67c32323be5310b767b7c&amp;amp;chksm=871b0157b06c88412d9b933805f5ca3a30df745a8ac147f35204e545c250f42e68c57312cbec&amp;amp;scene=21#wechat_redirect"&gt;雅虎开源首个色情图像检测深度学习解决方案&lt;/a&gt;》）使用了类似的架构，但不一样的是 Miles Deep 还能够区分裸体和多种特定的性行为之间的不同。就我所知，这是第一个公开的色情视频分类或编辑工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个程序可以说是使用 Caffe 模型进行视频分类的一种通用框架，其使用了 C++ 的 batching 和 threading。通过替换权重、模型定义和 mean file，它可以立即被用于编辑其它类型的视频，而不需要重新编译。下面会介绍一个例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Miles Deep 项目地址：https://github.com/ryanjay0/miles-deep&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;雅虎 NSFW 模型地址：https://github.com/yahoo/open_nsfw&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;安装&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Ubuntu 安装（16.04）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;依赖包（Dependencies）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;sudo apt install ffmpeg libopenblas-base libhdf5-serial-dev libgoogle-glog-dev&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外的 14.04 依赖包&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;sudo apt install libgflags-dev&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CUDA（推荐）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果要使用 GPU，你需要 Nvidia GPU 和 CUDA 8.0 驱动。强烈推荐；可提速 10 倍以上。这可以通过软件包安装或直接从 NVIDIA 下载：https://developer.nvidia.com/cuda-downloads&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CUDNN（可选）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是来自 NVIDIA 的额外的驱动，可以让 CUDA GPU 支持更快。在这里下载（需要注册）：https://developer.nvidia.com/cudnn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下载 Miles Deep&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;miles-deep (GPU + CuDNN) &lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;miles-deep (GPU) &lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;miles-deep (CPU)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也要下载这个模型。将 miles-deep 与该模型的文件夹放在同一个位置（而不是在模型文件里面）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbeFgm8ZC5626KHhWRaalMBphDURMKOgRuNGKQxahrWZ60GibHXBmYsLdg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注：是在一个 GTX 960 4GB 上测试了一段 24.5 分钟长的视频&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Windows 和 OSX&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我目前还在开发用于 Windows 的版本。但我没有 Mac，不过应该只需要做一些小修改就可以在 OSX 上运行。编译指令如下。https://github.com/ryanjay0/miles-deep#compiling&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;miles-deep -t sex_back,sex_front movie.mp4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这能找到后向和前向的性爱（sex from the back or front）场景，并输出结果到 movie.cut.avi&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;miles-deep -x movie.avi&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这能编辑去除 movie.avi 中所有的非性爱场景，并将结果输出到 movie.cut.avi。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;miles-deep -b 16 -t cunnilingus -o /cut_movies movie.mkv&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这能将批大小（batch size）减小到 16（默认为 32）。筛选出舔阴（cunnilingus）的场景，结果输出到 /cut_movies/movie.cut.mkv&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注：如果你的内存不够，可以减小批大小&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多种批大小情况下的 GPU VRAM 用量和运行时间：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbe42ymc2LdlOUT8mbU6Odo26vU4s9AEMNLC1NHxMAIJrFDjAPr2LrG0g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该结果是在带有 4GB VRAM 的 Nvidia GTX 960 上测试得到的，样本是一段 24.5 分钟的视频文件。当 batch_size = 32 时，处理 1 分钟的输入视频大约需要 0.6 秒，也就是说每小时大约 36 秒。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了 batching 之外，Miles Deep 还使用了 threading，这让其可以在分类的过程中截取和处理截图（screenshot）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;预测权重&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里是一个预测一段视频中每一秒的例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbeCpKmKP4O1G5q9JYjGxy60Zzm6LaNKagU1lxbUOe2dkITZsVyRSrPyQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通过你自己的 Caffe 模型使用 Miles Deep&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;找猫&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面这个例子是如果通过你自己的模型（或一个预训练的模型）使用这个程序：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;MODEL_PATH=/models/bvlc_reference_caffenet/&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;miles-deep -t n02123045 \&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;-p caffe/${MODEL_PATH}/deploy.prototxt \&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;-m caffe/data/ilsvrc12/imagenet_mean.binaryproto \&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&amp;nbsp;-w caffe/${MODEL_PATH/bvlc_reference_caffenet.caffemodel \ -l caffe/data/ilsvrc12/synsets.txt \ movie.mp4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这能找到在 movie.mp4 中的所有带有虎斑猫（tabby cat）的场景，并返回仅带有这些部分的结果 movie.cut.mp4。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码中的 n02123045 是虎斑猫的类别。你可以在 caffe/data/ilsvrc12/synset_words.txt 查找这些类别的代码。你也可以使用一个来自 model zoo 的预训练的模型：https://github.com/BVLC/caffe/wiki/Model-Zoo&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注：这个例子只是展示了其中的句法。但不知怎的，在我的经历中它的表现很差，很可能是因为分类有 1000 个。这个程序也能完美适合带有一个「other」类别的分类数量更小的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该模型是一个用 pynetbuilder 创建的带有残差连接（residual connections）的卷积神经网络（CNN）。这些模型都是 ImageNet 上预训练的。然后其最终层经过调整以适应新的分类数量和微调（fine-tuned）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如 Karpathy et al 的论文《Large-scale Video Classification with Convolutional Neural Networks》建议的那样，我训练了最上面 3 层的权重而不只是最顶层的，这稍微提升了一些准确度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbeiaKWjQdkMO8ufOwseLklShz53SslfsgoJbVLibianx0obr3OkibrsS1CMw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面使用不同的模型微调最上面 3 层所得到的结果，该结果是在 2500 张训练图像上测试得到的，这些图像来自与训练集不同的视频。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbeL79M854Ad4AX6U6k99JAcw623Usl3a8bibh8R2pic3mI8apuTl78Rpiaw/0?wx_fmt=png"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练损失和测试精度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8DLuIlocGmG4BCQ8jCHJbeQNQPsjelicr3ibjrvU7n4YKDgQLiaicSv5gV5NFeUOEN0mQycLrgRL7ZPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在所有测试的模型中，resnet50_1by2 在运行时间、内存和准确度上表现出了最佳的平衡。我认为全 resnet50 的低精度是因为过拟合（overfitting）的关系，因为它有更多的参数，也许其训练可以按不同的方式完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上面的结果是通过 mirroring 而非 cropping 得到的。使用 cropping 能够将在 resnet50_1by2 上的结果稍微提升至 95.2%，因此它被用作了最终的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 TensorFlow 微调（fine-tuning）Inception V3 也能实现 80% 的准确度。但是，这是使用 299x299 的图像大小，而不是 224x224 的大小，也没有 mirroring 或 cropping，所以它们的结果不能直接进行比较。这个模型可能也会有过拟合的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;编辑电影&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;给定对于每一秒的帧的预测，它会获取这些预测的 argmax（最大值参数）并创建这段影片的截断块（cut blocks），其中的 argmax 等于目标（target），而其分数（score）也比一些阈值要大。其中的差距大小、匹配每个模块中目标的帧的最小比例（minimum fraction）和分数阈值（score threshold）都是可以调整的。FFmpeg 支持很多编解码器（codecs），包括 mp4、avi、flv、mkv、wmv 等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;单帧 vs 多帧&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型并不使用任何时间信息，因为它分别处理每一张图像。Karpathy et al 的论文表明其它使用多帧（multiple frames）的模型的表现并不会好很多。它们难以应对相机的移动。将它们的慢融合模型（slow fusion model）与这里的结果进行比较仍然是很有趣的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其训练数据库包含了 36,000（和 2500 测试图像）张图像，分成了 6 个类别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;blowjob_handjob&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;cunnilingus&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;other&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;sex_back&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;sex_front&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;titfuck&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些图像的大小都调整为了 256x256，并且带有水平镜像（horizontal mirroring），并且为了数据增强（data augmentation）还随机剪切（cropping）成了 224×224 的大小。有很多实验没有剪切，但这能稍微提升 resnet50_1by2 的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前来说，这个数据集还受限于两个异性恋表演者。但鉴于这种方法的成功，我计划扩大分类的数量。因为这些训练很敏感，我个人并不会放出这些数据库；而只会提供训练出来的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;前向和后向性爱&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里的前向和后向性爱（sex front and back）是由相机的位置决定的，而非表演者的方向。如果女性表演者的身体面向相机，那么性器官的前面就展示了出来，这就是前向性爱（sex front）。如果女性的后面被展示了出来，就是后向性爱（sex back）。这创造了两种在视觉上不同的分类。其中在性交和肛交之间不做区分；前向性爱和后向性爱可能两者都包含。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;编译&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;克隆包含 Caffe 作为外部依赖包的 git repository&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;按步骤指令（http://caffe.berkeleyvision.org/installation.html）安装 Caffe 依赖包以用于你的平台。Ubuntu 指令（http://caffe.berkeleyvision.org/install_apt.html）。默认的是 OpenBlas。不要担心编辑 Makefile.config 或使用 Caffe 的问题。在 Ubuntu 16.04 上尝试这个，并在其上附加依赖包：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;sudo apt install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;sudo apt install --no-install-recommends libboost-all-dev&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;sudo apt install libopenblas-dev python-numpy&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;#Add symbolic links for hdf5 library#(not necessary on LinuxMint 18)cd /usr/lib/x86_64-linux-gnu&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;sudo ln -s libhdf5_serial.so libhdf5.so&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;sudo ln -s libhdf5_serial_hl.so libhdf5_hl.so&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;默认的是不带 CuDNN 的 GPU。如果你想用其它的工具，请编辑 Makefile 和 Makefile.caffe。注释掉和取消注释这两个文件中对应的行即可。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;make&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;证书&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码（包括训练好的模型）按 GPLv3 授权。Caffe 使用的是 BSD 2 授权。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 18 Nov 2016 09:34:47 +0800</pubDate>
    </item>
    <item>
      <title>干货 | 深度学习硬件架构简述</title>
      <link>http://www.iwgc.cn/link/3552552</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Fossbytes&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：周文璐、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;深度学习具有极高的计算需求， 要对深度学习应用进行开发并商业化，就需要找到合适的硬件配置。目前，在开发用于深度学习应用的高效硬件平台这一领域，竞争十分激烈。本文将介绍具体的硬件要求，并讨论未来对深度学习硬件的展望。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习在这十年，甚至是未来几十年内都有可能是最热门的话题。虽然深度学习已是广为人知了，但它并不仅仅包含数学、建模、学习和优化。算法必须在优化后的硬件上运行，因为学习成千上万的数据可能需要长达几周的时间。因此，深度学习网络亟需更快、更高效的硬件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;众所周知，并非所有进程都能在CPU上高效运行。游戏和视频处理需要专门的硬件——图形处理器（GPU），信号处理则需要像数字信号处理器（DSP）等其它独立的架构。人们一直在设计用于学习（learning）的专用硬件，例如，2016年3月与李世石对阵的AlphaGo计算机使用了由1920个CPU和280个GPU组成的分布式计算模块。而随着英伟达发布新一代的Pascal GPU，人们也开始对深度学习的软件和硬件有了同等的关注。接下来，让我们重点来看深度学习的硬件架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;对深度学习硬件平台的要求&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要想明白我们需要怎样的硬件，必须了解深度学习的工作原理。首先在表层上，我们有一个巨大的数据集，并选定了一种深度学习模型。每个模型都有一些内部参数需要调整，以便学习数据。而这种参数调整实际上可以归结为优化问题，在调整这些参数时，就相当于在优化特定的约束条件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWic45jgIOq2OCuFlIgqdSFLkN1LtQ73WJvUWYHZMsqTUpJFr0nz4Y91GHDibJCADvSbXKaNMBqy3SFA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图片：英伟达&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719456&amp;amp;idx=2&amp;amp;sn=8c6695450f107e3932ea7c15acc536a1&amp;amp;chksm=871b009eb06c8988f3462bd352566ad5fcda05766bf09a8dda2e1ca36b6f1fabde685e9e0830&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719456&amp;amp;idx=2&amp;amp;sn=8c6695450f107e3932ea7c15acc536a1&amp;amp;chksm=871b009eb06c8988f3462bd352566ad5fcda05766bf09a8dda2e1ca36b6f1fabde685e9e0830&amp;amp;scene=21#wechat_redirect"&gt;&lt;span&gt;百度的硅谷人工智能实验室（SVAIL）已经为深度学习硬件提出了DeepBench基准&lt;/span&gt;&lt;/a&gt;&lt;span&gt;，这一基准着重衡量的是基本计算的硬件性能，而不是学习模型的表现。这种方法旨在找到使计算变慢或低效的瓶颈。 因此，重点在于设计一个对于深层神经网络训练的基本操作执行效果最佳的架构。那么基本操作有哪些呢？现在的深度学习算法主要包括卷积神经网络（CNN）和循环神经网络（RNN）。基于这些算法，DeepBench提出以下四种基本运算：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;矩阵相乘（Matrix Multiplication）——几乎所有的深度学习模型都包含这一运算，它的计算十分密集。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积（Convolution）——这是另一个常用的运算，占用了模型中大部分的每秒浮点运算（浮点／秒）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;循环层（Recurrent Layers ）——模型中的反馈层，并且基本上是前两个运算的组合。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;All Reduce——这是一个在优化前对学习到的参数进行传递或解析的运算序列。在跨硬件分布的深度学习网络上执行同步优化时（如AlphaGo的例子），这一操作尤其有效。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除此之外，深度学习的硬件加速器需要具备数据级别和流程化的并行性、多线程和高内存带宽等特性。 另外，由于数据的训练时间很长，所以硬件架构必须低功耗。 因此，效能功耗比（Performance per Watt）是硬件架构的评估标准之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;当前趋势与未来走向&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWic45jgIOq2OCuFlIgqdSFLksgPeB588Sxlw9ODYz4RPiaX5TKibSq1pc1L6R3R5jSmnAac3jTTumIQA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;英伟达的GPU在深度学习硬件市场上一直处于领先地位。图片：英伟达&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英伟达以其大规模的并行GPU和专用GPU编程框架CUDA主导着当前的深度学习市场。但是越来越多的公司开发出了用于深度学习的加速硬件，比如谷歌的张量处理单元（TPU/Tensor Processing Unit）、英特尔的Xeon Phi Knight's Landing，以及高通的神经网络处理器（NNU/Neural Network Processor）。像Teradeep这样的公司现在开始使用FPGA（现场可编程门阵列），因为它们的能效比GPU的高出10倍。 FPGA更灵活、可扩展、并且效能功耗比更高。 但是对FPGA编程需要特定的硬件知识，因此近来也有对软件层面的FPGA编程模型的开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，一直以来广为人所接受的理念是，适合所有模型的统一架构是不存在的，因为不同的模型需要不同的硬件处理架构。 而研究人员正在努力，希望FPGA的广泛使用能够推翻这一说法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大多数深度学习软件框架（如TensorFlow、Torch、Theano、CNTK）是开源的，而Facebook最近也开放其 Big Sur 深度学习硬件平台，因此在不久的将来，我们应该会看到更多深度学习的开源硬件架构 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 18 Nov 2016 09:34:47 +0800</pubDate>
    </item>
    <item>
      <title>报告 | 从区块链到人工智能，CB Insights发布金融科技创业未来报告</title>
      <link>http://www.iwgc.cn/link/3552553</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CB Insights&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今天为大家推荐一篇 CB Insights 数据统计出的科技金融报告。CB Insights 使用自己的大型数据库，统计了在科技金融领域的众多创业公司。从银行到保险，区块链、人工智能等技术正在变革金融领域。机器之心对本报告每页的主要内容进行了编辑，但因为这是一份长达 116 页的报告，就不再粘贴图片。此外，CB Insights 在昨天又推出了 2016 年 第三季度全球科技金融风险投资的报告。对这两份内容感兴趣的读者可点击阅读原文下载并仔细研读。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic45jgIOq2OCuFlIgqdSFLkn5BLZp0pzypw92DGeXpQd9B5pmEXgJFelTFFjzftyg08QkVoLJmC7A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P1-8：CBinsights：金融科技的未来&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P9-12：创业公司成为技术发展和应用实现的重要推动力，是寻找新型商业模式的不确定性实验。创业公司是众包的 R&amp;amp;D，过去 5 年，在金融领域出现了 38135 家早期阶段的创业公司参与这些「实验」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P13：今天的新兴商业模式和创业公司会成为明天的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P14：即使世界顶级公司也更难保持优势了，如图为标准普尔 500 指数公司的平均保持时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P15：创业公司带来的威胁也在增长……创立一家科技创业公司的成本逐年下降。2000 年之后两大重要的推动力：开源和水平扩张、云和 AWS&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P16：技术被采用的速度越来越快&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P17-18：新型商业模式的用户增长更快&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P19：保洁&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P20：FedEX&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P21：酒店住宿领域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P22：汽车领域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P24：银行业公司分类（美国）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P25：银行业公司分类（欧洲）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P26-P28：新时代的商业战争不是「金刚 vs 哥斯拉」，而是巨头和大量创业公司之间的竞争：要么吃掉，要么被吃掉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P29：大部分创业公司都会失败，只有 4% 能走到第 6 轮&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P30-32：大部分可能会影响你的业务的创业公司都会失败，而成功成长起来的就会给已有的企业带来麻烦。对于这些威胁，个性化的服务是很关键的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P33-34：金融科技（fintech）领域很热很拥挤。金融服务占到了美国乃至全球经济的很大一部分。图为美国 GDP 中金融所占比重的年度变化（只在战争时期出现了明显下跌）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P35-36：金融服务这个价值数万亿美元的市场仍然大部分都是在线下，未来有望迎来金融科技创业的爆发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P38：移动的普及让金融科技公司能够直接面向消费者。图为 App Store 2011 年、2013 年和 2016 年年度 100 应用中的金融应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P39：人口分析：对于金融服务，千禧一代的看法有所不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;18% 的人在过去一年里更换了自己的主要银行&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;40% 的人愿意选择把谷歌作为银行&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;59% 的人认为金融产品的目标用户不是他们&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;79% 的人将他们与银行的关系看作是「相互交易」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P40：比起现有的金融服务公司，用户更喜欢新公司&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P42：银行业的 Uber 时刻：实体银行会像旅行、音乐和视频行业一样被革命吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P43：存在风险的总银行利润池= 109 亿美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P44：自 2011 年以来，风险资本支持的金融科技公司已经获得了 3006 比融资，总额高达 344 亿美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P46-49：2015 年第 4 季度的金融科技投资出现了一次暴跌，但很快就反弹了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P50：风险资本在 2015 年第 1 季度到 2016 年第 1 季度向金融科技公司投资了 5 亿美元。图为北美、亚洲、欧洲的比较（单位：百万美元）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P51：企业参与了约四分之一的金融科技交易（企业在金融科技公司的风险投资中的占比）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P52：高盛、花旗、桑坦德是风险投资金融科技公司的主要银行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P53：2015 年全球涌现出了 14 家金融科技独角兽，但 2016 年这一数字还是 0&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P54：24 家金融科技独角兽现在总价值达到了 680 亿美元；其中 15 家在支付或借贷领域……保险紧随其后&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P56：金融科技公司在 IPO 之后表现普遍不佳&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P57：风险投资金融科技创业公司的整体退出数量也在一定范围内不断波动&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P58：精明的风险投资者（Smart Money）仍然看好金融科技，交易活动在 2014 年达到峰值&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P59：Smart Money 主要投资了金融科技的哪些领域？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PoS 信用卡/另类贷款、不动产、保险科技、机构投资工具/数据&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P60-62：金融科技投资解读：比特币和区块链。图为比特币的价格走势，最近突破了 570 美元，这是自 2014 年 8 月以来的第一次&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P63：对比特币和区块链创业公司的投资。累计投资在 2016 年超过了 10 亿美元，共实现了 351 笔投资&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P64：在媒体的聒噪之中，对比特币的投资正在逐渐转向区块链&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P66：对钱包、挖矿和交易的投资也转向了区块链&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P67：战略性的投资支撑了区块链的发展。图为 2013-2016 年最大的 5 笔区块链/比特币投资的情况，其中橙色表示涉及到企业的投资&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P68：投资比特币/区块链的公司多种多样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P69-70：另一大投资热门领域是财富管理和个人储蓄。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自动财富管理增长迅速，但仍然只站到总市场的一小部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P71：预计美国智能投顾（robo-adviser）AUM 市场到 2020 年会超过 2 万亿美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P72：资本继续涌入智能投顾市场&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P73：Wealthfront 和 Betterment：AUM 从 0 美元增长到了 2016 年的 30 亿美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P74：但 AUM 增长速度并不快，有点让人失望&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P75：已有的企业已经快速采取的行动&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P76：自动储蓄应用为智能投顾带来了只是用移动的方法。图为自动投资账号的增长数量&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P79：出于对规模化的需求，一些玩家因为技术而被一些已有的企业看中了&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P80-81：支付 &amp;amp; 汇付科技也是金融科技领域的一大热门。PayPal 仍然在网络电子商务支付上占主导地位，但其未来可能取决于以下技术：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Processsing：在 2013 年，PayPal 买下了 Braintree，这家公司在上一年处理了价值 500 亿的移动和数字支付，其客户包括 Airbnb、Uber 和 StubHub。竞争者：Stripe、Ayden、ChasePaymentech&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Remittances：PayPal 在 2015 年收购的转账公司 Xoom 让美国用户可以轻松向另外 53 个国家的人转账。竞争者：Western、Union、Ria Financial&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Peer-to-Peer：PayPal 通过对 Braintree 的收购而拿下了 Venmo，这个工具可以让用户快速轻松发送资金。PayPal 正在将这项技术扩展到消费者-商户支付。竞争者：SquareCash、Facebook Messenger、Google Wallet&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Credit Lines：PayPal Credit 让消费者可以信用支付、平衡收支，6 个月不需要支付利息，之后的利息高达 19%。竞争者：Mastercard、Visa、American Express、Affirm&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P82：Venmo 支付量从 12.5 亿美元（Q1』15）增长到了 32 亿美元（Q1』16）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P83：Adyen 在 2015 年处理了 50 亿美元，成为了支付领域的独角兽&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P84：支付领域早期阶段创业公司的交易活动增长强劲&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P85：对转账创业公司的风险投资在 2015 年达到最高&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P86：在转账创业的大赛中，风险资本资助的公司有可能取得胜利吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P87：Vise 的投资地图&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P88：Mastercard 的投资地图&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P89：American Express 所在是收购与投资&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P93: 全球人寿保险额：2.7 万亿&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;全球非人寿保险额：1.4 万亿&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数字化消费中消费者期望值的主要转变&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P95：服务期望值：消费者对他们的在线体验或保险公司不满意&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P96：技术：在线销售保险市场的成长&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P97：产品：在现在的用户信息和消费喜好统计中有应用&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P98：保险科技投资：从 2011 年的 1.31 亿美元到 2015 年的 27 亿美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P99：到目前为止，2016 年最大的 5 笔科技金融交易有 3 笔是保险科技&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P100：今年第一季度，保险科技领域有一半的交易是在种子轮&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P101：如今的保险科技场景图：创业公司发展分布或新产品&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P103：保险公司在科技金融上的投资打破了记录&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P104：覆盖性连通使得 IOT 创业公司成为保险领域中的热门——保险业和索赔防范&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P105：再保险巨头有了更多参与&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P107：聊天机器人让金融领域疯狂&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P108：在保险领域新的 P2P 模型获得关注：消费者会拥抱他们吗&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P109：跨境投资者正在投资美国经济金融公司&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P110：人工智能对机构投资的影响：人工智能驱动的对冲基金创业公司的崛起&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P111：全球还未饱和的机遇&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P113：做支付的大科技公司&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P114：房地产众筹：会出现一家风投支持的获胜者吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P115：Digital Challenger Banks 的崛起&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P116：围绕区块链的热情已经从银行扩展到了其他主要产业应用&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;区块链+资本市场&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;区块链+保险&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;区块链+汇付&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;区块链+合规&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;区块链+借贷&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文为机器之心编译文章，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 18 Nov 2016 09:34:47 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 谷歌新人李飞飞：击碎玻璃天花板的华裔女科学家</title>
      <link>http://www.iwgc.cn/link/3537501</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：机器之心编辑部&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本周二，谷歌宣布斯坦福大学教授&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=4&amp;amp;sn=19fdaccf63daae29e72d2a738fd65e45&amp;amp;chksm=871b0d66b06c84708e3f803b2cec907365d5fd8a3d15401721a48bfaa4a49b5d37bded8e10a9&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=4&amp;amp;sn=19fdaccf63daae29e72d2a738fd65e45&amp;amp;chksm=871b0d66b06c84708e3f803b2cec907365d5fd8a3d15401721a48bfaa4a49b5d37bded8e10a9&amp;amp;scene=21#wechat_redirect"&gt;李飞飞加入其云团队&lt;/a&gt;。作为第一代中国移民，这位图像识别领域的杰出学者是如何从普林斯顿进入斯坦福，最后成为谷歌人工智能团队新任领导者的？让我们来了解一下这名华裔学者的传奇经历吧。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实现美国梦&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFHvtX1ibmMyw0k4tVhyVwAnjbnr2Peh5hDfNWs6QguvNooRDlmG697HA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;1976 年，李飞飞出生于北京，后来在四川长大&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你真想做一件事，全世界都会来帮你。16 岁刚来美国的那两年，李飞飞却只能从依靠自己开始。在帕西悉尼的白人圈子中，这位亚裔姑娘显得有点孤独，她像所有极客一样过着简单的生活。那个时候打工与学习几乎占据了她所有的时间。李飞飞明白，这就是普通新移民的生活，需要点牺牲和决心。幸运的是她的同学和高中数学老师在这时给了她莫大鼓励和帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1995 年，李飞飞进入普林斯顿大学攻读物理学，生活的大门终于渐渐为她打开。在周一到周五，她是普林斯顿物理系拿着高额奖学金的高材生，周末则必须回到 Parsippany 的洗衣房，置身于成堆的衣服中。她说「我爱普林斯顿。」因为这里有全世界最优秀的年轻天才。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而生活的艰辛也在随着年龄增长，她见过斯坦福的优秀博士毕业生为一张绿卡四处奔波，那时的她不明白也无法想象一张绿卡，一个留在美国的机会能难倒全世界那么多优秀的人才。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有过移民经历的人对平等有着更加深刻的体会，李飞飞也如此，她坚持主张高科技产业性别平等，支持种族多样化，还有反对特朗普。她曾在 Twitter 上调侃特朗普是个不懂科学没有眼界的人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;李飞飞说：「多样化的危机，也正是我们的社会正在应对的危机。『科技是没有灵魂的吗？』」她坦承自己对 AI 研究界的失望，因为这一领域不太欢迎不具代表性的少数群体。在她工作的部门里一共有 15 名全职人员，她是唯一的女性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;梦想与责任&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;并不是每个美国移民都能实现自己的梦想，但李飞飞做到了。当初她随父母举家搬到大洋彼岸。「他们来到这个国家是为了追求梦想。」李飞飞认为她也「应该能够追求自己的梦想。」这个「应该与能够」的选择做起来并不轻松。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1999 年，李飞飞从普林斯顿大学毕业，那时的华尔街一片辉煌，互联网泡沫的热潮接近顶峰，李飞飞接到了多家金融公司的工作邀请。然而她却没有从中选择任何一份工作来减轻家庭经济负担。她的父母支持她做出了最后选择，去西藏研究藏医。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然李飞飞知道自己终将回到学校，回到科研工作中来，读博士也是她的梦想，但西藏之行并非人生插曲。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在科学界藏医与中医一样存在很多争议，但这并不妨碍李飞飞对它的兴趣。她在媒体采访中提到，作为一个科学家，藏医可以在哲学和方法论层面上给她给多的理解。她非常看重具体科研项目在更大领域范围内的意义，每一项研究开始之前都要经过深思熟虑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFB67EIOFaYGTlRy1wfHpjtX2ktnf4FpicgwlxBGGyzgF5qRYVUpQH7icg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;她曾经放弃高盛的 offer，追随梦想来到西藏&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在李飞飞后来写给博士生们的信中可以看到她对科学探索的态度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;每个领域的每一篇论文都应该以独特的新视角进行研究。所以当你在机械地开展你们的工作前，请扪心自问——『我的研究会重新定义这一领域的未来吗？』这意味着发表论文的意义不是『这个方向没有人写过』，或者『让我来解决这个小问题，它的成果会很容易展示』；研究意味着『如果我做了，这一重大问题就有了更好的解决方案』，『如果我做了，我就真正开拓了这一领域』。你的研究成果应该能被很多人和行业直接使用，换句话说，你的选题应该有很多『客户』，你的解决方案应该是他们想要的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;西藏归来之后，李飞飞开始了加州理工的博士学业。博士期间母亲接连患上了癌症与中风，那是一段艰苦的日子，李飞飞说，「我们经历了很多困难，然后一起挺过来了。既要担起生活的责任，又要对得起自己的梦想」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;改变图像识别方向的人&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;是什么吸引谷歌一次性将李飞飞和她的门生李佳一齐请进公司，并委以重任的？显然是她的学术成就和影响力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 2009 年以来，李飞飞一直担任斯坦福人工智能实验室和斯坦福视觉实验室的负责人，并成为了终身副教授。在她 2014 年的简历上，有 95 篇在 Nature、PNAS、Journal of Neuroscience、CVPR、ICCV、NIPS 等顶级期刊与会议上发表的文章；联合发表的文章有 32 篇。从 2015 年到 2016 年，李飞飞署名发表的论文有 33 篇（斯坦福视觉实验室），还有一篇将在 2017 年发表在 CSCW 会议上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在不就之前，艾伦人工智能研究所推出了以人工智能为基础的免费学术搜索引擎&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720432&amp;amp;idx=1&amp;amp;sn=25c1224631a11ab1077fda9f2433e78e&amp;amp;chksm=871b0cceb06c85d840d98037614077761a0db0df6c75798d4fa2bc069c7380a02de81e0afed4&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720432&amp;amp;idx=1&amp;amp;sn=25c1224631a11ab1077fda9f2433e78e&amp;amp;chksm=871b0cceb06c85d840d98037614077761a0db0df6c75798d4fa2bc069c7380a02de81e0afed4&amp;amp;scene=21#wechat_redirect"&gt; Semantic Scholar&lt;/a&gt;。我们使用该引擎生成了李飞飞的论文引用量图解（注：搜索时请注意名字输入格式，Feifei Li 为另一位作者。），如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFibLnDY6icE6E9f8ibWiaW4vn07NEGAibxoAE89BrePloAQWk9aRb4obxoSA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFophp9aM2mKJVXnxic179wsJHo0WydODiaibXsdbOL15hhSYhfbnibmYPNw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;过去 3 年，李飞飞论文的平均引用量为 6738。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFDSftxppLcjibRovrPKLJFdIE9yMWLpnjjJ1iaHjSagIPPK2H7ibSzZ5LA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;基于可用数据，Semantic Scholar 估计李飞飞的引用量在 33215 到 44773 之间。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回溯过去，李飞飞在计算机视觉上的研究已经花费了 15 年。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2007 年，李飞飞和一位同事着手开始一项庞大的任务，为来自互联网的十亿张图片进行分类、打标签，从而为计算机提供样本。其中理论基础是如果机器观察到足够的事物，它们就能够在现实世界进行识别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们使用亚马逊 Mechanical Turk 这样的众包平台，邀请了来自 167 个国家的 5 万人帮助为其中的数百万张图像打标签。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，他们建立了 ImageNet 数据集。今天，这个数据集包含了使用日常英语标记的超过 1400 万张图像，跨越 21,800 个类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们用 Semantic Scholar 生成的图解中发现，李飞飞被引用最多的论文就是她于 2009 年在 CVPR 上发表的《ImageNet: A large-scale hierarchical image database》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9xhznsLIp5sSbIiayEITWLFu7SJtX2Z8kgbviaSEKhle3O4cMuGO8NqwOeN4RtZHS8OYicic4JgUibCiaQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?width=500&amp;amp;height=375&amp;amp;auto=0&amp;amp;vid=v0333il31b6" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;李飞飞Ted演讲&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;瓜分学界人才的科技巨头们&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的云业务负责人 Diane Greene 在新闻发布会上说，李飞飞和李佳的加入是谷歌正式将人工智能集团业务正式化的一部分。此后，该团队不会只专注于人工智能研究，而是致力于将尖端技术融入各种 Google Cloud 产品，例如让公司预测销售情况的软件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在宣布成立人工智能集团之前，谷歌还围绕着云服务部门产品路线图发布了一系列产品，介绍了他们如何扩大机器学习的使用。对于云计算来说，机器学习是一项关键的技术，它能训练大规模的 AI 网络，不断自我学习和提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Google Cloud 及其机器学习团队的产品经理 Rob Craft 也表示，这两名研究者将帮助谷歌「将机器学习的力量带入其他行业」，他们也将成为谷歌整合其研究单位及核心业务努力的一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，雇用具有机器学习和相关任务专业知识的人才并不便宜。这一行业内的激烈竞争导致谷歌这样的大公司经常会支付「NFL 球员签字费」级别的巨额资金，而越来越多的顶级学界研究人员也陆续加入了大公司的怀抱。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，学界内重所周知的大牛们基本被科技巨头筛了个遍，方式也是各种各样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最初，谷歌先是收购了多伦多大学的一家初创公司 DNNResearch。但实际上，这家公司只有三个成员，Geoffrey Hinton 和他的两个刚毕业的、曾经赢得 2012 年的 ImageNet 大赛的学生——Alex Krizhevsky 和 Ilya Sutskever（现在加入了 OpenAI）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今被称为谷歌最成功一笔收购的 DeepMind 也是竭尽全力的在挖空英国的人工智能人才。前几日 Business Insider 的一篇文章指出，牛津大学与剑桥大学最优秀的人工智能人才一直在被科技巨头收拢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而且在文章中重点提到，虽然微软和 Facebook 这样的美国科技巨头也正在招募牛津大学的研究生和教授，但是 DeepMind 似乎比其他的公司挖到的人才更多。DeepMind 从被谷歌收购了之后就已经把它的团队规模从在国王十字路时的 100 人扩大到了大约 250 人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不只是谷歌，以美国、中国为主力的科技巨头们正如同「风暴之眼」一般吸纳着一切尽可能的能量推动着公司在人工智能道路上的快速发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软在人工智能研究中一直处于第一梯队，在 9 月底宣布组建 5000 人规模的专注人工智能的工程和研发团队 Microsoft AI and Research Group。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前我们觉得已然落后了的苹果，在 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719828&amp;amp;idx=3&amp;amp;sn=5f72f8b9bbd0078d483ad233e578081a&amp;amp;chksm=871b022ab06c8b3ce2ef881c941e1ea532e3f465bbc1109c74e0aa0a72b283a8408dca7b353f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719828&amp;amp;idx=3&amp;amp;sn=5f72f8b9bbd0078d483ad233e578081a&amp;amp;chksm=871b022ab06c8b3ce2ef881c941e1ea532e3f465bbc1109c74e0aa0a72b283a8408dca7b353f&amp;amp;scene=21#wechat_redirect"&gt;10 月份拉拢到了 CMU 机器学习教授 Russ Salakhutdinov&lt;/a&gt; 作为该公司人工智能研究的负责人，开始招聘人才、组建团队。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;中国的 BAT 三巨头。百度因为有吴恩达，「声音」是最大的，我们对他们的人工智能研究也是了解最多的（开源深度学习框架 Paddle、硬件基准测算工具 DeepBench 等）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;阿里在今年的云栖大会上也「秀」了一把人工智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;腾讯也在最近成立了腾讯 AI Lab，新一轮的招兵买马不可避免。虽然在&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=1&amp;amp;sn=76e38d366841b567d38c4334df175eeb&amp;amp;chksm=871b0d66b06c8470d96d6faf9c636e0e0eed6d0e89e74abd3ecfe52c68f384a906c95efd1329&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720536&amp;amp;idx=1&amp;amp;sn=76e38d366841b567d38c4334df175eeb&amp;amp;chksm=871b0d66b06c8470d96d6faf9c636e0e0eed6d0e89e74abd3ecfe52c68f384a906c95efd1329&amp;amp;scene=21#wechat_redirect"&gt;机器之心的专访中&lt;/a&gt;他们对腾讯 AI Lab 没谈到太多内容，但隐隐可察觉出腾讯也要像谷歌一样在人工智能与自己的平台和产品结合上打通一条道路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在接近尾声的 2016 年，我们已经明显感觉到人工智能、机器学习、深度学习等字眼成为了科技界的主流。从害怕 AlphaGo 之后因过度炒作而经历新一轮寒冬，到语音识别、神经机器翻译等的一个又一个的技术突破，再到越来越激烈的人才竞争，一个新的时代即将到来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 17 Nov 2016 10:16:24 +0800</pubDate>
    </item>
  </channel>
</rss>
