<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | 机器能做噩梦吗？MIT开发出能生成恐怖惊恐图片的深度学习算法</title>
      <link>http://www.iwgc.cn/link/3189608</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#ffffff"&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#7f7f7f"&gt;&lt;span&gt;&lt;b&gt;机器之心编译&lt;/b&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;万圣节就要到了，本着吓死人不偿命的精神，麻省理工学院（MIT）Media Lab 今日上线了一个用人工智能吓人的网站 Nightmare（噩梦）：http://nightmare.mit.edu/#portfolioModal22。在这个网站上，研究者展示了利用人工智能算法生成的恐怖风格的图片，其中包括埃菲尔铁塔等地标建筑和人脸等一些结果。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW62dH7PLqwRcjNwKOmHB4Cp4gqJ3CaZnzAaHsIdldZxiasSJiav1Ifqgg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;地标：美国自由女神像、法国埃菲尔铁塔、日本东京塔&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWvtnepvzBIDpZwGibicRVRT4GkVmrxQahUvKKeeQjjV5iaj6ux7VKibvBIw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;人脸&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，利用深度学习软件生成恐怖图像已经不是什么新闻了。其中著名的有谷歌的 Deep Dream 生成的带有许多眼睛的狗脸的图片；还有前段时间中国出现的 Uber「幽灵车」事件的恐怖司机头像也有人认为是软件生成的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW2GJV4ohA9MJ5hL7HFRfCWnVX5SEpY0faJiac9JU1dRZVenibDHkP7KkQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌的 Deep Dream 生成的狗脸&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWJho6OEbibSv4CE2BLZ211IS5XbtN6B5GNXjr8dqzbVQ1wyjuiaFiaEI4g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Uber「幽灵车」司机&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MIT 还在这个网站上列出了一个恐怖和人工智能交织发展的超短历史：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2000 年前：恐怖的起点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;万圣节传统的神秘起源可以追溯到凯尔特人庆祝的古老异教节日。凯尔特人将这一天作为收获季节的结束和冬季的开端。他们相信这种季节的变换会打开死者世界的大门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1000 年前：人工智能的第一次迹象&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东西方文明都描绘了关于人造实体的传说故事——这些人造的存在能够思考、感知、帮助或伤害他人。在许多故事中，这些「生物」都脱离他们的创造者的控制，并获得了超越任何人预想的知识和能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1816 年：没有夏季的一年&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1816 年的春天经历了历史记录中最奇怪的气象现象：一个永无止境的冬天。这迫使三位作家将自己关在了日内瓦湖畔的豪宅中。玛丽·雪莱、约翰·威廉·波利多里、拜伦勋爵比赛看谁能写出最惊悚的故事。而他们所有人都获胜了。雪莱创造了弗兰肯斯坦；波利多里种下了吸血鬼文化的种子；拜伦则在他的诗作《黑暗（Darkness）》中通过地球上的最后一个人的讲述开启了世界末日惊悚题材的先河。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1840 年：第一个计算机程序诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能和恐怖惊悚开始交汇：拜伦勋爵（现代吸血鬼文学的创始之父）的妻子 Anne Isabella Milbanke 生下了计算机历史上一位先驱爱达·洛夫雷斯（Ada Lovelace）。她编写了世界上第一个机器算法，要知道，当时所谓的计算机器还仅存在于纸面之上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1930 年：恐怖惊悚兴起&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 世纪 30 年代的电影荧幕成为了在黑屋子里面吓人的前所未有的媒介。许多恐怖电影成了人们的消遣，其中包括弗兰肯斯坦、德古拉、木乃伊、隐形人、伦敦狼人……这也催生了一个有创造性的且有利可图的恐怖惊悚片行业。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1956 年：人工智能诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1956 年炎热的夏天，Marvin Minsky 和其它睿智的头脑聚集到了达斯茅斯学院。在一场创造力的爆发中，他们奠定了人工智能成长的基础：开发能够在西洋跳棋上击败人类、进行复杂数学计算……乃至能够生成英语句子等等的程序。有传言说计算机生成的第一个句子是：TRICK OR TREAT？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2016 年：人工智能驱动的恐怖&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几千年来，不同地域、不同宗教和不同文化的人都在创造吓人的方式。恐怖需要引发人内心的情绪才能在人类创造之中保留一席之地。在我们还不清楚人工智能的局限性的今天，这个挑战是尤其重要的：机器能够学会吓人吗？为了这一目标，MIT 推出了 Haunted Faces 和 Haunted Places：计算机通过深度学习算法和邪恶的灵魂生成恐怖惊悚图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，在这个网站上，MIT 还请求网友对他们的软件生成的恐怖图片进行评分。这些评分将作为 MIT 的这个恐怖图片生成模型的进化的训练数据，将使其能够生成越来越恐怖的图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据了解，研究者首先通过鬼屋和末日城市的图片对他们的人工智能算法进行了训练，然后将一些著名地标的图片输入该模型。经过处理之后，该模型能让这些图片带上阴郁的地狱风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW9xHIqMhs4KCsq8tfUrAVvK4J1dq5bfzGfKGJsrvClnZN6cib3ibHziadg/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果 Elon Musk 和 Stephen Hawking 的关于人工智能对人类生存的威胁的警告还不够吓人，MIT 的这个故意吓人的项目可算是做到了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道我们未来的计算机主人会不会使用这种生成的恐怖图片来恐吓我们呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，让我们来认识一下该项目的三位研究开发者：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWUaEoCSHiamfhOicd3vKGXONvJvSbJby58R8wnBYPRL7yWg8YR0S9svtA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part1：基于语言建模的词嵌入模型</title>
      <link>http://www.iwgc.cn/link/3189609</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part1，&lt;span&gt;全面介绍了词嵌入模型，接下来几天机器之心将继续发布 Part2、Part3，希望能对大家了解词嵌入有所帮助。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入简史&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;语言模型的简介&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;经典的自然语言模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W model&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CBOW&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无监督学习词嵌入（word embeddings）在许多自然语言处理的任务中都取得了前所未有的成功，因此它常被视为自然语言处理的万灵药。实际上，在许多自然语言处理架构中，它们确实几乎替代了诸如布朗聚类（Brown clusters）和 LSA 特征等传统型分布式特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 ACL（计算机语言学会）和 EMNLP（在自然语言处理中实证方法会议）的会议论文很大程度都是词嵌入的研究，有些人还认为词嵌入这种嵌入方法比 EMNLP 更加适合的自然语言处理。今年的 ACL 会议有了不仅一个，而是两个的词嵌入模型的研讨会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入之间的语义关系在外行人看起来就像变魔术。深度自然语言处理的讲座常以「国王－男人＋女人≈女王」的幻灯片来做开场白，一篇最近在 Communications of the ACM 的文章向词嵌入模型致敬，并称之为自然语言处理实现突破的主要原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文将会是本系列第一篇全面介绍词嵌入模型的文章，将讨论词嵌入模型的热度是否会持续下去及其原因。在这个介绍里，我们将尝试把在这个领域分散的论文串联起来，强调很多模型、应用和有趣的特征，并将在后续的文章中重点关注多语言环境下的词嵌入模型和词嵌入评估任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这第一篇文章将呈现目前的基于语言建模的词嵌入模型。在我们深度讨论很多的模型时，我们会挖掘它们的优点，希望能够在过去和当前的研究的背景下提供新的见解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于命名方式的简单小结：接下来我们将使用当前热门的「词嵌入（word embeddings）」术语，来指代词语在低维度向量空间的稠密表示。「词嵌入」和「分布式表征（distributed representations）」是两种可互换的表示方法。我们将特别强调「神经词嵌入（neural word embeddings）」，即运用神经网络训练的词嵌入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;词嵌入简史&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从上世纪九十年代开始，向量空间模型就已在分布式语义中得到了应用。当时，许多用于预测连续空间的词表征的模型已经被研究了出来，其中包括隐含语义分析（LSA：Latent Semantic Analysis）和隐狄利克雷分布（LDA：Latent Dirichlet Allocation）。想要详细了解词嵌入背景下的分布式语义的历史的读者可以看看这篇文章：https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2003 年创造了词嵌入这个名词，并且在自然语言模型中将其与模型参数一起联合训练。据了解 Collobert 和 Weston 于 2008 年首次展示了预训练的词嵌入的实际应用。他们里程碑式的论文《A unified architecture for natural language processing》不仅将词嵌入确立成了一种可用于下游任务的有用工具，还引入了现在已经成为了许多方法的基础的神经网络架构。但是让词嵌入最终能流行起来的是 Mikolov 等人在 2013 年创立的 word2vec，这是一个允许无缝训练和使用预训练嵌入的工具套件。在 2014 年，Pennington 发布了一个具有竞争力的预训练的词嵌入集 GloVe，标志着词嵌入已经成为了主流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入是目前无监督学习的成功应用之一。它们最大的好处无疑是它们不需要昂贵的人工标注，而是从未标注的现成大数据集中派生的。然后预训练的词嵌入就可以运用在仅使用少量有标注数据的下游任务中了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然而然地，每个前向传播的神经网络都把在词汇表中的词语当成输入，并把它们表示成低维空间中向量。然后，它们再通过反向传播进行调整，得出词嵌入作为第一层的权重。通常，这称之为「嵌入层（Embedding Layer）」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;产生词嵌入作为副产物的神经网络和 word2vec 这样的以生成词嵌入为特定目标的方法之间的主要区别是它们的计算复杂度。对于一个大的词汇集来说，使用非常高深度的架构来生成词嵌入的计算成本太高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是为什么直到 2013 年词嵌入才进入自然语言处理的舞台。计算复杂度是词嵌入模型的一个关键权衡，也是我们这篇概述中会重复出现的主题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个区别就是训练目标：word2vec 和 GloVe 都是用来生成广泛语义关系的词嵌入模型，这对许多下游任务有用；而这种方式训练的词嵌入对不依赖于这种语义关系的任务并无太多帮助。相反，常规的神经网络对于某个特定任务生成的词嵌入在别的任务往往功能有限。值得注意的是，一个依赖于语言建模这样的语义关系的任务能够生成类似于词嵌入模型的嵌入，这一点我们将在下节探讨。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外说明一点，word2vec 和 Glove 之于自然语言处理，也许就像是 VGGNet 之于计算机视觉，亦即一个普通的权重初始化——它能提供有用特征，而且无需长时间训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比较不同的模型，我们可以设想如下的标准：我们设想一串来自词汇库 V（其大小为|V|）的包含 T 个训练单词的的字符序列 w_1,w_2,w_3,⋯,w_T。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象我们的模型是一段包含 n 个单词的文段。我们将每一个单词与一个 d 维的输入向量 v_w（嵌入层中的同名词嵌入）和一个输出向量 v_w'（另一个词表征，其作用下面很久就会介绍）联系在一起。最终，对于每一个输入 x，我们相对于模型参数θ和模型输出分数 f_θ(x) 来优化目标函数 J_θ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言建模上的一项注意&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入模型和语言模型联系紧密。对语言模型的质量评价基于它们学习 V 词汇库的词语概率分布的能力。事实上，许多最新的词嵌入模型一定程度上尝试预测序列下一个会出现的词。另外，词嵌入模型的评价通常运用困惑度（perplexity）——一个从语言建模借来的基于交叉熵的评价标准。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们进入词嵌入模型的众多细节之前，让我们简单介绍一些语言建模的基础知识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体而言，语言建模力求在给定之前的词语的情况下，计算一个词语 w_t 的出现概率，也就是 p(w_t|w_{t−1},⋯w_}t−n+1})。运用链式法则和马尔可夫假设，我们就可以近似地通过之前出现的 n 个词得到每个词的概率乘积，从而得到整个句子或整篇文章的乘积：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWicOE3T44QVXmJJBnfWKckT1iccqtqAvyPlnZGtbdANbCicBKPGVC9EcMw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基于 n 元的语言模型中，我们可以用一个词的组分的 n 元的频率（frequency）来计算这个词的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW89AoRESTK3jPFEhlHHzu4YUicI6Tguzia2cp51tLMSgaqhMMqiczUXTog/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设置 n=2 产生二元模型，而 n=5 和 Kneser-Ney 则一同呈现平滑的五元模型——平滑的五元模型在语言建模中是公认的的一个强有力基准线。更多的细节，敬请参照斯坦福大学的演讲课件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在神经网络中，我们通过大家熟知的 Softmax 层来计算相同的目标函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWWz7wZibufQfmlfkAtMuAnOadPjsRsWEwfz3F34Wx16l1WdpQEWNFLUg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;内积 h^T v'_{w_t} 计算了词 w_t 的未标准化的对数－概率（log-probability），我们用在词汇库 V 中的所有词的对数－概率之和来把它标准化。h 是它的倒数第二层（见图 1 前向传播网络的隐藏层）的输出向量，而 v'_w 就是词 w 的输出嵌入，亦即在 softmax 层的权重矩阵中的表征。注意虽然 v'_w 可以表征词 w，但它是从输入词嵌入 v_w 独立学习的，因为向量 v'_w 和向量 v_w 的相乘对象是不同的（v_w 和索引向量相乘，v′_w 和 h 相乘）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW90nRVHqppUJW0O2Hgzxibsg4KnpkfznAzWxOicO5WUoJUKK8UdfT2GLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1: 一个自然语言模型（Bengio 等人，2006 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要计算每个词 w 在神经网络输出层的概率。想要高效地做到这一点，我们将 h 和一个权重矩阵相乘，这个权重矩阵每行都是对于在 V 中出现的词 w 所得的 v′_w。我们随后将得到的向量（我们通常称之为 logit，也就是前一层的输出）以及 d=|V| 传入到 softmax 层，softmax 层则把词嵌入「压扁」成一个词汇库 V 里面词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 softmax 层（对比于之前的 n 元计算）仅仅隐式地考虑之前出现的 n 个词。长短时记忆模型（Long Short-term Memory, 英文简称 LSTM），通常用来作自然语言处理模型，将这些词编码成状态 h。我们在下一章将会介绍的 Bengio 的自然语言模型，则是把之前的 n 个词通过一个前向传播层传入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请大家记住这个 softmax 层，许多后续介绍的词嵌入模型都将或多或少地运用它。运用这个 softmax 层，模型将尝试着在每一时刻 t 都最大化正确预测下一词的概率。于是，整个模型尝试最大化整个数据集的平均对数－概率:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWpcHJLCvDnQlKZcE77WSicYKhiafQzE0z1DXda8nHmsPaqr6WDbaW53yA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相似地，运用链式法则，模型的训练目标通常是最大化整个语料库的所有词相对于之前 n 个词的平均对数－概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWusgOAUobrxHnr6orodvhibE4W7OepmxR7NwnXg5zd0YQvM8uw7MicelA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果想在试验阶段从这个语言模型取样词，我们可以在每一时刻 t 贪婪地选择最高概率的词 p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})，或者用定向搜索。举个例子，我们可以用它来生成像运用了 LSTM 作为解码器的 Karpathy 的 Char-CNN 中的任意文本序列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;经典神经语言模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2013 年 [1] 提出的经典神经语言模型包含一个前向传播神经网络，它有一个隐藏层，用来预测文本序列的下一个单词，如图 2 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWfEyQxhGahfuuZq0geAAop5IoibjhibdaeqibXfHhVz28kpKGRnlOAb0QQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2: 经典神经语言模型（Bengio 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型的最大化目标函数就是我们在上文中介绍的典型的神经语言模型的目标（为了简洁，我们忽略规范化（regularization）这一项）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWn1icJveGOrgts4xhvSSibJPX0fpAldGN2koShDuqkmrbibXjyicWSj2uqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;f(w_t , w_{t-1} , \cdots , w_{t-n+1}) 是这个模型的输出，即 softmax 计算出的概率 p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})。n 在这里就是传入这个模型的之前 n 个词的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人首先提出词嵌入模型，它是一个在实数范围 R 内的词特征向量。他们的架构非常经典，是目前各种改进方法的原型。他们原始模型中的基础模块依然能在现在的许多神经网络和其他词嵌入模型中找到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;嵌入层：一个用索引向量和词嵌入矩阵相乘得出的词嵌入层；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;中间层：一个可包含一层或多层的中间层，例如，一个可以将之前出现的 n 个词非线性地组合在一起的全连接层（fully－connected layer）；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Softmax 层：一个最终层，输出词汇库 V 中词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，Bengio 等人发现了目前最先进模型中存在的两个核心问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现 2. 中间层可以由一个 LSTM 替代，这个已被最新神经语言模型使用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现最后的 softmax 层（更确切地说，是标准化项）是神经网络的瓶颈，因为计算 softmax 的计算复杂度与词汇库 V 中词的数量成正比，而个数量通常为成百上千，乃至几百万。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，如何在一个大词汇库中用较低的计算成本计算 softmax，成为了建立神经语言模型和词嵌入模型的一个关键挑战。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Bengio 等人对神经语言模型的的最初探索以后，计算机计算能力和算法还尚不允许在大词汇库上的训练。词嵌入模型的研究因而止步不前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Collobert 和 Weston [4]（因此被称为 C&amp;amp;W）在 2008 年展示了词嵌入模型在一个充分大的数据库中如何向下游任务携带语法和语义，并且提升性能。他们 2011 年的论文充分解释了他们的做法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的解决方法避免了对于 softmax 层的昂贵计算，其方法是采用另一个目标函数：&lt;/span&gt;&lt;span&gt;Collobert 和 Weston 的神经网络输出是正确词序列相对于不正确词序列高出的分数 f_θ，而不是 Bengio 等人的论文中用来最大化基于之前的词出现的下一个词概率的的交叉熵标准。他们为了这个目标函数采用了一个成对排名的标准，如下所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWAEQonuI5CpSO632Pmn7be0eQeRkf9wxDG3Rf3qTE4OU0XG8pBXibW1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型从所有包含 n 个词的窗口 X 中取样得到正确的窗口 x。对于每一个窗口 x，用 V 中的 w 代替 x 的中间词来产生一个不正确的版本 x(w)，而模型的目标就是最大化模型对于正确的窗口和不正确窗口的分数的距离。如图 3 所示，他们的模型架构类似于 Bengio 等人的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW8HspRpB0o7WThGibaVeZbbtN4BoNOMMVcDXCKN86QIlsDbACKyrR1mA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3: 去掉了排名目标的 C&amp;amp;W 的模型（Collobert 等人，2011 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算后的语言模型能够生成具有许多语义关系的词嵌入，例如国家能够聚类在一起，语法上接近的词在向量空间上相邻。他们的排名函数避免了 softmax 的复杂计算，且保持了 Bengio 等人论文中计算同样昂贵的完全相连的中间层（2.）（见图 3 中的 HardTanh 层）。他们对于 130000 个词的模型需要花费 7 周来训练的有一部分原因在于此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们现在介绍当今毫无疑问最为流行的词嵌入模型 word2vec，它源于 Mikolov 等人在 2013 年中两篇论文，且催生了上千篇词嵌入的论文。正因为词嵌入模型是自然语言处理中深度学习的一个关键的模块，word2vec 通常也被归于深度学习。然而严格上来说，word2vec 并不属于深度学习，因为它的架构并非多层，也不像是 C&amp;amp;W 模型一般运用非线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在他们的第一篇文章 [2] 中，Mikolov 等人提出了更低计算成本的学习词嵌入的两个架构。他们的第二篇论文 [3] 通过加入更多的提升了训练速度和准确度的策略来提升了模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些架构提供了对比于 C&amp;amp;W 模型和 Bengio 模型具有如下两大优点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们去掉了昂贵的中间层。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们运用语言模型来更多地考虑上下文。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们等等会讲到，他们的模型之所以成功不仅是因为这些改变，而更是因为某些特定的训练策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们会来看这两个架构：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;连续的词袋（CBOW）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;言模型只能通过观察之前出现的词来进行预测，且对于此类模型的评价只在于它在一个数据集中预测下一个词的能力，训练一个可以准确预测词嵌入的模型则不受此限。Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词，见图 4。他们称这个模型为连续的词袋（continuous bag-of-words，或者 CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWXSGmO8jAuKFwv9zmrvYCplt3EGa7zzYUGHNfQskm1FyDxLmnXaYTAg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4：连续的词袋（Mikolov 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 的目标函数和语言模型仅有着细小差异：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWbYXukF6Eicj6IqicqHRgFonWBLHib9ef3NasDyN9wnCQ4l4RlkMicmG40A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型并没有传入 n 个之前的词，而是在每个时刻 t 接收目标词的前后 n 个词的窗口 w_t。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词，如图 5 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWzfBaiaf9dNJkKtYCspNhMjnHj0TXRYMVB327oc96B6RaHqKjS1v63lQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5：Skip-gram（Mikolov 等人，2013）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;skip-gram 模型的目标因此用目标词前后的各 n 个词的对数──概率之和计算如下的目标：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWnjIBuKqBgQnAibFvxvM3Z24AB5qH8T05e50nw1JnoobMSKDOWQubIIQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更直观地解释 skip-gram 模型是怎样来计算 p(w_{t+j}|w_{t}) 的，让我们先来回顾 softmax 的定义：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWVT2D38N3IOCZzfl2WicwaOyg7zEgQjyvOc4ooZeR8uVAP3McspuxAOQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不计算目标词 w_t 基于前面出现的词的概率，而是计算周围词 w_{t+j} 对于 w_t 的概率。于是，我们可以简单地替换掉这些变量：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWiaNWtVic3M07x51F1hiaVYfLhG0I8CPG2he33YQ5vvs5FibCoV9B3FCKOg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 skip-gram 架构并不包括能够产出中间状态向量 h 的中间层，h 自然地成为对于输入词 w_t 的词嵌入 v_{w_t}。这也是我们为什么想给输入向量 v_w 和输出向量 v′_w 以用不同的表示，因为我们想要将词嵌入和自己相乘。用 v_{w_t} 替换 h，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWqDibSBVj9jFLGxEAZ0X8RNkYvpchHuCRoC7vqibjec11SjJibz7Muk8vg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 Mikolov 论文中的代号和我们的有细微差别，他们标注词语为 w_I，而周围的词为 w_O。如果我们用 w_I 替换 w_t，用 w_O 替换 w_{t+j}，然后根据乘法交换律交换内积的向量位置，我们能够得到和它们论文中一样的公式表示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWHicxcOHpT6EMTyHuBUU4GnxZtQMzY0FsHicc2QqzZHmQ5BhnWcqyl1Ow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一篇博文，我们将要讨论近似昂贵的 softmax 函数的不同方式，以及令 skip-gram 成功关键的训练决策。我们也会介绍 GloVe[5]，一个基于矩阵乘法分解的词嵌入模型，并讨论词嵌入和分布式语义的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;References&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Bengio, Y., Ducharme, R., Vincent, P., &amp;amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. http://doi.org/10.1162/153244303322533223 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. http://doi.org/10.1145/1390156.1390177 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp;amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from http://arxiv.org/abs/1103.0398 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models, 12. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Geoffrey Hinton最新论文：使用快速权重处理最近的过去</title>
      <link>http://www.iwgc.cn/link/3189611</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ArXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWmdMgiakHgSDWrrR1KsllMs0SZRo0CRAicTL5pibUnjfKXadSrw1If8BSw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;直到最近，对人工神经网络的研究在很大程度上还局限于仅有两种类型的变量的系统：代表当前或最近输入的神经活动以及学习获取输入、输出和收益（payoff）之间规律的权重。这种限制并没有什么好的存在理由。突触拥有在许多不同时间尺度上的动态，这表明人工神经网络可能能够受益于变化速度比行为（activity）慢但比标准权重远远更快的变量。这些「快速权重（fast weights）」可以被用于存储最近过去的临时记忆（temporary memories），它们能为过去（past）提供一种在神经角度上可行的实现这种类型的注意（attention）的方式，而且最近研究已经证明过去（past）在序列到序列（seq2seq）模型中非常有用。通过使用快速权重，我们可以避免存储神经活动模式的副本的需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWibYsQe9NyibQdMSNEOTYcWnD8iaITyibjTp9QeBY3D76iaccRSHBy1U7I8Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：快速关联记忆模型（fast associative memory model）；其中，红色线：持续边界条件，灰色线：慢速过渡权重，蓝色线：快速过渡权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWa8rshQMcNGvLF8FyKS0906lZTsCm9u4Il5SV6eU8ZAtlH2aHcyfedA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3：多层快速关联记忆模型（multi-level fast associative memory model）；其中，灰色虚线：更新快速权重和清空隐藏状态，红色线：持续边界条件，灰色实线：慢速过渡权重，蓝色线：快速过渡权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>专题 | 脑芯编：窥脑究竟，结网造芯</title>
      <link>http://www.iwgc.cn/link/3189613</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;9月份时，&lt;/em&gt;&lt;em&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=3&amp;amp;sn=e07ccb2264f7d003d6c81bfa6e5a91dd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=3&amp;amp;sn=e07ccb2264f7d003d6c81bfa6e5a91dd&amp;amp;scene=21#wechat_redirect"&gt;机器之心曾发文章宣布「机器之心」和「矽说」将共同推出系列文章「脑芯编」&lt;/a&gt;，揭秘类脑芯片的过去、如今与将来。本文是此专题的第二篇，窥脑究竟，结网造芯。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;〈二〉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几重卷积几重生&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;蜘蛛结网，是为了捕食昆虫；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;蜘蛛侠结网，是为了拯救世界；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;码农 Data Scientist (~ds~) 结网，是为了——&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;换一个角度看世界，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英语叫做： Representation。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你只想知道一个关于神经网络的常识，我认为上面这个单词是最不应该错过的。就像每个学模拟电子学的人，其实归根结底就是学了两个字——放大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;话接上回，我们说到，通过一系列乘累加和非线性激活函数，我们就可以实现一个神经元。而关键的问题就是如何把神经元们连起来。解决这个问题之前，我们先要明白神经网络的作用——通过一系列线性和非线性的变化重新将输入信息映射成为表达事物的本质的简化特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你觉得上面一句的每个字都认识，却不知道他在说什么，那么我们来看一个经典的例子——人类的视觉皮层（Visual Cortex）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;视觉皮层, 一场生物与 AI 的伟大握手&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;码农老师的生物课又来了……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你有没有想过当你看到生命中一个重要的人的时候，比如说基友（码农怎么会有妹纸？），你是看到是他/她的鼻子，眼睛，脸上的痘痘，昨晚熬夜的黑眼圈……但是这些东西最后都只留下了一个映像——我面基了。可是你有没有想过从你看到图像，到你得到的结论，无数的信息都已经没有过滤，你的脑子完成了一次将 4K3D 图像压缩成两个字的过程，到底发生了什么事？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWT15dRSNa56DWETYtTSrwiaLpZutAYVKw9JMr8RsjI0BVs5BjDeLcH6Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个过程就是从信息经过视觉皮层（神经网络？？）的过程。从前到后，他经过了几站：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）始发站——视网膜，比较像是一个电子系统的传感器，用来接收信号；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）快速交流道——LGN，他是将左右眼看到的信号重新编码后传递给视觉皮层，像是一个电子系统中的主控处理器与总线（请原谅我不说 LGN 的中文，因为说了你也记 不住）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（3）第一站——主视觉区 V1，第一层神经网络，司「边界检测（Edge Detection）」一职，这可能是神经元数量最丰富的一个区域；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（4）第二站——次视觉区 V2，第二层神经网络，司「基础特征提取」一职，归纳视觉信号的形状、大小、颜色、频率……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（5）第三站——V3，司「位置「，也是个过渡区，一条线上你有些站你不知道为什么会停~&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（6）第四站——V4/V5（MT）分支，深度神经网络，各司「色彩/运动」；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（6）V4 分支终点站 1——换乘 inferotemporal Cortex，近深度智能 TE 区，司」目标识别「~~~终于终于我认出基友来了，撒花~~&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（7）V5 分支终点站 2——换乘 Parietal Cortex, 进深度智能 MST 区，司「空间运动分析」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视觉皮层可能是目前为止人类认识的最透彻的大脑部分，不过，好像建立在无数的活体实验上。。。即使如此，还是有很多未知的空间亟待生物学家探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWwwja1MeVVSAz7EAFibpTn3almeJ8AfXIVub43z44a7zqyOy4ePX3OSg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道读到这里，对人工智能略有了解的你有没有觉得这堂生物课在哪里见过？先做边界检测，在再做特征提取，在进行分类识别，这不就是大名鼎鼎的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C N N&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LeNet/VGG/AlexNe&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;t&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积，让加速成为一种可能&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实在神经网络领域里，目前为止唯一能算的上前所未有成功的就是 CNN（Convolution Neural Network，卷积神经网络）。最早的 CNN 可以追溯到 98 年 Yann LeCun 的一篇如何识别手写数字的 paper，这里出现了第一个 CNN 的雏形 LeNet：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWbL3Qg3I7hgicawo40gfeIfJ8McqGhfdHUXNHPW0YTEROwS2VibVCK0wg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从结构上来，CNN 继承了视觉皮层中对信号处理「层」的概念，虽然不是那么的 100% 的吻合，但是 CNN 的初级层往往用来做「边界检测」这样的简单的特征提取，而在深度层重新组合初级层的信息成为抽象的再表达（Representation）, 最后交给事件的发生的相关概率归纳出事物的本质。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，一个比较不太准确的趋势是神经元的数量随层的深度逐渐减少，但是单个神经元的粗壮程度（输入数量）随层的深度逐渐增加。视觉皮层也具有相似的规律，V1 的数量多，但是结构比较简单，但到了 V4/V5，链接变得很复杂，但占的区域却比 V1 小的多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，这些都不做电路的人重点。对于硅工们而言 CNN 获得巨大成功的原因在于&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;它极大地节省了神经网络的硬件开销&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使神经元为单位作加速器成为了可能&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（1）CNN 定义了一种更高能效的元操作——卷积核&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于卷积是什么，大家可以去参考一篇《一文读懂卷积神经网络》（广泛地转载于各大公众号间），下图是我目前看到的最形象的卷积描述。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWK7vTTOjapGK5wbWTW6SrYZ0ZNtib4GuVqgPlMt0sv5C8TW9olVtUqUg/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;该图片源自网络，感谢原 gif 作者&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其本质就是对于一个区块，判断和自己系数组成的「基」区块的相似程度，得到的分数越高就越相似。这样，当一种「基区块」被赋予一种特征是，即使用于整张图片的特征提取，他的系数也是固定的，因此大量的系数加载操作可以被省略。同时，一个固定大小的「卷积核」成为了比「乘累加」更高阶、更高效的原子操作，在现代计算机体系结构中，实现越复杂，但操作越固定的加速器，其效率和速度的提升也就越大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（2）Pooling——是垃圾就要扔掉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNN 网络的另一个巨大贡献就是在卷积层和层之间，设置了一个」垃圾箱「，把上一层产生的无效信息都扔掉，避免了超大规模的数据传输和存储。大家把这叫做 Pooling，我又要来吐槽那个中国人给他取了个」池化「的名字，虽然我也找不到更好的名字，但根本无法帮助理解。Pooling 的策略很多，最常见的是 max pooling 就是留个最大的，然后又其他都扔掉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（3）「乱撸」？(ReLU)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet 后期发展到 AlexNet 后，激活函数也从 sigmoid 变成了 ReLu，他们的图形曲线大概如下所示。用脚趾头也知道，Relu 操作的实现就是把符号位为负置 0 就好了。至于 sigmoid 么，传承自经典机器学习回归理论，是 e 指数的除法操作，编译后简直就是一场噩梦，我们先把他当作一个古老的神话就好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWoK8EnmXGTcAaFuas4H1ER7IUZ7KTYhCW3OdLshGvU8Csj3pMMwYnbQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以上种种硬件实现的简化，加上 CNN 的巨大胜利，都当让硅工们看到了直接从电路角度优化的角度切入人工智能芯片的可能。但是，也发现了一个问题，传统的硬件加速的算法往往是做死的，比如椭圆加密，浮点乘除等等。但是 CNN 的元操作——卷积核——虽然模式固定，但是其每一层的卷积核数量和层数却是纷繁复杂，固定的硬件并不能实现网络的可塑性（structural plasticity）？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那该怎么办？下一次，我们先来回顾下 CPU 的发展历程，看看现代处理器的」形与令「。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「几重卷积几重生」——你看懂神经网络的卷积了？还有「重生」呢~限于篇幅，只能等番外了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就到这里，且听下回分解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;〈三〉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梦里不问形与令&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;世界上有两种管家&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种是 Batman 的 Alfred&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;能服务能做饭能伪装能打架&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;狠起来超人也不是干不过&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种是天朝的大内总管&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;掌印秉笔，啥事不会&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;老大又吩咐了就去传个话&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你脑子里的 CPU 是哪一种？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了神经元，知道了怎么把神经元连成网络，这个系列终于进入了主题——怎么实现神经网络。如果在这个问题上加一个条件，那就是「怎样用芯片实现神经网络的计算」？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在回答这个问题以前，让我们先去拜访两位长者——Alan Turing 和 John Von Neumann，目前大家公认的计算机之父。话说前者才是真的「苟利国家生死以，岂因祸福避趋之」，详见卷福主演的奥斯卡获奖电影《模仿游戏》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWYKQNNibibtjsU0Fr8SPLcEYZRq1G7vOteOgHG7rx1HV5l8vYE4ogjbOA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图灵-冯-诺依曼架构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了表达对大师的尊敬，我起了个很干脆的标题。大师之所以是大师，是因为他们定义了在 80 年前定义了通用计算机的数学模型和体系结构。在这过去的 80 年里，任何试图推翻这些结构的「投机」分子几乎都没什么好下场。但是，总有人希望推翻这个架构。先简单的描述下两位长者干了什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alan Turing 在 1936 年提出了一种具有普适性的逻辑计算模型，证明通过有限状态机完成输入数据的操作，可以实现任意复杂的逻辑运算。图灵机本身描述的场景在现在看来已经没什么意义，但是他第一次完整的定义普适计算机体系机构——一卷很长很长的带子（infinite length tape）通过一个有磁头 (head) 的有限状态表 (finite state table) 进行读取/处理/改写的机器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWZlxUNuDnMicZrjNwgpdYYAZ4l9blJfsNHSbwEClR87PjtNlGIX4JMdg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图灵机：带子、磁头和状态机&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9 年后，Von Neumann 把带子改叫做「Memory」，状态表叫做「CPU」，磁头改叫做「Connection (Bus)」，换了一副图，就有了史称「冯诺依曼架构」的现代计算机体系结构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWd3yyvtt5j4qR7vCmv1lEjx7ajicialS3DzoX6y0gM8L9BmWEIxEmGtLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;教科书上会说这个结构有啥特点，这是让你背的。其实很简单，图灵-冯诺依曼架构最大的特点是把计算任务分为了 2 个部分——数据存储 (memory) 和数据处理 (processor)。处理器几乎不能存数据，存储器几乎不能算数据。两部分用一种连接方式 (bus) 按一定规则通信。泾渭分明的特点让冯诺依曼架构处理事情起来特别有条理，就像「男主外女主内」的家庭角色分配一样，在硬件资源极度受限的情况下，成为了自动化发展的中坚力量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;冯诺依曼架构有一个升级版，叫做哈佛 (Harvard) 架构，把存储空间分为了指令 (instruction) 存储和数据存储，对应不一样的操作。目前的主流嵌入式微处理器基本采用这个架构，但 Anyway 这并不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;冯诺依曼架构在过去的 60 年称霸人间，如果这项专利申请成功的话，这一定是史上最赚钱的专利。可是，冯诺依曼架构在经历了各种法院撕逼后，被判定为一项没有收益人的专利……（Youyou Tu 和青蒿素在这面前简直不值一提）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;成也萧何 – x86 的不可一世&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然冯老爷子在自己的架构下发明了人类第一台计算机，ENIAC 和 EDVAC，但诺依曼的真正崛起还是要归功于 x86。如果你不知道 80x86 是什么，那只能说明我们已经有代沟了，嗯，很深深的代沟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Intel 自 1978 年推出 8086 后，x86 体系架构就一直是电脑（上到服务器，下到平板电脑）核心处理芯片的不二选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWULoPDCciaO6vyyJMfNl4YDVqzmsWWmKaq3JicIToq6yl55tB6GNXXXAg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Intel x86 i7 版图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;顺便做个普及，在冯诺依曼架构下，每个处理器会干的事情是有限制的，通常这个限制叫做指令集。它规定 CPU 的基本操作，没有指令集 (instruction set) 定义的复杂操作可以通过基本操作的组合来完成，比如指令集里没有乘法，那我们可以通过一定数量的加法来完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在冯老爷子的机构里，谁的指令集越大，可以访问的存储空间越大，谁就越牛逼。x86 的指令集从 8086 到 i7 不断扩张与膨胀，最终成为了一个会算双精单精、矢量图像，多核多线程多 Cache 的巨无霸。简单的说，到 2013 年的时候，史上最强 core 已经无所不能了。可是历史不断在重演一幕就是，当绝顶高手号称要独孤求败的时候，不知道哪里窜出来的毛小伙子可能一个起手式就把你撂倒了。圣经里大卫王这么干掉了 Goliath，《倚天屠龙记》里，张无忌这么称霸了光明顶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那谁是 x86 的张无忌呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;移动设备，RISC 的春天&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;独孤求败的 x86 其实有个致命的缺陷——能效，通俗地说就是「做一次」要花费的能量。可是每块肌肉都很发达的 muscleman 总是要比一般人多吃几碗饭吧。我们现在能买到的 i7 即使在省电模式也要消费超过 47W 的功耗。本省 47W 不算什么，但是苹果乔大叔的出现，让 47W 一下子很麻烦。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Iphone/Ipad 和一系列手持的充电设备对瓦级以上的功耗是非常敏感的！x86 的功耗导致它「充电 2 小时使用 5 分钟」的悲惨结局。肌肉男瘦身变成筋肉男的必然的命运。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这时，x86，或者说是 intel 的张无忌出现了—ARM Cortex RISC. 所谓 RSIC 就是精简指令集（Reduced Instruction Set），他能干的事情很有限，但是他的功耗低。X86 在其巅峰时期无数次地战胜过 RISC，以至于 ARM 出现时并有没足够重视他，那时候 Intel 还在和 AMD 抢 64 位 x86 的主导权呢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么无数次败下阵来的 RISC 可以最终成功呢？因为这次，他寻找到了一个 partner——加速器。在移动端的应用设备里，其实也有很对需要强大计算消耗的进程，这是 RISC 本省无法胜任的。但是，实际应用上，往往这些进程是有固定的模式和使用场景的。比如手机在通话时的语音编解码，拍照时的图像 处理（俗称「美颜」）和无线通信是的编解码。对于这样一个经常重复，且模式固定的高通量计算，可以在总线上加入一个专用模块（ASIC）加速，在处理专用任务是 ASIC 的能效又比通用处理器高很多。下图就是 ARM 有名的产品之一 A9，除了 CPU 外，它的浮点与超标量计算（NEON）都被移到了 CPU 外（一般来说，这不能算作加速器）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW1iaicRxC3bflgneKa5sKl1oFZfzYpoJPdMtNvh0ibv5zwbPqsh2f692UA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是开头的那个故事，你每天充的电不够「超人」吃的，与只能换个块头小，但是能够指挥其他人的总管。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;败也萧何 – 冯诺依曼瓶颈&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「泾渭分明，靠总线连」的冯诺依曼架构带来了单核/少核时代计算机的春天，但冯诺依曼架构的致命缺陷——冯诺依曼瓶颈——也悄悄地增长。随着摩尔定律的发展，远距离的搬移大规模数据早已取代了计算本身，成为制约高效计算的重要瓶颈，对于 x86 结构，有太多指令可以直接穿过总线访问存储空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 RISC+加速器的体系结构里，总线作为「总管」和「内务府」、「上书房」、「御膳房」间的桥梁，更是好不吃紧。当瓶颈出现在通信上时，冯诺依曼架构就体现出了它垂垂老矣的一面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个问题，在实时处理的人工智能场景下显得格外突出，信号从入到出，都是按照是数据流 (Data flow) 的传输模式一帧一帧地来。这一特征在类脑的神经网络实现中就更加明显。如果每一个卷积的系数都要去云深不知处的存储海洋里去寻找，那神经元的处理效率会非常低。简单地说：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谁脑子 TM 的是一半纯记忆&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一半纯分析的呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;脑子么，都是左右开工的。边走边忘，雁过留痕，却也是旧相识，恢复不出来一个一毛一样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，摆在类脑芯面前的路有三条：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）	采用冯诺依曼经典架构，把神经元计算写在指令集里，反正是超人，技多不压身；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）	采用 RISC+神经元/神经网络加速器，给「总管」再开个府呗；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（3）	放弃冯诺依曼架构，完全分布式硬件，像「数据流「一样的风骚走位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这三个选项都有不错的代表，我们慢慢来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梦里不问形与令，你知道计算机形（体系结构）和令（指令集）了么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心发布，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>资源 | 微软官方整理：用于Azure机器学习的免费数据集</title>
      <link>http://www.iwgc.cn/link/3180600</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Microsoft&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lee Scott&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDh403YYkBnSPp4nibgKKgTKBLickrN16xHFp5ibEKAvxa8ibiaKmLaElMQFg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要学习怎么使用微软 Azure 机器学习，最重要的是获取样本数据集和进行实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在微软，我们有大量的样本数据集可用。这些数据集已经在 Azure Cortana Intelligence Gallery 中的样本模型中得到了应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中一些数据集可以通过 Azure Blob 存储获取，所以可以直接链接到 Azure 机器学习实验；而其它的数据集则是以 CSV 格式提供的。下面列出的这些数据集都将提供直接的链接。你可以通过 Import Data 模型在你的实验中使用这些数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些数据中的剩下数据集都列在模块（module）面板中的 Saved Datasets 下；当你在 ML Studio 中打开或创建一个新实验时，你能在实验画布（experiment canvas）的左边看到它们。你可以直接将这些数据集拖拽到实验画布而将它们应用到你自己的实验中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下列出了一些可以免费使用的数据集：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;成年人收入普查二分类数据集&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个 1994 年的普查数据库的子数据集，使用了 16 岁以上的工作年龄的成年人的数据，其带有一个经调整之后大于 100 的收入指数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用人口学信息对人进行分类，以预测一个人年收入是否超过 5 万美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Kohavi, R., Becker, B., (1996). UCI Machine Learning Repository Irvine, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机场代码数据集（Airport Codes Dataset）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美国机场代码&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含每个美国机场，提供了机场 ID 编号和名字，以及机场所在的城市和州。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;汽车价格数据（Automobile price data，原始数据）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;按厂家和车型分类的汽车信息，其中包括价格、气缸数量和 MPG 等特征，以及保险风险评分（insurance risk score）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个风险评分最初是与汽车价格关联的，后来根据实际风险在一个被精算师称为符号化（symboling）的过程中进行了调整。+3 的值表示该汽车是有风险的，而 -3 的值则表示它可能是相当安全的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用回归或多变量分类，根据特征预测风险评分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Schlimmer, J.C. (1987). UCI Machine Learning Repository Irvine, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;自行车租赁 UCI 数据集（Bike Rental UCI dataset）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;UCI 自行车租赁数据集基于来自 Capital Bikeshare 公司的真实数据，该公司在华盛顿特区运营着一个自行车租赁网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含 2011 年和 2012 年每一天和每一小时的数据，总共有 17379 行。每小时租赁自行车数量的范围在 1 到 977 之间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Bill Gates RGB Image&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;已转换成 CSV 数据的公开可用的图像文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用于转换该图像的代码提供在使用 K-均值聚类模型的颜色量化（Color quantization using K-Means clustering model）的详情页面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;献血数据（Blood donation data）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个来自台湾新竹市输血服务中心献血数据库的一个子数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;献血者数据包括献血频率、总献血次数、自上次献血以来的时间和献血量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：目标是通过分类预测献血者是否在 2007 年 3 月献血，其中 1 表示目标区间内的一个献血者，0 表示没有献血者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Yeh, I.C., (2008). UCI Machine Learning Repository , CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;亚马逊网站的书评&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由宾夕法尼亚大学研究者采集（地址：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.cs.jhu.edu/~mdredze/datasets/sentiment/）。-参见论文《Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification》，来自 John Blitzer, Mark Dredze, and Fernando Pereira; 计算语言学协会 (ACL), 2007-&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原来的数据集包含 97.5 万条包含 1、2、3、4、5 评分的书评。这些书评都是用英语写的，截取自 1997-2007 年这个时间段。这个数据集已经被下采样成了 1 万条书评。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌数据（Breast cancer data）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由 Oncology Institute 提供的三个与癌症相关的数据集中的一个，其常常出现在机器学习文献中。结合了来自对大约 300 种组织样本的实验室分析的特征的诊断信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：基于 9 种属性分类癌症类型，其中一些是线性的，一些是按类别划分的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Wohlberg, W.H., Street, W.N., &amp;amp; Mangasarian, O.L. (1995). UCI Machine Learning Repository, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌特征（Breast Cancer Features）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了来自 X 射线图像的 10.2 万个可疑区域（候选项）的信息，其中每个区域都用 117 个特征进行了描述。这些特征是专有的，而且它们的含义没有被该数据集的创造者（Siemens Healthcare）揭示出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌信息（Breast Cancer Info）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了来自 X 射线图像的可疑区域的额外信息。每个样本都提供了对应 Breast Cancer Features 数据集行数的信息（如，标签、病人 ID、图像块相对于整张图像的坐标）。每个病人都有很多样本。对于患癌的病人来说，一些样本是积极的，一些样本是消极的。该样本有 10.2 万个样本。这个数据集有偏置的，其中只有 0.6% 的点是积极的，其余都是消极的。该数据集由 Siemens Healthcare 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Appetency Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_small_train_appetency.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Churn Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_small_train_churn.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Dataset Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的数据：http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction%20-%20orange_small_train.data.zip&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含来自法国电信公司 Orange 的 5 万个客户。其中每个客户有 230 个匿名的特征，其中 190 个数值特征和 40 个类别特征。这些特征是非常稀疏的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Upselling Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_large_train_upselling.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;能效回归数据（Energy Efficiency Regression data）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于 12 种不同的建筑外形收集的模拟能量分布。这些建筑按照 8 个特征进行了区分，比如：玻璃窗面积、玻璃窗面积分布和取向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用回归（regression）或分类（classification）来预测能效等级，其给出的两种响应是有实际价值的。对于多类别分类，响应变量被取舍到了最接近的整数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Xifara, A. &amp;amp; Tsanas, A. (2012). UCI Machine Learning Repository Irvine, CA:加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;航班延误数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自美国交通部收集的 TranStats 数据集中的乘客航班正常率数据。该数据集覆盖 2013 年 4 月到 10 月的统计，在上传到 Azure ML Studio 之前，该数据集处理如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该数据集经过过滤只覆盖美国本土的 70 个最繁忙的机场&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;废除了标记显示延误超过 15 分钟的航班&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;转航班数据也被消除&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;选择使用数据目录如下：Year, Month, DayofMonth, DayOfWeek, Carrier, OriginAirportID, DestAirportID, CRSDepTime, DepDelay, DepDel15, CRSArrTime, ArrDelay, ArrDel15, Cancelled&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;美国 2011 年 10 月飞机到达与离开的记录数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测航班延误&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：来自美国交通部的 http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;amp;DB_Short_Name=On-Time.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;森林火灾数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含来自葡萄牙东北部的天气数据，比如温度、湿度指数和风速，结合与森林火灾的记录。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：这是一项很难的回归任务，目的是预测森林火灾焚烧的地区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Cortez, P., &amp;amp; Morais, A. (2008). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[Cortez and Morais, 2007] P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 – Portuguese Conference on Artificial Intelligence, December, Guimarães, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9. 地址：&lt;/span&gt;&lt;span&gt;http://www.dsi.uminho.pt/~pcortez/fires.pdf.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;德国信用卡 UCI 数据集&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;UCI Statlog（德国信用卡）数据集（Statlog+German+Credit+Data)）使用了 german.data 文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集通过一系列的属性进行表述，根据人进行分类，每个样本表示一个人。此数据集中有 20 个特征，都是数字和类别，以及二元标签（信用风险值）。高信用风险标记为 2，低信用风险标记为 1。将低风险样本误分类为高风险的成本是 1，反之误分类高风险的成本是 5。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;IMDB 电影&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含 Twitter 上评估的有关电影的信息：IMDB 电影 ID、电影名和流派、生产年。该数据集中有 17K 的电影。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;鸢尾花两级数据&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在模式识别文献中，它可能是最知名的数据集。该数据集相对较小，包含来自三个鸢尾属植物分类的每种花瓣测量的 50 个样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：从测量中预测 iris 的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Fisher, R.A. (1988). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;电影 Tweets&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集是 Movie Tweeting 数据集的扩展版本，此数据集有 170K 的电影评估信息，从结构较好的 tweets 中提取。每个示例代表一条 tweet，数据元组：用户、IMDB 电影 ID、评估等级、时间标记、该 tweet 的点赞人数、转推人数。该数据集由 A. Said, S. Dooms, B. Loni and D. Tikk for Recommender Systems Challenge 2014 供用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;汽车MPG数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集是由卡耐基梅陇大学 StatLib 库提供的数据集的修正版本，此数据集曾被 1983 年 American Statistical Association Exposition 使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据列出了每加仑汽油各种类型机动车的消耗情况，同时也包含气缸个数、引擎排放量、马力、总重量和加速这样的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通途：基于 3 个多值离散属性和 5 个连续属性预测节约燃油。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：StatLib, Carnegie Mellon University, (1993). UCI Machine Learning Repository &amp;nbsp;Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Pima 印第安人糖尿病二进制分类数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 National Institute of Diabetes and Digestive and Kidney Diseases 数据集的一个子集。该数据集经过过滤只关注 Pima Indian 遗传的女性病人。数据包括血糖、胰岛素水平、生活方式这样的医疗数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测该主体是否有糖尿病（二分类）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Sigillito, V. (1990). UCI Machine Learning Repository」. Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;餐馆消费者数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一系列关于消费者的元数据，包括人口统计学和喜好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并测试推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Restaurant feature data&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一堆关于餐馆和餐馆特征的元数据，比如食物类型、餐厅风格、位置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并预测推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;餐馆评分数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;包含用户给出的对餐馆的评价，等级从 0 到 2 划分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并预测推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;钢退火多级数据集（Steel Annealing multi-class）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含一系列来自钢材退火实验的记录，数据包含测试钢材类型的物理属性（宽度、厚度、类型（线圈、薄片等））。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测任何二数类属性：硬度或强度，也可用于分析属性间的关联。钢材等级划分遵循一定标准，由 SAE 和其他组织定义。你可以寻求特定的等级，并了解所需要的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Sterling, D. &amp;amp; Buntine, W., (NA). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;望远镜数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;高能量伽马粒子爆发的记录，也带有背景噪声，都使用 Monte Carlo 处理方法模拟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模拟的目的是改进地表大气 Cherenkov 射线望远镜的准确率，使用统计方法微分想要信号（Cherenkov radiation showers）和背景噪声。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据已经过了预处理，以创建一个以指向相机中心方向为长轴的延长的聚类（elongated cluster）。这个椭圆的特征（通常被称为 Hillas 参数）是可以用于判别（discrimination）的图像参数中的一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测 shower 表征信号或背景噪声的天气图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：简单分类准确率对此数据意义不大，因为将背景时间分类为信号要比将信号分类为背景更糟糕。该数据可用来对比 ROC 图应该使用的不同分类器。同时也要注意背景事件（h 代表 hadronic showers）的数量是被低估的，在真实测量中，h 或噪声类代表主要事件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Bock, R.K. (1995). UCI Machine Learning Repository Irvine, CA: University of California, School of Information&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;天气数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 NOAA 的每小时地面天气观测（融合了从 2013 年 4 月到 2013 年 10 月的数据）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这份天气 数据包括了机场天气预报站的观测数据，时间从 2013 年 4 月到 10 月。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在上传 Azure ML Studio 之前，数据集要做如下处理：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;气象站 ID 要映射到对应的机场 ID 上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;与忙碌的 70 家机场无关的气象站需要过滤掉&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;日期按年、月、和天分为单独的列&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要选择的列包括：机场 ID、年、月、日、时间、时区、天空状况（skycondition）、能见度、天气类型、干球华氏温度（DryBulbFarenheit）、干球摄氏温度（DryBulbCelsius）、湿球华氏温度（WetBulbFarenheit）、湿球摄氏温度（WetBulbCelsius）、露点华氏温度（DewPointFarenheit）、露点摄氏温度（DewPointCelsius）、相对湿度、风速、风向、ValueForWindCharacter、本站气压（StationPressure）、气压趋向（PressureTendency）、气压变化（PressureChange）、 海平面气压（SeaLevelPressure）、 记录类型（RecordType）、每小时降雨量（HourlyPrecip）、（高度计）Altimeter&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;维基百科标准普尔 500 指数数据集（Wikipedia SP 500 Dataset）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;源自维基百科的基于标准普尔 500 指数中每家公司的文章的数据，以 XML 格式存储。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在将该数据集上传到 Azure ML Studio 之前，需要进行以下处理：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;提取每家特定公司的文本内容&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;移除 wiki 格式&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;移除非字母数字的字符&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;将所有文本转换成小写&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;已知公司类别已被加入&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意有些公司没有找到文章，所以该记录的数量小于 500.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;可以 CSV 格式下载的数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;direct_marketing.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/direct_marketing.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了关于一项直接邮寄活动的客户数据和关于他们的响应的指示。其中每一行代表一个客户。该数据集包含关于用户人口学信息和过去行为的 9 项特征，以及 3 个标签列（访问、转化和支出）。访问（visit）是一个二元行，表示了每次营销活动后客户的访问；转化（conversion）表示客户购买了一些东西；支出（spend）是指花费了多少钱。该数据集由 Kevin Hillstrom 为 MineThatData 电子邮件分析和数据挖掘挑战赛（MineThatData E-Mail Analytics And Data Mining Challenge）提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;lyrl2004_tokens_test.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/lyrl2004_tokens_test.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RCV1-V2 Reuters 新闻数据集中的测试样本的特征。该数据集有 78.1 万条新闻文章以及它们的 ID（该数据集的第一列）。其中每篇文章都已经 tokenized、stopworded 和 stemmed。该数据集由 David. D. Lewis 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;lyrl2004_tokens_train.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/lyrl2004_tokens_train.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RCV1-V2 Reuters 新闻数据集中的训练样本的特征。该数据集有 2.3 万条新闻文章以及它们的 ID（该数据集的第一列）。其中每篇文章都已经 tokenized、stopworded 和 stemmed。该数据集由 David. D. Lewis 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;来自 KDD Cup 1999 知识发现和数据挖掘工具竞赛（KDD Cup 1999 Knowledge Discovery and Data Mining Tools Competition）的数据集。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集可在 Azure Blob 下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://azuremlsampleexperiments.blob.core.windows.net/datasets/network_intrusion_detection.csv，其中包含了训练和测试数据集。训练数据集有大约 12.6 万行和 43 列，其中包含标签；3 列标签性质信息和 40 列数值与字符串/类别特征信息，都可用于训练该模型。测试数据集有大约 2.25 万个测试样本，和训练数据一样有 43 列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;rcv1-v2.topics.qrels.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/rcv1-v2.topics.qrels.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 RCV1-V2 新闻数据集中的新闻主题分配。一篇新闻可被分为多个主题。每一行的的格式是 1。该数据集包含 260 万个主题分配，由 David. D. Lewis 共享。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;student_performance.txt&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集来自 KDD Cup 2010 学生表现评估挑战赛（student performance evaluation）。这个数据集已被 Algebra_2008_2009 训练集采用（Stamper, J., Niculescu-Mizil, A., Ritter, S., Gordon, G.J., &amp;amp; Koedinger, K.R.（2010））&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 KDD Cup 2010 教育数据挖掘挑战赛中的 Algebra I 2008-2009 数据集可以在该竞赛的网站中下载：http://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集也可以在 Azure Blob 下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://azuremlsampleexperiments.blob.core.windows.net/datasets/student_performance.txt，其中的数据来自于学生辅导系统。其中提供了问题 ID 和简要描述，学生 ID，时间标记，同时还有学生在正确解决问题前的尝试次数。原数据集存储了 890 万条记录，这个数据集减少了取样数量，容量缩小至前 10 万行数据。这份数据每一条目有 23 个不同类型的分项，包括数值、类别和时间戳。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心经授权编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 揭秘微软量子计算研究：拓扑量子计算机</title>
      <link>http://www.iwgc.cn/link/3180601</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Nature&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Elizabeth Gibney&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;微软研究拓扑量子计算已经有十多年了。但是市面上关于其研究的具体内容并不多见。自然杂志采访了微软研究量子结构和计算小组成员 Alex Bocharov，他解释了为什么这家公司在量子计算上会选择一条不同于 IBM、谷歌等竞争对手的路。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一场打造「『通用型』的量子计算机」的竞赛。这种设备可以通过编程实现快速解决传统计算机无法解决的问题。它的出现会对制药、密码等领域带来革命性的变化。世界上多家重要的技术公司都在研究这一挑战，但是微软选择的方式比其对手更加曲折。IBM、谷歌和多家学术实验室选择了相对成熟的硬件，比如超导导线环（loops of superconducting wire），来制作量子比特（qubit）。由于它们具有在同一时间保持在开和关两种状态的混合或叠加态的能力，它们能驱动量子计算机做快速计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是微软却希望能以一种准粒子（quasiparticle）的状态编码量子比特：一种从物质的相互作用中出现的粒子状物体（object）。一些物理学家甚至还不确定微软在做的准粒子（叫非阿贝尔任意子，non-abelian anyons）是否真的存在。但是微软希望利用它们的拓扑性质，这种性质能使量子态对外界干扰具有极强的鲁棒性，来打造所谓的拓扑量子计算机。物质拓扑态的早期理论研究让三位物理学家赢得了今年的诺贝尔奖。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这家公司一直在研发拓扑量子计算，已经有十多年了。现在他们有研究人员为未来的机器编写软件并与学术实验室合作制作设备。Alex Bocharov 是一位数学家和计算机科学家，他微软研究的量子结构和计算组的成员。他对自然讲述了微软的这项研究。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDEWYhn9rUjrGpm9gse4l5P8Oz8OFVEAt10EP3qux38BsVicFuxFQDSmw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Alex Bocharov，&lt;span&gt;微软研究量子结构和计算组成员&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;微软是如何最终决定要做这个拓扑量子比特的？它可能是最难的量子计算硬件的。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们是以人为中心，而不是以问题为中心。和量子计算的领军人物，如 Alexei Kitaev、Daniel Gottesman，最值得注意的是，Michael Freedman，是我们量子计算团队发展的带头人。所以这是 Freedman 自己开拓的视野最终决定了做事情的方式，我们都跟着他。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); text-align: justify;"&gt;&lt;strong&gt;&lt;span&gt;IBM 和谷歌都在使用超导环作为他们的量子比特。你正尝试利用的量子比特是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的量子比特甚至都不是一种物质的东西。但是同样的&lt;/span&gt;&lt;span&gt;物理学家在对撞机中使用的基本粒子不是真正的坚实的物体。我们有的是非阿尔贝任意子（non-abelian anyons），比普通粒子更加模糊。它们是准粒子。被研究的最多的任意子种类出现在非常冷电子链中，而电子链被限制在一个二维表面的边缘。这些任意子的行为既像电子又像它对应的反粒子，它通常以密集的电导峰的形式在一维电子链的两端被观测到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;任意子状的粒子被作为一种独立的物体在 1937 年被首次预测，而且 Kitaev 在 1997 年也说过准粒子可以应用到量子计算机中。但是到了 2012 年，物理学家才首次宣称发现了它们。你可以肯定它们真的存在吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们非常确定这种最简单的物种确实存在。2012 年，荷兰代尔夫特科技大学的 Leo Kouwenhoven 观察到过它们。我不会说有 100% 的确定，但 Kouwenhoven 的观察已经被其他多家实验室再现过。这种激发（excitation）到底是什么并不重要，一旦这种粒子变得可以测量了，它们就可以用来执行计算了。现在的问题是，实验室正在把一些非常复杂的设备放在一起来产生大量的激发（excitation），并尝试开始做计算了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;任意子的开发似乎非常困难。那么相比其他种类的量子比特，使用任意子的优势又在哪里呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在大多数量子系统中，信息被编码到粒子的属性中，与周围环境最轻微的相互作用都会破坏它们的量子态。这意味着他们的操作精确度可能达到了 99.9%，我们成为三个九。在解决现实问题上，我们需要的精确度水平是十个九，所以你需要创造出一个大型阵列的量子比特，能让你来修正这些误差。拓扑量子计算有达到六个或七个九的潜力，这意味着我们不再需要做大量昂贵的误差校正了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;是什么让拓扑量子计算的鲁棒性这么好？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从环境和计算机的其他部分而来的噪音是不可避免的，这可能导致准粒子的位置与强度的波动。但是没问题，因为我们不会将信息编码到准粒子自己身上，但是我们会按顺序交换任意子的位置。我们称之为辫子，因为如果你画出在时间和空间上相邻的任意子对的一个交换序列，那么它们的轨迹线条看起来像辫子。该信息被编码成「拓扑」属性，也就是说，这个系统的集体属性只能跟随宏观运动而不是小波动来变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDcSibrodBZ8PCCnAnsCQYgxx1fZLmEnmCcfclqZS15zCSQTd0VibXiaeaA/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;辫子的数学理论或许可以作为未来拓扑量子计算机的基础&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;微软已经研究拓扑两个字计算十多年了，这其中所需要大多数量子比特都是假设的。为什么你们会坚持到现在？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是值得的，因为它带来的好处是巨大的，几乎没有坏处。微软是一家经济实力雄厚的公司。如果坐拥 1000 亿美元的现金，你会投资什么呢？比尔盖茨也投资了其他东西——根除疟疾和艾滋病病毒——未来这些研究可能都会需要用到量子计算。比如基因学到目前为止一直借助的是传统计算机，而 100-200 个量子比特计算机可能会给基因学研究带来巨大的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;微软有多少人参与了量子计算研究，你们的投入是多少？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大约在 35 到 40 个人，但是我不想冒昧地谈论资金的问题，也给不出大概的估计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;你们的团队一直在为这种量子计算机开发软件，有什么成果吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到目前为止，我们已经有了一个令人惊讶的成果，创造出了一个更有效的算法，它能减少量子比特相互作用的次数，叫做门（gate），只需要运行必要的计算，而这在传统的计算机上是不可能的。比如，在本世纪初的那几年里，人们认为在量子计算机上计算植物在光合作用中用到的铁氧还蛋白能级（energy level）大约 240 亿年。现在通过理论、实践、工程和仿真相结合，最乐观的估计表明，这项计算可能只需要一小时左右。我们还在继续解决这些问题，并逐步转向更多的应用工作，我们想到了量子化学、量子基因学，以及能在一个小到中等大小的量子计算机上解决的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;这算是占领先机吗? 因为一个可以处理这些问题的量子计算机可能是十年以后的事情。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去的问题是，量子计算机在假设上会不会不传统计算机表现更好这样的问题是不是问题。现在我们不仅想弄清楚它是不是可行的，还有如何实现它？我们需要闯过层层迷雾来解开这些问题，因为我们相信它本身将会成为一个完整的领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 消灭网络色情？人工智能并不能做到</title>
      <link>http://www.iwgc.cn/link/3180602</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自engadget&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Christopher Trout&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;去年，Twitter 收购了创业公司 Madbits，根据《连线》的报道，这家公司开发了一种在鉴别危险内容（NSFW）上可以达到 99% 识别率的程序。而在 9 月底，雅虎也开源了&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=1&amp;amp;sn=e99d95427ce67c32323be5310b767b7c&amp;amp;chksm=871b0157b06c88412d9b933805f5ca3a30df745a8ac147f35204e545c250f42e68c57312cbec&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719529&amp;amp;idx=1&amp;amp;sn=e99d95427ce67c32323be5310b767b7c&amp;amp;chksm=871b0157b06c88412d9b933805f5ca3a30df745a8ac147f35204e545c250f42e68c57312cbec&amp;amp;scene=21#wechat_redirect"&gt;其研发的深度学习人工智能色情内容过滤解决方案&lt;/a&gt;。但人工智能真的能够消灭色情吗？（额外的福利：一个训练数据集既可以用来训练识别模型，也可以用来训练生成模型。加利福尼亚大学戴维斯分校的学生 Gabriel Goh 就利用雅虎开源的 open_nsfw 训练了一个色情图像生成模型。警告：此链接的内容可能会引起一些人的不适。）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDL65dPF3hbZCn1zfBrQf0khohTRElUfBiaXMSQQB9N90EP4Kjx5MZWTw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;「我今天不会尝试进一步定义我所理解的这种物质（赤裸的色情），也许我永远都不会成功。我敢说我一看到就知道它是不是，而关于本案的这部电影，则并非如此。」──美国最高法院法官 Potter Stewart&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不能教会电脑我们自己都不明白的东西，但我们不会阻止雅虎对屏蔽色情信息的不断尝试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在1964年，最高法院推翻了对 Nico Jacobellis 猥亵的定罪，他是一位克利夫兰的剧场经理，被指控传播淫秽作品。有问题的那部电影是 Louis Malle 导演的「The Lovers」，讲述了由 Jeanne Moreau 饰演一名家庭主妇，厌倦了她的丈夫和马球比赛，在一个炎热的夜晚和一个年轻人私奔的故事。而说到「热」，我的意思是其中表现了粗重的呼吸，还有乳头一闪而过──准确地说，没有什么你在有线电视上看不到的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用几个简单的字，大法官Stewart 做到了几乎不可能的行为──创造了对于色情的定义：「我见即我知（I know it when I see it）」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美国人对于性的看法自1964年以后出现了很大改变。在 Jacobellis 在最高法院出庭后不久，美国出现了性解放运动，20世纪70年代又出现了色情业爆炸性的发展，而说到最近几年，网络出现了。今天任何能连接网络的人都可以在几秒钟内同时搜出射精和珍珠项链的图片。时间过了很久，我们却面临着相同的问题：我们仍没有对于色情和淫秽的普遍定义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDySjhVicPXgJGSNtvvHvSK0vGBdD5cMkLXkI7QH6hn2e0HnvoStiah3Qg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Jean Moreau 和 Jean-Marc Bory 在「The Lovers」中的表演，其中没那么露骨的画面（图片：Getty Images）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在互联网上人们可以不受限制地访问猥亵，肮脏和疑似猥亵的内容，这种情况催生了各种形式的内容审查，技术上，人们开始使用算法和先进人工智能程序来鉴别和清理色情内容。去年，Twitter 收购了 Madbits，一家小型初创企业，根据《连线》的报道，这家公司开发了一种在鉴别危险内容（NSFW）上可以达到99%识别率的程序。而在9月底，雅虎开源了其研发的深度学习人工智能色情内容过滤解决方案，其他公司也正在做着同样的事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;互联网巨头们近年来一直在投入巨资试图解决色情内容的问题。但他们面临的问题是，审查是一种封闭，淫秽本质上是主观的。如果我们无法对什么是色情达成一致，我们就不能有效地训练我们的计算机「我见即我知」。无论技术的复杂性或界定方式如何，色情识别程序仍需要依靠人类的判断来告诉它们什么是 NSFW。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「有时一个裸体儿童不止是裸体。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在互联网早期时代，美国的图书馆和学校基于搜索关键字过滤色情内容，以使内容满足儿童互联网保护法案（Child Internet Protection Act）的规定。顾名思义，这项条例旨在保护儿童远离互联网上的阴暗内容，特别是「那些在未成年人可以登录的电脑上出现的（a）淫秽；（b）儿童色情；（c）对儿童有害的图片。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但事与愿违。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在2006年纽约大学 Brennan 法律中心的一项报告中，研究者们对早期关键字过滤和其人工智能的后继者有如此的定义：「强势，经常不合理，审查工具」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「过滤器的复杂和无限增加的关键词让人类的表达方式越来越受限制，」报告中说。「它们孤立了单词和短语，让人类的表达价值减退。一个不可避免的结果将是这种方式会让医疗，司法，艺术和很多其他行业的研究遇到限制和阻碍。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这份报告发现目前流行的过滤器令人难以理解地屏蔽了 Boing Boing ，GLAAD，罗伯特·梅普尔索普和第三十届超级碗，同时经常反映了其创作者的政治和社会偏见。雅虎和谷歌已开始使用基于复杂图像识别的人工智能过滤器代替了关键词搜索，然而这些算法仍然依赖人类去教育它们辨别什么是可以接受的信息。而 Facebook 最近也发现，图片过滤比关键词好不到哪儿去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDMxYsfvHNBy6W7icmV1lOd5LOUo1UhRoCF2IKIleu9BkogCyDCGnNkJQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;（图片：美联社）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;社交网络上最近一次大事件正和此事有关。在9月初，Facebook 经历了一次大规模的抗议事件。一张普利策奖获奖照片遭到了 Facebook 新算法的屏蔽，这是一张摄于1972年越战中的照片，其中一名裸体的9岁女孩在凝固汽油弹空袭中哭喊逃亡。Facebook 最初以违反社区规则为由屏蔽了这张照片：「虽然我们认识到这张照片是标志性的，但很难创造一条规则在裸体儿童照片中分辨是否具有色情元素。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但就像《纽约时报》所说的，Facebook 的屏蔽出现后，成千上万的用户在他们的时间线上贴出这张照片以示抗议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「赤裸的儿童形象通常违反我们的社区标准，在一些国家这甚至属于儿童色情内容。但在这一事件中，我们认识到这张照片中存在的历史和广泛的重要性，它记录了一个重要的时刻。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不知道现在 Facebook 的系统中如何标记这张照片，但无论是人类还是人工智能，或者两者一起的审查体系，有一个底线不能被打破：「有时一个裸体儿童不止是裸体。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「有时挂着鞭子的男人并不只是挂着鞭子。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这不是 Facebook 第一次错误地过滤大多数人认为「干净」的内容。社交网络对于系统删除一些女性暴露乳房的照片也怨气颇多，这些照片其实只是一些护理和胸透的照片。最近 Facebook 上的另一课则有关艺术，他们错误地将一名黑人把自己涂成白色身体的裸体视频删除，随后被迫恢复。这名艺术家试图在黑人权利运动中唤起人们对警察暴行的注意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现实世界也充斥着艺术还是色情的争论。在1990年，辛辛那提当代艺术中心举行了罗伯特·马普尔索普（Robert Mapplethorpe）的摄影展览，中心及其主管被指控传播淫秽作品，随后控诉又被取消。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是美国第一次有一家博物馆成为这种官司的被告方，案子中的问题照片──描述了同性恋间的S&amp;amp;M──被共和党拿来成为了全国讨论的话题。控方认为，这次由国家艺术基金会资助的展览存在色情内容，而辩护方将其定义为艺术。这件案子证明了有时挂着鞭子的男人并不只是挂着鞭子。同时也证明了我们接触艺术的权利，无论多有争议，并不是总能获得保证。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的个人偏见仍将继续损害我们获得信息和言论自由的权利，尽管网络过滤系统已有很大发展。我们也许永远无法对 NSFW 的意义达成共识，然而没有普遍的定义，我们的机器只会遵循我们自己的判断进行工作。没有人可以定义我见为何物，也没有代码可以改变这种现状。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
    <item>
      <title>一周论文| 基于翻译模型(Trans系列)的知识表示学习</title>
      <link>http://www.iwgc.cn/link/3180603</link>
      <description>&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;引&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本期PaperWeekly的主题是基于翻译模型(Trans系列)的知识表示学习，主要用来解决知识表示和推理的问题。表示学习旨在将研究对象的语义信息表示为稠密低维实值向量，知识表示学习主要是面向知识图谱中的实体和关系进行表示学习。使用建模方法将实体和向量表示在低维稠密向量空间中，然后进行计算和推理。一般而言的应用任务为triplet classification 和link prediction.自从2013年TransE模型提出后，产生了一系列模型对TransE模型进行改进和补充,比如TransH、TransG等等。本期PaperWeekly主要提供了Trans系列的7篇文章供大家赏读。&lt;/span&gt;&lt;span&gt;paper目录：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、TransE，NIPS2013，Translating embeddings for modeling multi-relational data。&lt;br/&gt;2、TransH，AAAI2014，Knowledge graph embedding by translating on hyperplanes。&lt;br/&gt;3、TransD，ACL2015，Knowledge graph embedding via dynamic mapping matrix。&lt;br/&gt;4、TransA，arXiv2015，An adaptive approach for knowledge graph embedding。&lt;br/&gt;5、TransG，arxiv2015，A Generative Mixture Model for Knowledge Graph Embedding)&lt;br/&gt;6、KG2E，CIKM2015，Learning to represent knowledge graphs with gaussian embedding。&lt;br/&gt;7、TranSparse，AAAI2016，Knowledge graph completion with adaptive sparse transfer matrix。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;TransE:Translating Embeddings for Modeling Multi-relational Data&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;CNRS, Google inc.&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Embedding entities and relationships, Multi-relational data, link prediction&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;NIPS 2013/12&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何建立简单且易拓展的模型把知识库中的实体和关系映射到低维向量空间中，从而计算出隐含的关系？&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;传统训练知识库中三元组(head,relation,tail)建模的方法参数特别多，导致模型太复杂难以解释，并且需要很大的计算代价，很容易出现过拟合或欠拟合问题。而简单的模型在表现上与复杂的模型几乎一样，但更易拓展。TransE的训练过程如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibfS1Z7LMI99jub1DqHU8lsp7kJe9FKboF1wBXJvZs88icPWXZuZicbG2Q/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TransE模型的训练中，第12步是损失函数，对E和L做uniform初始化之后，让正确的h+l-t结果趋近于0，让错误的h‘+l-t’的结果变大，损失函数结果大于0取原值，小于0则取0，这种hinge loss function可以尽可能的将对和错分开，模型使用SGD训练，每次更新可以只更新这个batch里的三元组的向量，因为参数之间并没有冲突。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 WordNet&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/wordnet/download/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 Freebase&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;Code:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;https://github.com/thunlp/KB2E&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文提出了一种将实体与关系嵌入到低维向量空间中的简单模型，弥补了传统方法训练复杂、不易拓展的缺点。尽管现在还不清楚是否所有的关系种类都可以被本方法建模，但目前这种方法相对于其他方法表现不错。TransE更是作为知识库vector化的基础，衍生出来了很多变体。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TransH:Knowledge Graph Embedding by Translating on Hyperplanes&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Zhen Wang1, Jianwen Zhang2, Jianlin Feng1, Zheng Chen2&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Sun Yat-sen University&lt;br/&gt;microsoft&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;knowledge graph embedding, Multi-relational data&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;AAAI 2014&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;对知识库中的实体关系建模,特别是一对多,多对一,多对多的关系。设计更好的建立负类的办法用于训练。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;过去指示图库建模的方法参数过多, TransE在一定程度上解决了这个问题, 但是TransE过于简单，很难对一对多,多对一和多对多关系建模。所以为了平衡模型复杂度和建模效果，TransH将把关系映射到另一个空间（如下图 ）。 注意: 这种想法和Distant Model (Bordes et al. 2011)很相似，但是TransH用了更少的参数， 因为TransH假设关系是向量而不是距离。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="TransH_1" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibpAcQNEgw1IZJ4xnr6LnpkWmBHp78ew7R0Waf7F7PA6LhbiapAshibG7A/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型的一个亮点就是用尽量少的参数对复杂的关系建模。 下图罗列了相关工作的模型以及复杂度。图中可以看到从TransE到TransH并没有添加太多的参数（Unstructured只是TransE简化版）。Bilinear，Single Layer， NTN对关系或者实体进行了非线性的转换，作者认为是没有必要的（增加了模型复杂度）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="TransH_2" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibCIqfibRNZBV8YUDf4sdP8KsiaEDxmJve6NwuqgibnMePAW9rRpUwGmjIg/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TransH模型的训练和TransE类似 （SGD优化） ，下面是损失函数（因为一些限制，后面加入了拉格朗日乘数）。论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="TransH_3" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibticsWA0RiaRiceXAt57RhKTsQIzQ7eX6WcyPV5xTylqrCvelwf48Ro4FA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 WordNet:&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/wordnet/download/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 Freebase:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;Code:&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;https://github.com/thunlp/KB2E&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;1、TransE (Bordes et al. 2013b): 和TransH相比，它没有将关系映射到另一个空间，关系由一个向量r表示。&lt;br/&gt;2、Unstructured Model：简化版的TransE，假设r = 0。&lt;br/&gt;3、Structured Embedding： 使用了两个关系相关的矩阵，分别用于头h和尾t，评估函数为:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibqibjX3MaZfUMx5eZnCTHvjHfuRlwILhGhibdNiboU1ibRBOqFT0wm4Xmiaw/640?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该方法并没有抓住实体和关系之间的关系。&lt;br/&gt;4、Single Layer Model(SLM)：使用了神经网络，评估函数为:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibnViceibd41lsF3gaiamlAIPfreP9yNJvibH3zice9z0dAibfJiaET8ZFdHAmQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5、Distant Model (Bordes et al. 2011)：它将实体映射到另一个空间，然后假定关系是距离而不是向量（因为用了2个不同矩阵映射实体，所以对实体关系建模并不是很好）。&lt;br/&gt;6、Bilinear Model (Jenatton et al. 2012; Sutskever, Tenen- baum, and Salakhutdinov 2009)，Single Layer Model (Socher et al. 2013)，NTN (Socher et al. 2013)：他们都是使用非线性函数映射实体，这样模型表达能力虽然好但是太多参数也太复杂了（容易过拟合）。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;论文提出的TransH模型，为了解决TransE对一对多，多对一，多对多关系建模的难题。它权衡模型复杂度和模型表达能力。而且还设计了复杂取样的办法用于训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TransD: knowledge graph embedding via dynamic mapping matrix&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun Zhao&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;中国科学院自动化研究所 National Laboratory of Pattern Recognition (NLPR)&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;knowledge graph embedding, link prediction.&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;ACL2015&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;知识图谱中的link prediction。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;在link prediction上的TransE扩展模型，函数仍然为:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMib9Lvm53ffjN69icaibkYrpzoCnv3CZCeYSTG94a7MMHe6RnswhAMxyvqg/0?"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;br/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但h丄和t丄为entity向量h和entity向量t在该relation r上的投影表示。投影定义为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMib1TXficDwzJKjnBkpiaNE8VuYtibIMGUp89YibQhQGR0k3QtV3wKSnzK82Q/0?"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中(h_p)^T为某entity的投影向量，h为该entity的表示向量。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 WordNet&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 FreeBase&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;https://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如果TransD的所有投影向量为0，TransD就是TransE。类似的还有TransR/CTransR，他们对每个relation定义了一个mapping矩阵，参数更多计算复杂度更大。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;模型只涉及vector的相乘，因此计算复杂度较小，效果也取得了state-of-the-art，适合用于规模很大的知识图谱。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TransA:An Adaptive Approach for Knowledge Graph Embedding&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Hao Xian, Minlin Huang, Hao Yu, Xiaoyan Zhu&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;清华大学 State Key Lab on Intelligent Technology and Systems&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;knowledge graph embedding, elliptical equipotential hypersurfaces, metric learning.&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何解决了translation-based 知识表示方法存在的过于简化损失度量，没有足够竞争力去度量知识库中实体/关系的多样性和复杂性问题。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;知识图谱在AI搜索和应用中扮演着越来越重要的角色，但是它是符号表示，有一定的逻辑性的，因此如何表示这些关系就成了一个很大的挑战，为了解决这个挑战，很多模型如TransE, TransH, TransR纷纷被提出来，在这些模型中，基于几何关系的方法是很重要的一个分支，而基于几何关系的方法是使用K维的向量表示实体或者关系，然后利用一个函数f_r(h,t)来度量三元组(h, r, t)，而他们都是基于一个准则h+r=t。&lt;br/&gt;因此就使用了同一个损失度量h+r=t，这种损失度量其实是利用了在一个球形等价超平面，越接近中心，三元组的可信度越高，因此从未匹配的t中寻找合适的t就变得很苦难，同时这种方法也很难处理一对多，多对一，多对多的关系。因此这些方法不够灵活。&lt;br/&gt;具体可以从图1(a)看出。同时这种方法将等价对待向量中的每一维，但实际上各个维度的重要性是不同的，只有一些维度是有效的，其他维度可以认为是噪音，会降低效果，具体见图2(a).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此作者提出了另一种损失度量函数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibiaA0QwDodYZ9RdslIQksZerNKyzicc9ok9UwJXgzwsTpiaPN83vZzbzXA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过增加一个矩阵Wr，首先利用了一个椭圆等价超平面，解决了上述问题1，具体见图1(b)；同时利用LDL分解，公式变为:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibBQgn8lBxzqXhM40YmPAQtWUFiaSCEjURpJSfmia98fibicZ0GV4TkjDbSw/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中D_r就是一个对角阵，而对角阵中的每个值的大小，正好说明了每一维的不同重要程度，也就解决了上述问题2，具体减图2(b)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="figure 1" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibgwR9LoD85T6DcEf0CDrKMzdqvocNHPKMSMpKvunDuqNLKUicJpxTzYA/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图1&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="figure 2" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibr3zAQveUVc7IoqEv8EsnFKfdBVSicmVWJRQXWljaJJMmecluD6IlApw/640?wx_fmt=jpeg"/&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;图2&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 Wordnet&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 FreeBase&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;https://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如模型部分介绍的，当前的一些现有模型都是基于一个准则h+r=t，因此就使用了同一个损失度量h_r+r=t_r，只是在h_r和t_r的表示上有不同：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）TransE h_r = h, t_r = t&lt;br/&gt;（2）TransH h_r = h - (w_r)^T.h.w_r, t_r = t - (w_r)^T.t.w_r&lt;br/&gt;（3）TransR h_r = M_r.h, t_r = M_r.t&lt;br/&gt;（4）TransM则是预先计算了出每一个训练三元组的直接权重&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有很多类似的模型，这里就不再介绍了。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;感觉这篇文章的思路比较简单，就是针对当前模型的一些不足，更换了一个损失度量函数。但是几点还是值得学习的，首先通过图像来描述不同的损失度量函数，给人一个更直观的感觉；其次针对向量表示中的区别对待，感觉很有attention mechanism的感觉，对不同的triple关注向量表示的不同维度，以取得最好的效果，这点是非常值得借鉴参考的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TransG : A Generative Mixture Model for Knowledge Graph Embedding&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;清华大学 State Key Lab on Intelligent Technology and Systems&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;knowledge graph embedding, generative mixture model, multiple relration semantics.&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv2015&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;解决多关系语义(multiple relation semantics)的问题。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;传统的基于翻译的模型采用h_r+r= t_r(其中，h_r为头部实体，t_r为尾部实体，r为头部&lt;br/&gt;实体跟尾部实体的关系)，仅仅对一个关系赋予一种翻译向量。&lt;br/&gt;它们不能细分多关系语义，比如，(Atlantics, HasPart, NewYorkBay)和(Table, HasPart, Leg)两个的关系都是HasPart，但是这两个的关系在语义上不同，第一个是“部件”的关系，第二个是“位置”的关系。TransG能够解决关系的多语义问题。如图所示，多关系语义分析可以提高三元组的分类准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="figure 1" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibGvCun6jumMZ9gNxZjibzLa1siamsrWtEbJPS0mzN45rSqAwPwjpwQxxw/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TransG利用贝叶斯非参数无限混合模型对一个关系生成多个翻译部分，根据三元组的特定语义得到当中的最佳部分。最大数据相似度原理用来训练，优化采用SGD。实验结果在link prediction和triple classification这两种任务上都优于目前最好的结果，运行速度与TransE(最快的方法)成正相关，系数为关系语义部分的数目。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 WordNet&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/wordnet/download/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 Freebase&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;大多数都已介绍，这里就只说明CTransR，其中关系的实体对被分类到不同的组，同一组的实体对共享一个关系向量。相比较而言，TransG不需要对聚类的预处理。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;这篇文章的idea比较重要，考虑到一种关系存在的多语义问题，相当于对关系进行了细化，就是找到关系的隐形含义，最终从细化的结果中选出一个最佳的关系语义。这个在应用中很有意义，不同的语义可能需要不同的应对方法，可以借鉴。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;KG2E:KG2E_learning to represent knowledge graphs with gaussian embedding&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Shizhu He, Kang Liu, Guoliang Ji and Jun Zhao&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;National Laboratory of Pattern Recognition&lt;br/&gt;Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Distributed Representation, Gaussian Embedding, Knowledge Graph&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;CIKM 2015&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文所解决的问题是知识图谱的表示问题（即将知识图谱表示为低维连续向量空间），本文使用Gaussian Distribution 来表示实体和关系，提出了用Gaussian Distribution的协方差来表示实体和关系的不确定度的新思想，提升了已有模型在link prediction和triplet classification问题上的准确率。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;传统的表示学习的表示学习的方法和计算比较复杂，自TransE模型诞生后，很多模型都是在TransE的基本思想上加以改进，KG2E模型也是一样。&lt;br/&gt;KG2E模型使用高斯分布来表示实体和关系。&lt;br/&gt;模型实例见下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="model example" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibHCXyqDXSqCn2JF6ACDvlDqBx3iaBzpJ7NtLpu0fX2oFApERGCxkcPfQ/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个圆圈代表不同实体与关系的表示，它们分别于“Bill Clinton”构成三元组关系，圆圈大小表示的是不同实体或关系的不确定度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模型算法流程图如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="model algorithm" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibYjMeZsFYibdwZ1Z9v4dwD54xdp4VicwVgicepVDzc3vR3PEAeJdqHLkyA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;算法解读：&lt;br/&gt;输入：训练集三元组，KG中所有的实体和关系，以及其它的一些参数。&lt;br/&gt;输出：KG中所有实体和关系建模后生成的Gaussian Embeddings.（主要包含两个部分，均值（向量）和协方差（矩阵））&lt;br/&gt;line 1到line 4主要是数据的归一化&lt;br/&gt;line 5到line 15是算法实现部分：模型采用的是minibatch的训练方法，每一个minibatch的训练中都会进行负采样，并将负采样的样例和正例样例混合在一起学习，然后使用评分函数进行评估，要达到的目的是正例三元组的得分比负例三元组高或者低（高低取决于具体的评分而函数的设定）。在一次一次的迭代中不断更新结果，最后将得到的means和covariance进行正则化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;文章核心公式：&lt;br/&gt;（1）评分函数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="score function" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibSiapBvZIzf24gkcOGbUjQdCicNok0IXr3MXITa1OOxbV2uFcZo3uRBjA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）KL散度的能量函数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="KL energy function" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMibe6jaz2a8hwPbqWzJwe8EW7rRrdqG2yzL5EsEYiaQ0NJ0GZmYd0Hyggg/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（3）期望概率能量函数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="EL energy function" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMib2VvEo5EystR5QZNPGBpWENStMT5IjTL5jvfqHvxib4VxuwmgUs14UYg/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集：&lt;br/&gt;WN18 https://github.com/Mrlyk423/Relation_Extraction/blob/master/data.zip&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;WN11 https://github.com/dddoss/tensorflow-socher-ntn/tree/master/data/Wordnet&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FB13K https://github.com/dddoss/tensorflow-socher-ntn/tree/master/data/Freebase&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FB15K https://github.com/Mrlyk423/Relation_Extraction/blob/master/data.zip&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;创新点：&lt;br/&gt;1、以前的文章是属于point-based，KG2E是属于density-based的。&lt;br/&gt;2、提出了(un)certainty的概念，在建模过程中融入了关系和实体语义本身的不确定性的知识，使用高斯分布的协方差表示该实体或关系的不确定度，高斯分布的均值表示实体或关系在语义空间中的中心值。&lt;br/&gt;3、使用了新的score funciton：KL-divergence和expected likelihood&lt;br/&gt;应用场景：link prediction，triplet classification,knowledge reasoning&lt;br/&gt;不足之处：本文提出的方法在link prediction的many-to-many relations上的预测性能不是很好，主要原因是KG2E模型没有考虑实体的类型和粒度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h1 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TranSparse:Knowledge Graph Completion with Adaptive Sparse Transfer Matrix&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;中科院模式识别国家重点实验室&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Knowledge Graph Embedding,Sparse Matrix&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;文章来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;AAAI 2016&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;针对不同难度的实体间关系，使用不同稀疏程度的矩阵（不同数量的参数）来进行表征，从而防止对复杂关系欠拟合或者对简单关系过拟合。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文的模型与TransR类似，即对每一个关系r学习一个转换矩阵M_r,将h和t的向量映射到关系向量所在的空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过本文注意到knowledge graph中面临两个问题，分别是heterogeneous（有的实体关系十分复杂，连接许多不同的实体）和unbalanced（很多关系连接的head和tail数目很不对等）。如果只使用一个模型应对所有情况的话可能会导致对复杂关系underfit，对简单关系overfit。因此本文认为需要对症下药，复杂的关系就需要下猛药（用有更多的参数的复杂模型），简单关系就简单处理（较少的参数）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是怎么实现这样灵活的建模？在方法上本文借用了SparseMatrix，如果关系比较复杂就用比较稠密的矩阵，如果关系简单则用稀疏矩阵进行表达。文章假设关系的复杂程度正比于包含该关系的triplet数目，并根据两类问题提出了对应的稀疏矩阵初始化方法。不过并没有提出同时解决两类问题的统一方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、针对heterogeneity问题的模型叫做TranSparse(share)，模型参数sparse degree，theta_r，是由下列公式确定:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a title="alt text" class="" rel="gallery0" style="color: rgb(37, 143, 184); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMib98tDjZKotPLrathA9AW2rwCIeM2vTRLIyNic0qNicLd4HWibHRicgXvGvA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中N_r是该关系r所连接的triplet数目，N_r*是数据集中最大的关系triplet数目。通过这个sparse degree我们就可以确定参数矩阵的稀疏程度了。entity的向量通过下式进行转换：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;a title="alt text" rel="gallery0" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnABhQLT5cjDRs7liaPymcMib2Yp3PzLxib02cBA2GjhFgoeoXULKvjQCv1uoo24WIuXicibjia36HTVkIA/640?wx_fmt=png"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、针对imbalance问题提出的TranSparse(separate)方法也十分类似，即在关系的head和tail两端使用不同复杂度的matrix。sparse degree的公式与上面TranSparse(share)的几乎一样，只不过N_r和N_r*替换成了entity的个数。如果某一端要连接更多不同的entity，那么这一端就需要更复杂的模型来表征（matrix有更多非零参数）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;确定这个sparse degree之后，我们就可以初始化对应的稀疏参数矩阵了（原文中提到了Structured与Unstructured两种矩阵形式）。目标函数以及训练过程与其他工作一致，只不过在进行训练时我们只对矩阵中的非零部分进行更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后模型在triplet分类和链接预测任务上进行实验，相比于先前模型取得了更好的成绩，不过相比于TranD优势并不十分明显。提出的两个模型中TranSparse(separate)的表现更好。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;数据集 WordNet&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://wordnet.princeton.edu/wordnet/download/&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;span&gt;数据集 Freebase&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://developers.google.com/freebase/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;上面的相关工作已经介绍差不多了，这里不再赘述。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;TranSparse模型主要是为了解决关系和实体的异质性和不平衡性而提出，问题针对性强。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; text-align: justify; line-height: 1.75em; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;总结与展望&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;最近几年人们对知识表示方法的探究一直都在进行，知识表示学习对于计算机如何理解和计算知识的意义是重大的。在2013年embedding的思想出现之前，人们基本采用one-hot的表示方法来表示实体，近几年知识表示的核心思想就是如何找到合适的方法来将知识图谱emmbedding到向量空间，从而在向量空间中进行计算，并且也在这方面取得了不错的进展。但知识表示学习仍然面临着挑战，主要包括以下几个方面：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、对于多源知识融合的表示学习，如何将知识库中的文本等信息加入到学习中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、如何进行更加复杂的知识推理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、对于知识图谱无法表达的信息，应该进行如何表示和推理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4、如何在知识库中融入常识信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（参考自：清华大学刘知远老师的《知识表示学习研究进展》一文）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;致谢&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;感谢&lt;/span&gt;&lt;a click-to-target="::message.author" style="max-width: 100%; color: rgb(255, 76, 0); font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;anngloves、memray、yangzhiye、cheezer94、zhaosanqiang、zhang1028kun&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;和&lt;/span&gt;&lt;strong&gt;&lt;span&gt;gcyydxf&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;七位同学的辛勤付出，感谢&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;strong&gt;&lt;a click-to-target="::message.author" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;anngloves&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/strong&gt;&lt;span&gt;童鞋的整理和总结。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;广告时间&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的学术组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微信公众号：PaperWeekly&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkSMlEJFo90NY8rCXr3mJMBibduVHxMKSHzQtVkHz8kNwpjCKBiccGuqLE0WpPuAbdtEs6cTF5iabpAQ/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：PaperWeekly（&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;http://weibo.com/u/2678093863&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br/&gt;知乎专栏：PaperWeekly（&lt;/span&gt;&lt;a target="_blank" rel="external" style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important; text-decoration: underline;"&gt;&lt;span&gt;https://zhuanlan.zhihu.com/paperweekly&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br/&gt;微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 全面解析谷歌整体战略：关于人工智能、云服务和登月计划的未来</title>
      <link>http://www.iwgc.cn/link/3166699</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CB Insights&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、杜夏德、吴攀、武竞、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;CB Insights 今日发布了一篇关于谷歌业务战略的深度分析报告，盘点了谷歌在人工智能、云服务、虚拟现实/增强现实、电信与能源、运输与物流、硬件和服务平台等众多领域的业务和战略。其中人工智能是贯穿谷歌所有方面的创新的线索。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 1998 年谷歌在加利福尼亚州门洛帕克的一间车库变成一家公司以来，它已经发展成为了后 .com 时代企业创新的火炬手。谷歌对长远创新的亲睐体现在多个方面——从其广为人知的员工可用于个人项目的「20% 个人时间」政策（这个政策导致了 Gmail 和 AdSense 的诞生），到其在 2009 年成立的准独立的风险资本部分 Google Ventures，再到其面向未来创新的「登月计划」的 Google X 创新实验室（已改称 X）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，近些年来，该公司已经开始转变其实验性的方法、有风险的研发和分散式的公司结构了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2011 年执掌谷歌后，CEO Larry Page（现为 Alphabet 的领导者）就宣布谷歌将在「更少的箭头后放更多木材」，要向更重要的方向投入更多资源：从民主化的自下而上的创新方法变成更为自上而下的重点式战略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，据报道「20% 个人时间」政策在近些年来已经受到了限制，需要更多的管理批准和监督。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此同时，谷歌已经从核心的搜索和广告业务扩展到了更为广阔的业务领域，其涵盖了从消费硬件到汽车到电信到医疗到风险投资等众多领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年十月份的 Alphabet 组织架构重组就是为了这个庞大的规划网引入更大的结构、透明度和财政责任。为了同样的目的，该公司还在今年七月份聘请了摩根士丹利在财务纪律（financial discipline）享有盛名的资深高管 Ruth Porat 作为首席财务官。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一年来，这些举措已经给山景城应对收购和研发的方法带来了明显的改变。新的股权激励将会将员工的奖励和个人单位绩效挂钩。在 X 实验室，负责人 Astro Teller 曾写了需要推进登月项目「毕业」——「毕业」是指项目最终成长为 Alphabet 企业家族中可扩展的团队和产品。以 GV 为例，该投资部门已经很长时间没有领导了，其规模以及尤其的种子交易截至今年目前为止已经缩减了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于该公司幅员辽阔，我们使用了 CB Insights 科技市场智能平台中的许多工具来提炼 Alphabet 组织范围内各个单位的收购、投资和研究/专利活动，从而获得了一个关于其未来战略的数据驱动的视角。鉴于该公司运营范围的辽阔，我们不会触及每一项计划和部门，而只关注推动谷歌系向前的主要和反复出现的主题，其中包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;推进云和硬件：Alphabet 推动增长的领域已经不再限于广告了，也在研发、收购和投资有可能实现收入和带来利润的业务和领域，例如高级移动和智能家居硬件，尤其是云和企业服务。比如谷歌在 Nest 之后最大的收购 Apigee 就是一家已经上市的企业云公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工智能优先（AI-first）战略：该公司正在利用其人工智能/机器学习的专家人才（包括那些通过 DeepMind 收购吸纳进来的）来差异化自己在上述部门以及搜索和广告、整个面向消费者的网页服务和其它 Alphabet 单位的产品。谷歌新款的高端移动和智能家居设备就是提供一些人工智能服务的渠道。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关注增强现实/虚拟现实、自动驾驶和数字医疗：投资、收购和山景城的专利数据范围包括自动驾驶、可穿戴、人工智能驱动的医疗和为全球更大范围的人口提供网络接入。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;收购再次回升：该公司在 2016 年第 3 季度进行了 9 次收购，是自 2014 年第 3 季度以来最多的。这说明了该公司的兼并收购（M&amp;amp;A）意愿的复兴，因为其已经表现出了进一步拓展移动硬件（包括 Pixel 手机和智能家居中枢）、企业云服务、交通/物流、虚拟现实等领域的打算。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;GV 已经大大退出了种子市场：GV 已经基本上不再投资年轻的创业公司了。其种子投资上的行为已经较去年同期下降了 85%，在 2016 年上半年完全没有任何新的种子交易。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;规范其它登月项目：控制开支和强迫登月项目概划盈利的途径。这让该公司启用了资深人士和重组了团队以实现业务化，另外还招募了一些外部行业资深人士来推动 Loon 和自动汽车项目的商业化。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌核心的一些背景&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;收购&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;投资&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;谷歌&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;GV&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;谷歌资本&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;专利数据分析&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;按领域划分的 Alphabet 计划&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工智能&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;云与企业&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;消费硬件和平台&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;增强现实/虚拟现实&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;电信与能源&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;交通与物流&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;医疗保健和数字健康&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;金融科技&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结语&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;谷歌核心的一些背景&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们深入到 Alphabet 未来战略背后的数据之前，我们必须快速了解其目前最成熟和最盈利的业务线：谷歌搜索和广告。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;提醒一下，现在的谷歌包含搜索、地图、云和企业、谷歌品牌的硬件和操作系统（Chrome、安卓等）和 YouTube。其它所有单位（从投资部门（GV、Google Capital）到 X 部门）现在都直接隶属于 Alphabet。但我们将统一使用「谷歌」进行指代，不管它现在确实属于谷歌还是在 Alphabet 之前属于谷歌。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2016 年第 2 季度，该公司的总收入（top-line）和净收入（bottom-line）增长超出了分析预期。主要在谷歌的核心广告业务的推动下，Alphabet 的季度收入跃升了 21%，增长至 215 亿美元，利润则增长了 24%，超出了市场预估。这种强劲的势头的推动力来自向移动平台的成功过渡，其中包括新的移动广告格式和更好的效果评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管上个季度的结果大部分是积极的，但也存在一些可能会影响长期前景的趋势。首先，谷歌广告业务的收入中来自其自家网站的份额在 2016 年第 2 季度首次达到了巅峰的 80%，而在 2011 年时这个比例是 70%。这意味着未来的广告增长将前所未有地更加依赖于谷歌向其自己网站（例如搜索结果和谷歌新闻等等）的流量引导，而不是向网络成员的网站。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;移动广告通常比桌面广告的利润低，所以谷歌的成功是在每次点击的收入（CPC，当消费者点击广告商的广告时，广告商向谷歌的平均付费）更低的情况下实现的，其巨大的增量抵消一些这种情况的影响。2016 年第 2 季度在自家网站的 CPC 仅有两年前自家网站的 CPC 的 76%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;应用程序和连接设备数量的激增也带来了挑战，因为在 Facebook 和微信等应用程序占用大量用户在线时间，以及新一代设备（如智能家居中心）导流搜索流量成为潜在竞争对手的情况下，还不清楚谷歌的搜索引擎是否会继续占领主导地位（例如通过亚马逊的 Echo 智能家居设备和其 Alexa 语音助手直接进行的搜索）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，谷歌在全球数字广告支出中所占的份额持续下降，因为像 Facebook 这样的国内竞争对手以及百度和阿里巴巴等国际竞争对手都在持续增加市场份额。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的广告业务使公司长期以来一直保持稳定，同时它的财务手段也成为其登月计划（moonshot）的支撑。然而，尽管 Alphabet 的业务数量庞大，其财务业绩和增长前景仍然严重依赖于谷歌的原有业务（广告占了 Alphabet 2016 年第二季度收入的 89％）。其核心业务的卓越表现在其它不稳定的业务中是显而易见的，但是山景城（加利福尼亚州的一个县）已经敏锐地觉察到谷歌的收入来源是缺乏多样性的。当我们通过它的子公司深入了解 Alphabet 的活动时，我们将看到寻找新的增长途径是如何塑造了该公司的战略的收购。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;收购&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统认为谷歌是最有收购能力的科技公司之一，但它的收购步伐速度放缓，并导致了 Alphabet 架构下的机构重组。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 2001 年以来，该公司已经进行了近 200 次收购，引进外部人才和扩展新的部门，并在这个过程中创造了 Larry Page 的另一句「牙刷测试（toothbrush test）」技术格言，来确定并购目标是否值得。（目标是必须开发客户认为每天都不可缺少的产品。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的收购跟踪工具包括在山景城稳定的收购流中的每个收购记录。Alphabet 今年迄今（16 年 10 月 10 日）已经收购了 13 家公司，仅在上个月就进行了 3 次收购，其中包括 6.5 亿美元收购上市云公司 Apigee。我们使用 CB Insights Acquirer Analytics 工具跟踪谷歌自 2010 年以来的并购活动：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNH5sGqq31o3TlpiafwrgXmXmQGJnl8GM16Kas9KCCssibDaXpaX7bmkMQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这十年的大部分时间里，谷歌一直是技术并购的主导力量；活动在 2014 年达到顶峰，因为谷歌在第二季度收购了十多家公司，远远领先于那一年其它科技巨头的收购规模。然而，该公司的收购步伐大幅放缓，Alphabet 重组，并在 2016 年上半年大幅下降。最近一个季度的收购活动增加，虽然是否为昙花一现还有待观察，也或许是山景城适应了 Alphabet 的新收购速度释放的信号。到目前为止，2016 年第四季度，Alphabet 只收购了 Famebit，一个可以帮助商业品牌与 YouTube 上的视频创作者建立联系的平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了最新的财务紧张问题，近十年来大规模收购带来的低迷结果可能也促使了公司收购活动的暂停。该公司快速地收购了至少 7 家机器人公司——Schaft , Industrial Perception , Meka Robotics, Redwood Robotics ,Bot &amp;amp; Dolly , Holomni , 以及最著名的波士顿动力（Boston Dynamics）——触发了 2014 年的炒作高峰（如趋势图所示，如下），但这些公司从来没有合并成为一个高效的机器人公司。前安卓负责人 Andy Rubin 带领了机器人浪潮，但 Rubin 于 2014 年 10 月离开公司创办全球硬件创业孵化器 Playground Global。失去了有远见的人物可能会阻碍机器人部门的发展，不管是在谷歌或是在 Alphabet，这个名为 Replicant 的机器人部门从未真正合并成为一个完整的公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNe6iadTaCHBsarick3PHujzvctlIExWZSmnyCibccUT8xC8d3agBpTC1Ng/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相反，在 Alphabet 创立后，Replicant 直接进入管理层的视线，新公司 Alphabet 对其各公司的创收潜力进行了严格的审查。虽然像波士顿动力公司这样的子公司在 YouTube 上备受推崇，但是漫长的商业化道路导致该公司在 2016 年初被出售。最近，谷歌 2014 年对智能家居的重要战略环节——收购 Revolv，Dropcam（5.55 亿美元）和 Nest（3.2 亿美元，迄今为止收购的最大的初创公司）也被管理不善和员工流失的指控所困扰。Nest 的紧张局势随着 Nest 联合创始人兼首席执行官 Tony Fadell 在 6 月份的离开而公之于众。Fadell 辞职的事件中反复提到了智能家居领导层之间的摩擦，最引人注目的是 Dropcam 创始人 Greg Duffy 在 Medium 上的一篇博客（他的 Dropcam 团队被纳入 Nest 智能家居部门）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;将 Dropcam 卖给 Alphabet 是我的错误……可以说，我与当前 Nest 的领导在管理方式上有极大的差异。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nest 的问题可以看作是旧的管理结构的失败，即在公司扩展到新的市场和产品线时，没有正确地管理其各个子公司，在 Googleplex 背景下协调达成一致的目标和企业文化。事实上，Nest 部门（现在是一个独立的 Alphabet 公司）远离了该公司的新智能家居硬件的努力方向（刚于 2016 年 10 月 4 日的智能硬件大会上发布 Google Home）。作为 Alphabet 的事实上的智能家居公司，Nest 可能是成为亚马逊 Echo 设备的竞争者的最好的选择。然而，亚马逊设备对谷歌公司的主要搜索业务的明显威胁，以及与亚马逊竞争的日益激烈，可能导致谷歌得出结论：必须将开发智能家居作为公司核心，并由谷歌的执行官直接监督。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，这样的产品需要与谷歌的核心搜索和虚拟助理服务紧密集成。Nest 作为谷歌的非核心公司想要实现这一点，跨越部门间的界限和达成共识将有更多的麻烦。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;新成立的 Alphabet 结构可能有助于更成功的并购和不同业务的整合。Alphabet 旗下的不同部门将可以根据它们自己的战略利益和路线规划进行收购游说；但也会面临一个更为清晰的组织结构，能让 Alphabet 称霸天下，也会迫使其减少对投机项目的投入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管谷歌的机器人和智能家居的投资已经变成了警示性的故事，但谷歌在许多其它领域的收购却带来了毫无疑问更为积极的结果。2014 年谷歌收购的 DeepMind（金额在 5 亿到 6 亿美元之间）已经通过其高调的 AlphaGo 和 WaveNet 项目巩固了谷歌在人工智能研究方面的声誉，而且其技术也已经在谷歌的数据中心和翻译工具等产品中得到了应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了智能家居，谷歌与亚马逊的激烈竞争也带来了对云和企业服务的收购，以及一种完全不同的战略方法。谷歌正确地将云平台作为了其优先事项，因为其过去在这方面落后于亚马逊的 AWS 和微软的 Azure（尽管谷歌最近得到了一些战略性的云客户，其中包括苹果和 Spotify）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个领域，谷歌严重依赖于收购来补充其内部研发和在其平台之上提供增值服务。特别地，CEO Sundar Pichai 曾说过该公司的目标是通过稳健的、对开发者友好的服务进行竞争，而不是单纯的规模。我们的收购方分析数据也突出了这方面的努力：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNCUZQQkC6K8kq0iakD2noskxpThOufjLhFiaMOmuIgDcrKpbhE4kUOFqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个主要的例子是谷歌在今年 9 月对 Api.ai 的收购，这是一家帮助开发者开发对话式智能接口的创业公司。这能很好地和人工智能交织在一起，而人工智能则是谷歌的差异化战略的另一支柱（更多细节请参看行业部分）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌在这一领域的其它收购还包括 2014 年的 Stackdriver、Appurify、Firebase 和 Zync Render，以及过去两个月的 Apigee 和 Orbitera。事实上，谷歌 2016 年的一半以上的收购都涉及到企业应用或 B2B 云服务。其中许多都发生在 Recode 3 月份的谷歌在寻找企业云领域的目标上最活跃的报道之后。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;据最近离开谷歌的人说，那些收购请求出现最频繁的是这个领域：企业。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在云的推动下，谷歌得以关注服务中端市场的多个目标，旨在增加多样化的企业能力，这和对 Nest 的数十亿美元的重磅收购不同。其 1 亿美元收购的计费服务公司 Orbitera 是这种追赶策略以及其对一个灵活的、「多云（multi-cloud）」世界（其中企业将越来越依赖于多个供应商）的支持的象征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外值得一提的是，和押注前沿的有希望但未得到证明的领域相比（比如，自动驾驶汽车和机器人），Alphabet 的一系列云和企业收购更倾向于有明显的赚钱机会的较为成熟的公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 六月份对 Webpass 的收购也是来自于对成熟的电信领域的收购。Alphabet 运营 Fiber 业务的 Access &amp;amp; Energy 部门已经宣布了利用 Webpass 的无线技术降低资本开支和部署时间的计划（伴随着 Fiber 业务成本高昂的扩张）。在这样的背景中，Webpass 看起来像是又一个带来了即时影响的收购——降低了开始并提升了盈利能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不可否认，自 Alphabet 诞生以来的这短短一年时间，数据就已经深远地影响了这个新组织在平衡「登月项目」和财政责任上的尝试，并且明晰了其实现收入的路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;投资：谷歌，GV 和谷歌资本&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 的投资活动一直被竞争者和观察家们关注。一部分投资由下属的谷歌或其分支直接发起（例如 DeepMind 直接投资了远程医疗初创公司 Babylon）。但大部分投资来自 Alphabet 的两家主要投资机构：专注于早期初创企业的 GV（前谷歌风投）和对扩张期公司投资的谷歌资本（Google Capital）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些投资分支一直强调其投资策略与谷歌本身互相独立。在 2015 年 9 月，集团的运营策略发生了改变，GV 和谷歌资本成为了新控股公司下的两家分支公司。（但他们共同接受 Aphabet 的高级副总裁 David Drummond 的监督，此人同时监管集团的企业并购业务。）我们相应地单独分析他们的活动，但它们仍会同时出现在一些深入的剖析中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNGiankJeY6jR9KUwwuOMvNJ9f9LZcRI8GtrkiaXg4EC3csLMOa2NXuCpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如上图显示的，三家机构的投资活动数量在近期有很大波动。GV 的交易活动在 Alphabet 成立之前已经开始下降，而投资的增长来自于谷歌资本和谷歌本身的战略投资。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;谷歌&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从谷歌开始，公司的主要战略投资包括几次大型交易到一些「前沿」领域如增强现实，太空运输和探索。谷歌在 2014 年 10 月领投了隐形增强现实设备 Magic Leap 5.42 亿美元的一轮投资，在 2015 年 1 月又参与了 Space X 10 亿美元的 D 轮融资。有消息称谷歌正准备在未来对这家宇航公司继续投资 90 亿美元，以获得 7.5% 股权。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种投资活动的规模强调了山景城对于先进科技领域的重视。谷歌认为 AR/VR 是未来计算视觉呈现的核心。其 Magic Leap 的交易宣示了公司策略的进一步多元化（谷歌已有包括在 2016 年 10 月发布的消费级产品 Daydream 移动 VR 头盔，和其他早期产品，如 Cardboard，Google Glass 和 Tango）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此同时，谷歌向 SpaceX 进行了大笔投资，这将帮助 Alphabet 的「登月计划」为谷歌提供地理信息（Terra Bella，前 Skybox Imaging 项目），同时帮助提升全球互联网覆盖面积（Access and Energy，前 Project Loon 等计划）。廉价高效的地球卫星发射将会为对这两个方面提供便利，而 SpaceX 的首席执行官埃隆·马斯克对此也志趣相投，他决心建立一个以卫星为媒介的全球通信网络。谷歌直接资助了卫星服务公司 O3b Networks ── 而 O3b 已被欧洲卫星通信公司 SES 以 14 亿美元的价格收购──这些投资也与上述活动相关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了战略投资，在 2016 年 4 月谷歌建立了 Area 120，一个为公司内部员工准备的创业孵化器。这些措施是为了防止公司人才的外流。孵化器的名字中提及挤出 20% 的时间进行创业，就像谷歌的其他传统一样，这已经变成了一个正式的，明确的计划。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GV&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 风投机构自 2009 年成立以来，已经成为了风投生态中的重要一环，它一直是最为活跃的风投公司。我们接下来分析 GV。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有这样一个事实：Bill Maris，作为创始人和首席执行官在 8 月初离开了 GV。有匿名消息称这次人员变动与上级公司 Alphabet 的重组有关（在 Maris 治下的 GV 以自主决定权而闻名）。目前事件的双方仍公开表示友好，Maris 这样评论：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我的离开是因为所有事情都很棒……我与 Alphabet 之间没有问题。但 Alphabet 的改变对所有人都有一点影响。我们（GV）从第一天开始就是独立的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然如此，在后 Aphabet 时代里，创始人的离去仍将成为主题，这一幕也许将会在公司的其他部门继续呈现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过 CB Insight 的投资分析工具分析 GV 近期的投资活动，我们可以看到 GV 的投资速度自 2013 年末到 2014 年初的顶峰（约每季度 30 笔投资）以后，有逐渐下降的趋势。这与我们之前 GV 正在减少新投资活动的结论相同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNVB6JqkFpCibiatjqnecBicEthKvPibeFKh6aAw5pz3BAnPynLquWnxPHAg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GV 在 2009 年以 10 亿美元资本起家，每年膨胀 50 亿美元。随着 GV 资金来源的充裕，它逐渐参与进了大型投资，如 2014 年优步 12 亿美元的 D 轮投资和 Jet.com 在 2015 年的 B 轮投资。CB Insight 的投资分析工具显示了 GV 持续增长的中型交易，在 2016 年第一季度，公司一跃进入了大型投资的行列，包括 2 月份对 Oscar（40 亿美元）和 Magic Leap（7.94 亿美元）的投资。总的来说，数据显示 GV 的大型投资脱离了 2015 年以前的中位数，出现了急剧上升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNTfXTib0FSOBMNRDOblRMY7CrKL5YdCbgR3DYFrI1DCr8011icxBIvokw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;交易数量的减少和投资金额的增加表明 GV 已完全退出天使轮投资市场，这个曾经它赖以为生的领域。在过去的两年里，它的天使轮投资活动每年减少 85%，在 2016 年上半年则完全没有这种投资出现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNdFGt31QxuSZKcwhsGJXOYlaRSHt28cHLVS8LKv38bMSnFM6uaE48icw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bill Maris 在去年 12 月承认了这种变化，认为在前期投资中的机会正在减少。在 今年 8 月 Maris 离职的采访中，他同样指出 GV 目前对于投资形式存在限制：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;当你有 25 亿美元的资金，进行种子轮投资就是浪费时间了。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，GV 融资项目的区域也越来越固定了──以美国为中心。公司 2014 年启动了 12.5 亿美元的欧洲投资专项资金，由五位合伙人进行管理。然而，在后 Alphabet 时代，这个项目在 2015 年 12 月宣告终止，其中资金被回收并投入 GV 品牌再造项目。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在大约一年半之前，欧洲的分部进行了不到十项投资，其中最大的交易是对宾馆预定网站 Secret Escapes 6000 万美元的 C 轮融资（Octopus Investments 是这次投资的另一个领投者）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNtia539fiau4cgUmnEVLRtJ8jXs1suHcrWggn2kdVFUpiczFMK09Kta68Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从行业上看，GV 一直被其「独立」的策略所束缚，公司表面上追求高风险，但潜在高回报的登月式的项目，实际上却与传统风投公司别无二致。Maris 的个人魅力渲染了谷歌对于登月式项目的追求。医疗领域的投资为这种看法做出了注解，正如 Maris 在 2015 年文件中所说：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;如果你今天问我，我们会活到 500 岁吗？答案是肯定的……如果有人让你在很多钱和能活很久中进行选择，你会怎么选？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的确，GV 一直在强调他们会投资医疗初创企业。他们的投资范围从数字医疗公司（Flatiron Health）到供应商（One Medical），也包括新方向如基因医疗（Editas，Foundation Medicine，23andMe）。近年来，公司正在这些公司上投入越来越多的资金。在 2015 年 3 月，Maris 在接受彭博社的采访时披露，其时 GV 已将 36% 的资金投入生命科学领域，而在 2013 年，这个数字只有 6%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNMnRwibt5YJQJUrcz4wSZBKz952VxF8d7zCGTL1JW870ic4OJTVWPD6Eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNJCDkEuP1woDHZ28WFyDVJEWE8Ju6JgGIibLeDrLCtm3WCyibfTQm9lrw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GV 的投资和 Alphabet 其他部门 Verily 与 Calico 的投资共同正在为变革性医疗研究助力。如此重视医疗行业无疑是因为 Bill Maris 拥有生命科学背景；事实上，这位前生物科学公司管理人直接推动了 Calico，Alphabet 神秘的抗衰老研究部门。在 CB Insight 中，我们可以深入了解 GV 对于其他领域的投资，跨度从 AR/VR 领域到无人机，互联网金融，网络安全，再到人工智能。GV 的投资组合与 Alphabet 令人眼花缭乱的资本操作有着很多重合。至少 6 家 GV 投资的公司最终都被山景城收购，其中值得注意的是 2014 年的 Nest。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一方面，优步突然变成了 GV 甚至 Alphabet 投资策略中潜在的不和谐音符。随着竞争对手的压力，优步开始将重心转向他们的自动驾驶汽车，而谷歌此前也投资了拼车应用 Waze（收购于 2013 年）探索共乘车市场。随着这些步骤的实施，在共乘车服务上他们正在对优步形成威胁。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为回应，优步正在逐渐稀释管理层中 Alphabet 的地位，交易负责人 David Drummond 离开了优步董事会。这家打车公司同时驱逐了董事会观察员 David Krane，后者是 GV 的合伙人，现已成为 Bill Maris 的继任者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNq9LRRTao3hzib2KrbXHrmsERrYhYWQiaz7zSDV0hwCXwbsoiaJAiabkKibA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;谷歌资本&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌资本是 Alphabet 风投家庭中年轻的一员，于 2013 年创立。因其资金充裕，公司的投资方向明显不同于集团内老一代的同僚，谷歌资本主要参与初创企业的后期融资阶段。据称，他们每年的投资数量为 30 亿美元。一个健康的数字，但略微少于 GV。正如其宣称的，谷歌资本将自身定位于盈利导向（而不是战略导向）的投资者。当然，其不断增长的资金仍正在利用专业知识，招募基础和其母公司谷歌的威望作为其核心卖点。集团的其他公司承诺给对谷歌资本共享资源。Edward Kim，被投资公司 Gusto 的首席技术官赞赏这种工作方式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;他们真的从谷歌内部找到了一个人，一个能够解决我们问题的人。相比资金，他们其实带来了更多技术上的帮助。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自成立以来，谷歌资本一直维持比 GV 更低的活跃度。他们每季度的成交数量维持在 1 至 3 笔，只在 2015 年第三季度超过了这一数字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHN12PDJmeVbCLicwj93CniaTGSJLcQZ1hYMyYV5BnpX7oNnOs9wb9fVZAQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌资本通常参与 2500 万至 10 亿美元的融资轮，这不属于很大的交易。其中一些包括 CloudFlare 的 D 轮融资（11 亿美元），FanDuel 的 E 轮融资（27.5 亿美元），和 Oscar 的 C 轮融资（40 亿美元）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNpSdlh1H0icG29dSic65iaibFfgYYt9oUrqM9Bb9aVeOYkgQuZ5x3wJTTow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌资本的投资方式反映了硅谷大多数高端投资公司的习惯，与那些著名对冲基金，或老虎基金与富达投资的科技投资共同基金相似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GV 很少在谷歌资本投资之前对同一家公司进行投资。前者确实出现在了谷歌资本投资的一些公司的共同投资人名单上，但这一般都是在追逐利益时出现的巧合。当然，随着 GV 正在逐渐远离早期投资市场，两家投资机构的重叠区域可能会越来越多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNGtQlObdRJNyuc07vKJdxXLC8rmicvff2FRy1j7Awp7FWvicSRTcgwVYg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌资本目前最值得一提的交易是对 Care.com 的首次公开市场投资。今年 6 月，谷歌资本宣布了对这家护理服务公司 4635 万美元的投资，该公司于 2014 年 1 月份上市。这次交易意味着公司的投资部门跨越私人与公开市场，这与那些共同基金和对冲基金的业务相同，正如谷歌资本的合伙人 Laela Sturdy 在回答公司对私人和公开市场领域之间的立场的问题时所说的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Care.com 体现了本公司的投资喜好。我们一直专注于成长阶段的公司，我们唯一的目标是帮助他们成长为拥有谷歌体量的巨头。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可能会看到未来谷歌资本进行更多的 PIPE（私募资本投资公开市场），又或许他们将固守传统的私人投资领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;专利数据分析&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 CBInsights 专利数据，我们还筛选出公司研究活动的趋势。这项分析在执行时有几个注意事项，主要是，专利申请过程在应用发布前有一个明显的时间差。这个延迟时间从几个月到两年不等。我也拿谷歌做过该方面分析，排除了其外部收购公司带有的专利。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外值得注意的是，谷歌一直以来对专利所持的态度。过去，公司高管包括 Larry Page 和 Sergey Brin 他们自己都反对申请过多的专利，这会威胁到硅谷的创新精神。乔布斯发布第一台 iPhone 时，谷歌只有 38 项专利。到了 2011 年，谷歌高级副总裁总法律顾问 Kent Walker 描述了公司对专利制度的普遍厌恶，他们希望见到改革措施：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;专利不是创新。这是一种阻止他人创新的特权。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，随着智能手机的诉讼在本世纪初加剧，谷歌被迫转变了立场。2012 年，它以 125 亿美元的价格收购了摩托罗拉手机业务，这是迄今为止该公司最大的收购，该收购为谷歌不断增长的 IP 库带来了丰富的手机专利。谷歌自己也开始迅速提交专利应用申请。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNCLfzbhTcPTc99T16cpVJ9qSsZBtibXWA5hnmKuzgs1EzJX7vFZyribdg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关键数据突出在与谷歌前沿产品计划相关的专利上。Mountain View 的专利点亮了其谷歌眼镜计算设备及其他可穿戴设备研究。「Balloon」也在 2014 年问世，它从今年年初到现在一直处于下面这张列表的首位，Project Loon 的气球动力互连网络开发一直在持续。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNXSicKAEDz3aRqQIfC2BNp3rnFv8OSckOPiaKrcSQdGSmRdmPQ0rXOpjg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关键词「车辆（vehicle）」上升的频率也反应出谷歌在自动驾汽车领域上的投入，他们一直在扩大自动驾驶的测试团队，还在寻找汽车制造商合作伙伴。2012 年，带有汽车关键词的应用数量激增，而且最近几年一直在增长，包括专利数据可能还不完整的 2014 年，所以当这些档案公布于世时，汽车应用的数量实际上甚至会更多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNAzAEy0WGbpvmuIvTpv5qAfmVSMhGVrqv7ghVpicaoQFia1DpgcCiceQfg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多这些专利产生计划（moonshot），当然包括自动驾驶机车项目，已经占领了谷歌的 X 实验室。自从 2010 年建立以来，X 实验室一直在尝试成为一个成功的致力于尖端前沿的企业研究机构，其他像 PARC 和贝尔实验室最终都因其母公司而失败了（至少在资金上）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的趋势工具挖掘了数百万条媒体关于技术趋势的报告后显示，「moonshot」一词的流行度一时间接近了「Google X」。换句话说，谷歌的实验室与 moonshot 概念紧密相关，像传统的企业创新实验室运营商一样，Alphabet 已经不再避开专利系统了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHN6x7qV89EIxfbTUQVbWMnQy3JXNohw9AhsbgXYiaWknAlnZpPTADVr9g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一节中，我们将细究遗爱谷歌的一些优先项目，以及它们是如何融合进 Alphabet 的特定产业策略中的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Alphabet 在各领域的战略&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和其投资部门很像，Alphabet 其他的子公司（包括谷歌）的业务都涉足了不同的领域。这里，我们对 Alphabet 涉足的重点领域进行分组深入分析。再次强调，下面的列表中不包括这家公司的所有业务活动，而是综合概述了其目前的兴趣领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年 10 月在其最新高端 Pixel 智能手机的发布会展示中，谷歌 CEOSundar Pichai 说世界正在从「移动至上」转向「人工智能至上」。谷歌与人工智能这一时髦领域有着密切的联系，而且占领这一领域的欲望越来越强烈，并成为了最活跃的人工智能公司卖家。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNytc2crLejeEREP4IrFBftZJcibNiawzdTxvzSfZspyYdHDa6SsrKZtibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在其公司内部，谷歌大脑以一个 X 项目成功凸显。去年，Astro Teller 将谷歌大脑描述成「谷歌的生产价值所在，可以抵得上 GoogleX 的总成本了，」这个小组开发了 tensorflow，并提高了从翻译到语音搜索的核心技术。在 Alphabet 今年第二季度盈利的电话会议上，Sundar Pichai 也对投资人重复强调了机器学习的重要性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;机器学习是驱动未来的引擎... 谷歌内部目前有超过 100 个团队在使用机器学习，从街景到 gmail 到语音搜索等等。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了实体产品外，一个脱胎于 DeepMind 的系统已经帮助其耗电量巨大的数据中心减少了成本和实现了环境保护功能，将能源使用效率提升了 15%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌正在将机器智能和学习部署到它的所有业务上，我们也在几个关键领域对其人工智能方面的业务活动做出分析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;云&amp;amp; 企业&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的云和企业服务已快速上升至其优先项目中了，这一点在其疯狂的收购中可见一斑。进一步推进有利可盈的服务市场延伸了 Alphabet 所强调的能直接带来财务上涨的机会的逻辑（上一季度，亚马逊 AWS 部门产生近 100 亿美元收入）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Alphabet 成立谷歌云事业部 Google Cloud Enterprise 后，谷歌立即挖来了 VMWare 的前 CEODiane Greene 任职谷歌的云计算事业部的高级副总裁。谷歌云包括了 Google for Work、云平台（针对 AWS）、和 Google App。谷歌已经开始重新定位其品牌。2016 年 9 月，Google for Work 品牌更名为 G Suite。视频群聊和没人爱的 Google+网络也正在转向企业用户。我们已经看到，谷歌正在收购和建立众多对开发人员友好的服务来区分其平台。然而，谷歌也在很多业务中利用了机器学习技术，以便在与对手的竞争中抢占先机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌大脑和公司的人工智能收购业务有助于推动这些进展，同时也肩负着提醒旁观者 Mountain View 在人工智能领域的领先地位的责任。然而据我们的趋势分析显示，就其云产品而言，谷歌在媒体报道上依然落后于微软 Azure 和 AWS。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNhmoRT3yoeKdes7fyhqLL4qMFEbvqVCTV7ibAhdmCGMibXOHev7fvcM8Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然谷歌一直落后于亚马逊、微软，甚至是 IBM，但早期的回报总是积极性的。今年第二季度谷歌的「其他收入」（包括云及许可费用，硬件、及其他非广告业务）为 22 亿美元，并以 33% 的年增长率上升。公司高管们很快就注意到了，云服务是这一增长的主要驱动力，而且将仍然是谷歌找到创收新途径的战略基石。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;消费硬件&amp;amp;平台&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经涉足了这个领域，但是 Alphabet 面向消费者的项目一直在努力平衡他们在金融实用主义上对激进项目的偏好。Nest 最近的已经出现在各个角落，但是其他部门的业务还在面临自己的困难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌搞出了 X，而 Advanced Technology and Projects（ATAP）的研发部门也在开发「epic shit（史诗级项目）」，并与公司保持适当的整合。但是，像其他的部门一样，针对于消费者的研发部门今年也经历了更换重要领导。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌自从把摩托罗拉前总裁 Rick Osterloh 拉回董事会主持新一轮洗牌时，汇集了像 ATAP, Chromecast, Nexus, Pixel 智能手机和谷歌眼镜（后者原先由 Tony Fadell 负责）不同的面向消费者团队。这个新的硬件部门最近搁置了 Project Ara，自 2013 起来一度大肆宣传的模块化智能手机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌一直对其自主手机硬件品牌产品 Nexus 和 Pixel 不太满意，现在 Pixel 取代了 Nexus，并与运营商签约了正式分销协议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的消费设备跨越各种智能手机、平板电脑、笔记本电脑，以及一个运行 Andromeda 的混合设备，一个统一了安卓和 Chrome 的操作系统。该公司继续在很大程度上轮流依赖于多家制造商伙伴（三星、HTC、华为等等）为其生产设备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，谷歌谷歌刚刚宣布了 Pixel 线的品牌制造厂商，并促成了合作伙伴 HTC 与富士康合作（据说导致华为退出该项目）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些新手机与 Home 和 Daydream 一起在 10 月的「Made by Google」大会上发布。该产品的闪电问世为 Osterloh 的新硬件部门定下了基调，以谷歌为中心的品牌推广透露着该公司有点模仿微软 Surface 产品线的意味，更加接近苹果在硬件设计和软件服务上的业务布局。新的 Pixel 设备的溢价定价比对了之前 Apple 的 iPhone7 系产品的定价。谷歌又一次在一个成熟的行业中追逐一块高利润的馅饼。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌也在展望对话智能平台的未来，从 Allo 的即时讯息到智能家居。此外，新的 Pixel 智能手机已经有内置捆绑人工智能的功能，包括新的虚拟谷歌助手，以及 Pixel 用户的照片和视频无限存储。至少现在，谷歌正在为其品牌 Pixel 和家居设备保留了语音助手（通过 Allo 的机器人聊天可以用上这个语音助手，但是没有整合进来）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌一直在利用人工智能来区分其消费云产品，比如其带有自然语言搜索和自动脸与对象识别的照片产品&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在硬件和即时讯息这两个业务上，谷歌已经晚了一步，但是这些举措都对山景（Mountain View）产品在人工智能技术和作为搜索平台中心上奠定重要地位起到了关键作用。每个通过一个 Alexa-或者 Siri 驱动的设备的查询都威胁到了谷歌当前收入模型的基础。即便谷歌在这里取得了成功，一个以语音为中心的搜索形式仍有可能颠覆其已经依赖了十几年的传统网页搜索广告的显示模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，如果谷歌能将其设备和服务通过其人工智能技术含量区分开来，它将有机会创造出一个新的、潜在的高利润业务线，甚至可以通过拓展市场份额加强其与苹果和亚马逊的对抗。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;增强现实（AR）/虚拟现实（VR）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 的增强现实/虚拟现实的经典战略已经开展成了多个并行运行的项目。该公司在这项业务上已经从预算虚拟现实（Cardboard），到 AR（谷歌眼镜）硬件再到 VR（Daydream）和 AR（Tango）平台，还有其前面提到的 Magic Leap。在我们的增强与虚拟现实研究简报中，我们已经研究了谷歌和其他科技公司。从产品创新来看，Daydream 头戴设备很有趣，它一改之前笨重的塑料套而使用了轻便的布状织物，是一款面向大众消费者的虚拟现实概念的新设计产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNr5Qib9W2btyXc0EwEDKdfNfMiaFWQViacaSqR5Wpre3IG3eTfgr54ibcFA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，围绕谷歌 AR/VR 的讨论大部分都是讨论 VR 而不是 AR，这并不意外，因为它的 AR 可穿戴产品 Google Glass 失败了，而 Daydream 成为了谷歌在此领域中的旗手。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNIzia5cqrhVAt8LTrahsoz6ZuHP1wndlOwDiaLKiaicxHIqE0gP0FxAh2EA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通信 &amp;amp; 能源&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 通过外部投资（SpaceX、O2b Networks）、收购（Titan Aerospace，现在是 Project Skybender）以及一些内部项目，投入了数十亿美元改进全球互联网的接通性。Google Fiber 要做的事已经演变成了对市场中传统电信服务提供商的打击，也就是那些为大半城市群提供千兆网络和电视服务的一方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，Fiber 已经被部署光纤网络的成本困扰一段时间了，更不要提来自在位者的条例和法规难题了（Fiber 曾被指为 Alphabet「Other Bets」中最大的单项支出，Other Bets 是该集团对登月项目经济报告的涵盖性术语。）一旦资本支出自由流通，Fiber 如今正被邀请接受 Alphabet 的经济审查，8 月份来自高层的一份措辞严厉的要求就可见一般：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet CEO 拉里·佩奇命令 Fiber 减少客户攫取成本到目前的 1/10，同时要求 Fiber 主管 Craig Barratt 砍掉一半职员，从 1000 到 500。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像我们上面提到的，对 Webpass 的收购就像一剂药膏，直接缓和了这些损失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其他项目包括谷歌的 Project Fi mobile virtual network operator（MVNO），谷歌称该项目是一项实验，动机是改进现任运营商。一个 MVNO，本质上也就是从无线和市场手机服务商那里购买带宽放到自己品牌、价位和支撑方案下面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Link、Skybender 和 Loon 项目针对的是偏远地区和新兴市场的完全不同的人群，但这显然是「所有人都接入网络」理念的延展。和其它 X 单元一样，Loon 项目最近已经走出了 X，得到了来自 WildBlue 的行业老兵 Tom Moore 的领导，以推动该项目实现商业化。Loon 项目也利用了谷歌的机器学习之力，部署了能够优化气球的位置和方向的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只要这些「登月项目」从 X 毕业，就很容易预想它们会被收纳到 Alphabet 的 Access &amp;amp; Energy 的旗帜之下。有传言说这个 A&amp;amp;E 单位将会换个新名字，但目前仍然还会包括 Alphabet 在能源方面的工作。Sunroof 项目是其中的一项计划，Makani 机载风力涡轮机（在 2013 年收购）如果成功，也是另一个自然的候选项目。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;交通和物流&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌自动驾驶汽车项目于 2010 年在 Google X 成立；其表现和公众知名度使之成为了 Google X 这个部门事实上的代言人。该公司已经相应地进行了投资，有传言说谷歌为这个长期项目准备了 100 亿美元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就在 Alphabet 的重组之前，该项目也雇佣了自己的行业老兵——来自 Hyundai America 和 TrueCar 的 John Krafcik。这被广泛地解读成是使该项目正式实现独立的开始。在 4 月份的一个采访中，Astro Teller 将其描述成是「正在从 X 毕业的过程中」。（尽管 X 已经从谷歌分离，但该汽车项目目前仍然保留了原来的名字）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，这个谷歌汽车项目的测试场地已经从山景城扩展到了德克萨斯州、亚利桑那州和华盛顿州。在 5 月份时，它也与一家主要的汽车制造商（Fiat Chrysler）确定了首次合作关系，并且还在 7 月份引入了一个法律领导。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，一些长期的团队成员已经离开了这一项目。8 月份时候我们看到项目 CTO Chris Urmson 离开，一些工程师也离开了并创立了 Otto 和 Nuro.ai 等公司。随着其他玩家的加入，谷歌在这一领域的独特地位已经受到了挑战，正如我们 Trends 趋势工具对自动驾驶相关热点的跟踪那样：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNQe5q6Ru5IskvNLn8VB1HmMXzRbDKgicvjeFSbHENKrGjBTqYmvaUfWg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在物流方面，Alphabet 有谷歌 Express 同日送达服务，这项服务在今年 2 月份已经扩展到生鲜杂货店。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 X 旗下，还有 Wing 无人机项目，该项目最近和 Chipotle 进行了合作在弗吉尼亚理工测试卷饼送递服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和其它部门一样，这两个快递项目都是针对电子商务巨头亚马逊的无人机和其它物流项目的防御，以应对其对谷歌的产品搜索流量的威胁。它们也将 Alphabet 带入了与 Instacart、FreshDirect 和 Uber 的竞争中。我们已经将 Uber 评价为了自动驾驶和驾乘共享的有力竞争者。Wing 项目也与越来越多自动快递无人机创业公司形成了竞争关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;医疗保健和数字健康&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如我们所见，Bill Maris 领导的 GV 已经投资了很多医疗保健和数字健康领域的创业公司。尽管我们目前还不清楚在 GV 的新领导下，这个趋势是否还会继续，但 Alphabet 旗下已经有两个从事生命科学研究的分支机构了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 Verify（即原来的谷歌生命科学（Google Life Sciences））自 2015 年 12 月以来启动很多项目，包括智能葡萄糖感应隐形眼镜、纳米诊断和用于抵抗震颤的 stabilized spoons（收购自 Lift Labs）。Verily 也与一些顶级的医疗保健品牌建立了合作，其中包括 Johnson &amp;amp; Johnson（Verb Surgical）、GlaxoSmithKline（Galvani Bioelectronics）和 Dexcom（连续血糖监测）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Verily 是另一个还没有明确的商业化规划的 Alphabet 部门。Verily 已经出现了人才流失的状况，一些人回到了谷歌的怀抱，一些人则成了竞争对手。充满怀疑的观察者也在质疑 Verily 项目的有效性和实用性，其中包括斯坦福大学疾病预防学教授。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;人们需要搞明白这些玩意对于市场和公司的意义──一个新世界──或者我们正在谈论的东西将很快展现影响……后者是难以想象的。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时 Calico 展现了登月哲学的真正精神，通过研究年龄基因和挑战衰老疾病来延长人类寿命。不同于 Alphabet 的其他分支，Calico 从集团外雇佣了医疗专家。在 2015 年 9 月，谷歌披露了这家分公司的预算达到了 24 亿美元，在必要的情况下可以增至 49 亿美元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Calico 一直笼罩在谜团之中，它只有很少的具体产品，却更重视进行长期研究项目（Verily 在其基因研究中所言）。Calico 的网站内容简单，但仍然显示他们正与许多著名公司合作，包括 AbbVie，AncestryDNA，同时还有很多大学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;金融科技&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们曾详细介绍了 Alphabet 突然切入金融科技领域，所以这里只是简要介绍一下。在投资领域，GV 和谷歌资本在科技金融的投资中占有重要地位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHNlSGCdGVC9r9lSn1vKCicDmJwIaj4u0vS3fNumaon8zF18uIraoMxc5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;保险技术是其中的焦点，Alphabet 在 2015 年中参与了至少 6 次这方面的合作与投资。这包括 Nest 与 American Family 的合作，和现在已经结束的 Google Compare，CoverHound 和 Compare.com 的伙伴关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibEqKGrib6p0EJy7MBHwtMHN9exXXKu4kgicwicwWwl02ZhPUIERFicmTHBlZ6Nm2JqutIw3ib9zibicxHDw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年六月，谷歌又宣布禁止在其所有网站上出现发薪日贷款的广告。在支付领域，谷歌停止了实物的 Google Wallet 卡服务，但仍在继续运营 Android Pay 平台。后者面临的竞争对手不仅是苹果，还包括 Android 授权的很多公司包括三星，他们也都在自己的手机上开发出了自己的支付平台。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总而言之，Alphabet 目前正在公司结构的转型期。公司的目标已经转变为面向更协调的目标和多元化的盈利。Alphabet 已经在努力使长期以来分工不甚明确的各个分支目标更加清晰。观察家和股东们都欢迎新的分支，硬件和软件团队，并满意公司专注于领导行业的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，公司内部仍有冗余，大量 X 计划在过渡期间仍然存在。Alphabet 目前正将重心转向利润和商业潜力，同时以更为集中的方式应对竞争对手的挑战，它已选择了一个主要武器：人工智能。人工智能将是 Alphabet 在未来新市场中的杀手锏。但深耕人工智能是否能为谷歌带来成功仍有待观察。这主要取决于执行，以及人工智能的应用能否在运输，云服务，医疗和消费级硬件等各项领域中同时展现出竞争力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 21 Oct 2016 16:14:09 +0800</pubDate>
    </item>
    <item>
      <title>专访 | 英国机器人初创企业Emotech：如何打造一款类人脑情感互动机器人？</title>
      <link>http://www.iwgc.cn/link/3166700</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;技术顾问：赵巍、砚晨、张俊&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;em style="max-width: 100%; color: rgb(136, 136, 136); line-height: 1.75em; text-align: justify; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;今年 5 月，英国机器人技术创企 Emotech 宣布完成总额 1000 万美元的 A 轮融资，将开发一款拥有自己性格的人工智能机器人 Olly 。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;Emotech 的起点是 2014 年 8 月庄宏斌和 Chelsea 一起获得 UCL 学生创业大赛的第二名，这是 UCL 年度针对从本科到博士在内的全体学生的创业竞赛，负责人 Timothy Barney 直接授命于现任英女皇负责提升英国高校的创业水平。最初的产品叫 Happy Cube，目的改善白领在办公室工作的情绪。&lt;br/&gt;&lt;br/&gt;虽然当时的产品仅是拥有浅层人工智能的桌面小装置，与现在功能强大的 Olly 无法相比，但这是庄宏斌探索「改变人类与机器关系」的第一步, 也是公司被命名为 Emotech, Emotion+Technology 的原因。&lt;br/&gt;&lt;br/&gt;近日，「机器之心」独家对话了这家来自英国的初创企业。Emotech 的 创始人庄宏斌、联合创始人 Chelsea 、Jan，以及科学家团队中的 Pawel、Zarf 和 Pedro 分别就公司团队、技术、产品以及人工智能行业等问题接受了「机器之心」的采访。&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;关于产品和技术&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：请介绍一下你们的产品，它能完成怎样的任务？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;我们目前在开发新的产品设计，今年 11 月左右会公布，到时再详细地介绍。简单来说，Olly 是世界上唯一具有独立个性的家用机器人，类人脑的情感互动是它的重要特色。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;机器之心：HB 有丰富的产品管理经验，能否介绍下 Olly 的产品思路和理念？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;产品的思路源于之前做社会网络时对产品给用户带来价值的理解，2007 年人人网用户数量大爆发其中的一个关键因素是产品策略上的优化让用户找到失去联系的老同学/朋友，这里面是情感紧密联系的。当时一直在思考如何衡量产品给用户带来的价值，比如从表面的数据上，两个用户在吵架，带来的「UGC」要远比一句「我喜欢你」多得多，但显然价值和体验恰恰相反。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibZF0R7M7A97iajltKvGkDSI7CseOht6CJLDe511R69yr7cpwaLuHINBXGeuBLibL7LIiaDDDuIeBNCA/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;br/&gt;当然这只是一个简单的例子说明情感的重要性，深入下去会发现这其实在多方面有情感的理论根基，包括认知心理学和神经科学等等。所以我坚信下一代的交互模式情感是不可或缺的。技术上人工智能的发展和产品上情感设计的升级，让个人机器人成为可能。&lt;br/&gt;&lt;br/&gt;等产品公布时可以比较系统和详细的讲。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：Echo 和 Google Home 等都有巨大的数据支持，作为一家创业公司，你们的技术在哪些方面有竞争优势？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;我们对于竞争的的理解：&lt;br/&gt;&lt;br/&gt;第一，数据固然重要，但算法和模型也尤其关键，我们专注的领域和用户群比大公司要窄，而且我们同样拥有在这些领域的顶级科学家（语音识别、类人脑、增强式机器学习）；&lt;br/&gt;&lt;br/&gt;第二，边际效益递减，数据并非越多越好；&lt;br/&gt;&lt;br/&gt;第三，Data-efficient Learning 也是我们的强项之一。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：在英语用户使用中学习到的经验能否迁移到汉语环境中，能迁移多大比例，还是要重新学习？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;/span&gt;语音信息的特征可以被学习和迁移到不同语言，这已经被很多研究验证了；另外有些数据（比如文字）可以通过很多线上资源低成本地获取，而有些数据（比如用户交互模型）是可以被抽象，与具体语言分离的。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：Olly 能够学习不同用户的个性，在家庭使用中，Olly 如何区分不同角色？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;产品设计理念上，我们参考了很多家里宠物尤其是狗和不同家人之间的关系和互动；技术上，声音、样子等等都可以成为角色区分的方法。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;机器之心：预计语音助手技术何时成熟，迎来市场爆发？有进入汉语市场时间表吗？&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;从技术方面，硬件上 MEMS 麦克风发展非常迅速，虽然还需要提高，但 1-2 年内质量和成本上都要大大优于现在；而目前随着深度学习的发展，我们已经看到很多可以通过算法方面来降低硬件要求的例子。&lt;br/&gt;&lt;br/&gt;当然不同领域会有不同，比如车载的语音助手和家居语音助手的要求是不一样的。市场方面语音交互是和 GUI、触屏交互同一个量级的概念，但我们相信市场成熟需要的时间应该比智能手机花的时间短。&lt;br/&gt;&lt;br/&gt;明年年底左右中文的 Olly 就会开始售卖。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：很多公司在开发语音助手时更倾向于基于已有的平台开发软件和服务，因为更简单，你们为什么要选择做更难的硬件呢？对于智能助手来说，硬件实体带来的最主要区别是什么？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;一方面实体机器人有多方面优势，包括数据收集和情感互动等, 我们尤其在 Olly 的动作方面做了很多吸引人的有趣设计；另一方面我们团队很多人都有创造一个实体机器人的梦想。最后就像滑雪一样，高级道的缆车排队的人远比低级道少得多。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;机器之心：你们的产品应该是做开放域问题的 chatbot, 请问开放域 chatbot 难点有哪些？你们成功解决了哪些？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;我们的产品和 chatbot 还是有很多不同点。但就 chatbot 来说，基本的挑战主要集中在&lt;br/&gt;&lt;br/&gt;1. 理解语义；&lt;br/&gt;&lt;br/&gt;2. 确定内容；&lt;br/&gt;&lt;br/&gt;3. 确定如何表达。&lt;br/&gt;&lt;br/&gt;对于我们而言，在场景理解，情感和数据有效性等方面有很多优势，这对 Olly 和用户互动上有很大的帮助。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：开放领域的 chatbot 比特定领域的 chatbot 难度更大，为什么选择这个难度更大的领域而不是先从哪个领域内的 chatbot 做起呢？比起巨头，你们又有哪些优势？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;第一，我们做的事情和 chatbot 有一些共同点，但还是很不一样的；&lt;br/&gt;&lt;br/&gt;第二，仅从难度方面探讨，难度小的事情没什么意思；&lt;br/&gt;&lt;br/&gt;第三，相比起「开放域」或「通用」，我们更倾向于认为我们是找到了一个简洁优美，并且高度可扩展的方向。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：实体类的 chatbot 可以获得更丰富的 context 信息，包括语调，表情，历史对话，位置等，处理这么多类型的 context 是一个比较困难的问题，你们如何处理？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;Olly 的核心 AI 引擎本质上就是多重模态的；我们通过低层算法处理各种输入，然后用高层算法合并。举个例子不同算法处理音频和视频信息，而提取出来是什么人正在说话。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：要实现深层次的个性化需求需要来源广泛的各类数据，请问作为作为创业公司你们如何获得？或者是如何与其他公司合作？&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;做一个令人兴奋的产品是关键，大家会觉得一起合作有前景，有意思。这里面既有我们寻求的合作伙伴，也有一些是看到我们的报道后，主动找到我们的。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：在获取多样化的数据之后，实现有意义的个性化还需要深层挖掘各数据之间的关系，并具有一定的长时记忆和推理能力，这些都是技术上的难点，请问 Olly 在技术上有什么创新？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;我们的 AI 引擎是一个高度可扩展的层级结构，这个架构有一个很大的好处就是支持我们对不同类型数据进行处理。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;机器之心：你们如何保护用户的隐私？用户的历史数据你们如何处理？在用户隐私和提供个性化服务之间，你们的平衡理念是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;我们对用户的隐私极度重视。首先我们只收集经过用户同意的数据，而且用户拥有对自己所有数据的完全控制；其次我们尽量进行本地化实时处理，如果需要通过云端，则尽量只传输提取后的特征数据，而且本地和云端都会经过严格的加密。&lt;br/&gt;&lt;br/&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：human-inspired computing 是否可以理解为用机器学习理解问题？数据规模和质量的挑战都很大，如何解决问题？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;strong&gt;Emotech&lt;/strong&gt;：并不一样，比如飞机并不需要像鸟一样扇动翅膀，具备理解能力的算法并不需要像大脑一样。深度+一些特定的增强式学习方法会被用到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;关于团队&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：目前公司规模及技术人员比例是怎样的？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;目前 15 名全职员工，会继续扩中到 25-30 名左右，其中 80% 为技术人员科学家。英国和美国是全球人工智能储备最多的国家，Emotech 会继续以伦敦为核心研发中心，同时开始向硅谷发展，吸收优秀的人工智能专家及市场领域的资深人员，为明年的年产与销售做准备，努力让 OLLY 成为世界上最有趣的个人机器人。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：Zaff 角色多重，这是保持创造力的方式吗？&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;角色多重，在各跨界中都有专业水准并不是我们的用人标准，但似乎公司充满创造力，勇于改变和冒险的气氛吸引了这样的团队成员。创始人庄宏斌除了产品设计外，也是单板滑雪的狂热爱好者，在创业的巨大压力和紧迫时间下，还是会在冬季每个月去瑞士 Zermatt 峰报道；Chelsea 是科班出身的当代艺术投资专家，至今还在最大的艺术杂志《hiart》撰写专栏；Pedro 是乐队的贝斯手，还出过专辑；Pawel 热衷钢琴作曲等等 &lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：对于一家聪明人聚集的人工智能公司，平时有哪些团队建设？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;吃吃喝喝是团队的重要娱乐之一，尤其是在伦敦美食地图 Szu 博士的领导下，这个拥有 14 国母语的超级国际化团队善于在伦敦的东区和 central 寻找各国美食，并在肉食动物和素食者之间找到平衡；宏斌、Jan、Pawel 都是单板迷，在计划冬天的集体滑雪；音乐和艺术也是团队的共同爱好，2 个钢琴，1 个古筝，1 个萨克斯风，1 个贝斯，等等。&lt;br/&gt;&lt;br/&gt;娱乐之外，充电也是重要内容。公司支持团队成员相关或者不相关的学习，比如参与心理学互动研究、摄影、人工智能垮领域交流等。始终相信，没有不相关的学习，只有不断开拓未知领域，勇敢冒险，才能真正创造世界上最好的人工智能创业团队。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;关于人工智能行业&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;机器之心：英国的很多人工智能创业公司都走上了被收购的道路，你如何看待这种现象？被收购是好事还是坏事？Emotech 未来是否走同样的路？&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;首先很多美国的创业公司也被收购，另外很多英国人工智能公司被大公司类似 Google，Amazon 和 Apple 收购也从一个侧面反映了英国 AI 的技术积累和先进性。在去年 TechCrunch 获奖之后，我们已经收到不少来自产业基金的投资意向，但是为了最大程度的保持独立，按照我们的规划打造世界上最好的人工智能团队，生产最有趣的机器人，我们选择了现在的投资人。&lt;br/&gt;&lt;br/&gt;但是，我们并不拒绝各种形式的跨界合作，开放平台与各种智能设备整合，或者 Olly 主体的特殊定制，还是人工智能软件与其他硬件厂商的合作，我们都非常乐意尝试，只要不违背我们向用户提供最好的「hands free 的人工智能体验，让用户体验更好的生活方式的原则」。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：Yann LeCun 说生成模型是机器学习领域过去十年最有趣的进展，你们认为近期的机器学习和人工智能有哪些重要的发展？&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;我们非常惊喜地看到很多进展其实是硬件端带来的发展，很多解决方案是在相对简单但是通过大规模数据训练出来的模型。当然在理论方面的进展也是惊人的，比如对联结式模型机制的更深入理解所带来更优的方式优化和参数使用（例如注意力机制）；另一方面是对多模式数据的联合处理，比如结合语言和视觉数据。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：DeepMind 大大提升了强化学习的热度，能否请 Pedro 给没有背景的专业读者解释说明一下？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;大家应该比较熟悉图片分类的算法，这类算法需要判断每给定一张图片中是否有一只猫或者房子等，这样的任务是一张张图片完成但相互间不会影响。与这类算法不同，经典增强式学习的问题的例子包括让一个机器人走出迷宫或者下围棋，在这类问题里，机器人当前的每一步（左转或右转，下哪一格等）都会影响到后面的步骤，这就是增强式学习最基本的特点：解决有连续性相互影响的问题，过程中机器需要在连续性的很多步中选择正确的答案（却不知道最终结果是什么）。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：在对话和自然语言处理领域目前亟待解决的技术难题是哪些？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;绝大部分当前机器学习应用的挑战在于很多模型在特定的任务/领域可以取得非常出色的表现，但却很难应用到更广义的领域。对话，自然语言处理和语音识别也有这样的问题。当然具体来讲有很多难点需要更多的投入，比如说当多个用户用同一个设备时，需要特定的处理，再比如说，经常是即便语音识别已经达到非常高的水平，系统并不理解用户的意图或者失去对话中的当前状态；另外自然语言生成也是很有挑战的领域。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：bot 是目前很多科技巨头都看好的方向，认为是将来可以解除自然对话连接服务的一种全新的产品形态，你们对此怎么看？如果长期看好的话，目前制约 bot 发展的技术难点和产品难点各是什么？&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech&lt;/strong&gt;&lt;span&gt;：更全面地说，人工智能带来全新的产品形态，人工智能是这整个星球的趋势，而且我们坚信会是一个比移动互联网要更大的市场，因为它不光可以变革移动互联网上绝大部分的应用，还产生新的交互方式以及有物联网等基础；学术界已经积累了好几十年，工业界刚刚开始。技术难点方面前面的问题已经有提到了。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;机器之心：团队中有些神经科学背景的专家，如何看待神经科学与人工智能之间的关系？&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;strong&gt;Emotech：&lt;/strong&gt;&lt;span&gt;我们可以看到很多人工智能上的进展是得到神经科学方面的启发，包括深度学习和一些关键计算机视觉方面的突破；人类拥有这个星球上最高级的智能系统，显然有很多方面值得研究，比如小孩的学习能力远比目前的计算机系统要先进；Olly 的核心是一个类脑人工智能引擎，使其情感化及高效学习；当然，如以上提到的：飞机并不需要像鸟一样扇动翅膀。&lt;br/&gt;&lt;br/&gt;人工智能技术的成熟可能会带来一种全新的产品开发思路，而产品是一个综合学科，显然人工智能相关的产品其科研性会更突出，但依然是建立在满足用户需求的基础上。这部分等我们的产品出来后我们会比较系统地讲一次，这样能更形象一些。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 21 Oct 2016 16:14:09 +0800</pubDate>
    </item>
  </channel>
</rss>
