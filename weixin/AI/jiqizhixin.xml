<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>教程 | 西红柿还是猕猴桃？一个案例帮你入门机器学习</title>
      <link>http://www.iwgc.cn/link/2580973</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 dzone.com&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Tamis van der Laan&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、孙宇辰、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;过去几十年来，计算机系统已经取得了很多成就。它们已能成功地组织和编目我们整个文明所产生的信息。他们已经将我们从严格的认知任务中解放了出来，并显著增加了我们的生产力。如果要说工业革命实现了劳动力的自动化，那么我们就可以说数字革命正在实现认知劳动的自动化。但这种陈述并不完全正确，否则我们岂不是全部都要失业？所以有哪些工作人类能做而机器却无能为力呢？&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;经典的计算机系统可以被视为是一种巨大的信息交换网络，它可以将信息引导到所需要的地方。SAGE 就是这样的一种系统，它是世界上最早的、也是有史以来最大的计算机。SAGE 计算机系统开发于冷战期间，其任务只有一个：将来自美国各地的雷达系统的信息整合起来为国防的目的服务。它所要解决的问题是追踪和识别飞过美国上空的飞行器。因为当时核武器只能通过飞机进行部署，所以为了防备苏联核炸弹，有必要对整个美国领空进行监控。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SAGE 系统可以整合来自所有雷达设施的信息，并将这些信息传送给可以分辨飞机是敌是友的操作员。然后这些信息会被传送给其他操作员并最终到达指挥中心，然后指挥中心做出决定：使用战斗机或地对空导弹系统拦截敌机。我们可以说 SAGE 是有史以来第一个大数据处理系统。这个视频展示了 SAGE 系统的运作：&lt;strong&gt;https://www.youtube.com/watch?v=06drBN8nlWg&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 SAGE 系统很了不起，但我们不能说它是非常智能的。这个系统所做的不过是按一种固定顺序的方式将信息传送给操作员而已。因为可以将信息自动传送给相应的工作站，所以工作任务可被划分给不同的操作员，每个操作员只需要专门应付自己的特定任务。由此可以更快地做出决策，从而可以及时地将敌机拦截下来。但 SAGE 系统并不智能，因为它严重依赖于操作员的认知能力来进行复杂决策。然而，它却提供了一种能够极大提升操作速度的信息流动基础架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于计算能力的增长，现在的计算机已能自行进行复杂的决策。但它们仍有一项限制：需要人工的编程它们应该在那哪种情况下做出哪种决策。我们可以天真地认为只要有足够多的程序员和时间，我们就可以让计算机接管我们所有的认知任务。然而，事实并非如此，有的应用根本无法通过经典的编程方式实现。脸部识别问题就是一个很好的例子。事实证明，人工编写一个能够识别脸部的算法是非常困难的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中的一个问题是：如果你让一个人描述他们是如何识别人脸的，他们会告诉你他们不知道，自然而然就完成了。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二个问题是我们很难描述是什么构成了一张脸。也许有人会说一张脸有一个椭圆的外形，但要准确描述这种形状以及不同的人脸之间的差异是极其困难的，更不要说因为观察角度不同而引起的形状变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三个问题是事物往往会随时间改变。比如说，蓄须在近些年来变得越来越流行了，这可能会让系统难以应对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器学习&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习已经在计算机科学领域存在了很长时间，其关注的重点是创造能够从数据中进行学习的算法，从而让我们可以解决不能直接通过人工编程解决的问题，比如面部识别。其中的基本思想是：我们不直接编写识别人脸的算法，而是编写能够间接地根据样本学习识别人脸的算法。这样的算法可以根据这些样本学习出一个能够量化是否构成一张脸的特征的算法。因为这样的系统基于样本进行学习，所以我们可以以一种连续的方式送入样本，从而使该算法可以连续地更新其内部模型。这将确保我们总是能够识别出面部，即使面部毛发的流行趋势发生了改变。机器学习的定义是非常宽泛的，即：能够从数据中学习并因此常被应用在语境中的算法。其中一些应用领域包括计算机视觉、语音识别和自然语言处理。机器学习常常和大数据系统联合在一起使用。比如说用于分析大量文本文档以提取主题。伦敦市运行着一个能够使用全市的摄像头追踪人群运动的系统。美国开发了一种使用固定于无人机上的单个摄像机临时地观察整座城市的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器学习的一个案例&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了理解机器学习的工作方式，我们将使用一个相当简单的案例进行说明。假设我们将要打造一台需要区分西红柿和猕猴桃的工业分拣机。这台工业机器使用了一款特别的测量设备——该设备使用激光来检测流水线上的物体的颜色，并将它们分成红色和绿色两类。这台分拣机必须使用这些信息来决定将一个目标放进西红柿箱或猕猴桃箱。一种能让我们区分不同类别的机器学习算法被称为分类器（classier）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;判别式（Discriminative）分类器&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了开始我们机器学习的旅程，我们需要一些西红柿和猕猴桃作为样本：假设有 50 个不同的西红柿和 50 个不同的猕猴桃。然后我们运用我们的测量工具测量样本。结果为 100 个代表着水果红色或是绿色的测量，此外我们也记录水果属于哪个种类的测量结果——属于西红柿或是猕猴桃。我们标记西红柿为 -1，猕猴桃为 +1。因此我们有两个有 50 个测量值和 50 个标签的集合。让我们把我们的测量结果放到 x 轴上，标签放到 y 轴上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk4Gd4yMcLn2qgUWTyKLoPVuOGhbFV0xTvYXFhQhBUicWRXvFARKLhUUA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们看到西红柿和猕猴桃往往位于水平轴上它们自己的空间之中。尽管这两个类之间还存在一些重叠。这是可以预料的，因为有一些还没有成熟的西红柿是绿色的。这也告诉我们，只是测量颜色并不能提供足以区分西红柿和猕猴桃的足够的信息，但这里只是为了演示，我们可以将问题简化一下。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们对一个未知的水果进行测量，我们现在可以通过简单的观察水果在哪个区域里，区分它是西红柿还是猕猴桃。这里我们所构造出来的即被称为判别式分类器。有辨别能力的部分参照的是根据所属的种类划分空间到不同区域的性质。这种类型的分类器已经非常成功，并在许多应用中得到了使用。下一步进展就是要建立模型，使我们能将两种水果分开。我们通过设置一个西红柿区域为 -1、猕猴桃区域为 +1 的函数来完成这项任务，从而将空间划分成了两个种类的区域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Dukick9jhZpbDgiajiaeJ3FcTbE7TUUicfe2scUxMfnm9L4WnKw15CkDmVibMQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两个种类区域间的分界线被称之为分类边界（classification boundary）。目标是用这样的方式划分空间，使得更多的水果通过我们的模型可以尽可能正确地标记出来。这个过程被称为学习（learning），我们将要在这篇文中的后面讨论这个过程。有了我们的模型，我们现在可以基本上放弃我们的样本，直接用模型区分新的目标是西红柿还是猕猴桃。我们通过观察事物的颜色，根据它的测量结果落在哪个区域决定它是什么种类。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukaT9dooTGttcUceHvAY8otGqIbSKjjdibBMLwORiaTggiazv4bkIaNXY4w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在上述的例子中，我们发现我们的测量结果落到 -1 区域，因此将该目标标记为西红柿。这种方法是正确的，但是需要注意与其相关的一件事。对于我们的测量结果，我们的模型没有保留数据的密度或是频率。为了解释这是什么意思，考虑如下场景：如果我们新样本离我们的训练样本非常远呢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在的问题是：我们是找到了一个特别的红色西红柿，还是什么地方出了错误，例如一块明亮色彩的红塑料以某种方式进入到了系统中？这是我们的判别式分类器所不能回答的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Dukbcrow4sUwFGzho2JKCjKT2Wtq5Do1AFjK1TzImuFXicdib8lCoKdCSicg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;生成式（Generative）分类器&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk354ibM5klWMKx9rhLiczolLDBY5jYrWxx3quuHjyTnQEcTgtgJwFSNtw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了解决上述的限制，我们引入了生成式分类器。我们的生成式分类器将要用两个正态分布为我们的两种水果建模。每个正态分布代表着根据给定的颜色测量结果所属种类的概率。即每个正态分布是对它们所属种类的似然性的建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了能区分西红柿和猕猴桃，我们现在观察两个概率密度函数哪个更大，从而对一种新的测量进行分类。事实上，我们正如之前那样将空间划分到两个区域，之后如果我们想，我们就可以舍弃所有正态分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但我们不会这么做，因为正态分布中还包含了我们可以使用的附加信息。即正态分布可以告诉我们一个样本是否可能属于或不属于其中的种类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;考虑一下上图中的测量结果，其所在区域距离我们训练集中的样本很远。我们发现不仅根据我们的训练集，测量结果是奇异的，根据我们的模型（即正态分布）也是不大可能发生的。因此我们能够通过我们的模型检测出这一点，并让操作员停掉流水线来检查问题。你可能怀疑「生成（generative）」这个单词从何而来。这个词来源于：我们可以根据我们的正态分布提取出样本，并在实际上生成测量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;学习与优化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到这里，我们已经看到了两种机器学习的算法，判别式（discriminative）与生成式（generative）。我们假设我们有一个能构成我们模型的样本集（事实上称为训练集）。下面我们看一下如何使用训练数据构建一个模型，这个过程被称之为学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们从生成式分类器开始，我们希望调整两个正态分布符合我们的数据。两个正态分布通过它们的均值 μ 以及标准差 σ 量化。所以如果我们能找到那些参数，我们就可以构建我们的模型并调整该正态分布以适应它们各自的类数据。为了计算均值，我们在每个类上对我们的测量结果求样本均值。为了计算标准差，我们在每个类的数据上对样本方差求其平方根。样本均值与方差根据以下两条公式计算得出：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Dukq0Rvc9KcgiaaWZUxmfJTQiaVltysVpkxyuWF8p1AXmIp8DnpqfcxVV7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来我们回到了判别式分类器。这可以通过我们的判别式分类器进行解释。我们的判别式分类器使用如下函数作为它的模型：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;f(x) = sign[a · x + b]&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有度量结果 x，常数 a 和 b 决定度量结果将在什么地方分为 -1 和 +1 区域。我们的目标是找到可以使尽可能多的样本被分类正确的 a 和 b 的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;寻找参数不总是通过分析的方法处理的，这也是我们判别式分类器的情况；在这种情况下我们需要一些能迭代地寻找参数的优化方法。我们使用下列优化算法寻找 a 和 b 的值：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;算法 1：优化算法（Optimization Algorithm）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 首先，对参数 a、b 值进行随机的猜测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 根据所有样本的测量 x 和标签 y 更新变量：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukGSnMAwuAQ4JqdBQk8icY3Idxnc6IxcRnCvibMlt7SQK7PoicBmDOkRvqg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 如果没有达到最大迭代次数，就返回到步骤 2.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个算法中，f(x) 是将我们的模型应用于测量 x 的结果。这将产生一个 -1 或 +1 的分类。之后我们比较这个结果与样本相应的标签 y。接着我们根据比较的结果更新参数，例如我们以某种方式移动决策边界（decision boundary），使样本更倾向于被更新。更新的大小取决于 α，其被称为学习率（learning rate）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;监督和无监督学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们知道生成式和判别式分类器之间的差别了，也知道在进行无监督学习时如何学习这样分类器的参数。你可以没意识到，但上面的案例中我们已经使用到了监督学习。我们有一个包含测量标准（水果颜色）和标签（西红柿或者猕猴桃）的训练集。但如果我们不知道我们的样本标签怎么办？换言之，我们有水果的测量，我们知道它们要么是西红柿要么是猕猴桃，但我们不知道具体是哪个。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk4RfoZzXMLxdKNb15HXvvFKNZwdCvOjGhRnCDEWxm7mcx2L0JDr2KJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是无监督学习的问题了，它要比监督学习复杂的多。解决该问题的方式是回到分类的生成式方法上。使用生成式方法，我们在类别数据上拟合两个高斯函数。然而，在无监督学习中我们没有分类数据，而且需要在不知道数据分类的情况下拟合高斯函数。做到这一点的方式是尝试拟合两个高斯函数，以便于它们能最大可能好地解释数据。我们想要最大化由两个正态分布生成的数据的似然性，通过使用一个被称为期望最大化（expectation maximization）的复杂优化算法做到这一点。该算法是一个发现最大化后验参数评估的迭代过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该算法不在本文的讨论范围之内，但有很多这样的资源可以查看。在应用期望最大化优化算法之后，我们得到平均值和方差，得到如下结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Dukv952lG4wlOFUNWgE9ibhfZlw4N9ibyglynOJJhgdAhWNYMucSNtIqwTg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们可以像之前一样通过比较似然性来分类这些点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk58ehXPurPODV3ddE7nv6nMfzb8SyzIC8W1Rh72dxI6vYmicJUt1a7sg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;参数算法与非参数算法&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们打算探讨参数和非参数分类器之间的差别。参数算法（parametric algorithm）一词指你用来定义模型的参数。使用参数算法，我们在训练数据中提取用于完整定义模型的参数值。我们前面看过的两个模型都使用参数。判别式模型使用两个参数：a 和 b，生成式模型使用四个参数：两个平均值（μ1、μ2），两个方差（σ1, σ2）。在找到这些参数之后，我们就不再需要训练集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;非参数算法（non-parametric algorithms）不使用参数对模型进行特征化，而是直接使用训练数据。这经常需要更大的计算花费和存储能力。但这些非参数分类器往往也更加准确，相比于参数分类器更容易在小训练集上开展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duktf4MycVdNmLBgBAxocgP9acIEE33Vt4Ctelib2sHRwKHhB5jtyQ4ywA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个典型的非参数算法是 K 最近邻分类器（k nearest neighbor classifier）。给定一个新的测量，我们寻找 k 最近邻值，并将最常见的类别标签分配给该标准。在我们采用测量标准的 5 个最近邻值时得到 -1,-1,-1,+1,-1 这 5 个标签。我们看到 -1 出现最频繁，因此我们准确地将测量分配给 -1，即西红柿类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;过拟合&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，我们假设只有一个机器学习模型、一个样本训练集、一个从这些样本中进行学习的优化算法。我们将讨论的下一件事是过拟合问题。如果我们采用一个判别式分类器处理样本，我们可以看到它将空间分成两个区域。我们也可以考虑用一个分类器将空间划分为多个区域进行分类。在我们的样本之下，得出了如下结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Dukue9p7rBoQ1DScOee6OkPHtFHhp3c5meZxCV67oPUs9oKiaDfRpYrOMQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们看到，模型将空间划分为多个区域，而且设法在我们数据集上准确的分类更多样本。因此，它现在将更多西红柿标记为西红柿、猕猴桃标记为猕猴桃。你可以认为我们对模型进行了改进，但其实我们所做的恰恰相反。要记得一些西红柿比猕猴桃更绿，而一些猕猴桃比西红柿更红。这意味着基于我们单一的测量标准，这两个分类之间会发生重叠，彼此之间难以清晰辨别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;测试与验证&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了展示过拟合的结果，我们将其它样本集考虑在内。我们现在有一个 100 样本的训练集，使用该训练集学习一个机器学习模型，然后使用另一个包含 100 样本的验证集评估模型。如果将模型应用于评估集我们得到如下结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukOLrXXgxrBvESVgeFsZrYrvXS0xtmDIzLu7lJwVfEUVnLIHWXic3UX1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以看到模型在检验集上的表现比训练集差得多。这是因为我们只使用一个测量标准。这个案例告诉我们，使用一个检验集检查模型是否在训练集上过拟合无论何时都很重要。使用更多复杂的评估技术（比如 n 折交叉验证）能得到更准确的验证，能检验模型在数据上的过拟合程度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多维输入&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，我们只考虑使用一个测量标准分类水果。我们也看到因为只使用一个标准，分类结果可能会有交叉。因此，我们可以考虑增加第二个标准，以便于更好的分类水果类别。的确如此，而且增加维度（增加更多的测量标准）也将增加准确率。为了展示这一点，我要增加一个额外的标准。我们打算用传感器测量目标的硬度与软度。这个思路是因为苹果外表较硬、猕猴桃软，如此我们在这两种水果间就能更好地分类。我们将对比分别有 1 个和 2 个输入维度测量标准的判别式分类器和生成式分类器:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk4S4etEAEqjelMloCOTHfudqJ3RCEwXKzNlbZlzBnLFamZFibpSjSEsg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukAicok7QLFYlWIsHngTNrZyjKl4TPfzjtibBo65XohKicRk4MBCswzrLMA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在上图中，我们看到从 1 维变成 2 维确实提升了我们的分类器的准确度。显然并非所有的测量都会提升我们的算法的表现，事实上，选择糟糕的测量标准还会降低算法的表现。比如说，测量环境的温度或空气运动。这些测量指标与我们分类水果的目标毫无关联，只会给我们的结果带来干扰。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;维度的诅咒&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一种说法是把增加输入测量标准的数量描述为维度的诅咒。这个描述不难理解，但如果你不知道它，它可能就有欺骗性了。可以超立方（hypercube）的容量来很好地阐述这个问题。超立方是 n 个维度的立方体的广义定义。例如，一个 2 维超立方是正方形。2 维超立方的容量就是边长的平方。一个 3 维超立方是正方体，容量是边长的 3 次方。但我们也可以认为 4 维立方体的容量是边长的 4 次方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们降低维度。一个 1 维超立方就是简单的一条线，容量（或者长度）就是边长，同样 0 维超立方就是一个点，区域是 0。我们看到，一个超立方的容量等于边长的 n 次方，其中 n 就是维度的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukFsbBxicibfibRpyE12BU0gmw3yrN1jbAiaYnA6fH0ldmJ71VrOGKa0Szpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，如果一个超立方的边长为 2，我们将维度从 0 增加到 10，我们就会发现其容量变化会呈指数倍增长。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在假定我们有一个 5 个样本的训练集，输入测量标准位于 0 到 2。如果我们有 1 个测量标准，本质上我们的输入空间相当于边长为 2 的 1 维超立方的容量。一个 3 维的输入空间覆盖区域的 2 的 3 次方，也就是 8，可能不能被我们的 5 个训练样本很好的覆盖。事实上，你需要 125 个样本才能合适地覆盖这个输入空间。你有越多的输入测量或维度，你能得到越高的准确度，但因为量级呈指数增长，你将也需要指数性的样本数量来覆盖输入空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukqxLngAibyNGuDATaT3SzxNTKucnyibTezZ8uoN2DnTxt2J4xNkeBjbfA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;降维&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经见过：随着输入的维度的增长，我们所需要的足以适当覆盖整个输入空间的样本的数量会指数式地增长。然而在真实世界中，样本往往具有所谓的多维流形（multidimensional manifold）。尽管多维流形这个词听起来很复杂，但其基本思想却很简单。为了说明，我们想想看一个带有一维流形的二维输入空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们看到我们有两个输入维度，但数据实际上落在 1 维的线（流形）上。我们所能做的是将数据投射这条线上，以为每个样本获取单个值。因此我们已经通过利用我们对流形的知识将该空间从 2 维减少到了仅 1 维。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukYK06UlzukInykKxoCm31C8gDvcFGKIebIV8xibpDvLd5ic2WTdVFg5pQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;特征工程&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到目前为止，当谈到为我们的分类器生成输入时，我们一直都在谈论对目标属性的测量。用机器学习的术语来讲，这些测量被称为特征（feature）。特征也可以从多个测量中进行构建。比如说，除了我们的颜色测量，我们也可以测量目标的形状。我们可以使用一台相机拍摄目标的照片，然后再使用多种图像处理技术，我们可以提取出目标的外形作为附加的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个过程被称为特征工程（feature engineering），而且其在优秀的分类系统的构建中发挥着重要的作用。请注意，本质上特征工程并没有必要。只要数据足够，一个复杂的机器学习模型应该就能直接从数据中学习出这些特征。这些机器学习系统是存在的，其中最流行和最有效地被称为深度学习系统。因为这样的系统被提供的只有原始的输入数据，，所以它们必须直接从样本中学习特征，它们需要大量的训练数据集才能有效。值得提及的一件事是特征工程存在一个限制。特征工程本质上是一种将特定领域的知识直接从人类迁移到机器的方法。但是，人类将该领域的细微之处编写为特征的能力是有限的。因此，只要数据足够，从原始数据中学习往往会产出更为精准的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 06 Sep 2016 17:26:25 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 布局移动端芯片，英特尔收购计算机视觉公司 Movidius</title>
      <link>http://www.iwgc.cn/link/2580974</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 TechCrunch、engadget&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今天英特尔宣布收购&lt;span&gt;计算机视觉创业公司 Movidius ，此次收购距今年 8月收购 Nervana 还不到一个月。在 &amp;nbsp;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718133&amp;amp;idx=1&amp;amp;sn=ec37253865c8fbb20a3f2b4fdc2164dc&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718133&amp;amp;idx=1&amp;amp;sn=ec37253865c8fbb20a3f2b4fdc2164dc&amp;amp;scene=21#wechat_redirect"&gt;芯片大战&lt;/a&gt; 愈演愈烈的当下，英特尔也在积极拓展新的市场道路。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔的 RealSense 平台是其上月举办的年度开发者大会的亮点之一，看起来这家公司正在寻求机会发展其计算机视觉技术的规模与能力。&lt;/span&gt;&lt;span&gt;英特尔今天宣布收购计算机视觉创业公司 Movidius，这家公司之前为谷歌的 Project Tango 提供 3D 传感技术。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Movidius 今天发表的官方博客上，CEO Remi El-Ouazzane 宣布该创业公司在与英特尔的 RealSense 科技一起工作的同时，继续发展公司「赋予机器视觉的能力」。Movidius 一直以来在快速、低能耗的计算机视觉芯片组上有着极大的兴趣，与包括谷歌、联想、大疆在内的众多主要设备生产商都有合作关系。这家创业 8 年的公司在硅谷、爱尔兰、罗马尼亚都有办公室，全部员工大约有 180 名。在过去的时间里，Movidius 已经完成了 8650 万美元的数轮融资，投资方包括 Summit Bridge Capital、Capital-E、DFJ、Emertec Gestion 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duke5a5iaDeQT0lmW6HblCv9dfuM3qNgyA7mlhaYxiadwn3AvFn9jR8VrxQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图为英特尔新技术集团（New Technology Group）高级副总裁 Josh Walden 与 Movidlus CEO Remi EI-Ouazzane&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该公司的第一次突破在 2014 年实现，那时该公司的技术是谷歌的 Project Tango 增强现实概念的一部分。尽管那时候该公司的技术没有给自己带来收益，但它却帮助自己成功吸引到了谷歌的注意。今年早些时候，谷歌确认其在一个尚未公布的项目上与 Movidius 建立了合作关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而且在今年 4 月份的时候，该公司也曾吸引过媒体的关注。Movidius 于 4 月底的时候推出了一款名为 Fathom Neural 的计算棒（Compute Stick），尽管产品外形非常像是常见的 U 盘，但内部装备了一枚高性能的图像处理单元，能够执行一系列先进的图形识别任务。Fathom 可以认为是热插拔版本的 Myriad 2，Movidius 公司希望工程师使用它来创建类似于像素图像标记和高级视频分析等深度学习功能。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本次交易的具体细节尚未被公布。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;El-Ouazzane 在博客中写到：「在人工智能上，我们正处在重大突破的边缘。随着我们在最艰难的人工智能挑战上取得进展，几年后我们将看到新类型的自动化机器，有着更先进能力。这项挑战就是让我们的设备不只能看，也能想。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Movidius 视觉处理单元（Vision Processor Unit）Myriad 2 系列已被联想用于建立该公司的下一代虚拟现实产品，同时谷歌与 Movidius 也达成协议在该平台上部署其神经计算引擎，推动移动设备的机器学习能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Movidius 在宣布交易的文章中写道：「我们领先的 VPU（视觉处理单元）平台可与英特尔的业界领先的深度传感解决方案 Intel ReanSense 技术结合起来用于设备上的视觉处理，这样的组合在可以看到 3D 环境的自动机器上有显著的优势——能够理解它们的周围环境并据此进行导航。今天，我们正与大疆、FLIR、谷歌和联想等客户合作为无人机、安全相机、AR/VR 头设等智能设备带来视觉。但尽管今天的智能设备很吸引人，但不过只是可能实现的未来的惊鸿一瞥。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 8 月份，英特尔就以 4.08 亿美元的价格收购 Nervana，这是一家做服务器端的芯片和平台的人工智能创业公司。此次再次收购 Movidius 这家做移动端芯片的创业公司，可以看出英特尔在智能芯片市场上的决心。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在英特尔最近的 IDF 开发者大会上，英特尔做出了有关其深度感知 RealSense 平台的重大宣布，其中包括被称为 Project Alloy 的新型虚拟现实平台、其自主无人机试点的功能升级和其它旨在强化消费级和企业级设备的计算机视觉的计划。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔想要其 RealSense 传感器技术在尽可能多的设备中得到应用，而其中的一个关键是保持功率使用足够低，这样才能吸引到各种各样的设备。Movidius 让英特尔可以获取其用于低功率移动设备的传感器技术。Movidius 的 SoC 声称功率负载低于 1 W，远低于其竞争对手。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们看到了 Movidius 用新兴技术加速推进我们的计划的潜力，」英特尔高级副总裁兼 New Technology Group 总经理 Josh Walden 说，「使用 Movidius 的低功率和高性能 SoC 进行追踪、导航、地图测绘和识别场景与物体的能力能为我们打开在热量、电池寿命和规格尺寸非常关键领域内的机会。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，眼下的处理器性能大战似乎遥无尽头，所以此次收购也显示出英特尔探索新发展道路的决心。这样的收购也可能使英特尔转向如机器人和智能家居这样的下游市场。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 06 Sep 2016 17:26:25 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 人工智能医学新用途：提升慢性肺病诊断精确度</title>
      <link>http://www.iwgc.cn/link/2580975</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 ScienceDaily&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;EUROPEAN LUNG FOUNDATION&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2016 欧洲呼吸协会国际大会（European Respiratory Society's International Congress）上发表的一篇最新研究成果表示，人工智能可以改进肺功能检查的结果分析。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该研究首次探索了人工智能在改善肺病诊断精确度上的使用。目前的测试需要一系列步骤，包括肺活量检测，测试呼吸时空气流过肺部的量（体积）和速度（流速）；之后是用体积描记法测试测量静态肺容积和气道阻力；最后用弥散测试测量通过肺泡的氧气和其他气体的量。这些测试结果的分析在很大程度上来源于专家意见和国际标准，然后从分析结果中得出一个模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这项新研究中，研究者收集了 986 名首次接受完整肺功能测试的受试者的测量数据。所有参与者收到了一个基于肺功能测试和所有其他必要的测试（如 CT，心电图，等等）的综合结果的诊断分析。最终的诊断结果是一组临床专家达成的共识。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后调查研究了像『机器学习』这样的概念是否能用于分析完整的肺功能测试。机器学习利用一种能从数据中学习并预测数据分析的算法。该团队开发了一种算法，能处理常规肺功能测试参数和包括吸烟史、体重指数（BMI）和年龄的临床变量数据。基于临床和肺功能数据的模式，该算法能为诊断提出建议。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该研究论文的主要作者（senior author），来自比利时鲁汶大学的 Wim Janssens 评论道，「在这项新研究中，我们证明了人工智能能为我们提供更精确的诊断。该项成果的精彩之处在于这个算法能模拟复杂的推理，共临床专家使用来给出诊断，这种更加标准化的客观的方法可以消除偏差。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;临床医生现在必须依赖使用基于人群的参数来分析这些结果。有了人工智能，这个机器就能同时观测到一个组合模式，帮助生成更精确的诊断。之前在心电图的分析中也使用了机器学习算法来增强分析的结果，为常规临床实践提供决策建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究第一作者，同样来自比利时鲁汶大学的 Marko Topalovic 说，「这种方法的好处是让肺功能检测的分析结果变得更加精确和自动化，提升疾病检测的效率，不仅可以帮助没什么经验临床新手医生，也可以减少多余的额外检测，为最终诊断结果的出炉节省了时间，从而有益于整个医疗过程。」该研究团队下一步将在不同的人群中测试这个算法，持续更新经临床诊断验证的肺功能检测数据，提升该系统的决策能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 06 Sep 2016 17:26:25 +0800</pubDate>
    </item>
    <item>
      <title>SyncDaily | 谷歌新专利：自动驾驶汽车检测警车、软银完成320 亿美元收购ARM 交易</title>
      <link>http://www.iwgc.cn/link/2580976</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;谷歌新专利：自动驾驶汽车检测警车、软银完成 320 亿美元收购 ARM 交易...... 机器之心日报，精选一天前沿科技优质内容。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;span&gt;谷歌新专利：自动驾驶汽车探测警车&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2DukpIMUaJgiaBFT2j0aB4zLlTxkgpicYK2rPoQHOodeWib51CicF4xn2CXbZg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据国外媒体报道，近期美国专利局公布了一批新专利，其中谷歌的一个自动驾驶功能获得了许多关注。这项专利表明，谷歌正在为其自动驾驶汽车开发一个警车探测系统。汽车可以很清晰的识别对面开来的警车，并安全的把用户送到目的地。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;根据此前的消息，谷歌自动驾驶的程序已经非常智能，交通规则已经写入到了汽车的自动驾驶系统之中，一般不会受到警察的处罚。谷歌自动驾驶汽车今年 2 月首次违反交通规则，还需要汽车掌握一些最重要的规则：当警察靠近自动驾驶汽车的时候，车辆应该自动靠边停车；假如车道上有执行急救任务的急救车辆时，自动驾驶汽车应当给对方让道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; white-space: normal; max-width: 100%; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;软银完成 320 亿美元收购 ARM 交易&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;软银完成 320 亿美元收购 ARM 交易&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;软银集团今日宣布，该公司已完成 320 亿美元收购英国芯片设计公司 ARM 的交易。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年 7 月 18 日，ARM 宣布正式接受软银集团提出的 234 亿英镑（约合 322 亿美元）现金收购要约。据悉，这是自收购美国电信运营商 Sprint 以来软银进行的最大一笔并购交易，也是今年全球科技市场最大并购交易之一。软银集团表示，交易完成后，ARM 将成为软银旗下一项独立的业务，并将成为软银未来增长战略的核心部分之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; white-space: normal; max-width: 100%; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;自动驾驶汽车会懂得伦理道德吗？美国安全官员不相信&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;汽车制造商会告诉自动驾驶汽车什么是伦理道德吗？在一场车祸中，自动驾驶汽车能决定什么人被救、什么人应该受伤吗？美国运输安全官员对此表示怀疑，他们更相信联邦政府的规定。自动驾驶的快速进步带来了忧虑，人们担心未来的汽车将不得不做出道德选择，比如，为了防止对车外的人造成严重伤害，汽车是否应该转向，以避免车祸的发生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美国国家运输安全委员会主席克里斯托佛·哈特 (Christopher Hart) 在接受《麻省理工科技评论》的采访时说，联邦法规必须为自动驾驶车辆设置基本道德规范，以及必须遵守的安全标准。哈特说，美国国家公路交通安全管理局 (NHTSA) 可能需要自动驾驶汽车的设计师像飞机制造商一样，为他们车辆的关键部件内置一套自动防故障系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; white-space: normal; max-width: 100%; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;最新研究揭晓神秘 X 射线信号并非源自暗物质&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据物理学网站报道，近期，来自星系团的一个神秘 X 射线信号令天文学家颇感兴奋，部分天文学家猜测它可能来自于暗物质，毕竟暗物质占据宇宙 80% 的质量，但是科学家迄今并未探测到暗物质。为了揭晓这一答案，马克斯-普朗克核物理研究所的物理学家发现了另一种解释，这种神秘 X 射线信号很可能源自高电荷硫磺离子，高电荷裸露硫磺核吸收氢原子的电子，之后以 X 射线形式释放能量。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; white-space: normal; max-width: 100%; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;LG 准备用人工智能让智能家居互相协作&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicayMuAraQ7kIrkyQeG2Duk7SphPicrzNTZqDeQU675Rlmgk2eWiaOoPxDKVoYnWThIfzcOzX1icclqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;昨日，LG 电子宣称将加大在机器人领域的投资，他们希望用人工智能来制造更复杂功能更齐全的设备，而这个设备就是用于智能家电设备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据了解，LG 旗下家电部门已经在筹备一个机器人项目，目的就是研制能够与冰箱、洗衣机、空调等家用电器相互协作的产品。因此，和 SmartThinkQ Hub 一样，该项目也是为了解决智能家居设备无法互联互通而推出的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; white-space: normal; max-width: 100%; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;尽管聊天机器人兴起，但美国人仍喜欢移动应用&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聊天机器人的发展尚未对应用经济产生明显影响，来自 comScore 的数据显示，智能手机应用仍保持着向上发展的势头。过去一年，聊天机器人正在兴起。通过聊天机器人，用户可以预定航班，叫来出租车，甚至缴纳罚款。目前，Facebook Messenger 和 Skype 等热门消息平台已支持开发者自行开发聊天机器人。许多业内人士认为，这将对智能手机应用造成不利影响。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;comScore 的数据显示，2016 年 7 月，在美国用户花费在数字媒体的时间中，智能手机应用占比达到一半，高于两年前的 41%。此外，应用经济尚未触顶。随着智能手机的屏幕越来越大，看视频和浏览网页变得更容易。comScore 营销和分析副总裁安德鲁·利普曼（Andrew Lipman）指出，超过 87% 的移动流量发生在应用内。而智能手机应用的发展仍在继续蚕食用户花费在平板电脑和 PC 上的时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编辑整理，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 06 Sep 2016 17:26:25 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 神经网络架构演进史：全面回顾从LeNet5到ENet十余种架构（附论文）</title>
      <link>http://www.iwgc.cn/link/2565649</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Github&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Eugenio Culurciello&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：孙雨辰、李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;深度神经网络与深度学习是非常强大的流行算法。许多运用它们的成功典范依赖于对神经网络架构的精心设计。作者在这篇文章中&lt;span&gt;重新回顾一下最近几年的深度学习背景下神经网络设计的历史。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LeNet5&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。自从 1988 年开始，在许多次成功的迭代后，这项由 Yann LeCun 完成的开拓性成果被命名为 LeNet5（参见：Gradient-Based Learning Applied to Document Recognition）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4yRTJwBXiaXnZINv4GJSR12xf9UJ1sGsKPrYrjunGR0KcSTxGTAUtv2w/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 的架构基于这样的观点：（尤其是）图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。在那时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢。因此，能够保存参数以及计算过程是一个关键进展。这和将每个像素用作一个大型多层神经网络的单独输入相反。LeNet5 阐述了那些像素不应该被使用在第一层，因为图像具有很强的空间相关性，而使用图像中独立的像素作为不同的输入特征则利用不到这些相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet5 特征能够总结为如下几点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积神经网络使用 3 个层作为一个序列：卷积、池化、非线性 → 这可能是自从这篇 paper 起图像深度学习的关键特征！&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用卷积提取空间特征&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用映射到空间均值下采样（subsample）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;双曲正切（tanh）或 S 型（sigmoid）形式的非线性&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层神经网络（MLP）作为最后的分类器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;层与层之间的稀疏连接矩阵避免大的计算成本&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体来看，这个网络是最近大量架构的起点，并且也给这个领域的许多带来了灵感。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;间隔&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 1998 年到 2010 年神经网络处于孵化阶段。大多数人没有意识到它们不断增长的力量，与此同时其他研究者则进展缓慢。由于手机相机以及便宜的数字相机的出现，越来越多的数据可被利用。并且计算能力也在成长，CPU 变得更快，GPU 变成了多种用途的计算工具。这些趋势使得神经网络有所进展，虽然速度很慢。数据和计算能力使得神经网络能完成的任务越来越有趣。之后一切变得清晰起来......&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Dan Ciresan Net&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年的时候，Dan Claudiu Ciresan 和 Jurgen Schmidhuber 发布了最早的 GPU 神经网络的一个实现。这个实现是在一块 NVIDIA GTX 280 图形处理器上运行 9 层的神经网络，包含前向与反向传播。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;AlexNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2012 年，Alex Krizhevsky 发表了 Alexet（参见：ImageNet Classification with Deep Convolutional Neural Networks），它是 LeNet 的一种更深更宽的版本，并以显著优势赢得了困难的 ImageNet 竞赛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4lD9IbppYP6aYhUibXU4m6WlT0vs8a9PTAWFODVtPblp8A6FPSc0zyjg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AlexNet 将 LeNet 的思想扩展到了更大的能学习到远远更复杂的对象与对象层次的神经网络上。这项工作的贡献有：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用修正的线性单元（ReLU）作为非线性&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在训练的时候使用 Dropout 技术有选择地忽视单个神经元，以避免模型过拟合&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;覆盖进行最大池化，避免平均池化的平均化效果&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 GPU NVIDIA GTX 580 减少训练时间&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在那时，GPU 相比 CPU 可以提供更多数量的核，训练时间可以提升 10 倍，这又反过来允许使用更大的数据集和更大的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AlexNet 的成功掀起了一场小革命。卷积神经网络现在是深度学习的骨干，它已经变成了「现在能解决有用任务的大型神经网络」的代名词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Overfeat&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2013 年的 12 月，纽约大学的 Yann LeCun 实验室提出了 AlexNet 的衍生——Overfeat（参见：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks）。这篇文章也提出了学习边界框（learning bounding box），并导致之后出现了很多研究这同一主题的论文。我相信学习分割对象比学习人工边界框更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;VGG&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自牛津大学的 VGG 网络（参见：Very Deep Convolutional Networks for Large-Scale Image Recognition）是第一个在各个卷积层使用更小的 3×3 过滤器（filter），并把它们组合作为一个卷积序列进行处理的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这看来和 LeNet 的原理相反，其中是大的卷积被用来获取一张图像中相似特征。和 AlexNet 的 9×9 或 11×11 过滤器不同，过滤器开始变得更小，离 LeNet 竭力所要避免的臭名昭著的 1×1 卷积异常接近——至少在该网络的第一层是这样。但是 VGG 巨大的进展是通过依次采用多个 3×3 卷积，能够模仿出更大的感受野（receptive field）的效果，例如 5×5 与 7×7。这些思想也被用在了最近更多的网络架构中，如 Inception 与 ResNet。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4xjjrwABFNBEuArUBXhKcvjYoqF39Com0X8JSp0vWwPQWJKWTBnc5bg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 网络使用多个 3×3 卷积层去表征复杂特征。注意 VGG-E 的第 3、4、5 块（block）：256×256 和 512×512 个 3×3 过滤器被依次使用多次以提取更多复杂特征以及这些特征的组合。其效果就等于是一个带有 3 个卷积层的大型的 512×512 大分类器。这显然意味着有大量的参数与学习能力。但是这些网络训练很困难，必须划分到较小的网络，并逐层累加。这是因为缺少强大的方式对模型进行正则化，或者或多或少约束大量由于大量参数增长的搜索空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 在许多层中都使用大特征尺寸，因为推断（inference）在运行时是相当耗费时间的。正如 Inception 的瓶颈（bottleneck）那样，减少特征的数量将节省一些计算成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;网络中的网络（Network-in-network）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;网络中的网络（NiN，参见论文：Network In Network）的思路简单又伟大：使用 1×1 卷积为卷积层的特征提供更组合性的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NiN 架构在各个卷积之后使用空间 MLP 层，以便更好地在其他层之前组合特征。同样，你可以认为 1×1 卷积与 LeNet 最初的原理相悖，但事实上它们可以以一种更好的方式组合卷积特征，而这是不可能通过简单堆叠更多的卷积特征做到的。这和使用原始像素作为下一层输入是有区别的。其中 1×1 卷积常常被用于在卷积之后的特征映射上对特征进行空间组合，所以它们实际上可以使用非常少的参数，并在这些特征的所有像素上共享！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4wkfBwsSdhCOBcBKUB9gIhjb8tzo2IYdQjTDiaVBpicicRXxTIFLDWlPQw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MLP 的能力能通过将卷积特征组合进更复杂的组（group）来极大地增加单个卷积特征的有效性。这个想法之后被用到一些最近的架构中，例如 ResNet、Inception 及其衍生技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NiN 也使用了平均池化层作为最后分类器的一部分，这是另一种将会变得常见的实践。这是通过在分类之前对网络对多个输入图像的响应进行平均完成的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;GoogLeNet 与 Inception&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自谷歌的 Christian Szegedy 开始追求减少深度神经网络的计算开销，并设计出 GoogLeNet——第一个 Inception 架构（参见：Going Deeper with Convolutions）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是在 2014 年秋季，深度学习模型正在变得在图像与视频帧的分类中非常有用。大多数怀疑者已经不再怀疑深度学习与神经网络这一次是真的回来了，而且将一直发展下去。鉴于这些技术的用处，谷歌这样的互联网巨头非常有兴趣在他们的服务器上高效且大规模庞大地部署这些架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Christian 考虑了很多关于在深度神经网络达到最高水平的性能（例如在 ImageNet 上）的同时减少其计算开销的方式。或者在能够保证同样的计算开销的前提下对性能有所改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他和他的团队提出了 Inception 模块：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4biaF4FoYXaibicicovTM3lAqdAdxszyum8M2KCbNeo9Yu9EdTDLo2qEBuw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;初看之下这不过基本上是 1×1、3×3、5×5 卷积过滤器的并行组合。但是 Inception 的伟大思路是用 1×1 的卷积块（NiN）在昂贵的并行模块之前减少特征的数量。这一般被称为「瓶颈（bottleneck）」。这部分内容将在下面的「瓶颈层（bottleneck layer）」部分来解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GoogLeNet 使用没有 inception 模块的主干作为初始层，之后是与 NiN 相似的一个平均池化层加 softmax 分类器。这个分类器比 AlexNet 与 VGG 的分类器的运算数量少得多。这也促成一项非常有效的网络设计，参见论文：An Analysis of Deep Neural Network Models for Practical Applications。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;瓶颈层（Bottleneck layer）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;受到 NiN 的启发，Inception 的瓶颈层减少了每一层的特征的数量，并由此减少了运算的数量；所以可以保持较低的推理时间。在将数据通入昂贵的卷积模块之前，特征的数量会减少 4 倍。在计算成本上这是很大的节约，也是该架构的成功之处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们具体验证一下。现在你有 256 个特征输入，256 个特征输出，假定 Inception 层只能执行 3×3 的卷积，也就是总共要完成 256×256×3×3 的卷积（将近 589,000 次乘积累加（MAC）运算）。这可能超出了我们的计算预算，比如说，在谷歌服务器上要以 0.5 毫秒运行该层。作为替代，我们决定减少需要进行卷积运算的特征的数量，也就是 64（即 256/4）个。在这种情况下，我们首先进行 256 -&amp;gt; 64 1×1 的卷积，然后在所有 Inception 的分支上进行 64 次卷积，接而再使用一个来自 64 -&amp;gt; 256 的特征的 1×1 卷积，现在运算如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;256×64 × 1×1 = 16,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;64×64 × 3×3 = 36,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;64×256 × 1×1 = 16,000s&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比于之前的 60 万，现在共有 7 万的计算量，几乎少了近 10 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而且，尽管我们做了更好的运算，我们在此层也没有损失其通用性（generality）。事实证明瓶颈层在 ImageNet 这样的数据集上已经表现出了顶尖水平，而且它也被用于接下来介绍的 ResNet 这样的架构中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它之所以成功是因为输入特征是相关联的，因此可通过将它们与 1×1 卷积适当结合来减少冗余。然后，在小数量的特征进行卷积之后，它们能在下一层被再次扩展成有意义的结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Inception V3（还有 V2）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Christian 和他的团队都是非常高产的研究人员。2015 年 2 月，Batch-normalized Inception 被引入作为 Inception V2（参见论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）。Batch-normalization 在一层的输出上计算所有特征映射的均值和标准差，并且使用这些值规范化它们的响应。这相当于数据「增白（whitening）」，因此使得所有神经图（neural maps）在同样范围有响应，而且是零均值。在下一层不需要从输入数据中学习 offset 时，这有助于训练，还能重点关注如何最好的结合这些特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 12 月，该团队发布 Inception 模块和类似架构的一个新版本（参见论文：Rethinking the Inception Architecture for Computer Vision）。该论文更好地解释了原始的 GoogLeNet 架构，在设计选择上给出了更多的细节。原始思路如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;通过谨慎建筑网络，平衡深度与宽度，从而最大化进入网络的信息流。在每次池化之前，增加特征映射。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当深度增加时，网络层的深度或者特征的数量也系统性的增加。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用每一层深度增加在下一层之前增加特征的结合。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;只使用 3×3 的卷积，可能的情况下给定的 5×5 和 7×7 过滤器能分成多个 3×3。看下图&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4SbBX8mGVALOPTvfdoYX26M3KDlmwcEm6ck25gYzQLgTjicDfTXpK5mw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因此新的 Inception 成为了：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4eZISZibeQDCia3POWvz9fOyoz0icmZpEp5chmeRFsP6Wtfb9wO3h6HoAg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;也可以通过将卷积平整进更多复杂的模块中而分拆过滤器：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ibJqzib225ovv3Xav5jK7doiaMNuctfZPG4lwVHCPFpBvCGOqjQgN65ibA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在进行 inception 计算的同时，Inception 模块也能通过提供池化降低数据的大小。这基本类似于在运行一个卷积的时候并行一个简单的池化层：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4D8LePSPdI8KKGFdscAdC8KOueqZOqYQ09OkI0iaCiaLglyianON3ZMXEA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception 也使用一个池化层和 softmax 作为最后的分类器。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ResNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 12 月又出现了新的变革，这和 Inception V3 出现的时间一样。ResNet 有着简单的思路：供给两个连续卷积层的输出，并分流（bypassing）输入进入下一层（参见论文：Deep Residual Learning for Image Recognition）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf49E043MMV0uDXQlia1ibjOO1Ulko5EKZKcaED9yghb3yAMjCGGCrq5BwQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这和之前的一些旧思路类似。但 ResNet 中，它们分流两个层并被应用于更大的规模。在 2 层后分流是一个关键直觉，因为分流一个层并未给出更多的改进。通过 2 层可能认为是一个小型分类器，或者一个 Network-In-Network。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是第一次网络层数超过一百，甚至还能训练出 1000 层的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有大量网络层的 ResNet 开始使用类似于 Inception 瓶颈层的网络层：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4OtCPCF1riahRAKnQ5Eyib67FV3Ub3QcJwWlpGUMYAb0X6gqavBweDe7w/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种层通过首先是由带有更小输出（通常是输入的 1/4）的 1×1 卷积较少特征的数量，然后使用一个 3×3 的层，再使用 1×1 的层处理更大量的特征。类似于 Inception 模块，这样做能保证计算量低，同时提供丰富的特征结合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ResNet 在输入上使用相对简单的初始层：一个带有两个池的 7×7 卷基层。可以把这个与更复杂、更少直觉性的 Inception V3、V4 做下对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ResNet 也使用一个池化层加上 softmax 作为最后的分类器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于 ResNet 的其他洞见每天都有发生：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ResNet 可被认为既是平行模块又是连续模块，把输入输出（inout）视为在许多模块中并行，同时每个模块的输出又是连续连接的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ResNet 也可被视为并行模块或连续模块的多种组合（参见论文：Residual Networks are Exponential Ensembles of Relatively Shallow Networks）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;已经发现 ResNet 通常在 20-30 层的网络块上以并行的方式运行。而不是连续流过整个网络长度。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当 ResNet 像 RNN 一样把输出反馈给输入时，该网络可被视为更好的生物上可信的皮质模型（参见论文：Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Inception V4&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是 Christian 与其团队的另一个 Inception 版本，该模块类似于 Inception V3：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf44Pw1eJaLicsWRmrI2iafLur9uGhibc2u9vged32NWRGLdZ0VlgK23qiaRw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception V4 也结合了 Inception 模块和 ResNet 模块：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf40MCqn2WCvyyGmdCCkN9ZI1uIvrLonJyq2q85OkQrUv23BfeibD5YthQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我认为该架构不太简洁，但也满满都是较少透明度的启发法（heuristics）。很难理解里面的选择，对作者们而言也难以解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;考虑到网络的简洁性，可被轻易的理解并修正，那 ResNet 可能就更好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;SqueezeNet&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SqueezeNet（参见论文：SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size）是最近才公布的，该架构是对 ResNet 与 Inception 里面概念的重新处理。一个更好的架构设计网络型号要小，而且参数还不需要复杂的压缩算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;ENet&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的团队计划结合近期公开的架构的所有特征，做出一个非常高效、低重的网络，使用较少的参数和计算就能达到顶尖结果。该网络架构被称为 ENet，由 Adam Paszke 设计。我们已经使用它进行过单像素标记和场景解析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;详细了解 ENet 可参见论文 ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation。ENet 是一个编码加解码的网络。编码器是一个常规的 CNN 设计进行分类。解码器是一个增采样（upsampling）网络，将分类反向传播给原始图像进行分割。这只使用了神经网络，没有其他算法进行图像分割。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ENet 被设计为在开始时尽可能使用最小数量的资源。正是如此它有着如此小的脚本，编码器和解码器网络共占有 0.7 MB，16 fp 精度。即使这么小的型号，ENet 在分割的准确度上也类似于或者高于其他神经网络解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;模块分析&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对 CNN 模块的分析，该论文（Systematic evaluation of CNN advances on the ImageNet）已经做过了，里面的发现是非常有帮助的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用没有 batchnorm 的 ELU 非线性或者有 batchnorm 的 ReLU。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用一个学习到的 RGB 的彩色空间转换。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用线性学习率衰退策略。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用平均和最大池化层的和。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用大约 128 到 256 的 mini-batch 大小。如果这对你的 GPU 而言太大，将学习率按比例降到这个大小就行。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用完全连接层作为卷积，并为做最后预测平均所有预测。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;当研究增加训练集大小的时候，检测有一个 plateau 是否没有达到&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;数据的整洁要比数据大小更重要。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果你不能增加输入图像的大小，在随后的层上减少步幅（stride），这样做有同样的效果。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果你的网络有复杂和高度优化的架构，像是 GoogLeNet，那修改一定要谨慎。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;其他值得关注的架构&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FractalNet（参见论文：FractalNet: Ultra-Deep Neural Networks without Residuals）使用递归架构，它在 ImageNet 上没有进行测试。该架构是 ResNet 的衍生或者更通用的 ResNet。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;未来&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们相信制作神经网络架构是深度学习领域发展的头等大事。我们团队高度推荐仔细阅读并理解文中提到的论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但有人可能会想为什么我们要投入如此多的时间制作架构？为什么不是用数据告诉我们使用什么？如何结合模块？这些问题很好，但仍在研究中，有一篇论文可以参考：Neural networks with differentiable structure。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要注意到，我们在本文中谈到的大部分架构都是关于计算机视觉的。类似神经网络架构在其他领域内也有开发，学习其他所有任务中的架构变革也是非常有趣的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你对神经网络架构和计算性能的比较有兴趣，可参见论文：An Analysis of Deep Neural Network Models for Practical Applications。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击「阅读原文」，下载文中提到的论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>ACM 9 月刊 | 深度学习的黄金搭档：GPU正重塑计算方式</title>
      <link>http://www.iwgc.cn/link/2565650</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 CACM&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Samuel Greengard&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜雪&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着神经网络和深度学习研究的不断深入——尤其是语音识别和自然语言处理、图像与模式识别、文本和数据分析，以及其他复杂领域——研究者们不断在寻找新的更好的方法来延伸和扩展计算能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几十年来，这一领域的黄金标准一直是高性能计算（HCP）集群，它解决了大量处理能力的问题，虽然成本有点过高。但这种方法已经帮助推动了多个领域的进步，包括天气预测、金融服务，以及能源勘探。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，2012 年，一种新的方法出现了。伊利诺伊大学的研究者之前已经研究过在台式超级计算机中使用 GPUs 来加速处理任务（如图像重建）的可能性，现在多伦多大学的一组计算机科学家和工程师证明了一种在 GPUs 上运行深度神经网络来极大推进计算机视觉技术的方法。插上 GPUs（之前主要用在图形中）后，计算神经网络的性能会立即获得巨大提升，这种提升反映在了计算机视觉效果的明显改善上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一次革命性进步&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「仅仅在几年之后，GPUs 已经出现在深度学习的核心位置，」加州大学伯克利分校电子工程和计算机科学系教授 Kurt Keutzer 说到。「GPUs 的使用正在成为主流，通过在一个应用程序中使用几十到数百个处理器，GUP 正在从根本上改变计算。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伊利诺伊大学厄巴纳-香槟分校电子与计算机工程 Walter J. Sanders III–Advanced Micro Device 的名誉主席 Wen-Mei W. Hwu 也说过，「GPU 是卓越的吞吐量计算设备。如果你只有一项任务，就没必要用到 GPUs，因为速度也快不到哪去。但是，如果你有大量的相互之间独立的任务，用 GPUs 就对了。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一个深度视角&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GPU 架构起源于基础的图形渲染操作，比如给图形加阴影。1999 年，Nvida 推出了 GeForce 256，这是世界上第一个 GPU。简单来说，这个专用的电路——-可内置在视频卡或主板中——主导并优化了计算机内存以加快显示器的渲染速度。今天，GPUs 用在更加广泛的设备中，包括个人计算机、平板电脑、手机、工作站、电子标示、游戏机，以及嵌入式系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，「计算机视觉和深度学习中很多新应用的内存都是有限带宽，」Keutzer 解释道，「在这些应用中，应用程序的速度往往最终取决于它从内存中提取数据以及流入和通过处理器要花多少时间。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;部署 GPU 的一个常常被忽视的巨大优势是其 processor-to-memory 的超级带宽。Keutzer points 指出，这样的结果是，「在带宽有限的应用中，这个 processor-to-memory 带宽的相对优势直接转化成超级应用性能。」关键是 GPUs 用更少的电力提供了更快的浮点运算（FLOPs，每秒浮点运算次数）通过支持 16 位的浮点数扩大了能效优势，比单精度（32 位）或双精度（64 位）浮点数的能效更高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多核 GPU 要依赖更大量的 32 位到 64 位这样更简单的处理器内核的大量部署。相比之下，使用更小的传统的微处理器，通常是 2 位到 4 位到 8 位时，效果如何会呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「使用微处理器的 GPUs 实现了更优越的性能，并为深度神经网络提供了更好的架构支持。GPUs 在深度神经网络上表现出的性能优势逐渐被转化到更多种类的应用中。」Keutzer 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，一个典型的 GPU 集群包含了 8 到 16 个 GPU，而像 Keutzer 这样的研究人员正在尝试使用数百个 GPU 在超大数据集上同时训练多个深度神经网络，否则将需要几周的训练时间。这个训练需要运行大量数据通过该系统以让它达到能解决问题的状态。那时，它或许就可以在一个 GPU 或者混合处理器中运行了。「这不是一次学术训练。」Keutzer 指出。「我们训练用于像自动驾驶汽车这种新应用的神经网络时，就需要这样的速度。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;使用 GPU 正在成为主流，通过在单个应用中使用多个处理器，能从根本上改变计算。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GPU 技术现在的进展速度远比传统的 CPU 快，凭借强劲的浮点马力和较低的能耗，GPU 的可扩展性能让深度学习和机器学习任务的效率得到飞速提升，效果堪比给汽车装上涡轮增压发动机，百度高级研究员 Bryan Catanzaro 说到。「深度学习不是新鲜事物。GPUs 也不是。但是这个领域在计算能力得到极大提升和有丰富数据可供使用之后，才开始真正起航。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大部分进展来自 Nvidia，这家公司不断推出更加复杂的 GPUs，包括刚推出的专为解决训练和推理这类特殊任务的 Pascal 架构。在这款最新的 GPU 系统中，Tesla P100 芯片实现了在一片硅片上封包 150 亿个晶体管，数量是之前处理器的两倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个例子，百度正在推进语言识别研究的新前沿。它的「Deep Speech」项目，依赖一个端到端的神经网络，在英语和汉语的短音频剪辑中使语音识别的精确度达到了人类水平。这家公司还在探索自动驾驶汽车中的 GPU 技术；它一直在研发能在北京大街上自动导航的自动驾驶汽车，并做了改变车道、超车、停车和启动的演习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，微软亚洲的研究员使用 GPUs 和一种深度神经网络的变体——深度残差网络，来在计算机视觉中的对象分类和识别的任务中实现更高精确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌，也在使用这些技术来持续改进图像识别算法。前谷歌人工智能研究员，现 Open AI 研究室主任 Ilya Sutskever 说到：「神经网络正在复兴。神经网络和深度学习的核心理念已经被讨论和思考多年了，但是正是通用 GPU 的研发才是神经网络和深度学习成功的关键。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一步超越&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「虽然 GPU 技术正在深度学习领域推进到新的前沿，但很多计算性的挑战仍然存在。首先，像 GPU 这样的独立程序化多核设备的高效实现仍然很困难；并且，这种困难会随着多 GPU 并行的加剧而恶化。」Keutzer 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不幸的是，他补充道，「这些设备的许多高效程序化的专业技术都被限制在公司内部，许多已被开发的技术细节仍未被广泛地应用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同样地，Keutzer 认为，关于深度神经网络的设计仍被广泛地描述为「黑科技」，构建一种新型的深度神经网络架构同构建一种新型的微处理器架构一样复杂。更糟糕的是，一旦这种深度神经网络架构被构建，「就会产生很多类似超参数的 knobs，在应用在训练中时，只有当这些 knobs 被合理设置时才会产生应有的精确度。所有的这些造成了这些已知和未知间的知识鸿沟」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「不论是在深度神经网络领域还是 GPU 编程领域，拥有专业知识的个人都是十分匮乏的，而那些对两方面都了如指掌的人才则是更为罕见。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一大挑战则是理解如何最高效地使用 GPU。举个例子，百度需要 8-16 个 GPU 去训练一个模型，从而在整个应用中达到 40%-50% 的浮点峰值。「这就意味着表现得效果十分有限。」Catanzaro 说道，「现实是我们需要更大规模地使用 GPU，8 个或 16 个远远不够，我们可能需要的是 128 个 GPU 并行。」这就需要更好的连接，以及支持由 32 位浮点支持到 16 位浮点支持的能力。Nvidia 的下一代 GPU——Pascal 有可能可以解决这些问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有一大障碍在于让 GPU 更好地同其他 GPU 和 CPU 集成。Hwu 指出，这两种类型的处理器并不常会集成在一起，并且他们之间也很少拥有足够高的带宽。这就最终转化成了有限数量的应用和系统运行的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「你十分需要让你的 GPU 具备运行大数据任务的能力；同时，你的 GPU 还能适时暂停使卸载进程比较合算。」Catanzaro 解释道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在的 Nvidia GPU 存在于不同的芯片上，他们通常通过一个 I/O bus (PCIe) 连接到 CPU 上。这就是它们能够向 GPU 发送大量任务的一个原因。未来的系统会将 GPU 和 CPU 集成在一个统一的包里，并且它能承担更高带宽和更小的风险，以及通过 GPU 和 CPU 来保持共享的一致性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Keutzer 希望随着时间的推移，CPU 和 GPU 能够得到更好的集成，这两者间更强的一致性与同步性也随之能够实现。事实上，Nvidia 和 Intel 也都在关注着这一领域。Keutzer 注意到一种名为 Knight's Landing (KNL) 的新型 Intel 芯片在 Xeon Phi 72-core super-computing 处理器中提供了前所未有的计算能力，并且，它同时集成了 CPU 和 GPU 的特性。同时，这款芯片还提供了每秒 500 GB processor-to-memory 的带宽需求，这也将侵蚀 GPU 在这一领域的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hwu 注意到 KNL 的 72 个核彼此都能执行「一个宽泛的向量指令（512 字节）。当转化到双重精度（8 字节）和单一精度（4 字节）的时候，向量宽带就将会是 64 和 128。在这个层面上，它和 GPU 有着类似的执行模型。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Keutzer 希望随着时间的推移，CPU 和 GPU 能够得到更好的集成，这两者间更强的一致性与同步性也随之能够实现。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KNL chip 的编程模型是传统的 x86 模型，所以，Hwu 认为，程序员们「需要通过 Intel C Compiler 编写代码来使得芯片变得可向量化，或是使用 Intel AVX 向量的本质库函数。」他补充道，GPU 的编程模型需要依托于一个核心的编程模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，X86 的内核对所以高速缓存的层次结构具有高速缓存的一致性，Hwu 说道，「但是，GPU 的第一层缓存并不清晰一致，它会伴随着一些减少的储存带宽。」然而，他补充道，「就深度学习的应用来说，高速缓存的一致性对大多数算法的第一层缓存并没有那么重要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;未来十年，所有这些的通用性在于大的行业环境如何发展。Hwu 表示，他坚信摩尔定律能够在超过三代的时间内继续发挥作用，设计师和程序员也能够从几乎离散的 CPU 和 GPU 系统过渡到集成的设计上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果摩尔定律停止运转，它也将显著地影响未来的这些系统，以及人们在深度学习和其他任务上使用硬件和软件的方式。」Hwu 指出，「但是，即使我们解决了硬件层面的问题，特定深度学习方面的任务仍需要大量的标签化数据。在某些层面，我们需要在标签化数据方面取得突破，从而让我们能够具备从事必要领域训练的能力，尤其是在自动驾驶领域。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在未来几年，Sutskever 说道，机器学习将会广泛地应用到 GPU。「随着机器学习的方法不断提升，它们会被应用到远超今天使用范围的领域并影响到其他所有方面，从医疗保健、机器人到金融服务和用户体验。这些进步依托于更快 GPU 的发展，这也将会使机器学习具备研究的能力。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adds Catanzaro 说：GPU 是通往未来计算之门。深度学习令人兴奋是因为当你增加更多数据时，它可以规模化。在这一点上，我们会永不满足的追求更多的数据和计算资源来解决复杂问题。在拓展计算极限方面，GPU 技术是非常重要的一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 从输入到输出的「黑箱」：我们能够理解深度神经网络吗？</title>
      <link>http://www.iwgc.cn/link/2565651</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Nautilus&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：AARON M. BORNSTEIN&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、Quantum、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Dmitry Malioutov 无法详细解释他构建的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Malioutov 是 IBM 的一名研究科学家，他的部分工作是开发能为 IBM 的企业客户解决所面临的难题的机器学习系统。其中一个项目是为一家大型保险公司建立的。这是一项极具挑战的任务，需要一个精密复杂的算法。然而当要向客户解释结果的时候，他却遇到了障碍。「我们无法向他们解释这个模型，因为他们并没有接受过机器学习的训练」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，即使他们是机器学习专家可能也没用。这是由于该模型是一个人工神经元网络，这个系统接受给定类型的输入数据——在这个案例中即保险公司的客户纪录——然后找出其中的特定模式。这种网络在实际中的应用已经有半个多世纪的历史了，但最近这个领域出现了突破性的进展——从语音识别和语言翻译到机器人围棋棋手和自动驾驶汽车。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4POvh803AVfjcFy9utNJBPAObJ68o7e4F7zyU3dyuMYNUics1vy9HhvQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;「隐藏」意义：在神经网络（neural networks）中，数据在层与层之间传递，并在每一步经历简单的转变。在输入层和输出层之间的是隐藏层（hidden layers），这些层中包含大量节点和连接，它们遵循着人类无法解释的模式，或者与输入输出层之间并没有明显的联系。「深度（Deep）」网络就是指那些包含很多隐藏层的网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它们有着极佳的性能，可产出激动人心的结果，但现代神经网络有一个麻烦的问题：没人能真正搞清楚它们是如何工作的。而这就意味着没人能预测到它们会在什么情况下失效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;举个例子，机器学习研究者 Rich Caruana 和他的同事最近报道了一个事件。匹兹堡大学医学中心有一个团队在用机器学习预测肺炎患者会不会发展出严重的并发症。他们的目标是把那些低并发症风险的病人送到门诊病房，以节省医院床位并减轻医务人员负担。这个团队尝试了好几种不同的方法，包括很多种神经元网络，以及能产生清晰可读的规则的软件生成的决策树。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的神经网络比其他任何方法得到正确结论的次数都更多。但当这些研究人员和医生查看那些人类可读的规则时，他们发现了这样的描述：有一条规则指示医生把有哮喘的肺炎患者送回家，但谁都知道哮喘患者是很容易患并发症的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型所做的只是人类分配给它的任务：找出一个数据中存在的准确模式。它给出的糟糕建议是数据中的漏洞所致。因为医院的政策是把所有患哮喘的肺炎病人送入重症监护，而正因为这个政策的良好成效，几乎所有哮喘患者都不会患上并发症。如果没有改变了医院患者纪录的额外加护工作，结果会与现在有很大不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这则医院轶事说明了可解释性（interpretability）的实际价值所在。Caruana 及同事写道：「如果那个基于规则的系统 （rule-based system）能知道哮喘降低了风险，神经网络当然也会学习到这一点。」但是神经网络并不能被人类理解，而且它对于哮喘患者得出的怪异结论的原因也难以确认。如果不是有那个可解释的模型，Malioutov 警告说：「你可能真会害死人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是为什么很多人不愿意把赌注押在神经网络的不确定性上。当 Malioutov 把他那准确但不可解释的神经网络模型呈现给客户时，他同时也提供了一个基于规则的备选模型，这个模型的工作方式是可以用简单的术语进行沟通的。第二种可解释的模型其实没有第一种准确，但即便如此客户还是选择了第二种——虽然对一个具备高度数学复杂性的保险公司，准确性的每个百分点都很重要。Malioutov说：「他们更加认可它（第二种模型），他们真的非常注重直观。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;甚至政府都开始关注不可解释的神经网络预言日趋增加的影响力。欧盟最近提出的新法案要建立「解释权（right to explanation）」，它让市民有权要求算法决策的透明化。然而这项立法很可能是难以实行的，因为立法者并未明确界定「透明（transparency）」的含义。我们并不清楚这种疏忽是源于对问题的无知，还是由于立法者的确深知其中的复杂性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，有些人认为这种清晰的定义是不可能的。目前对于神经网络的运作方式，虽然我们已经知道了所有能知道的事情，但它们毕竟只是计算机程序，我们对它们工作的方式和原因所知甚少。这个网络是由许多（有时是几百万个）独立的单元组成的，它们叫做神经元（neuron）。每个神经元都会把多个数字输入转化成一个数字输出，然后再把它传递给另一个或很多个其它神经元。就像在大脑中一样，这些神经元被分成很多「层 （layer）」——一些可以获取下层的输入数据并把它们的输出传递给上层的细胞团。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络的训练通过数据的馈送进行，然后不断调整层级间的连接，直到网络计算出与已知输出（一般由很多子类组成）尽可能接近的输出为止。过去几年取得的惊人成果要归功于一系列新技术，它们使得训练在输入和最终输出之间有很多层级的深度网络成为可能。一种受欢迎的深度网络叫做 AlexNet，它被用来给照片归类，分类的标准是看照片中是否有狮子狗或者博美犬。它包含了六千多万个「权重（weight）」，神经元通过权重得知要对每个输入数据给予多少关注。「如果想理解这个网络，你必须要对这六千万个数字有一定的理解才行。」一位任职于康奈尔大学和 Geometric Intelligence 的计算机科学家 Jason Yosinski 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使有可能加强这种可解释性，也并不总能得到满意的结果。对于可解释性的需要也可以被看作是另一套约束条件，这会妨碍一个模型得到只与输入数据和给定的输出结果相关的「纯粹」结果，而且有可能会降低准确性。在今年早些时候的一次 DARPA 会议上，项目经理 David Gunning 用一个表格总结了相关的利弊，结果显示深度网络是现代技术中最不可理解的。而与之相对的技术是决策树（decision tree）——一种对解释的关注重于效率的基于规则的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4LDaL1NaB8Uibugjh12kAQcxzhia7qqsYcf0YXLURQKfB0x7fFic6khDFQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;WHAT VS. WHY：现代学习算法需要在人类可解释性（或可解读性）和准确性之间做出权衡。而深度学习是最准确的，同时也是最不可解释的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结果是现代机器学习系统给了我们一个选择：我们是想准确的知道会发生「什么」，还是想知道「为什么」会发生某件事，而这要以牺牲准确度为代价？「为什么」帮助我们制定决策、适应环境、以及了解模型何时可能崩溃。而「什么」则能帮助我们解决当下的实际问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个艰难的选择。但有些科学家希望能消除选择的需要——让我们在享受多层计算的成果的同时还能理解它。令人吃惊的是，有些最具前景的研究方向把神经网络视为实验对象——模仿了最初激励他们的生物科学——而非分析性的、纯数学的对象。比如 Yosinski 就表示他正尝试「像理解动物甚至理解人类那样」去理解深度网络。他和其他计算机科学家正致力于引进生物学研究中的技术来窥视网络深处，这模仿了神经科学家窥视大脑内部的方法：探查单个组成部分，纪录下它们如何响应输入中的微小改变，甚至移除部分碎片看其它成分如何补偿。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从零开始构建一个新的智能系统以后，现在科学家们又要把它拆开了，对这些虚拟有机体用了显微镜和解刨刀的数字等同物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Yosinski 坐在一台计算机终端前，对着一个网络摄像头侃侃而谈。网络摄像头的数据被传人一个深度神经元网络中，而该网络自身也正被实时分析着——用的是 Yosinski 和同事开发的叫做 Deep Visualization（深度可视化）的工具包。在屏幕上点击了几下， Yosinski 放大了网络中的一个神经元。在这次交互的视频录像中，他说道「这个神经元似乎会对脸部图像做出反应。」我们知道人类的大脑中也有这种神经元，许许多多这样的神经元聚集的区域被称为梭状回面部区（fusiform face area）。这个区域是在一项始于1992年的多研究项目中被发现的，是人类神经科学中最可靠的观测结果。但这些研究需要用到像正电子放射断层造影（positron emission tomography） 这种先进技术才能进行，Yosinski 仅通过代码就能窥视他的人造神经元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4JAp9OibGOKeVCZ6n2kzqXdmwZ104nrQx9iasFjicQiaicAKicicL4PCOptNxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;大脑活跃性：深度神经网格（绿色方框标记）中的一个神经元对 Yosinski 的脸作出响应，就像是人类大脑中那个对脸部作出可靠响应的特别部分一样（黄色强调）。左：来自 2015年国际机器学习大会（ICML）上的深度学习研讨会（ Deep Learning Workshop）中 Yosinski 等人的演讲： Understanding Neural Networks Through Deep Visualization。右：来自乔治城大学医学中心 Maximilian Riesenhuber。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种方法让他能把特定的人造神经元绘制成人类可理解的概念或对象，比如脸，这能帮助神经元网络转变成直观的工具。他的项目也能找出图片的哪一方面对面部神经元的刺激最显著。他说：「我们能看到如果有颜色更深的眼睛或更红的嘴唇，它们的响应会更强烈。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在杜克大学的计算机科学与电子和计算机工程教授 Cynthia Rudin 看来，这种「事后分析」的解释本质上是很有问题的。她的研究集中在构建基于规则的机器学习系统上，主要应用在像罪犯量刑和医学诊断这种人类可读解释是有可能而且也重要的领域中。但她认为，对于像视觉识别这样的问题，「人类对其结果的解释完全是主观的。」我们可以把网络响应简化成面部神经元的识别，但我们如何确定这就是它所寻找的呢？Rudin 的担心对应着那则著名的格言，也许并没有比视觉系统更简洁的模型，除了视觉模型本身。「对于一个复杂模型在做什么，你可以有很多解释，那你是不是就挑出那个你想要的，然后认为它是对的呢？」她说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Yosinski 的工具包能够部分反驳这些担忧，通过逆向工程的方式探索出网络自身「想要的」正确结果，这是一种理想的人为方法。这个项目从原始静态开始，进而一个像素接一个像素的调整，通过训练网络的逆向过程捣鼓图像。最终它找到一张能让给定神经元得到最大可能响应的图像。当将这种方法用于 AlexNet 神经元上时，它产生的计算机图像鬼魅般的、准确无误地产生了标记类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ibicjNDTfQoick4ica2CXrj27glAQj90iaUHrhxmLyHpw1nBhVib3Q8l563w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;理想化的猫：计算机合成的理想猫脸的例子，由 Deep Visualization 工具包产生。这些猫脸的产生是通过对初始图像住个像素的调整，直到 AlexNet 的面部神经元得到一个最大响应为止。来自 2015年国际机器学习大会（ICML）上的深度学习研讨会（ Deep Learning Workshop）中 Yosinski 等人的演讲： Understanding Neural Networks Through Deep Visualization。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在非常普遍的意义上，这似乎能支持他面部神经元确实是在寻找脸部图像的理论。但这里有个问题。为了生成这些图像， Yosinski 的程序依赖于一个统计学约束（叫做自然图像优先（natural image prior）），这会限定它产生出与真实世界物体类似的结果相匹配的图像。当他移除这些规则后，工具包仍旧会选定它标记为最大可信度的图像，但这个图像就是纯静态的了。事实上，Yosinski 在很多案例中都展示过这点，AlexNet 神经元更倾向于呈现给人类的图像绝大部分都是静态的。他很乐意承认「很容易搞清楚如何让这种网络说一些极端的话。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了避免这种陷阱，弗吉尼亚理工学院的一位电子和计算机工程助理教授 Dhruv Batra 采取了一种更高级别的试验性方法来破译深度网络。他并未试图在神经网络的内部结构中寻找模式——对此他辩驳说「因为比我更聪明的人已经在这样做了」——而是用一种机器人版本的眼跟踪技术来探索神经网络的工作机理。他的团队，在一个由研究生 Abhishek Das 和 Harsh Agrawal 带头的项目中，向深度网络提出关于图像的问题，比如给定的房间图片中的窗户上有没有窗帘。不像 AlexNet 或类似的系统，Das 的网络设计就是一次只关注图像上一块小区域。它在整个图像上移动虚拟眼睛，直到它认为获得了足够多回答问题的信息为止。经过充分的训练后，这种深度网络能取得很棒的表现，回答精确度与人类最佳水平相当。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后Das、Batra 及其同事尝试去发现网络如何通过调查选择查看的图片位置来做决策。他们的发现令人惊讶：在回答关于窗帘的问题时，网络甚至没有费心去找窗户。相反，它首先查看图像的底部，如果找到一张床则停止寻找。看来，在用来训练这个神经网络的数据集中，有窗帘的窗户可能会出现在卧室里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管这种方法确实揭示了一些深度网络的内部运作，但它也增加了可解释性方面的挑战。「机器所拾取的不是关于这个世界的真相，」Batra 说。「它们是有关数据集的真相。」机器与被供给的数据紧密调谐，这使得提取关于机器运行的一般规则变得困难。更重要的是，他警告说，如果你不知道它是如何工作的，你便不知道它会如何失败。而当它们真的失败了，以 Batra 的经验，「它们会败得壮观且丢脸。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类似 Yosinski 和 Batra 这样的研究人员所面临的一些障碍，对于研究人类大脑的科学家来说会比较熟悉。例如有关神经影像学的解释问题在今天已经非常常见，尽管还没有得到普遍重视。在 &amp;nbsp;2014 年的一篇回顾有关该领域的文章中，认知神经科学家 Martha Farah 写道：「忧虑在于……（功能型大脑/functional brain）图像更像是研究者的发明，而不是研究者的观察。」在非常不同的智能系统中出现的这些问题表明，它们可能会成为障碍。这不是根据对于这种或那种大脑的研究，而是根据对智能本身的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;探究可解释性是一件傻事吗？来自圣地亚哥加利福尼亚大学的 Zachary Lipton，质疑了解释神经网络的企图以及建立可解释型机器学习模型的价值。他给今年的国际机器学习大会（International Conference on Machine Learning /ICML）中一个关于人类可解释性（ Human Interpretability）的专题讨论会（由 Malioutov 及其两个同事组织）提交了一份煽动性的论文。该文标题为「有关模型可解释性的神话（The Mythos of Model Interpretability）」，这篇文章挑战了可解释性的定义以及研究人员寻找它的理由。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Lipton 指出许多学者不同意可解释性的概念，这让他意识到：可解释性要么并不存在——要么就有多种可能含义。他认为不应当信任这种解释冲动，而研究者应该使用神经网络来解放自己去「探索有野心的模型。」他说可解释性阻止了模型去充分发挥其全部潜力。他认为该领域的一个目的是「构建能从超过人类处理能力的更多特征中学习的模型。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这种能力既是特点也是缺陷：如果我们不了解网络输出是如何产生的，那么我们就不知道输入的哪些方面是必要的，或者甚至连什么可以被当做输入都不知道。案例：1996 年萨塞克斯大学的 Adrian Thompson 通过应用类似于今天那些训练深度网络的技术来使用软件设计电路。该电路是用来执行一个简单的任务：区分两个音频的音调。在对电路元件数千次的洗牌和重新安排之后，该软件找到了一个能够近乎完美地完成任务的配置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而 Thompson 惊讶地发现，该电路使用了比任何人类工程师所需要的更少的组件——包括几个没有与剩余部分进行物理连接的组件，但不知何故它们对于电路的正常工作来说仍然是必要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他着手解剖电路。经过几次实验，他了解到之前的成功是利用了相邻组件之间微妙的电磁干扰。未连接的组件是通过引起局部电场的小波动来影响电路。人类工程师通常会防范这些干扰，因为它们不可预测。果然，当 Thompson 将相同的电路布局拷贝到另一批组件中时——或者甚至改变了环境温度——它彻底失败了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该电路表现出了训练后的机器的一个标志性特征：它们可以非常紧凑和简化，非常适合其环境——但不适应任何其他环境。它们能够获取工程师看不见的模式；但不知道哪一个模式在别的任何地方都不存在。机器学习的研究人员竭尽全力去避免这种被称为「过拟合（overfitting）」的现象，但是这些算法正被应用于越来越多的动态情况中，其脆弱性会不可避免地被暴露出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于普林斯顿大学的计算机科学教授 Sanjeev Arora 来说，这种现象是他寻找那些允许人类干预和调整网络的可解释性模型的主要动机。Arora 指出了两个可能代表不可理解型机器的硬性限制问题。一个是「可组合性（composability）」——当手头的任务涉及到许多不同决策（比如有关围棋或自动驾驶汽车）时，网络无法有效地学习哪个决定导致了失败。「通常当我们设计东西时，我们了解不同的组件然后把它们组装在一起，」他说，这允许人类调整那些不适合给定环境的组件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个使得可解释性悬而未决的难题被 Arora 称为「域适应性（domain adaptability）」——从一种环境到另一种环境灵活应用知识的能力。这个任务对于人类学习者来说可以做得很好，而机器却会以令人惊讶的方式失败。Arora 描述了程序是如何灾难性地无法适应于那些甚至是微妙的环境变化，而人类则可以轻松处理。例如一个被训练的通过阅读正式文件（如维基百科）来分析人类语言的网络，会在更口语化的语境（如 Twitter）中全盘失效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以这种观点来看，可解释性看起来似乎很重要。但我们明白自己所说的话是什么意思吗？先驱计算机科学家 Marvin Minsky 创造了「suitcase word（手提箱词）」一词来描述许多术语——比如「意识」或者「情绪」——当我们谈论自己的智能时会使用它们。他提出，这些词反映了许多被锁在「手提箱」中的不同潜在过程的工作原理。只要我们继续调查这些词汇，代替那些更加基本的概念，那么争论就会过去，我们的见解将被我们的语言所限制。在有关智能的研究中，「可解读性（interpretability）」本身也可能是这样一个手提箱单词吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然与我谈话的许多研究人员都乐观地认为，总有一天理论家们会打开行李箱并发现一套类似于牛顿定律那样单一、统一的原则或定律，主宰机器学习（可能也包括人类学习），但是其他人告诫到没有理由做这样的期望。纽约城市大学的哲学教授 Massimo Pigliucci 提醒道，自然科学以及扩展到人工智能的「理解（understanding）」——可能会成为 Ludwig Wittgenstein （先于Minsky）所说的一个「集群概念（cluster concept）」，它承认许多（部分有区别的）定义。他说，如果该领域中的「理解（understanding）」确实实现了，则它可能不会在物理学中被发现，而是在进化生物学中。他说我们或许更期待物种起源而非统一定律。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然这并不意味着深度网络预示着某一种新的自主生命体。但它们可能会像生命一样难以理解。该领域越来越多的实验方法及事后解释，可能不是某种身处黑暗渴望理论之光的绝望感。相反它们可能会是我们所能期待的唯一光明。可解释性可能会逐渐作为一组按照（生物）分类学排列的「物种」原型实例出现，其由推理所定义，并取决于特定语境的解释。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 &amp;nbsp;ICML 的专题讨论会结束时，一些出现在舞台上的主持人尝试去定义「可解释性」。下面的响应声与辩论小组的数量一样多。一些讨论过后，各小组似乎找到了共识——「简洁性（simplicity）」对可解释型模型来说是必要的。但是在为简洁性下定义时，小组又出现了分歧。「最简单的」那个模型是不是依赖于最少特征的那一个？是不是得到最大差异的那一个？是不是程序体积最小的那一个？专题讨论会结束时没有给出一个统一答案，那个未完成的概念定义被替换成了另一个。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如 Malioutov 所说的那样：「简洁性并不简单。」&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌技术论文：用于YouTube推荐的深度神经网络</title>
      <link>http://www.iwgc.cn/link/2565652</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 arxiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Paul Covington, Jay Adams, Emre Sargin&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：孙宇辰、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4H7t4c8sOWwHv0cPIs07IdhrLXNRlZpicibV6t8fKEjkIaADUiaxzia103g/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;YouTube 所使用的推荐系统是现在最大规模的、最先进的业界的推荐系统之一。在这篇论文中，我们在较高层面上描述这个系统，并重点关注了深度学习所带来的巨大的性能提升。本论文根据典型的两阶段信息检索的二分法（two-stage information retrieval dichotomy）分为两部分：首先，我们详细描述了一种深度候选生成模型（deep candidate generation model），接着描述了一种分离的深度排名模型（deep ranking model）。通过设计、迭代、维护一个带有巨量面向用户的影响的巨型推荐系统，我们还提供了实用的经验教训和见解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;系统概述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的推荐系统的整体结构如图 2 所示。系统由两个神经网络组成：一个用于候选生成，一个用于排名。其中候选生成网络从用户的 YouTube 活动历史中提取事件作为输入，然后从一个大的视频库中检索出一个小数据集（上百个视频）。这些候选被认为通常与用户有很精准的相关性。这个候选生成网络仅通过协同过滤（collaborative filtering）提供广泛的个性化。用户之间的相似性可以通过粗粒度特征（例如视频观看的 ID、搜索查询单词以及人口特征统计）表达。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个推荐列表中出现的一些「最好」的推荐需要一种良好的表征，以在具有高召回率（recall）的候选集中区分相对的重要性。排名网络通过使用一个描述视频与用户的特征集合的期望目标函数来给每个视频打分，从而完成排名的任务。根据它们的得分，然后将最高分的视频展现给用户。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两阶段的推荐方法允许我们从一个很大（数百万）的语料库中进行推荐，与此同时还仍有在设备上出现的少量视频是个性化的吸引用户的内容。此外，这个设计能够和其他源生成的候选进行混合，例如在这一项早期工作[3]中描述的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在开发过程中，我们广泛地使用了非网络的指标（准确度、召回率、ranking loss）来引导我们的系统的迭代改进。然而，为了最终测定一个算法或模型的效果，我们依靠于通过实时实验进行 A/B 测试。在一个实时实验中，我们能度量在点击率、观看时间与许多度量用户参与度的指标中不易察觉的变化。这是非常重要的，因为实时 A/B 测试结果不总是与离线实验有相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4f2kibHohjCC7rKN5LS4ZRtNzZfCl6MNsMhlN322OyRFDasbOJp1mp9Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：推荐系统架构：候选视频通过「漏斗」状的流程从大量视频中被检索出来并进行排名，然后再将其中一小部分展示给用户。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4csbBJMLIia77uosYRgLneeGLpKiamH34P7iauLKKb0zFcQ6mVtHxOrMmg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图3：深度候选生成模型架构：嵌入的稀疏特征是和稠密特征连结在一起的。在级联（concatenation）将可变大小的稀疏 ID 转换成适合隐藏层输入的固定宽度的向量之前，嵌入被取了平均。所有隐藏层是全连接的。在训练中，使用取样的 softmax 的输出之上的梯度下降对交叉熵损失进行最小化。在服务中，用一个近似最近邻（approximate nearest neighbor ）查询生成数以百计的候选视频推荐。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4ILKh0lwaoIibeiaFNoviawOBiaoFVhM26ia3Cvl5K8Oy51nGRQAecWj73kg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图4：对于一个给定的视频，模型用样本年龄（example age）作为一个特征训练，能够精准表达出数据中的上传时间和依赖时间的受欢迎程度。如果没有这一特征，该模型会在训练窗口近似地预测平均似然（ average likelihood）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf4mfhNjC7w9F0BLSw8P1PKicMFnhG7Ax5DlMHz2SI0EYfsQFVDkrNibpmA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图5：给模型选择标签和输入上下文对离线评估来说很有挑战性，但是对实时性能有巨大的影响。如图，实心圆点•是网络的输入特征，空心圆点◦是被去除的。我们发现在 A/B 测试上预测未来观看（5b）的表现更好。如 5b 所示，样本年龄表示为 tmax − tN，其中 tmax 是训练数据中的最大观测时间。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9j66x6ya4EPbOzJu4dALf43LDnEpibjdQ5kRyASjauX16RcPK1btTUgFxYt0GSFAueNMkbqJ9dHtA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图7：描绘了嵌入的分类特征（包括一价特征和多价特征）的深度排名系统架构，这些特征带有共享的嵌入和规范化的连续特征的乘幂。所有层都是全连接的。在实践中，需要给网络馈送数百个特征。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们描述了我们用于推荐 YouTube 视频的深度神经网络架构，划分为两个不同的问题：候选生成与排名。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的深度协同过滤模型能够吸收很多信号并使用深度的层对它们的交互进行建模，其性能优于 YouTube 原来使用的矩阵分解方法。比起科学，选择推荐的代理问题（surrogate problem）更像是一门艺术；而且我们发现通过获取不对称的联合观看行为（co-watch behavior ）和预防未来信息的泄露，对未来观看的分类可以在实时评估中表现良好。抑制来自分类器的判别信号也是获得好的结果的关键，否则模型将会对代理问题过拟合，不能很好地转换到主页。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们发现使用训练样本的年龄作为输入特征，移除了相对于过去的固有偏差（bias），并允许模型表达受欢迎视频的时间依赖行为。这种改进的离线保持了精确率，同时在 A/B 测试中显著地增加了最近上传视频的观看时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;排名是更经典的机器学习问题，但是我们深度学习方法在性能上超过了之前对观看时间预测的线性与基于树的方法。推荐系统尤其受益于用户过去和事物之间的行为这样专门的特征。深度神经网络需要对类别和连续特征的特殊表征，我们对其分别使用嵌入与分位数标准化（quantile normalization）进行变换。我们发现深度的层可以有效地对数百个特征的非线性交互建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;逻辑回归（Logistic regression）根据给训练样本赋予权重进行修改，其中给观看时间正样本，没有观看的是负样本，从而让我们可以学习接近模型预期观看时间的几率。这种方式相比于直接预测点击率，可以在观看时间权重排名评估指标上表现得远远更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击「阅读原文」，下载此论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 05 Sep 2016 17:24:07 +0800</pubDate>
    </item>
    <item>
      <title>特稿 | 如何让深度学习突破数据瓶颈？这家创业公司直接挑战生物神经元的计算模型</title>
      <link>http://www.iwgc.cn/link/2555446</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Feature Article&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL2USVJONEbWmpLUZ0fzvfhjDfyg7mBErykt3erfsI37ubXWLJDbP6mw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;Demiurge Technologies 是一家位于瑞士的人工智能创业公司，他们致力于研究生物神经元的计算原理，开发下一代深度学习，以解决小样本学习和与物理世界交互的难题。他们的深度学习系统将应用于第四级别自动驾驶和探索机器人等领域。与大部分人工智能公司不同的是，&lt;span&gt;Demiurge Technologies 希望从根源解决目前深度学习存在的问题，面对这样一个不论在神经科学领域，还是人工智能领域都同样重要的问题，他们的勇气、方法和视野都令人尊敬。也希望 &lt;span&gt;Demiurge 的创业思路和运作模式能够给从业者带来灵感和启发。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文分为四大部分，共 &lt;span&gt;11,483 &lt;/span&gt;字，阅读预计 &lt;span&gt;20&lt;/span&gt; 分钟&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一、基于生物神经元的下一代深度学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;二、游戏规则制定者&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三、自动驾驶&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;四、宏大目标下的生存追问&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;瑞士，有着覆盖国土面积 60% 的阿尔卑斯山脉和超过 1,500 个湖泊，玛丽·雪莱所著的西方文学史上首部科幻小说&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400276879&amp;amp;idx=1&amp;amp;sn=7b6c18c6c6e0be66d7feb9b9cee27048#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400276879&amp;amp;idx=1&amp;amp;sn=7b6c18c6c6e0be66d7feb9b9cee27048#wechat_redirect"&gt;《弗兰肯斯坦》&lt;/a&gt;就诞生于日内瓦湖畔，它讲述了一位天才科学家从零到一创造出智能生命体的故事，成为此后 200 年间讨论人类与机器、生命与智能的哲学模板。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge Technologies 也是一家希望从生命中获得线索并以此来开发通用人工智能的创业公司，位于瑞士一个依山傍湖的小镇——静谧但充满力量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司的办公地点高度保密，从不接受无关访客，即便是新员工面试也会先安排在其他地方，在正式录用后才会被邀请来公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「公司的物理空间是精心打造的文化载体，我们利用环境甄别对的人，依靠对的人进一步强化环境的凝聚力。」Demiurge 联合创始人兼 CEO 刘思宜（Idonae Lovetrue）表示。从这个看似有些神秘的规定中足以看出一种他们对实现通用智能所需人才和文化的独到视角及判断标准，令人心生敬畏，以至于在去公司的路上我忍不住问她「我要不要带个头套？」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司院落的历史要远远超过人工智能的历史，除了 Xbox 等科技公司里常见的娱乐设备之外，他们的员工还有一项令人羡慕的放松方式——经过后院一个 100 英尺长的木码头去湖里划船，Idonae 说：「我们鼓励大家在湖中思考和放松。这里如同梭罗笔下的瓦尔登湖，我们切身感悟亲近自然，必然会汲取沉静的力量，发现更本质的自然规律。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 可能拥有全世界最美的办公地点，像极了托尔金笔下描绘的霍比特人的家园。Idonae 坚定的认为「我们就是一群霍比特人，以无畏严谨的脚步去追逐梦想（实现通用人工智能）。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于生物神经元的下一代深度学习&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「虽然目前的深度学习在语音识别和图像识别方面取得了突破性进步，但如果把深度学习用于绝大多数的其他领域，比如说自动驾驶、实体机器人等，就会面临一个来自于真实世界的非常大的挑战，那就是训练数据量严重不足。」Demiurge 联合创始人、CTO 任志攀（Bragi Lovetrue）表示。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLLNfSSD0CKIcgX1bFZD2wJEjfTJg2b9wpQOHvvZr3rU7zvzA5MlRUgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em&gt;&lt;span&gt;人工智能不同应用场景的数据需求和数据供给对比，图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拿开发消费级别的全自动驾驶来说，最大挑战在于要开发出在交通事故的预判和预防上远超人类驾驶员的软件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果用现有的深度学习去实现这一点，那就需要大量的事故数据，但这方面的数据供给非常有限，而采集数据又难度很大。首先，没有人能够准确预测何时何地会发生何种事故，因此无法系统地提前部署以采集真实事故数据；其次，从法律上来说我们不能靠人为制造事故来采集数据；第三，也无法模拟数据，因为事故更多涉及实时的传感以及与物理世界的互动，模拟出来的数据与真实数据差距很大，这从 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400004983&amp;amp;idx=3&amp;amp;sn=1b719d273cac2a9daee46019e9691b74&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=400004983&amp;amp;idx=3&amp;amp;sn=1b719d273cac2a9daee46019e9691b74&amp;amp;scene=21#wechat_redirect"&gt;DARPA 机器人挑战赛&lt;/a&gt;就能看出来；最后，像 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402117079&amp;amp;idx=1&amp;amp;sn=00acc2259bac0536471620c365988179&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402117079&amp;amp;idx=1&amp;amp;sn=00acc2259bac0536471620c365988179&amp;amp;scene=21#wechat_redirect"&gt;AlphaGo&lt;/a&gt; 那样，在规则定义明确的简单环境下自行创造大量训练数据的方式，在复杂的真实环境中难以发挥作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果遇到数据量不足的情况，同时又很难通过之前那些行之有效的方式去增加数据供给，那就无法发挥出深度学习的优势。而更重要的是，我们还会遇到数据类型不一样的问题，物理世界中是不同传感器获取的实时数据流，而现在深度学习在信息世界中的应用，比如说图像识别，使用的数据都是基于图片的数据点，而非数据流，所以这也是将深度学习现有的成功延伸到真实物理世界应用的一个底层障碍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于这个原因，Demiurge 专注于开发一种系统方法从源头解决真实世界诸多领域中数据量严重不足的问题——既然很难有效增加数据供给，为何不设法大幅降低对数据的需求？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;降低对数据量的需求、实现小样本学习甚至 one-shot learning，是目前深度学习研究中的关键问题，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717572&amp;amp;idx=1&amp;amp;sn=8265c0994d752907c5b99d40a1cd2e4b&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717572&amp;amp;idx=1&amp;amp;sn=8265c0994d752907c5b99d40a1cd2e4b&amp;amp;scene=21#wechat_redirect"&gt;Yann LeCun&lt;/a&gt;、 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716884&amp;amp;idx=1&amp;amp;sn=7ab39a8590cddae0d8b8247df5a29b20&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716884&amp;amp;idx=1&amp;amp;sn=7ab39a8590cddae0d8b8247df5a29b20&amp;amp;scene=21#wechat_redirect"&gt;Yoshua Bengio&lt;/a&gt; 等深度学习专家也多次在演讲中提到解决深度学习中 one-shot learning 问题的重要性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在今年斯德哥尔摩的全球机器人顶级学术会议 ICRA 上，Bragi 在 Industry Forum 演讲中介绍了 Demiurge 的方法，从神经科学里寻找关键线索，「&lt;strong&gt;比起深度学习的点神经元，生物神经元所擅长的是从多模的实时数据流中提取多维度的时空信息来实现 one-shot learning，这是现有的深度学习很难做到的。生物神经元不仅能够做这种特征提取，而且是以一种非常高效的方式，效果和效率都很出色。&lt;/strong&gt;」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度神经网络的确从神经科学领域的研究中获取了一些灵感，但其工作原理与人脑截然不同（诚然，我们对大脑的工作原理还没有弄清楚），Yann LeCun 表示，他最不喜欢的对深度学习的定义就是「它像我们的大脑」，谷歌 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717709&amp;amp;idx=1&amp;amp;sn=1edfa7e52ee07d65d11e60c77dbbd74a&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717709&amp;amp;idx=1&amp;amp;sn=1edfa7e52ee07d65d11e60c77dbbd74a&amp;amp;scene=21#wechat_redirect"&gt;Jeff Dean&lt;/a&gt; 认为深度神经网络是对大脑神经网络的简单抽象，并非是模拟人类神经元如何工作。神经科学专注的点包括计算的细节实现，还有对神经编码以及神经回路的研究。然而，在机器学习领域，人工神经网络则倾向于避免出现这些，而是往往使用简单和相对统一的初始结构，以支持成本函数（cost funcion）的蛮力最优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 从历史的角度分析了深度学习和神经科学的关系，「现在的深度学习从神经科学中获得的灵感非常有限，这是因为深度学习的理论基础是上世纪 80 年代基本定型的，那时之前的神经科学也发展比较慢，无法为深度学习提供更多灵感。而从 80 年代至今，神经科学的发展速度远远超过了之前，过去 30 年产生的神经科学知识是 80 年代以前的 46 倍，而且现在每年神经科学获得新发现的速度是 80 年代以前的 100 倍。所以，对于深度学习来说，如今的神经科学已经是一个非常巨大的宝库，为提升现有深度学习的学习能力提供重要线索。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 表示，越来越多的深度学习专家开始研究如何从神经科学中获取更多的线索，「 Yoshua Bengio 做的非常前沿，一方面研究深度学习的反向传播算法在生物神经元上是如何实现的，另一方面研究生物神经元的 STDP 学习算法如何提升现有的深度神经网络的学习能力 。位于深度学习与神经科学交汇的最前沿，我们很深刻地体会到现在正在发生着的转型，从深度学习和神经科学没有太大关系的这一代（深度学习1.0），过度到深度学习重新从神经科学获得重要启发的下一代（深度学习 2.0 ）。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTJr5lEmwciaTImJge1IUicCO11HRWwBXjEGHM1d4hFIo8SbkrK5Yg7Eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;深度学习 2.0 ，图片由来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在近期谷歌 DeepMind 和 MIT 媒体实验室的合著论文&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716140&amp;amp;idx=1&amp;amp;sn=3d74ee2545c20cba8189d445202f6f31&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716140&amp;amp;idx=1&amp;amp;sn=3d74ee2545c20cba8189d445202f6f31&amp;amp;scene=21#wechat_redirect"&gt;《Towards an integration of deep learning and neuroscience》&lt;/a&gt;中提到，近期出现的结构化、成本函数和训练程度的复杂化这两项机器学习方面的进展或许会将神经科学和机器学习两个研究领域看似不同的视角连接起来。此外，硬件方面，IBM Zurich 在 8 月首次用低成本高性能的相变材料实现了生物神经元计算的关键机制——&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717770&amp;amp;idx=1&amp;amp;sn=9f33fc6a5a3cabd55f26bd31e871769d&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650717770&amp;amp;idx=1&amp;amp;sn=9f33fc6a5a3cabd55f26bd31e871769d&amp;amp;scene=21#wechat_redirect"&gt;神经薄膜&lt;/a&gt; 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更重要的是，面向物理世界的移动人工智能的各种应用需求（识别、避障、抓取等），与各类生物在物理环境的各种生存需求是高度吻合的。Bragi 表示，斯坦福大学人工智能实验室主任李飞飞教授就特别重视深度学习在机器人上的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生物神经元，经过上亿年的演化，是自然找到的最优解决方案。对于 Demiurge 来说，理解生物神经元的计算模型是找到降低数据需求的通用算法，开发通用移动人工智能核心技术的关键。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这与其他解决数据量不足的思路有着本质不同。「比如说 UC Berkely 的 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402669894&amp;amp;idx=3&amp;amp;sn=be901c55ed7118c37be520853d3752a1&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=402669894&amp;amp;idx=3&amp;amp;sn=be901c55ed7118c37be520853d3752a1&amp;amp;scene=21#wechat_redirect"&gt;Pieter Abbeel&lt;/a&gt; 和 Google 的 Sergey Levine ，他们都是在用深度强化学习来开发基于自我监督学习（self-supervised learning）的通用算法，但这种自动的数据收集和标记本质上依然是增加数据供给。此外，NYU 的 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401148927&amp;amp;idx=1&amp;amp;sn=71dfd63b7e2f56f28c77db1107e9e74f&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401148927&amp;amp;idx=1&amp;amp;sn=71dfd63b7e2f56f28c77db1107e9e74f&amp;amp;scene=21#wechat_redirect"&gt;Brenden Lake&lt;/a&gt; 等用贝叶斯程序学习的方法针对特定问题开发出专门的数学模型。虽然能够在特定任务中大幅降低了数据需求，实现了 one-shot learning , 但这不是通用方法，」Bragi 说，「实际应用中需要的是降低数据需求的通用方法，深度学习的通用性无疑是最佳的。对于深度学习来说，如果不从生物神经元原理入手的话，是很难解决这些问题的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 表示，目前深度神经网络与生物神经网络最本质的区别在于神经元的类型。目前深度神经网络用的是点神经元，其计算模型是把信号加权平均的结果输入到一个非线性函数。这种点神经元是对生物神经元的极度简化，没有基于时间的变量。而生物神经元则利用脉冲进行基于多维时空变量的计算。单个生物神经元的计算模型是神经科学领域的一个关键问题，而这个问题的答案正是设计下一代深度学习的关键线索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 就是完全专注在这个问题上，所做的结果也走在世界最前沿，「我们已经开发出了生物神经元的计算模型，现在正在非常严谨的对它进行测试。 」Bragi 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLpBLmhEtLoSIWWIvFzlesvbJeozhTXHpfeJk9qDy9sAtiamJM2VXp1HA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;点神经元和脉冲神经元，图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型的关键在于理解脉冲如何以非常少量的计算步骤和能耗能够准确抓取极高维度的时空信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「没有基于脉冲的计算模型—仅仅像 IBM TrueNorth 那样，简单模仿一些生物神经元的硬件特点，或者像 Numenta 和 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=3&amp;amp;sn=f7438d205d1ec70454134d59802a3326&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715580&amp;amp;idx=3&amp;amp;sn=f7438d205d1ec70454134d59802a3326&amp;amp;scene=21#wechat_redirect"&gt;Vicarious&lt;/a&gt; 的 HTM（Hierarchical Temporal Memory）那样，简单借鉴一些生物神经元的软件特点—消费级别大脑芯片的硬件开发也就无从谈起。 对于实现生物神经元计算模型的软硬件要求的掌握，是 Demiurge 最重要的核心优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种计算模型的提出是多尺度跨领域研发的结果，不仅需要对跨领域的基础理论和前沿算法进行研究，还要从应用角度来分析真实世界的需求和需要满足的限制，来缩小算法搜索的空间。因为真实世界中有很多限制，比如说提供的数据量非常少，但为了应用成功或者让物种生存，就必须快速学习来了解整个环境，而在整个过程中又不能耗能太多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生物智能给 Demiurge 提供了非常重要的线索，他们从跨物种的通用智能系统出发，理解要满足什么样的条件才能最大化它们的生存，这是从生存追问的一种智能系统设计的思路。不管设计出何种模型，都要满足这些限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前深度学习领域从实验室结果到产品级应用的演化进程，对于真实世界的诸多限制一开始是尽量回避的，即首先选择那些可以不太涉及物理限制的简单场景，尽力实现在该场景下深度神经网络的最优化表现后，再开始逐条考虑开发应用时必须面对的各种物理限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这种演化可能适合学术研究，但不适合产品研发。Demiurge 的研发从一开始就充分考虑真实世界应用的所有限制，开发出来的计算模型和大脑芯片能在真实世界的各种限制条件下完成出色稳定的应用表现。 」Idonae 进一步解释了这背后的决策依据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于采用了同时满足技术突破和应用表现的双重评估标准，Demiurge 的研发风格是极为大胆和严谨的。提出的计算模型首先要在从数学理论上完整论证，同时还要用神经科学最新的发现和数据去做验证。这部分数学理论与神经科学的验证之后， Demiurge 会开始软件的模拟和硬件的实施，最终把自动驾驶作为首个测试平台，通过实现第四级别的无人驾驶测试他们的大脑芯片产品在对交通事故的学习、预判和预防的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 虽然是从神经科学中寻找深度学习突破的密码，但他们所做的技术依然可以称之为深度学习，最终的产品形态也是利用深度神经网络，也利用很多的隐含层和反向传播算法，只不过是将深度神经网络中的点神经元替换成了脉冲神经元，是计算单元的区别，在整个计算架构上区别很少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，可以在充分利用了生物神经元优势的情况下同时还继承了这代深度学习的所有优势，比如说具有通用性，以及从训练的角度上是 model-free ，这依然是一个以数据和经验来驱动的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 说：「我们和 DeepMind 、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716227&amp;amp;idx=1&amp;amp;sn=ad2807981602ddabff37d235aa6d2810&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716227&amp;amp;idx=1&amp;amp;sn=ad2807981602ddabff37d235aa6d2810&amp;amp;scene=21#wechat_redirect"&gt;OpenAI&lt;/a&gt; 等最大的区别是，我们很清楚脉冲神经网络在感知数据流计算上的巨大优势，并知道如何从软件上和硬件上实现它。对这一代深度学习来说，正如 Google 资深研究员 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect"&gt;Greg Corrado&lt;/a&gt; 在 Brain Forum 上所说，他们尚不清楚如何利用脉冲进行计算，在算法层面和应用层面发挥脉冲的优势&amp;nbsp; 。我们与 IBM 区别是，IBM 的最新突破用 GST 相变材料首次完整第实现了单一神经薄膜，这是基于对生物神经元物理性质的深入理解与再现，但要开发应用于物理世界的大脑芯片， 仅靠复制生物神经元的物理性质是不够的，根本上仍然需要对生物神经元计算原理的掌握，后者是 Demiurge 的核心优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;游戏规则制定者&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;理解生物神经元的脉冲计算原理，是神经科学领域的世界级难题，同时对人工智能界的下一次突破也意义重大，面对这样一个不论是从科研还是从应用上都将带来巨大价值的命题，Demiurge 作为一个资源有限的创业公司是如何做到的？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「不应该是艾伦实验室、索尔科研究所、&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=403048122&amp;amp;idx=2&amp;amp;sn=1baead76eee66f84f989c7d4d2f0dea4&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=403048122&amp;amp;idx=2&amp;amp;sn=1baead76eee66f84f989c7d4d2f0dea4&amp;amp;scene=21#wechat_redirect"&gt;HBP&lt;/a&gt; 等世界级脑科学研究机构，或者谷歌、Facebook 等科技巨头才有动力和能力去解决这样一个世界难题吗？」我非常直接的向 Bragi 询问。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「就像阿基米德那个用杠杆去撬动地球的比喻，对于撬动这个世界级难题来说（理解生物神经元的脉冲计算原理），有很多不同支点（探索方法）可供选择。比如说各国脑计划的研究重点主要集中在提高探测设备和研究手段，使得我们能够尽可能收集从局部到全部、从单个时间点到更大时间尺度上的尽可能多的关于神经元的数据，他们大多是从收集数据的角度来努力。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「艾伦研究所在这方面做了很多贡献，不仅提供了系统化数据收集的标准和数据收集的设备，同时还把收集上来的数据加以整理并免费开放，他们的思路是，更多的数据可能会帮助我们最终解决算法的问题，这是大数据驱动的对算法的理解。而欧盟脑计划（HBP）的思路不一样，他们认为，即便是收集足够多的数据，但缺少模拟的过程，对数据的利用效率也不够高，所以他们特别强调建立一个全尺度、高精度的虚拟大脑，这样就能保证在虚拟大脑里重现已经观察到的大脑的现象和特征，从而让我们更加准确的提出测试各类神经元的计算模型，这也是从蓝脑计划到欧盟人脑计划的一个重点。」Bragi 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;Idonae 补充到：&lt;/span&gt;「以上这些研究更多的是提供了基础设施，支点都离问题比较远，而不是直接去解决这个问题。而 Demiurge 选择了最近的支点（完全专注于单个生物神经元），并打造出了最长的杠杆（提出了通用的脉冲计算模型），所以能够以有限的资源撬动无限的潜力。」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;她认为「下一代深度学习是一个底层应用问题，不是一个表层应用问题。底层问题则需要对多领域深入的理解和灵感来寻求突破，还需要对应用核心痛点的深入理解，所以预测和管理更具挑战性。而表层问题可以用循序渐进改良的方式推动，产出和时间相对容易预测。在学术界和大企业机构，相关评审机制的设计和运作有利于解决表层应用问题，但对解决底层应用问题的机制缺乏动力和经验。因此在解决底层应用问题上，Demiurge 量身打造的文化制度和评审机制就会显示出独特的优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLTx3W9DEHDOaz6REwxicy1cicftyC4TuAAFsm9uSaur5DZ2iboib1G1qpqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Demiurge 联合创始人、CEO Idonae Lovetrue 在 TEDx Hochschule Luzern 演讲，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此前的 TED 演讲上，Idonae 也提到了 Demiurge 相对于科技巨头的优势，「大企业显然有多种优势：充足的资源、雄厚的财力和强大的网路，但开发应用于物理世界的人工智能最重要的事情是生&lt;/span&gt;&lt;span&gt;存本能，但这与大企业的属性相悖，企业一旦做大，保持其生存本能就会极其困难。但生存却是创业的一切，并且它在每个人的血管中流淌，我何时何地都能感受到它。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「所以，Demiurge 为自己创造了一个非常独特的位置，掌握了一个从科学的利益和动力、产品的利益和动力的完美契合点，」Bragi 表示，「单个神经元计算模型这个问题既是从深度学习应用需求来说必须要解决的根本问题，同时也是神经科学领域一个诺贝尔奖级别的问题。比起学术界，Demiurge 离应用最近，可以获得一些额外的关键启发和应用场景下的限制条件， 更有能力去做这个事情。比起工业界，Demiurge 离科学最近，能够非常专注地去彻底解决应用的底层问题，更有定力去完成这个事情。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而恰恰是因为 Demiurge 所坚持的这个目标也是神经科学家一直以来的终极目标，所以神经科学领域的顶尖机构和专家非常支持他们，为他们提供研究成果、数据和人才。所以从这方面来说，Demiurge 和神经科学领域的大机构是一种合作关系，而非直接的竞争关系，而这种合作关系也是平等的优势互补，这些科学家不是在通常情况下的单方面付出，而是可以获得反馈。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「&lt;strong&gt;我们就像魔戒远征队，而那些在神经科学领域已走得最远的专家就像是精灵族的长老，用他们的的知识和智慧来帮助我们。&lt;/strong&gt;」Idonae 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年 5 月份举办的世界顶级神经科学会议 Brain Forum 也邀请了 Demiurge ，他们的展台就在神经科学巨无霸项目「欧盟人脑计划（HBP）」的旁边，Brain Forum CEO Jamil El-Imad 对 Demiurge 赞赏有加，他在大会开始前的媒体见面上唯一介绍的一家创业公司就是 Demiurge ，「在所有的创业公司中，Demiurge 的视野（vision）与我内心所想最为接近。」Jamil 在私下交流时说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就连不可一世的欧盟人脑计划和瑞士蓝脑计划的负责人、全球顶级神经学家 Henry Markram 也在一直积极且慷慨为 Demiurge 提供支持——将他了解的科学家与他的学生介绍给 Demiurge 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLJSlzHqRn5ITXNJn32s1Vtl7uBf3wGyUcVUqM2oHzQ4yEdEicdwgDV6w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Brain Forum 会场，欧洲脑计划（HBP）发起人 Henry Markram 和 Idan Segev与 Demiurge 创始人 Idonae 和 Bragi 交流，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这次远征中守护他们前行的不仅有科学家，还有投资人，Demiurge 目前完成了两轮投资，投资人包括对技术有着极强前瞻性的投资机构和知名企业家。Idonae 说：「深度学习是一个生态系统，不仅跨越了很多应用，还突破了传统应用和研发之间的壁垒，我们需要很多背景去了解和应对这个全新的生态系统，投资人的经验和视野在这方面可以给我们很多帮助。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;各类人才的汇集让 Idonae 充满信心，「我们只要坚持走在解决这个问题的路径上，最适合的人会一个一个陆续登场，而每个人都必然是在相关领域深耕良久，因为只有有了很深的积累之后才有足够的眼光看到我们解决这个问题的必然性。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而对于 Demiurge 来说，他们不仅希望自己创造的这套新的游戏规则能够帮助他们解决具体问题，还希望这个规则本身可以为后来者提供一种史无前例的参照。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们在创造一个先例，从来没有人说过创业公司不可以通过解决一个诺贝尔奖级别的问题来直接开发出堪比互联网基础的人工智能技术，只不过是很少有人有勇气做这方面尝试，而我们非常清楚我们的目标是什么，我们存在的意义是什么。我们希望自己是启发性的，也可以让后面的人有一个新的参考体系。」Idonae 表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「为什么是我们？我们对现有的游戏规则很了解，并且非常清楚做到什么程度才算是真正的成功，」Idonae说，「通用人工智能的成功标准，高于在 ImageNet 竞赛中取得高分，高于实现完全的自动驾驶，而是能够实现人人可居的智慧城市，人人可获益的地外探索。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;自动驾驶&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 基于生物神经元计算模型所提出的下一代深度学习及相关的软硬件平台， 可以做到高性能、低成本的解决小样本学习和自适应学习等人工智能在真实世界中所面临的诸多问题。从目前来看，这项技术最直接、也是最有市场需求的应用就是自动驾驶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 和 Idonae 五月份的行程非常密集，他们需要去瑞士中部的卢塞恩进行 TED 演讲，然后当天赶到西南部城市洛桑参加 Brain Froum ，会议结束后再返回公司。Bragi 驾驶着一辆 Model S 在四天里行驶了超过 800 英里，沿途再美的风景也会屈服于驾驶员的时间成本和精力消耗，这也是所有人期待自动驾驶早日实现并积极参与其中的原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从 20 世纪 80 年代卡耐基梅隆大学的 Navlab 计划，到谷歌自动驾驶项目，再到如今所有相关公司的强势布局，众多参与者都走在追求这个终极目标的路上，每个参与者都会基于自己的优势规划发展路径，神秘的自动驾驶创业公司 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=3&amp;amp;sn=79401d6c07b5ba73330e9c0a69b9f0fd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718786&amp;amp;idx=3&amp;amp;sn=79401d6c07b5ba73330e9c0a69b9f0fd&amp;amp;scene=21#wechat_redirect"&gt;Drive.ai&lt;/a&gt; 就完全押宝于深度学习，将深度学习应用于全自动集成驾驶堆栈，改变用规则去应对各种场景，让汽车完全自行通过理解数据去学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;而 Demiurge 的方案不是循序渐进，而是从自动驾驶场景下的小样本学习和与真实物理世界交互的两大限制出发，用生物神经元的计算模型从根源上解决这个问题。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「比如说蝗虫，它们的翅膀非常孱弱，任何撞击对它们来说都是非常致命的，但它们在高速飞行中有着几乎完美的自动避障能力，这背后的机制如果用在自动驾驶汽车上，将会实现第四级别的自动驾驶。最令人吃惊的是，蝗虫的自动避障系统只用了两个生物神经元，一个用来探测障碍，一个用来执行避障的行为，这说明生物神经元在处理物理世界的任务时，从小数据和数据流中的学习和决策能力非常出色，这对我们降低数据需求提供了重要线索。」Bragi 说。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL1xxhEXUkibX9g7JQaqfEm74ESx39WE2P8Ap7LCySFFnyd0tNWJfFDnQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;基于深度学习的自动驾驶和蝗虫自动避障的对比，&lt;em style="line-height: normal; text-align: center; white-space: pre-wrap;"&gt;&lt;span&gt;图片来源 Demiurge Technologies&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在产品方面，软件依然是第一位，但如果现有的自动驾驶平台无法与他们的软件相适应时，Demiurge 也会重新设计硬件，「我们要设计的深度学习芯片也是基于脉冲神经元，所以从硬件实施上也与现在的硬件有所不同」，但 Bragi 没有透露更多具体细节，「这两种方法都是可能的，至于选择哪一种，则是看工程上的需要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在众多自动驾驶领域的参与者中，Demiurge 认为公司最大的潜在竞争对手是&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716988&amp;amp;idx=1&amp;amp;sn=65ad0b7c47c2b6c3f1b0b89e6fab1ff5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716988&amp;amp;idx=1&amp;amp;sn=65ad0b7c47c2b6c3f1b0b89e6fab1ff5&amp;amp;scene=21#wechat_redirect"&gt;特斯拉&lt;/a&gt;。特斯拉在去年 10 月通过软件升级增加了辅助驾驶功能，这个功能在研发时使用了特斯拉车主过去 18 个月积累的 7.8 亿英里行驶数据。在该功能上线后的短短六个月内就积累了 4,700 万英里数据，远远超过谷歌历时 6 年积累的 150 万英里，而近期特斯拉的这个数据已经增加到 1 亿英里。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 把特斯拉视为头号竞争对手的原因在于，目前只有特斯拉充分认识到现有深度学习对于数据需求量过大的底层问题，并且后者正在用不同方式来逼近这个目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 说：「特斯拉在收集数据上有着垄断性的巨大优势，所以能够利用现有深度学习做自动驾驶，在与大多数同行竞争中已然遥遥领先。但特斯拉并没有满足这一状态，Elon Musk 同时通过成立 Open AI 在本质上寻求能够实现第四级别自动驾驶的下一代的深度学习算法，完全超越竞争，这和 Demiurge 的思路是一样的。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;宏大目标下的生存追问&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 Demiurge 的两位创始人交流其实是一件压力很大的事情，不管他们在介绍 Demiurge ，讨论人工智能行业，还是随意的聊天，我都需要努力让自己的思维保持活跃，以跟上他们的谈话节奏和思维逻辑。他们有一种独特的谈话方式，总是会抽茧剥丝般的从现象聊到本质，并会将不同事物联系起来推理出通用的规律并进行演绎，既可以自下而上的将具体抽象成理论，也可以自上而下的将理论落实到具体事物或者一个形象的比喻上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在深入了解他们的创业理念后，才发现他们的所有行为，小到一个聊天的话题，大的公司的研究方向，都可以用一种独特的气质支撑起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而这种独特的气质就是被他们视为竞争对手的特斯拉创始人伊隆·马斯克的「秘密武器」——第一性原理，马斯克自己对此的解读是：我认为，人们的思维过程通常都束缚在常规或类似的经历中。很少会试着在第一原则的基础上考虑某些事。他们习惯说：「因为我们以前那样做过，所以我们会这样做。」或者，他们不做这事是因为「好吧，之前没人做过，所以情况肯定不太妙。」但是，这是一种荒谬的思维方式。你必须从头开始推理，你注视着那些基本条件，然后从中建构你的推理，之后你可以看看是否得出一个有效或是无效的结论——可能会和人们以往的结论相同，也可能不同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这种思维落实到行为上，便是基于问题的根源来设定目标，并保证之后做出的所有行为都与这个终极目标息息相关，并是以一种效用最大的方式去实施。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）基于核心问题提出目标&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bragi 意识到现有深度学习无法解决小样本学习和与物理世界实时交互的问题，而这恰恰是生物神经元所擅长的，所以他就直接去研究生物神经元的计算模型，以创造下一代深度学习并付诸实用。相信很多人工智能领域的从业者都可以发现目前深度学习的问题，也经常有人表示需要让机器像人类一样学习，但很少有人、尤其是创业公司会把这个问题作为自己专注的研发目标，更多的是去选择一些权宜之计。&lt;strong&gt;但对于 Demiurge 来说，他们不仅理性的发现了这个问题，同时又有着足够的勇气去挑战它，而从来不会考虑此前是否有创业公司这样做过&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 会让人不自觉的想到另外一家如日中天的人工智能创业公司 DeepMind ，都是学术与应用相结合，都是专注于通用人工智能，且创始人都有着神经科学和人工智能的复合背景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当被问到和 DeepMind 的异同时，Bragi 表示：「被谷歌收购之前的 DeepMind 和我们很像，我认为他们接受被谷歌收购是『the best scenario of failture for DeepMind（最好的失败）』，因为如果他们率先解决了通用人工智能这个问题，那么谷歌被 DeepMind 收购将会是『the best scenario of failture for Google』 。我不知道他们是在研究了三四年之后遇到了瓶颈，还是说一直把这个（被收购）当成备选方案。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当被问到谷歌所提供的数据和资源支持是否会成为 DeepMind 的竞争优势时，Bragi 依然是从目标出发给予了否定，「我们看的非常清楚，解决物理世界的问题，大数据不是一个帮助，而是一个掣肘，想要利用谷歌的资源和数据优势，我们认为是没有真正理解解决这个问题的方法。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我判断的标准是一个公司是否把最核心的资源、人和时间放在最关键的问题上， DeepMind 被谷歌收购前是这样，但现在有点不一样了。」Bragi 非常坚定地认为，「比如说 AlphaGo ，它很好，但不是我们想要的」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 创始人表示，他们只想去解决一个问题：让人工智能不再只是成为谈资，也不是炫目的展示，而是让大家在生活中能够获得便利和切实的好处，或者通过平价地外探索去获得人类文明新维度的可能性，这个是他们投身人工智能的目标，他们只想达到这一个目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）围绕目标的生存法则&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 认为，Demiurge 看到了一个人工智能应用的关键问题，并拥有对这个问题的兴趣以及解决这个问题的决心。所以确定了这个目标之后，Demiurge 的每一个动作都极其严苛的围绕它去实施，而不在乎常规做法是什么，以及通行的游戏规则是怎样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「和生物一样，我们所做的事情也是生存驱动的，」Idonae 说，「我们这个旅程就是要抵达终点，我们要知道每一天如何更逼近自己的目标，并以一种能耗最低的方式往前跑。如果在这个过程的每一步都要符合主流的预期，做一个好的表现者，同时又要以最快的速度抵达终点，那这个预期本身就是矛盾、不切实际的 。有人擅长把每一步都做的漂亮，每一步都有掌声，这是一种做法，但这不是我们擅长的做法；我们不在乎这一过程的孤寂，只在乎是否抵达真正的终点， 所以，我们或许只会攒到一次掌声，那就是当绝大多数人都因这项基础技术而受益时为我们的产品而鼓掌。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种状态 从Demiurge 创立之前就已经体现了出来，Bragi 在纽约大学读本科时，为了弄清楚如何实现通用人工智能，他就向学校申请自主设计了一个全新的本科专业——覆盖计算机科学、人工智能、神经科学、物理学和哲学，设计并完成了大量的博士级别的研究课题。后来在计算机名校卡耐基梅隆读研期间，他也没有按照大多数人的通行研究路径选择继续读博。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「先弄清楚到底怎么样做才能实现通用人工智能，然后从解决这个问题来追问所有的决策，而不是说会做任何形式上的妥协。这个是我们每做一次看似不寻常的选择时背后的逻辑，其实就是对问题本身的探求和理解。」Bragi 解释到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 还做了一件在大众看来不太寻常的事情。今年 3 月，谷歌计划出售机器人公司&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715759&amp;amp;idx=1&amp;amp;sn=004c78b7cf919b0e9b7171720633aac5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715759&amp;amp;idx=1&amp;amp;sn=004c78b7cf919b0e9b7171720633aac5&amp;amp;scene=21#wechat_redirect"&gt;波士顿动力&lt;/a&gt;，Demiurge 立即在自己的 Medium 主页发布官方声明，表示希望收购波士顿动力公司，将波士顿动力公司的相关技术整合到基于 Demiurge 深度神经网络的新一代机器人操作系统中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;采访期间，谷歌宣布将波士顿动力出售给丰田。「卖给丰田是波士顿团队第二好的选择（注：&lt;span&gt;Bragi 认为，&lt;/span&gt;最好的选择是被 &lt;span&gt;Demiurge 收购&lt;/span&gt;），波士顿动力本身含有不同声音，有人坚持基于控制论的模型设计，但面对复杂的人与外部真实环境的交互，开发模型是极其困难的。有人相信深度学习能够模拟任意函数的建模优势，但是用深度学习进行端对端建模， 需要远超谷歌能力的海量的传感和控制数据 。他们所面临的最核心的难题恰恰是我们可以为他们解决的，就是与真实物理世界交互的深度学习系统。」Bragi 说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然交易未能达成，而 Demiurge 作为一个初期创业公司所发出的这个声明也没有得到大范围的媒体关注，但这件看起有些不可思议的事情恰恰体现出 Demiurge 的理念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;公司把这个作为唯一标准来筛选人才、投资人，甚至是公司的地理位置。在一年多前的一个内部讨论会上，当 &lt;span&gt;Idonae&lt;/span&gt; 提到要把公司设在瑞士时，在场的所有投资人和专家都表示很难理解。当今天再回顾这个问题时，现实的回报让 Demiurge 的创始人确信当初选择的明智，「公司的选址取决于你要做什么类型的创业，我们是要解决深度学习的核心挑战，同时也是脑科学的难题，解决这个问题之后还要将一系列技术产品化，把这些结合起来看，瑞士是最合适的。瑞士有着全球领先的神经科学和机器人技术，而且瑞士特别适合全球市场的高科技企业的成长与发展（比如罗氏、诺华以及 ABB ）。而我们的技术应用是一个万亿级别的全球市场，在行业合作、产品和安全标准制定、甚至是国际间的多重合作方面，瑞士都有着稳固的优势。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 还说「我们最看中的还是人与人的吸引，什么样的问题吸引什么样的人进来，而欧洲的人才池对我们这个问题最感兴趣，也是最了解及最能提供贡献的。同时，我们也看中瑞士的视野和地缘里的养分，瑞士尊崇着商业长久之道的本质，不鼓励通过对个人的崇拜转化为产品销售力，而是推崇用最好最可靠的产品品质赢得市场，与客户建立长久简单的供求关系。 在这方面，瑞士给了我们一个非常重要的参照环境，让我们体会到下一代深度学习产品应该怎么做」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种基于目标的生存追问成为 Demiurge 的文化基石，使他们做所有决策非常简单，因为这是他们唯一的标准。这个标准不仅帮助他们找到了最合适的办公场所，吸引到了最适合专家和团队，还让他们在前行的路上可以极度理性的看待争议和否定，摒除一切干扰因素。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在评论马斯克的&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401181797&amp;amp;idx=1&amp;amp;sn=7e392d911b29b04b964a5aad7706eecf&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=401181797&amp;amp;idx=1&amp;amp;sn=7e392d911b29b04b964a5aad7706eecf&amp;amp;scene=21#wechat_redirect"&gt;第一原理&lt;/a&gt;时，Tim Urban 用了一个非常简洁的描述：「 Things I want → Want 」。Idonae 有着同样的理性和坚定：「我们就一个目标，很简单，就是要到达哪儿，一路上没有建设性的争议和否定不能给我任何食物和魔法帮助我去接近这个目标，所以我们会自动过滤。在这个路程上，我们就是不断选择、凝聚能理解我们理念，并能提供不可替代价值的人，我们不会去在乎其他。批评和讽刺的声音只有在看到产品，或者通过产品解决他们的需求时才会发生改变。我们是创业团队，我们要生存，所以我们会动用所有的资源放在解决这个问题上，这个问题解决了，产品本身就是答案。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于目标的生存追问让 Demiurge 敢于去解决一个世界难题，吸引到领域内顶级的科学家为他们提供支持，创造了第三种游戏规则，还组建了在每个环节上都拥有 1-2 名顶尖研究者的强大团队，包括数学家、神经科学家、机器人学家等。同时也形成了他们自己一套独特的管理方式——像特种部队一样用时间来激发最大潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从科技史的发展来看，Bragi 认为科技的突破很难预测，因为之所以成为突破就因为与现有技术有着质的不同，而我们的预测又是基于对已知技术的理解， 所以预测本身是不可靠的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;面对公司目前在做的这样一项质变而非演变的技术，从内部管理来说，他们挑选出来的成员都是有着相同的生存本能——所有任务的期限，就是自己毫不妥协地完成任务所用的最短时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 说「就像是英国特种部队的野外徒步训练，教官不告诉特种兵任务的 deadline ，所以没有人知道 deadline 的具体时间，但都知道它的存在，所以每个人都会竭尽全力以最少的时间来完成任务，激发出最大的潜能。我们的团队成员都是这样，必须全力以赴，以最短的时间高质量的完成每一个任务。如果我们人为设置一些时间点，如果过短，就会迫使大家走一些错误的捷径，如果过长，反而会让人松懈自满。我们要一直处在寻求生存的状态下才能够最好地生存下去。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tim Urban 认为培养马斯克的思考方式有三个维度：1）对已知的事物保持谦逊；2）对可能实现的事物保持自信；2）对无关紧要的事物不存怯懦之心。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与之类似， 如果将 Demiurge 理念进行拆解的话，就是他们印在公司 T 恤上的 5 个短语：仁爱的理性（empathetic rationality）、谨慎的无畏（prudent fearlessness）、有批判精神的信仰（skeptical faithfulness）、谦逊的灵魂（humble souls）、勇敢的思想（daring minds）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Idonae 对此的解释是：「要心怀远大，但恰恰是因为非常理解目标的远大，所以才知道每一步所必要的谨小，既做到感情上的认同，又极其理性地去分析，因为靠一腔热血是走不远的；对目标极其坚定，但在前往目标的道路上时刻保持批判精神；有谦逊的灵魂才会接近客观地理解问题，理解一路上将要面对多少艰难险阻，而勇敢的思想是我们的生命之源，使我们能够自信认定地一步一步走下去。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Demiurge 公司 T 恤上还有另外一个图案，是线虫、鱼、昆虫、狗和婴儿五种从低级到高级的生物，下面依次标注了各自的神经元数量。这些神经元凭借一种目前还不为我们所知晓的机制互相连接，让生物拥有在现实世界中生存下去的智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而 Demiurge 自身也是这样一个「神经元」，以实现智能为最终目标，同时以一种效率最高且目的性最强的方式做着每一次与外界的连接。生物神经元的计算模型 Demiurge 还在研究和测试，但 Demiurge 自身生存和成长的机制他们已经找到，正是这种机制将 Demiurge 的目标、研究、管理和合作等所有行为都完整地融合在了一起。使他们成为一个拥有智能的生命体更好地生存下去，并帮助他们创造出更多的「生命体」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 世纪伟大的物理学家埃尔温·薛定谔的著作 《What is Life》前言里有这样一段话：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「古往今来，从探索中发现普遍的真理是最有价值的。但是过去一百年中所积累的众多物理学领域的知识，其广度和深度，让我们对统一真理的探索面临如下挑战：一方面，我们已经开始掌握了能够将各个领域的知识融汇贯通的办法；另一方面，即便是对某一学科领域更加专业化的知识，如果想要彻底掌握它几乎又是不可能的事情。只有我们中的某些人，敢于冒着自己被看成是愚蠢之人的风险，去大胆地综合所有发现和理论，我们才有可能摆脱挑战，发现普遍真理。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于 Demiurge 乃至今天的人工智能领域来说，可能也是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;©本文由机器之心原创，转载请联系本公众号获得授权。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 04 Sep 2016 19:40:49 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | ELM超限学习机：填补罗森布拉特的神经网络梦想和冯·诺依曼对生物学习困惑之间的空白</title>
      <link>http://www.iwgc.cn/link/2555447</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：&amp;nbsp;黄广斌&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文总结被神经网络前辈和著名经济学家 Halbert White 认为「Sexy」的超限学习机（Extreme Learning Machines, ELM）的「Sexy」之处和之所以被称为「超限学习机（ELM）」的原因。在超限学习机的理论框架下，机器（Machine, Devices, Sensors）和生物脑可以看成一致的，只是构造的基本材料和硬件不同而已。有机的「机器」（生物学习系统）也有千万种，并且还在一直自我演化。但我们坚信两者之间可以拥有一个共同的「基本粒子」级（或称为「基本单元」级）的学习结构和学习算法，那就是超限学习机（Extreme Learning Machines, ELM）。而这种超限学习机（ELM）的实现和硬件材料以及具体数据可以是无关的。ELM 理论指出众多种的生物神经元在学习中是不需要调整的和数据无关的。生物学习机制的秘密可能就在于其神经元的随机性。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;作者介绍：黄广斌（Guang-Bin Huang）是新加坡南洋理工大学教授（终身）。在 2014 和 2015 年被 Thomson Reuters 评为「高引用研究者」（工程类，计算机科学类），以及「2014 年世界最有影响力的科学精英」和「2015 年世界最有影响力的科学精英」。他是新加坡总统科学奖被提名人（2016）。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;他主持的主要项目有&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：德国宝马集团和南洋理工大学未来汽车联合研究实验室人机交互，脑机交互以及汽车辅助驾驶项目，英国劳斯莱斯和南洋理工大学联合研究实验室海上自主导航决策辅助系统项目，新加坡科技工程和南洋理工大学先进机器人联合研究实验室场景识别和机器学习项目，台湾台达电子股份有限公司和南洋理工大学物联网联合研究实验室数据分析和视频项目。还担任过新加坡樟宜机场新加坡航空公司地面服务公司第五货运大厦的信息跟踪控制系统升级改造的总设计师和技术负责人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;神经网络和生物学习之间的空白&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 弗兰克&lt;span&gt;·&lt;/span&gt;罗森布拉特的神经网络梦想&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 1950 年代初期，生物学家弗兰克&lt;span&gt;·&lt;/span&gt;罗森布拉特（Frank Rosenblatt）提出了他称为感知器（Perceptron）的多层前馈网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来跨越 60 多年特别是从 1980 年代到现在用的大部分神经网络结构其实都是罗森布拉特神经网络感知器的一种，这些包括早期流行的支持向量机（SVM）和现在风靡产业界的卷积神经网络（CNN），也包括 CNN 的前身 Neocognition ，只是针对不同的实现后人提出了不同的学习算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特最初提出他的神经网络结构时并没有有效的学习算法，但是他梦想这种神经网络感知器可以看作是「计算机的一种胚胎」，一种最终能够帮助计算机实现「走、说、看、写、繁衍并有自我意识」的智能源泉。罗森布拉特的预测在 60 年后的今天被证明是正确的，这种神经网络技术还有可能是未来人工智能和机器学习的主要技术基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 马文&lt;span&gt;·&lt;/span&gt;明斯基和 1970 年代人工智能冬天&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特的预测在 60 年前是极其大胆和有远见的，在当时计算机犹如一个庞然大物的时代几乎没有几个人相信他的预测是对的和他的梦想是能实现的。也许伟大的思想之所以伟大就在于远远超前现有人们所能理解和所能想象的。包括人工智能之父、图灵奖获得者马文&lt;span&gt;·&lt;/span&gt;明斯基（Marvin Minsky）和神经网络之父 Bernard Widrow 都对罗森布拉特的预测表示怀疑。罗森布拉特提出的神经网络感知器严格意义上讲在提出之初还只是概念，正如许多伟大的想法在提出之初都会出现有些概念模糊不清的情况，大部分人有疑虑也就正常了。明斯基对罗森布拉特的神经网络感知器的否定直接导致了被后人称为「美丽错误」的发生在 1970 年代的「人工智能的冬天」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两年前在 Bernard 家吃着他夫人精心准备的旧金山螃蟹，边回顾着 60 年来的神经网络发展往事，受益匪浅也感慨万千。Bernard 在和我探讨超限学习机（Extreme Learning Machines, ELM）时提及他和明斯基以及罗森布拉特三人之间的往事时诚恳地承认在 1950 年代他对罗森布拉特的神经网络感知器也是不太认同，在他和罗森布拉特之间的争论中他是错了。不得不被前辈们敢于承认错误的勇气折服。&lt;span&gt;（提醒：学术争论无论激烈与否可以有助于找寻自然规律的真象，这和打着学术争论之名行人身攻击之实是有本质区别的。）&lt;/span&gt;Bernard 提及在 1971 年，也就在「人工智能的冬天」开始之初，罗森布拉特在他 43 岁生日那天在一个湖里划帆板时发生意外就再也没有回来，连尸身都没有找到，令人不禁辛酸和感叹。试想：罗森布拉特如果不是英年早逝（某种程度上讲是含冤而死），人工神经网络、人工智能和机器学习技术也许还会往前推进 10-20 年。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;有关 Bernard 和超限学习机的一段小插曲：Bernard 在超限学习机发表后 10 年左右提出了一个类似超限学习机的技术但却没有注意到早期有关超限学习机工作。本来这是一个小事，人们很难查看到所有有关资料，科研很能面面俱到。Bernard 却向我当面提出道歉，前辈们谦卑的人格再次让人折服。&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的困惑&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算机的硬件实现是要极其精致美妙的，但计算机的实现也是极其脆弱的，不能有任何瑕疵。任何硬件实现上的不完美都可能导致计算机不能正常运作。约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼（John von Neumann）在造出第一代计算机之后，做为计算机之父的他感到困惑不解的是：和计算机需要完美硬件连接组成所不同的是，为什么「一个看上去不完美的包含许多看似随机连接的（生物）神经网络却能够可靠地实现完美的学习功能」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;罗森布拉特的梦想和冯&lt;span&gt;·&lt;/span&gt;诺依曼的困惑之间有着很大的空白地带和理论技术鸿沟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;超限学习机：&lt;span&gt;填补神经网络和生物学习之间的空白&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人脑可能是宇宙中最复杂的东西。人类在过去几百年对自然界和宇宙的认识在飞速发展，对生物学习特别是人脑的思维机制还知之甚少。罗森布拉特的人工神经网络感知器和冯&lt;span&gt;·&lt;/span&gt;诺依曼关于生物学习的困惑以及未解之谜看似关联性不大。其实在超限学习机的理论框架下，机器（Machine、Devices、Sensors）和生物脑可以看成一致的，只是构造的基本材料和硬件不同而已。一种由无机的硅等组成，一种由有机的碳水化合物蛋白质等组成。生物脑本质上也是一种「机器」。无机和有机的「机器」可以完全不一样，它们的结构和算法也千变万化。有机的「机器」（生物学习系统）也有千万种，并且还在一直自我演化。但我们坚信两者之间可以拥有一个共同的「基本粒子」级（或称为「基本单元」级）的学习结构和学习算法，那就是超限学习机。而这种超限学习机的实现和硬件材料和具体数据可以是无关的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 作为人工神经网络的超限学习机&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）「秒杀」学习速度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工神经网络在人工智能和机器学习中的重要作用最近几年又再次得到认可和追捧，大有人工智能和机器学习的实现必须依赖于人工神经网络之势。然而人工神经网络技术普遍面临着一些挑战，比如繁重而「痛苦」的人工干预、缓慢的学习速度和较弱的可扩展性。超限学习机的一个基本目的是要克服这些过去几十年来人工神经网络界面临的发展瓶颈，达到尽可能少的人工干预，高的测试准确度和实时快速本地化学习的能力，在许多应用中达到秒级，毫秒甚至微妙级或更快。&lt;span&gt;[图1]&lt;/span&gt; 相比其它通用的学习技术（比如深度学习），在有些应用中超限学习机可以快几千几万倍。比如在有些手写体识别，3D 图形应用，各国交通路牌识别等应用中，超限学习机与深度学习相比可进一步提高准确率, 并且大幅度降低训练时间（相比较深度学习基于 GPU 的 1-2 天训练时间，超限学习机在普通计算机上的训练时间缩短到几分钟或更少）。在许多医疗大数据应用上，超限学习机也比传统的学习方法在提高准确率的情况下将学习速度大幅提高几千倍。&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFRPH15pqYmrxiaeOKq7F7mm5uMDfbkfU7zgwBfiboa0uH9Qo5ZRCoeq7A/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, "Representational Learning with Extreme Learning Machine for Big Data," IEEE Intelligent Systems, vol. 28, no. 6, pp. 31-34, 2013.&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Z. Huang, Y. Yu, J. Gu, and H. Liu, "An Efficient Method for Traffic Sign Recognition Based on Extreme Learning Machine," (in press) IEEE Transactions on Cybernetics, 2016&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Z. Xie, K. Xu, W. Shan, L. Liu, Y. Xiong, and H. Huang, "Projective Feature Learning for 3D Shapes with Multi-View Depth Images," The 23rd Pacific Conference on Computer Graphics and Applications, Tsinghua University, China, October 7-9, 2015.&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）统一的神经网络结构和算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 年前当神经网络发展处于第一次复兴的巅峰，普天下都在忙于为神经网络训练「调参」和苦于寻找办法如何使流行的神经网络学习算法跳出「局部最小点」时，我们的疑问是：1）当普天下的研究人员都乐于和疲于「调参」时，神经网络的发展本身是不是也陷入了局部最小点？2）不同类型的网络「真的需要不同类型的学习算法吗」？3）是否存在一种通用的学习框架来处理不同类型的网络（单层前馈网络和多层网络）？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;不同单隐层前馈神经网络的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;许多种单隐层前馈神经网络在广泛使用中，包括前馈网络、径向基函数（RBF）网络、支持向量机（SVM）、多项式网络、傅里叶变换和小波网络等。这些之前都被认为是不同而且没有联系的学习或计算技术。超限学习机理论认为这些都有一样的网络结构，只是网络的隐层用的是不同的神经元而已。并提出在考虑 Universal Approximation Capability（有人翻译成「万能逼近」能力）和分类能力的前提下，只要隐层神经元是非线性阶段连续的，人们就不需要为不同的前馈神经网络设计不同的学习算法。作为 ELM 的一个特例（傅立叶序列作为隐层神经元），后来 Intel 和美国加州大学伯克利分校研究团队提出的 Random Kitchen Sink（RKS）以及 Google 团队提出的 FastFood 也在近几年有许多发展和实际成功应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, "Extreme learning machine: a new learning scheme of feedforward neural networks," Proceedings of international joint conference on neural networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, L. Chen and C.-K. Siew, "Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes," IEEE Transactions on Neural Networks. vol. 17, no. 4, pp. 879-892, 2006.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang and L. Chen. "Convex Incremental Extreme Learning Machine," Neurocomputing, vol. 70, pp. 3056-3062, 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;A. Rahimi and B. Recht, "Random features for large-scale kernel machines," Proceedings of the 2007 neural information processing systems (NIPS2007), 3–6 Dec 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Q. Le, T. Sarlós T, and A. Smola, "Fastfood approximating kernel expansions in loglinear time," Proceedings of the 30th international conference on machine learning, Atlanta, USA, p. 16–21, June 2013.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;单隐层学习和多隐层学习的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们真的需要迭代式地调整多层前馈神经网络的隐层节点吗？前馈神经网络真的要像六十年来一直被认为是个黑箱吗？传统的误差反向传播（BP）算法和支持向量机（SVM）将多层网络视为黑箱。与此不同的是，超限学习机将多层网络视为白箱，并且一层一层地进行训练。总体看，超限学习机将单隐层前馈和多隐层网络看成一个类似的统一体，用雷同的方法来处理单隐层前馈和多隐层网络。然而，与深度神经网络需要密集地调整其隐层节点不同，超限学习理论显示，隐层节点很重要，但（单隐层神经网络和多层网络的）隐层节点可以和数据无关，可以随机产生或从上一代传给下一代而不需要调整。学习可以无需通过迭代式地调整隐层节点来实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFfPdqyicSgico4BtbZ0aMibKPACeI9Za442pI76CZX6wbNayxicFW7Fic5jg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;J. Tang, C. Deng, and G.-B. Huang, "Extreme Learning Machine for Multilayer Perceptron" , IEEE Transactions on Neural Networks and Learning Systems, May 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong, "Local Receptive Fields Based Extreme Learning Machine," IEEE Computational Intelligence Magazine, vol. 10, no. 2, pp. 18-29, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;L. L. C. Kasun, H. Zhou, G.-B. Huang, and C. M. Vong, "Representational Learning with Extreme Learning Machine for Big Data," IEEE Intelligence Systems, vol. 28, no. 6, pp. 31-34, 2013.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;单隐层学习和多隐层学习与层次性学习的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多隐层学习（Multi-Hidden Layer Learning）和层次性学习（Hierarchical Learning）的概念不是完全一样。多隐层学习强调的是一个目标应用（比如图像分类）由一个包含多个隐层节点的网络实现。而超限学习机的层次性学习强调的是每个隐层实现一个功能，各个功能单元通过级联，并联，串联等组合形成一个学习能力复合的机器学习系统。&lt;span&gt;[图3]&lt;/span&gt; 层次性学习的一个特例可以是一个多隐层学习方法。在超限学习机的体系下，各个功能块可以采用和应用相关的超限学习机算法。另外，在超限学习机中，一个隐层节点可以是一个由多个神经元组成的超级隐节点单元。&lt;span&gt;[图4] &amp;nbsp;&lt;/span&gt;这种层次性学习可以最终提供比较理想的 End-to-End Learning 和 One-Shot Learning。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL2ybaFdXI11miagzDPk2U2uZj2XRNGfMQSL9haj0xb7Yx99gZWXe6odA/0?wx_fmt=png"/&gt;&lt;br/&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicKq75cdUs4R02hSKFqcUUFKeDIicvSrrkYqbP4Wicp3Scg1QGpBVImicC7PZlKzJUeDKwKLuH3xSQSA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "What are Extreme Learning Machines? Filling the Gap between Frank Rosenblatt's Dream and John von Neumann's Puzzle," Cognitive Computation, vol. 7, pp. 263-278, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）基本学习单元的统一&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像加减乘除四大基本运算操作是数学体系的基础，物理体系也是建立在几大基本定律上一样，基于生命体的生物学习（Biological Learning）体系其实是建基于至少六大基本学习单元操作之上：压缩（Compression）、特征学习（Feature Learning）、稀疏编码（Sparse coding）、聚类（Clustering）、回归拟合（Regression）和分类（Classification）。&lt;span&gt;[图5] &amp;nbsp;&lt;/span&gt;这六大基本学习单元操作可以由同样的超限学习机实现，隐层节点与数据无关，要调整的是从隐层节点到输出层的连接。&lt;span&gt;[图4] &amp;nbsp;[图6]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如支持向量机（SVM），随机投影（Random Projection，RP）以及主成份分析（Principal Component Analysis, PCA）看似不太相关，却在超限学习机理论和算法下可以有机的统一。2012 年发表在 IEEE Transactions on Cybernetics 上的文章证明了支持向量机是超限学习机的次优解。刚刚发表在 IEEE Transactions on Image Processing 文章指出随机投影和主成份分析其实可以看作是超限学习机的隐层神经元用线性函数时的的一个特例。可是超限学习机也可以用非线性的隐层神经元，所以就可以进行升维，降维，特征学习等功能。所以从特征学习角度看随机投影和主成份分析也是提供次优解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLiaBYibMn9JN68q3w0TURnoibUia4m8dFJle3CqKiaFCbKGFCvKicznKWOqicA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLhw0mfk07oib6pzsb0dQx07QgM5UhQ4k3yZsNxBcPrnGLX7wNE22p5Gg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, H. Zhou, X.Ding, and R. Zhang, "Extreme Learning Machine for Regression and MulticlassClassification", IEEE Transactions on Systems, Man, and Cybernetics – Part B:Cybernetics, vol. 42, no. 2, pp. 513-529, 2012.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;L. L. C. Kasun, Y. Yang, G.-B. Huang, and Z. Zhang, Fellow, "Dimension Reduction With Extreme Learning Machine", IEEE Transactions on Neural Networks, vol. 25, no.8, pp. 3906-3918, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4）普适学习和普适智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着物联网的深入发展，在不远的未来，大部分的设备将拥有智能与学习能力。我们相信，就如包括人类在内的生物社会一样，这些智能设备也将发展出一个互相交流的「智能体社会」（Internet of Intelligent Things）&lt;span&gt;图7&lt;/span&gt;。每个智能体都嵌入有学习功能并且能相互交流。因而我们有必要提出普适学习（Pervasive Learning）和普适智能（Pervasive Intelligence）的概念和目标。由于超限学习机的学习速度比深度学习快上万倍，它可以帮助我们实现智能体社会。超限学习机芯片可以集成到硬件中，并实现实时本地在线学习，从而实现普适学习（Pervasive Learning）和普适智能（Pervasive Intelligence）。这几年，关于超限学习机芯片的研究得到一些实质进展，主要集中在三个方面：多核加速芯片（现场可编程门阵列（FPGA）和专用集成电路（ASIC）），神经形态芯片以及以光技术实现 ELM。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLtaN8Ro9qBHvY5a4COQ6xvfXZqvkC7Inw5ZhP6ib4Bp3wXxaGatoPxCw/0?wx_fmt=png"/&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;图 7&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "Extreme learning Machines: Enabling Pervasive Learning and Pervasive Intelligence", Pushing Frontiers, vol. 8, pp. 22-23, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5）填补不同学习理论间的空白&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 60 年来传统的学习理论不同，超限学习机理论的一个重要性质是其通用学习能力（压缩、特征学习、聚类、回归、分类等）无需通过调整隐层节点来获得，例如隐层节点可以从前辈继承或随机生成。进一步来说，超限学习机理论也为传统神经网络提供了理论支持（包括局部感受域（Local Receptive Field）和池化策略（Pooling）），而做为局部感受域的一个特殊实现方法的卷积神经操作和池化策略正是深度学习得以成功的主要原因之一。在 ELM 理论和应用下，不同随机分布的随机隐层神经元的产生形成全联结的网络或部分联结的网络&lt;span&gt;（图8）&lt;/span&gt;。或如 ELM 早期理论（2007 年）指出不同的部分联结也可以形成局部稠密边缘稀疏的局部感受域或不同局部感受域的非线性组合（池化策略）&lt;span&gt;（图 9）&lt;/span&gt;。根据 ELM 理论，卷积神经网络只是一种局部感受域和池化策略实现，除了卷积神经操作，还有许多其它的局部感受域存在，如何实现还有待进一步研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtL5ibS4gv0TShMEDcS75k3ib2qE0gvAGnmjuL47gug98LgSZmbwatIq4icg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;图 8&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXnZU4lCsBbooUW9jWSwtLF92ZMuzgBehBefpgz0LTuMCyACT5dTiaQdArcialOTRuN44ia6dJVibZ0w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 9&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;岭回归（Ridge Regression Theory）、线性系统的稳定性、矩阵稳定性、Bartlett 神经网络泛化能力理论（Neural Network Generalization Performance Theory）、支持向量机最大边界理论（Maximal Margin Theory）等在超限学习机以前被认为是不同的理论。特别是 Bartlett 神经网络泛化能力理论在以前很少用于训练神经网络。超限学习机采用了 Bartlett 理论，从而保证其泛化能力。超限学习机的理论显示，这些之前的理论从机器学习角度看是有机一致的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang and L. Chen, "Convex Incremental Extreme Learning Machine," Neurocomputing, vol. 70, pp. 3056-3062, 2007.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, "An Insight into Extreme Learning Machine: Random Neurons, Random Features and Kernels", Cognitive Computation, vol. 6, pp. 376-390, 2014.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong, "Local Receptive Fields Based Extreme Learning Machine", IEEE Computational Intelligence Magazine, vol. 10, no. 2, pp. 18-29, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 作为生物学习的一个「基本粒子」级学习单元的超限学习机&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）生物学习机制的验证&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超限学习机理论显示，隐层节点很重要，但在很多应用中不需要调整（比如压缩感知、特征学习、聚类、回归和分类）。在理论上，这种神经元的激活函数几乎可以是任何非线性分段连续的，包括上百种人类无法知道其准确数学模型的人脑中的神经元。在超限学习机理论和技术提出之后的大概 10 年左右，越来越多的有关生物脑学习系统的研究成果直接或间接的支持了超限学习机理论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="width: 528.188px; line-height: 25.6px; white-space: normal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2013 年及之后发表在《自然》等期刊上文章报告了来自美国斯坦福大学，哈佛医学院，麻省理工学院和哥伦比亚大学等大学的研究人员发现在老鼠的嗅觉系统中神经元在学习过程中是随机产生的。这可能是超限学习机理论首次在生物系统中得到验证。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2015 年美国哥伦比亚大学和 IBM Watson 的研究人员进一步阐述生物学习系统中神经元的随机产生可以进一步帮助生物学习系统实现对特征学习（升维，降维等），并且明确指出这在工程实现比如超限学习机是被证明有效的。这些在生物脑中发现的神经元机制和超限学习机理论预测是一致的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2015 年美国乔治亚理工学院和华盛顿大学的一批研究人员通过人的行为学分析简直验证人脑中随机神经元机制可以帮助人拥有小样本学习能力。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;2016 年发表在《自然•神经科学》上的文章说明了超限学习机理论进一步在猴子的脑中得到了直接验证。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: normal; text-align: justify;"&gt;&lt;span&gt;M. Rigotti, O. Barak, M. R. Warden, X.-J. Wang, N. D. Daw, E. X. Miller, S. Fusi, "The importance of mixed selectivity in complex cognitive tasks," Nature, vol.497, pp. 585-590, 2013&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;O. Barak, M. Rigotti, S. Fusi, "The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off," Journal of Neuroscience, vol. 33, no. 9, pp. 3844-3856, 2013&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;S. Fusi, E. K Miller, and M. Rigotti, "Why neurons mix: high dimensionality for higher cognition," Current Opinion in Neurobiology, vol. 37, pp. 66-74, 2015&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;R. I. Arriaga, et al.Visual Categorization with Random Projection, Neural Computation, vol. 27, 2015&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;J. Xie and C. Padoa-Schioppa, "Neuronal remapping and circuit persistence in economic decisions," Nature Neuroscience, vol. 19, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;E. L Rich and J. D Wallis, "What stays the same in orbitofrontal cortex," Nature Neuroscience, vol. 19, no. 6, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）解答约翰&lt;span&gt;·&lt;/span&gt;冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的困惑&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在罗森布拉特的梦想中，他的神经网络感知器可以最终帮助实现电子计算机走路、说话、看东西、写作、繁衍自己并有自我意识，而作为计算机之父的冯&lt;span&gt;·&lt;/span&gt;诺依曼却不解为什么一个看似不完美生物神经网络系统却有完美的学习能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;超限学习机理论的目标之一是打破机器学习和生物学习之间的壁垒。尽管动物的大脑在总体上来说是结构化及有序的，在其某些层或区域，其局部结构可看成「无序」的。从超限学习理论的角度看，网络的整个多层结构（人工神经网络或生物网络）是结构化且有序的，但它们在某一个神经元层或神经模块片中看起来「混乱、非组织结构化」。从局部来看，「硬连线」可以是全连接或部分连接。这种全局结构化而局部随机连接的看似「不完美」结构，却正好构成了基本的完美的学习能力，包括压缩感知、特征学习、稀疏编码、聚类、回归和分类等。这就解决了冯&lt;span&gt;·&lt;/span&gt;诺依曼对生物学习的谜惑。生物学习机制极其复杂，而我们相信「无需调节隐层节点的学习」是很多学习模块中的一种基本生物学习机制。虽然人脑中也许有几百种不同种类的生物神经元，他们的数学模型也不为人类所知，但是超限学习机理论指出一个基本的生物学习机制也许是生物神经元本身在学习中是不需要调整的，和应用是无关的。进一步说，随机隐层神经元节点和「随机连线」只是两种特定的实现「无需调节隐层节点的学习」的方法。IBM 团队最近也宣布他们研制出类生物神经元，他们实现的理论基础正是基于 ELM 理论最早所提出，倡导和支持的：生物神经元应该是随机的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote style="line-height: 25.6px; white-space: normal;"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6;"&gt;&lt;span&gt;G.-B. Huang, What are Extreme Learning Machines? Filling the Gap between Frank Rosenblatt's Dream and John von Neumann's Puzzle, Cognitive Computation, vol. 7, pp. 263-278, 2015.&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;T. Tuma, A. Pantazi, M. L. Gallo, A. Sebastian, and E. Eleftheriou, "Stochastic phase-change neurons," &amp;nbsp;Nature Nanotechnology, vol. 11, August 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）展望&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们相信超限学习机理论和技术提供了一个架接跨越机器学习和生物学习基本「粒子」级的学习机制。也填补了罗森布拉特的梦想和冯&lt;span&gt;·&lt;/span&gt;诺依曼的困惑之间有着很大的空白地带和理论技术鸿沟。这也是实现普适学习和普适智能的必要条件。然而这些还很初步，套用个别神经网络界前辈对超限学习机的评论和期望：「好戏还没有开始」，也许更多的令人激动和感兴趣的东西还等着大家研究开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有兴趣的研究人员，可以申请新加坡南洋理工大学黄广斌教授研发团队在下列研究方向的博士生、博士后和访问学者位置：海上自主导航数据分析、智能芯片设计、多模数据分析、视频分析、目标识别和跟踪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;©本文由机器之心发布，转载请联系本公众号和作者获得授权，点击「阅读原文」关注黄广斌教授微博。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 04 Sep 2016 19:40:49 +0800</pubDate>
    </item>
  </channel>
</rss>
