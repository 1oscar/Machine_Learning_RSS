<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | 在语音识别这件事上，汉语比英语早一年超越人类水平（附论文）</title>
      <link>http://www.iwgc.cn/link/3204725</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：吴攀、李亚洲&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;几天前，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;微软语音识别实现了历史性突破&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;，英语的语音转录达到专业速录员水平，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心也独家专访了专访微软首席语音科学家黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt; ，了解到词错率仅 5.9% 背后的「秘密武器」——CNTK。但微软的成果是在英语水平上的，从部分读者留言中我们了解到对汉语语音识别的前沿成果不太了解，这篇文章将向大家介绍国内几家公司在汉语识别上取得的成果（文中提到的论文可点击阅读原文下载）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;10 月 19 日，微软的这条消息发布之后在业内引起了极大的关注。语音识别一直是国内外许多科技公司发展的重要技术之一，微软的此次突破是识别能力在英语水平上第一次超越人类。在消息公开之后，百度首席科学家吴恩达就发推恭贺微软在英语语音识别上的突破，同时也让我们回忆起一年前百度在汉语语音识别上的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cviag8vMQYUdriajb2nmBmvwSBT0tbKoDFMSGlutZ3JdicAPn38IQvCRiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;吴恩达：在 2015 年我们就超越了人类水平的汉语识别；很高兴看到微软在不到一年之后让英语也达到了这一步。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;百度 Deep Speech2，汉语语音识别媲美人类&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 12 月，百度研究院硅谷人工智能实验室（SVAIL）在 arXiv 上发表了一篇论文《Deep Speech 2: End-to-End Speech Recognition in English and Mandarin（Deep Speech 2：端到端的英语和汉语语音识别）》，介绍了百度在语音识别技术的研究成果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9clIVppxH7FBeCf98MwHicHyqadAuqKtGZM8AQcCe00XpXEwcYQBYXN3Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们的研究表明一种端到端的深度学习（end-to-end deep learning）方法既可以被用于识别英语语音，也可以被用于识别汉语语音——这是两种差异极大的语言。因为用神经网络完全替代了人工设计组件的流程，端到端学习让我们可以处理包含噪杂环境、口音和不同语言的许多不同的语音。我们的方法的关键是 HPC（高性能计算）技术的应用，这让我们的系统的速度超过了我们之前系统的 7 倍。因为实现了这样的效率，之前需要耗时几周的实验现在几天就能完成。这让我们可以更快速地迭代以确定更先进的架构和算法。&lt;strong&gt;这让我们的系统在多种情况下可以在标准数据集基准上达到能与人类转录员媲美的水平。&lt;/strong&gt;最后，通过在数据中心的 GPU 上使用一种叫做的 Batch Dispatch 的技术，我们表明我们的系统可以并不昂贵地部署在网络上，并且能在为用户提供大规模服务时实现较低的延迟。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文中提到的 Deep Speech 系统是百度 2014 年宣布的、起初用来改进噪声环境中英语语音识别准确率的系统。在当时发布的博客文章中，百度表示在 2015 年 SVAIL 在改进 Deep Speech 在英语上的表现的同时，也正训练它来转录汉语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当时，百度首席科学家吴恩达说：「SVAIL 已经证明我们的端到端深度学习方法可被用来识别相当不同的语言。我们方法的关键是对高性能计算技术的使用，相比于去年速度提升了 7 倍。因为这种效率，先前花费两周的实验如今几天内就能完成。这使得我们能够更快地迭代。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=f0339hys92j&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别技术已经发展了十多年的时间，这一领域的传统强者一直是谷歌、亚马逊、苹果和微软这些美国科技巨头——据 TechCrunch 统计，美国至少有 26 家公司在开发语音识别技术。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是尽管谷歌这些巨头在语音识别技术上的技术积累和先发优势让后来者似乎难望其项背，但因为一些政策和市场方面的原因，这些巨头的语音识别主要偏向于英语，这给百度在汉语领域实现突出表现提供了机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为中国最大的搜索引擎公司，百度收集了大量汉语（尤其是普通话）的音频数据，这给其 Deep Speech 2 技术成果提供了基本的数据优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过有意思的是，百度的 Deep Speech 2 技术主要是在硅谷的人工智能实验室开发的，其研究科学家（名字可见于论文）大多对汉语并不了解或说得并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这显然并不是问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Deep Speech 2 在汉语上表现非常不错，但其最初实际上并不是为理解汉语训练的。百度美国的人工智能实验室负责人 Adam Coates 说：「我们在英语中开发的这个系统，但因为它是完全深度学习的，基本上是基于数据的，所以我们可以很快地用普通话替代这些数据，从而训练出一个非常强大的普通话引擎。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c0icKPaM9FU1ic1UjIzLkkeuQr75iaAzSd9t52W5wb7MNa3SOGvGaRa82Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于英语和普通话的 Deep Speech 2 系统架构，它们之间唯一的不同是：普通话版本的输出层更大（有 6000 多个汉语字符），而英语版本的只有 29 个字符。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该系统能够识别「混合语音（hybrid speech）」——很多普通话说话人会组合性地使用英语和普通话。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Deep Speech 2 于 2015 年 12 月首次发布时，首席科学家吴恩达表示其识别的精度已经超越了 Google Speech API、wit.ai、微软的 Bing Speech 和苹果的 Dictation 至少 10 个百分点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据百度表示，到今年 2 月份时，Deep Speech 2 的短语识别的词错率已经降到了 3.7%（参考阅读：&lt;span&gt;http://svail.github.io/mandarin/&lt;/span&gt;）！Coates 说 Deep Speech 2 转录某些语音的能力「基本上是超人级的」，能够比普通话母语者更精确地转录较短的查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度在其技术发展上大步迈进，Deep Speech 2 目前已经发展成了什么样还很难说。但一项技术终究要变成产品和服务才能实现价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;科大讯飞的语音识别&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度的 Deep Speech 识别技术是很惊人，但就像前文所说一项技术终究要变成产品和服务才能实现价值，科大讯飞无疑在这方面是做得最好的公司之一。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科大讯飞在自然语言处理上的成就是有目共睹的，在语音识别上的能力从最初到现在也在不断迭代中。2015 年 9 月底，机器之心对&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect"&gt;胡郁的一次专访中&lt;/a&gt;，他就对科大讯飞语音识别技术的发展路线做过清晰的介绍：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞很好地跟随了语音识别的发展历史，深度神经网络由 Geoffrey Hinton 与微软的邓力研究员最先开始做，科大讯飞迅速跟进，成为国内第一个在商用系统里使用深度神经网络的公司。谷歌是最早在全球范围内大规模使用深度神经网络的公司，谷歌的 Voice Search 也在最早开创了用互联网思维做语音识别。在这方面，科大讯飞受到了谷歌的启发，在国内最早把涟漪效应用在了语音识别上面，因此超越了其他平台。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞最初使用隐马尔可夫模型，后面开始在互联网上做，2009 年准备发布一个网页 demo，同年 9 月份安卓发布之后开始转型移动互联网，并于 2010 年 5 月发布了一个可以使用的手机上的 demo；2010 年 10 月份发布了语音输入法和语音云。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;整个过程中最难的地方在于，当你不知道这件事情是否可行时，你能够证明它可行。美国那些公司就是在做这样的事情。而科大讯飞最先领悟到，并最先在国内做的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到今年 10 月份刚好过去一年，科大讯飞的语音识别技术在此期间依然推陈出新，不断进步。去年 12 月 21 日，在北京国家会议中心召开的以「AI 复始，万物更新」为主题的年度发布会上，科大讯飞提出了以前馈型序列记忆网络（FSMN, Feed-forward Sequential Memory Network）为代表的新一代语音识别系统。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cXGicdTiciazC0I6f8l77vqeEoGMRrULEcqV6dyaDwescVk19W9wj4PGDQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在此论文中，我们提出了一种新的神经网络架构，也就是前馈型序列记忆网络（FSMN），在不使用循环前馈的情况下建模时间序列中的 long-term dependency。此次提出的 FSMN 是一个标准的全连接前馈神经网络，在其隐层中配备了一些可学习的记忆块。该记忆块使用一个抽头延时线结构将长语境信息编码进固定大小的表征作为短期记忆机制。我们在数个标准的基准任务上评估了 FSMN，包括语音识别和语言建模。实验结果表明，FSMN 在建模语音或语言这样的序列信号上，极大的超越了卷积循环神经网络，包括 LSTM。此外，由于内在无循环模型架构，FSMN 能更可靠、更快速地学习。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来通过进一步的研究，在 FSMN 的基础之上，科大讯飞再次推出全新的语音识别框架，将语音识别问题重新定义为「看语谱图」的问题，并通过引入图像识别中主流的深度卷积神经网络（CNN, Convolutional Neural Network）实现了对语谱图的全新解析，同时打破了传统深度语音识别系统对 DNN 和 RNN 等网络结构的依赖，最终将识别准确度提高到了新的高度。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来，科大讯飞又推出了全新的深度全序列卷积神经网络（Deep Fully Convolutional Neural Network, DFCNN）语音识别框架，使用大量的卷积层直接对整句语音信号进行建模，更好的表达了语音的长时相关性，比学术界和工业界最好的双向 RNN 语音识别系统识别率提升了 15% 以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBVRabcLJkjdEGhbwAo4ITz6PAOAEYhlPpV44E9gY5mKna244FKmicFA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;DFCNN 的结构图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DFCNN 的结构如图所 示，DFCNN 直接将一句语音转化成一张图像作为输入，即先对每帧语音进行傅里叶变换，再将时间和频率作为图像的两个维度，然后通过非常多的卷积层和池化（pooling）层的组合，对整句语音进行建模，输出单元直接与最终的识别结果（比如音节或者汉字）相对应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;搜狗语音识别&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;纵观整个互联网行业，可以说搜狗作为一家技术型公司，在人工智能领域一直依靠实践来获取更多的经验，从而提升产品使用体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在前几天的锤子手机新品发布会上罗永浩现场演示了科大讯飞的语音输入之后，一些媒体也对科大讯飞和搜狗的输入法的语音输入功能进行了对比，发现两者在语音识别上都有很不错的表现。比如《齐鲁晚报》的对比结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;值得一提的是，得益于创新技术，搜狗还拥有强大的离线语音识别引擎，在没有网络支持的情况下依旧可以做到中文语音识别，以日常语速说话，语音识别仍然能够保持较高的准确率。这一点科大讯飞表现也较为优秀，两者可谓旗鼓相当。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;整体体验下来，搜狗在普通话和英文的语音输入方面表现，与讯飞相比可以说毫不逊色，精准地识别能力基本可以保证使用者无需进行太多修改。此前在搜狗的知音引擎发布会上，搜狗语音交互技术项目负责人王砚峰称「搜狗知音引擎具备包括端到端的语音识别、强大的智能纠错能力、知识整合使用能力以及多轮对话和复杂语义理解能力」，这些都有效保证了搜狗语音输入在识别速度、精准度、自动纠错、结合上下文语意理解纠错方面收获不错的表现。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;八月份，搜狗发布了语音交互引擎——知音，其不仅带来了语音识别准确率和速度的大幅提升，还可以与用户更加自然的交互，支持多轮对话，处理更复杂的用户交互逻辑，等等。知音平台体现出搜狗在人工智能技术领域的长期积累，同时也能从中看出他们的技术基因和产品思维的良好结合。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cfJmmVV0ZbP4CBWibicb3SlSQjNya8q0BtjlBiauIQNo7tvoWpl10YcFkA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;搜狗知音引擎&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;搜狗把语音识别、语义理解、和知识图谱等技术梳理成「知音交互引擎」，这主要是强调两件事情，一是从语音的角度上让机器听的更加准确，这主要是识别率的提升；另一方面是让机器更自然的听懂，这包括在语义和知识图谱方面的发展，其中包括自然语言理解、多轮对话等技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cV8PxINicWJXJ9FEawYWavia1CNzde2ibyot94tV8ibSibIw0tUoFNA2gdvw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;语音识别系统流程：语音信号经过前端信号处理、端点检测等处理后，逐帧提取语音特征，传统的特征类型包括 MFCC、PLP、FBANK 等特征，提取好的特征送至解码器，在声学模型、语言模型以及发音词典的共同指导下，找到最为匹配的词序列作为识别结果输出。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKZW6DiaAocs5146zTGq6G0RBwNr4SuZgnYUOPJTIibRuiauDAaZUYLOSA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;CNN 语音识别系统建模流程&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据搜狗上个月的一篇微信公众号文章写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;在语音及图像识别、自然语言理解等方面，基于多年在深度学习方面的研究，以及搜狗输入法积累的海量数据优势，搜狗语音识别准确率已超 97%，位居第一。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过遗憾的是，搜狗还尚未公布实现这一结果的相关参数的技术信息，所以我们还不清楚这样的结果是否是在一定的限定条件下实现的。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像TechCrunch 统计的美国有 26 家公司开发语音识别技术一样，中国同样有一批专注自然语言处理技术的公司，其中云知声、思必驰等创业公司都在业内受到了极大的关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLnHUIeJZIyticlic6SpxIt7b3dIdAldSU3w3tCgj84DXeHOSzbC6tbEQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上图展示了云知声端到端的语音识别技术。材料显示，云知声语音识别纯中文的 WER 相对下降了 20%，中英混合的 WER 相对下降了 30%。&lt;br/&gt;&lt;br/&gt;在今年 6 月机器之心对云知声 CEO 黄伟（参见：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect"&gt;专访云知声CEO黄伟：如何打造人工智能「云端芯」生态闭环&lt;/a&gt;）的专访中，黄伟就说过 2012 年年底，他们的深度学习系统将当时的识别准确率从 85% 提升到了 91% 。后来随着云知声不断增加训练数据，如今识别准确率已经能达到 97% ，属于业内一流水平，在噪音和口音等情况下性能也比以前更好。&lt;br/&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect"&gt;思必驰的联合创始人兼首席科学家俞凯&lt;/a&gt;是剑桥大学语音博士，上海交大教授。他在剑桥大学待了 10 年，做了 5 年的语音识别方面的研究，后来做对话系统的研究。整体上，思必驰做的是语音对话交互技术的整体解决方案，而不是单纯的语音识别解决方案。因此在场景应用中，思必驰的系统和科大讯飞的系统多有比较，可相互媲美。&lt;br/&gt;&lt;br/&gt;当然，此领域内还有其他公司的存在。这些公司都在努力加速语音识别技术的提升。语音识别领域依然有一系列的难题需要攻克，就像微软首席语音科学家黄学东接受机器之心专访时所说的那样，「理解语义是人工智能下一个需要攻克的难题，要做好语音识别需要更好的语义理解，这是相辅相成的。」&lt;/span&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</title>
      <link>http://www.iwgc.cn/link/3204726</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part2，全面介绍了词嵌入模型， Part1请点击&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" style="font-size: 12px; text-decoration: underline;"&gt;&lt;em&gt;&lt;span&gt;技术 | 词嵌入系列博客Part1：基于语言建模的词嵌入模型&lt;/span&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csHIKEblezwKfQKC9eNPNmd6x1ONG6bMsRjOsJHz6XDqTKEtFx4ev8A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于Softmax的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层次Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微分Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNN-Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于采样（Sampling）的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有适应的重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目标采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;噪音对比估计&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;低频的标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其他方法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;选择哪一种方法？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文是我们词嵌入及其表示系列的第二篇。在上一篇博文里，我们提供了词嵌入模型的概述，并介绍了 Bengio 等人在2003年提出的经典神经语言学习模型、Collobert 和 CWeston 在2008年提出的 C&amp;amp;W 模型，以及Mikolov 在2013年提出的 word2vec 模型。我们发现，设计更好的词嵌入模型的最大挑战，就是如何降低softmax 层的计算复杂度。而且，这也是机器翻译（MT）（Jean等人[10 ]）和语言建模（Jozefowicz等人[6 ]）的共通之处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇博文里，我们将要重点介绍过去几年的研究中 softmax 层的不同近似方法，它们其中的一些被运用在语言建模和机器学习。在下一篇博文里，我们才会介绍别的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了统一以及便于比较，让我们简单重新介绍一下上一篇博文的重点：我们假设训练集是一串包括 T 个训练词的字符序列 w1,w2,w3,⋯,wT ，每一个词来源于大小为 |V| 的词汇库 V。模型大体上考虑n个词的上下文 c。我们将每一个词和一个 d 维度的输入向量（也就是表示层的词嵌入）vw 以及输出向量 v′w（在softmax层的权重矩阵的对于词的表示）联系在一起。最终，我们相对于我们的模型参数 θ 来优化目标函数 Jθ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们一起来回顾一下，softmax 层对于一个词 w 在上下文 c 出现的概率的计算公式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cSM7Vh72kwdkmn1icCfS58XRTmCwEZLEogeNaUybOAPU1Riam40mKjfBQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，h 是倒数第二层的输出向量。注意到和之前提到的一样，我们用c来表示上下文，并且为了简洁，舍弃目标词 wt 的索引 t。计算softmax的复杂度很高，因为计算 h 和 V 里每个词 wi 的输出向量的内积需要通过一部分的分母的求和来获得，从而得到标准化后目标词 w 在上下文 c 的概率。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来我们将会讨论近似 softmax 所采用的不同方法。这些方法可以分成两类：一类是基于 softmax 的方法，另一类则是基于采样的方法。基于 softmax 的方法保证 softmax 层的完好无损，但是修改了它的结构来提升它的效率。基于采样的方法则是完全去掉 softmax 层，优化其它目标函数来近似 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于 softmax 的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层次的 softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这让我们把对一个词出现概率的计算分解成一连串的概率计算，我们将无需对所有词作昂贵的标准化。用 H-Softmax 来替代单一的 softmax 层可以把预测词的任务带来至少50倍的速度提升，因此适用于要求低延迟的任务，比如 Google 的新通讯软件 Allo 的实时沟通功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cTFc71iaicfxRXvC23pN5ewrIYkZRSgAmFvUbaAicGKibLyOt7tpHFDnl1w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：多层词的 softmax&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把常规的 softmax 想成是只有一层深度的树，每个 V 中的词都是一个叶子节点。计算一个词的 softmax的概率则需要标准化所有 |V| 个叶子的概率。反之，如果我们把 softmax 当成一个每个词都是叶子的二叉树，我们只需要从叶子节点开始沿着树的路径走到那个词，而无序考虑其它的词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为一个平衡的二叉树的深度为 log2(|V|)，我们只需要通过计算最多 log2(|V|) 个节点来取得一个词最终的概率。注意到这个概率都已经经过了标准化，因为二叉树中所有叶子节点的概率之和为1，所以形成了一个概率分布。想要粗略地验证它，我们可以推理一个树的根节点（图1中的节点0），它的所有分支必须相加为1。对于每个接下来的节点，概率质量分解给它的分支，直到最终到达叶子节点，也就是词。因为这其中没有损失概率，而且所有词都是叶子，所有词的概率的总和必须为1，所以分层次的 softmax 定义了在 V 上所有词的一个标准化的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体而言，当我们遍历树的时候，我们需要计算每个分支点的左右两个分支来计算这个节点的概率。正因如此，我们要为每个节点指定一个表示。对比于规律的 softmax，对于每个词，我们因此不需要 v'w 的输出词嵌入——反之，我们用给每个节点 n 都指定词嵌入 v′n。因为我们有 |V|−1 个节点，而每一个都拥有一个唯一的表示，H-Softmax 参数都和普通的 softmax 几乎一样。我们现在可以计算给定上下文 c 一个节点 n 的右分支（或左分支）的概率了，方式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAicibJK1LvFZ28em66ZtbEteGvlvOnWicJGceVWRRHWiaKRaiaiaCpTiapNpA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个方法和普通的 softmax 的计算方式几乎一致。现在，我们不计算h和词嵌入 v′w 的点乘积，而是计算每个树节点的h和词嵌入 v′w。另外，我们不计算整个词汇库里所有词的概率分布，我们仅仅输出一个概率，在这个例子中这个概率是 sigmoid 函数的节点 n 的右分支的概率。相反地，左分支的概率是 1−p(right|n,c)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9coiaLqKEFJs9fo63g3FTiaQQ4v5cdAojvAwHKmXSEhbGubm5w2rqlkvBA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：多层次的 softmax计算&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由此易得一个词w在上下文c中的为左右分支的概率之积。举个例子，在上下文「the」、&lt;span&gt;「dog」&lt;/span&gt;、&lt;span&gt;「and」&lt;/span&gt;、&lt;span&gt;「the」&lt;/span&gt;之中，在图 2 中词「cat」的概率可以通过计算从节点1向左，经过节点2转右，再在节点后转右所得的概率来计算。Hugo Lachorelle 在他的课程视频 (https://www.youtube.com/watch?v=B95LTf2rVWM)中提供了一个更详细的介绍。Rong[7] 也很好地解释了这些概念，并且推导了 H-Softmax 的导数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，树的结构十分重要。直观上来说，如果我们让模型在每个节点都来学习二元分类，比如我们可以让相似的路径获得相似的概率，我们的模型应该可以获得更好的表现。基于这一点，Morin 和 Bengio 给树提供 WordNet中的 synsets 的聚类。然而，他们的模型表现却不如常规的softmax。Mnih 和 Hinton [8] 用一个聚类算法来训练树结构来低轨地把词分成两堆，并且他们的模型在一部分的计算中表现和常规的 softmax 相当。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是，只有在我们提前知道想要预测的那个词（以及它的路径）时，我们才能够加速训练。在测试阶段，当我们需要预测最可能出现的词时，尽管缩小了范围，我们仍然需要计算所有词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，我们不需要用&lt;span&gt;「左」和&lt;span&gt;「右」&lt;/span&gt;&lt;/span&gt;来指定子节点，我们可以用一个对应路径的位向量来索引节点。在图 2，如果我们假设用比特 0 来表示向左和比特 1 来表示向右，我们可以用0-1-1来表示一条左－右－右的路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们回顾一下，一个平衡二叉树的路径长度是 log2|V|。如果我们设置 |V|=10000，这就相当于一条大约长度为13.3 的路径长度。相似的，我们可以用平均长度为13.3的路径的位向量来表示每一个词。在信息论中，这指代一串信息长度为 13.3 比特的字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;字的信息内容小结&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先回忆，一个词 w 的信息量（信息熵）I(w)是它的概率的负对数 I(w)=−log2p(w)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在数据集中所有词的熵H就是在一个单词库的所有词的信息熵的期望：H=∑i∈Vp(wi)I(wi)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也可以把一个信息源的信息熵想成是用来编码这部分信息所用的比特数。对于抛掷一枚公平的硬币，每次需要1比特；而对于一个总是输出相同符号的信息源，我们只需要0比特。对于一个平衡二叉树，我们平等对待每一个词，每个词 w 的熵 H 将拥有同样的信息量 I(w)，因为每个词都有同样的概率。在一个 |V|=10000 大小的平衡二叉树中平均的词信息熵 H 就恰好是它的平均路径长度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKR0xvouGYoYt3RoRFNPt3CQyfWOqibfR0ibIDoRSpPeuwZDDibg4NUkmA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们之前讲过，树的结构十分重要。值得注意的是，利用树的结构不但可以获得更好的表现，更可以加速运算：如果我们将更多信息加载进树中，那么更少信息的词将不意味着更短的路径，因为有些词出现频率更高，就可以用更少的信息去编码它。一个 |V|=10,000 长度的信息库的词信息熵大约为 9.16。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，考虑到出现频率，我们可以减少一个词汇库中每个词中的平均比特数。在这个例子中，我们从 13.3 减少到 9.16，相当于加速了 31%。Mikolov 等人 [1]把哈夫曼树运用在多层次 softmax，把更少的比特赋给更常出现的词。比如，「the」这个最常见的英语单词，在树中只需要最少比特数的编码，而第二常见的单词将会被指定第二少比特数的编码，以此类推。虽然我们仍然需要用相同数量的词去编码一个数据集，然而有更高频率出现短的编码，所以平均而言，去给每个词编码只需用更少的比特数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个类似于哈夫曼编码的编码又被称为信息熵编码，因为每个编码词的长度大约是和我们观察到的每个符号的熵成正比。Shannon [5] 在他的实验中建立了英语的信息率的下界，每个字母大约是 0.6 到 1.3 比特；根据每个词的长度为 4.5，这相当于每个单词为2.7到5.85比特。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这个和语言建模（我们在上一篇博文中讲到的那样）联系起来：语言模型的评价标准的困惑度应该是 2H，其中H就是信息熵。一个一元的熵是 9.16，因此它有一个非常高的困惑度 2^9.16=572.0。我们可以将这个值更加具体化，观察一个困惑度为572的模型就像从一个信息源中选择单词，而每个单词有572种选择，每种选择概率相等，且互相独立。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这么说吧：Jozefowicz 等人在 2016 年开发的这个最新的语言模型，在 1B Word Benchmark 中，每个词拥有 24.2 的困惑度。这个模型因此需要大约4.60比特来编码一个词，因为 2^4.60=24.2，非常接近于 Shannon 描述的实验下界。我们能否，以及如何使用这个模型去建立一个更好的多层次 softmax 层？这些问题仍留待我们去探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微分 Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Chen等人[9]介绍了一个经典 softmax 层的变形—— 微分Softmax（Differentiated Softmax，英文缩写为D-Softmax）。D-Softmax 基于不是所有的单词都需要同样多的参数：许多高频词可以拟合许多参数，而非常低频词则只能拟合很少的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了做到这一点，他们不用常规 softmax 层的 d×|V| 大小的包含输出词嵌入 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cCbHjibecAfibMSdDsCe46w79x89D4aibEYKGowQnSBJHExd1Gnsk7ialwA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的稠密矩阵，而用一个稀疏矩阵。然后他们把 v′w 排列成块，根据他们的频率排列，而每块的词嵌入的维度都是 dk。块的数量和他们的向量大小均为可以优化的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdExer2zIaU8T1CH179unBARXsWPnKoV9bxWPMrRgZhResAia55v2CaQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图3：微分 softmax（Chen等人(2015)）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图3中，分区A的词嵌入的维度是 dA（这些是高频词的词嵌入，因为它们被赋予更多的参数），而分区 B 和 C 的词嵌入分别有 dB 和 dC 维度。注意到所有不属于任何分区的区域，也就是图1中的这些没有阴影的区域，都设为 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前的隐藏层 h 都被当成是把每个对应维度的分区的特征串在一起。在图3中的h由大小分别为 dA、dB 和 dB 的分区组成。D-Softmax 不计算矩阵－向量的乘积，而是计算每个分区的乘积和它们在h的分区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为许多词只需要相对来说较少的参数，计算 softmax 的复杂度降低了。对比 H-Softmax，这个加速在测试阶段仍然存在。Chen 等人（2015）发现 D-Softmax 是在测试阶段最快的方法，而且是最准确的模型之一。然而，因为它给低频词赋予了更少的参数，D-Softmax 对于低频词的建模效果并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CNN-Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个对经典 softmax 层的改进受到了 Kim 等人最近的对于通过一个字母层次（character-level）的 CNN 的输入词嵌入 vw 的研究启发。Jozefowicz 等人（2016）建议对输出词嵌入做相同的事情，即通过一个字母层次的CNN。注意到如果我们像在图4中的在输入和输出有一个 CNN，生成输出词嵌入 v′w 的 CNN 必须和生成输入词嵌入 vw 的 CNN 不一样，就像是输入词嵌入和输出词嵌入必须不一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csXJPjEiaclKa88v8e40TNrbw6sibibcMnkPwpmAQmWE3c9GNk054uzm9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图4: CNN-Softmax（Jozefowicz 等人 ，2016年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然这个仍然需要计算常规 softmax 的标准化，这个方法很大程度上减少了模型参数的数量：我们现在不存储d×|V|的词嵌入矩阵，而仅仅是追踪 CNN 的参数。在测试阶段，输出向量 v′w 可以提前计算，所以模型的表现不会受损。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，因为字母都在练习空间中表现，且因为得到的矩阵都倾向于学习一个把字母映射到词的平滑函数，基于字母的模型常常会难以区分拼写相似而意思迥异的词。为了避免这个问题，研究者们加上一个通过每个词学习的连接系数，就能很大程度地减少常规的 softmax 和 CNN-softmax 的表现差异。调整修正项（correction term）的维度，研究者就可以取得模型大小和表现好坏的平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者也强调不需要用 CNN-softmax，而是把前一层h的输出传入一个字母层次的 LSTM，它输出词语的方式可以是一次输出一个字母。因此，每一个时间步中，softmax 输出的并不是词，而是字母的概率分布。然而，他们不能得到和这一层相当的表现效果。Ling 等人[14] 为机器翻译采用一个相似的层，得到了具有竞争力的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于采样的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上文中讨论的这些方法仍然保持着 softmax 大体的结构，而基于采样的方法则完全和 softmax 层无关。它们的方式是近似其它的损失函数的 softmax 的分母的正则化来。然而，基于采样的方法只是在训练时有效——在推断（inference）中，完全的 softmax 仍然需要计算来获得一个正则化后的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了直观上看 softmax 分母在损失函数的影响，我们将会推导我们相对于我们模型 θ 的损失函数 Jθ 的梯度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在训练过程中，我们希望给训练集中的每个词 w 减少模型的交叉熵损失函数（cross-entropy loss）。这就是我们的 softmax 的输出的负对数。如果您不确定 softmax 和 cross-entropy 的关系，请看看Karpathy的解释 (http://cs231n.github.io/linear-classify/#softmax-classifier)。我们的损失函数如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c2s5OvdiblDD98icF2pVmSM6wdkrAlq1RBdFnlPGLyIjuNp9z2IRJ54icA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意在实际操作中，Jθ 就是在整个数据集的所有负对数概率的平均值。为了获得这个推导，我们可以把 Jθ 分解成 ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cc28a98FibJJaqM9Z5ZcK3HEMNtRtn6bNXpnicKy1OcJibMI3iaLuQrHJyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cX5fBZSjYZiaosEaZjj18eIs7LQjxbY21SZ8B0ibQ1ianrmAiclPM5OibXCQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了简洁，并且和 Bengio 和 Senécal [4 , 15]的标号对应起来（注意到第一篇文章中，他们计算了正对数的梯度），我们用 −E(w) 来代替 h⊤v′w 的点积。我们的损失函数于是看起来像这个形式：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9czrTnJnxb1YtXMcPgKx9kNWYJs85PvcEdXXicsF9UnsNvGpexEjM6bwg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了反向传播，我们现在可以计算相对于我们的模型参数 θ 的 Jθ 的梯度 ∇：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibb57T6eIt9RBpXOfYfcyqX50QzZsFT0env3mKaibo3MddDlyKKw9QPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 logx 的梯度是1x，运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBicpdmib8xb0PPWxoarS5NiaBnx7W8ribT4mx1UTgsSibxu1qtWmKWaZhFw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以在和中移动梯度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cIl0KSVjhsIkjq4YibsnQcqic1HHAcIf79mS8VCr3l0c8mtkc62twy6kw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 exp(x) 的梯度就是 exp(x)，再一次运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cuJLJr4kkCOs1xO1SlQNp8vcDKGribAMyicmDPOTnibt6YXGtF4NPFYM7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把它写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cmRm7iayjzaYcu6PyvMEaDW09yaoYW1QkIicKMAJZPCo97XFwWeMFEYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意到：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDXVdfWYA3OCXLOz1AY6znenNyWcYL3cP4Kv61UibFkdz9wL0Wgkya4w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只是 wi 的 softmax 概率 P(wi)（为了简洁，我们忽略它对上下文 c 的依赖）。替换它，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyWdiaqibr7K72GKqdLuoGFUKSphg8cpCVmPzguicqVickjHw9s98FicjTYA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，重新把负系数放到和前面，可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cic7kKz3l2sNOZDk87dXXEYQyo0leyTPgSGDbQiaakLVqzgyznveLtczQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio和Senécal（2003）发现了梯度最终由两部分组成：一个是给目标词 w 的正反馈（上一个式子中的第一项），一个是给其它词 wi 的负反馈，由它们的概率 w（第二项）反映出来。我们可以发现，这个负反馈只是对于所有V中的词 wi 的梯度 E 的&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDicOh0m3Iia8DqYUqxDW1bGaU0oQbmjZvstS0BrmAooJCKTT0jtV0SPQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cJQZ6YAALBzZAYfiaic1rTT8VPuK2TE08A2JicWbz1aFnL3Xx2dhxkia03g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在大多数的基于采样方法的难点是去近似负反馈，让它更容易地去计算，因为我们不想将所有在 V 中的词的概率加起来。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以用蒙特卡洛方法去近似任何概率分布的期待值 E，也就是所有概率分布的随机样本的平均值。如果我们知道网络的分布，即 P(w)，我们就可以从中采样 m 个词 w1,⋯,wm ，然后近似得到如上的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1iaKjjtjgwu7GvJIibul4C6o5NI9pyTTLKS0V8EBRW9YrbibKUYEFcbsw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，为了从概率分布 P 中采样，我们需要计算 P，但这其实是我们最开始想要避免的。于是，我们找到另一个概率分布 Q，我们称之为建议分布（proposal distribution）。Q 应该便于从中采样，而且可以用来作为蒙特卡洛抽样法的基础。我们偏向于选取和 P 相近的概率作为 Q，因为我们希望得到的近似期望能够尽量准确。在语言建模中一个非常直接的选择就是用一元文法分布（unigram distribution）作为 Q 的训练集。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也就是经典的重要性抽样方法（Importance Sampling，英文缩写 IS)：它使用蒙特卡洛抽样，通过一个提议分布Q来近似一个目标概率分布 P。然而，这仍然需要给每个采样的词 w 计算 P(w)。为了避免这个，Bengio 和 Senécal (2003年) 用Liu [16]中提出的有偏估计量（biased estimator）。这个预测函数可以在 P(w) 当成乘积计算时使用，这正是我们要处理的情况，因为每个除法都可以转化成乘法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从根本上来说，我们不必花大代价计算 Pwi 并得到梯度 ∇θE(wi) 的权重，我们只需要利用提议分布Q来得到权重的一个因子。对于一个有偏见的 IS，这个因子就是&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKKOburZqykXNAxdQf9icVJVN3aLgwfm9fBTUibRdibbDIF4DWbmvicJYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cOEicmHnbo25icNKia8pWXTU616hOJ54IFXccicv4fqO6CyPqvicLiar8lQIA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;且&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ccFUe9xS5QmuicPSeRDuwFk04jyCRbViaUmq8kD2Alokmo43yYSQ1qTibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用 r 和 Q 而非 Bengio 和 Senécal (2003年, 2008年) 论文中的 w 和 W 来避免重名。因为我们可以发现，我们仍然计算 softmax 的分母，但是用提议分布 Q 来替换分母的标准化。因此，我们的用以近似期望的有偏差的预测函数如下：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用越少的样本，我们的预测就会越差。另外，我们需要在训练阶段调整我们的样本数量，因为如果样本数量太小，在训练阶段，网络的分布 P 可能和一元分布 Q 分岔，导致整个模型无法收敛。因此，Bengio 和 Senécal 为了防止可能的分岔采用了一个计算有效样本大小的方法。最后，研究者们声称这个方法相对于传统的softmax能够加速19倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自适应性重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 和 Senécal (2008年) 注意到对于重要性采样方法，替换掉越多复杂的概率分布，比如二元（bigram）和三元（trigram）分布，那么接下来在训练阶段，将对于从模型的真实分布 P 来对一元分布 Q 的分岔的对抗将无效，因为n元（n-gram）分布看起来和训练好的神经语言模型的分布挺不一样。作为替代模型，他们提出了一个用在训练阶段调整适应过的 n 元分布来更好地跟随目标分布P。最后，他们根据一些混合方程来插值（interpolate）一个二元分布和一个一元分布，而这些混合方程的参数都是他们用不同频率组的 SGD 训练的，训练目标为最小化目标分布 P 和提议分布 Q 的 Kullback-Leibler 差异。在实验中，他们声称训练效果提高了大约100倍。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目标采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jean 等人（2015年）提出了在机器翻译中运用具有适应性的重要性采样方法。为了让模型更好地适应 GPU 上的并行计算及有限内存，他们将目标单词的数量限制到采样最少必须达到的数量。他们将训练集分区，在每个分区只保留一定数量的单词，形成了总词汇表的一个分集 V′。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这根本上表现了一个独立的提议分布 Qi 可以在训练集的每个分区 i 中运用，这给所有词汇表分集 V′i 内的词赋予同等的概率，而其它的词则概率为0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;噪音对比估计方法&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;噪音对比估计 （NCE）（Gutmann 和 Hyvärinen） [17] 由 Mnih 和 Teh [18] 作为一个比重要性采样方法（IS）更稳定的采样方法提出，因为我们发现 IS 具有一定风险出现建议分布 Q 和分布 P 分岔的情况，而这就是需要优化的地方。不同于之前的方法，NCE 并没有试图直接预测一个词的概率，而是用一个辅助性的损失函数来同时最大化正确词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆 Collobert 和 Weston（2008年）的成对排序标准，它将正数窗口排列在「受损的」窗口之前，这一点我们在上一篇博文已经讲到。NCE 做类似的事：我们训练一个用来从噪音区分目标词的模型。因此，我们可以将预测正确词的任务简化到一个二元分类任务，其中模型试图从噪音样本中区分正确、真实的数据，如图4所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1adCl7nogIcJmZlT6iaictdVDibsHibvcz0WvSUIWCN8uTc71Ctictxo0Ag/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4:噪音对比估计&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于每个词，给定了 n 个在训练集之前出现的词 wt−1,⋯,wt−n+1，我们可以从噪音分布 Q 来生成 k 个噪音样本 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为在 IS 中，我们可以从训练集的一元分布中采样。因为我们需要数据标签来完成我们的二元分类任务，我们指定在上下文 ci 中所有的正确词 wi 为真（y=1），而所有噪音采样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为假（y = 0）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不去计算我们的噪音样本的期望&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9crbV8CnzHCuqRCvbQZYciaSZb81K8vdOmkCCADiaHUib264CuwAHrjJAiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为获得这个期望仍然需要把所有 V 中的词加起来从而预测负标签的标准化的概率，而是再次用蒙特卡洛法近似求平均值：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cYUABqwbDqciaP5mUnGmAbibOib5UeOQ0cIHrAREkby6UgSgtW95l5KPxQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以简化为：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1LaD38BuaEYl5c2wdusjTH2TBDArzibdVMIxAfEtPiaUN65ecBGpaugA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为每个在上下文 c 中正确的单词 wi 生成 k 个噪音样本，我们有效地从两个不同的分布中生成词：正确的词从训练集 Ptrain 的实际分布中采样且依赖于它们的上下文 c，而噪音样本则来自噪音分布 Q。我们因此可以用两个分布的混合模型来表示采样到正样本或负样本的概率，它们基于分别的样本数量来取得权重：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdOuP3fNY2ZVe92AE4XicusmNHaZbeLicw0XJd6eAF5PSuicDxhWicGc66w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据这个混合模型，我们现在可以计算一个样本来自于训练分布 Ptrain 的概率，它就是一个 y 对于 w 和 c 的条件概率：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cvMiaS2C3ZspQOrhOAAZqy6oQqsVicUsPq8JxBLJukcrtT6u0kicicgicUqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它可以简化成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c6zxKO7iboQ6eLdo4G20xOltzNibicAAQw1sZpicBSEyichL0L2WAyN3nPgg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们不知道 Ptrain 的值（正是我们希望计算的），我们用我们的模型 P 的概率来替换 Ptrain：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLibByWY4fXbOmYMZp7kAo6hT3vYXUlzs8XnVeZ6Kghic4L7ibkNiaTcNYg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测噪音样本（y=0）的概率因此就是简单的 P(y=0|w,c)=1−P(y=1|w,c)。请注意计算 P(w|c)，也就是给定它的上下文 c，一个词 w 的概率本质上就是我们对于 softmax 的定义：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cwkssghydynxK27TJQzmTbnZ6mnxtIO6iaSR6ge7e1gbn4qs6vXKbDaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了记号简便和不被混淆，让我们把 softmax 的分母命名为 Z(c)，因为这个分母仅依赖于 h，它从 c 中生成（假定一个固定的 V）。Softmax 于是看起来像这样：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csqnNicNhT0hpxAu6ZicOxxpn0oLs8UD3R2XF3AGWRK23Lzdib7Pyga7mw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mnih 和 Teh（2012 年）和 Vaswani 等人 [20] 实际上将 Z(c) 固定在 1，他们声称这样不会影响模型的表现。这个假设有一个良好的附带效果，那就是可以减少模型的函数数量，同时保证模型可以自己标准化，而不需要依赖特意标准化 Z(c)。确实，Zoph 等人 [19] 发现即使模型学习这个参数，Z(c) 和 1 非常接近，而且具有小的方差。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们在上面的 softmax 算式中可以设 Z(c) 为 1，对于在上下文 c 中的词 w，我们就得到了如下的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以插入上式的这一项来计算 P(y=1|w,c)：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAsQ2v4wTem2aVYqItLdrVXKaic3v4Oex5KkiaVEQu8YFKIwBbLvDj3xw/0?wx_fmt=png"/&gt;&lt;br/&gt;插入这一项到我们的 logistic 回归的目标中，就能得到完整的 NCE 损失函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cl0ribiczQ3gEwcxPTWAjjJ9zZTibyb1vnAb2ZZP7IchESrj0L2CyZT5Sw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意到 NCE 有一个很好的理论上的保证：可以证明当我们增加噪音样本的数量 k 时，NCE 的梯度趋向于 softmax 函数的梯度。Mnih 和 Teh（2012 年）提出 25 个样本就足够使模型表现能够和常规的 softmax 相当，且能够提升 45 倍的运算速度。对于 NCE 的更多信息，Chris Dyer 发表了一些非常好的笔记 [21]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个 NCE 的缺点就是，因为不同的噪音样本来源于对每个训练词 w 的采样，噪音样本和它们的梯度都不能在稠密矩阵中存储，这减少了在 GPU 上使用 NCE 的好处，因为它不能受益于快速稠密矩阵乘法运算。Jozefowicz 等人（2016 年）和 Zoph 等人（2016 年）独立地提出在所有训练词中共享噪音样本，从而 NCE 的梯度可以用稠密矩阵运算来计算，在 GPU 上更加高效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 IS 的相似性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jozefowicz 等人（2016 年）的研究认为 NCE 和 IS 不仅都是基于采样的方法，同时也高度关联。他们发现 NCE 用一个二元分类的任务，IS 可以类似地用一个代理损失函数（也称上界损失函数，surrogate loss function）表示：IS 并不像 NCE 一样用一个 logistic 损失函数去做二元分类任务，而是优化用一个 softmax 和一个交叉熵损失函数做一个多元分类的任务。他们发现当 IS 做多元分类的任务时，它为语言建模提供了一个更好的选择，因为损失函数带来了数据和噪音样本的共同更新，而不是像 NCE 一样的独立更新。事实上，Jozefowicz 等人（2016 年）用 IS 做语言建模，并在 1B Word Benchmark 数据上却取得了优异表现（就像上文所述）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;负采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mikolov 等人（2013）发现并推广的目标——负采样（NEG），可以被看成是 NCE 的一个近似。就像我们之前所说的，NCE 可以被证明是在样本数量 k 增大后对 softmax 损失函数的近似。NCE 的简化算法 NEG 避开了这个保证，因为 NEG 的目标就是学习高质量的词表征，而不同于在测试集上获得低困惑度这一个语言建模的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NEG 也用一个 logistic 损失函数来最小化训练集内的词的负对数可能性。让我们回忆一下，NCE 计算给定上下文 c 一个来自实际的训练概率分布 Ptrain 的词 w 的概率，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibXFmOoB9PA90Cr3ZiaavZeG2ua1ejWBU57oGECs4oePpk7LmejmPXxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 NEG 的主要差别在于 NEG 只是用简化 NEG 的计算来近似这一个概率。因此，它将最昂贵的项 kQ(w) 设为 1，我们就得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ctmfyXQQxokVicmhgzPWTFB12I5CPJssCGVq7ibEibsKvqe0RYSg05sHHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当 k=|V| 而且 Q 是一个均等分布时，kQ(w)=1 刚好为真。在这个情况下，NEG 等价于 NCE。我们设 kQ(w)=1 而不是其它常数的原因可以从重写这个算式来得到，因为 P(y=1|w,c) 可以变型成 sigmoid 函数&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;如果我们将它插入回之前的 logistic 回归函数中，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cUFzzWicd7WsEIdEAwrfYgXvwofgx7WFuztAt5jtpibdCCUFWoGjtgibew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;稍微简化一下，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKbv5qqXtiaibiaRPN8CNXtxhZTuSfMicckRwJC3uuT9nR1NibmFfUXlJx0A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cQbUic1UBuFDUNXblhmdsEu1pE2LqGQEpnicNdbdicKJEQpTRyoaAaXUtg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终得到 NEG 损失函数：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cgO2fzKgP75TWM0zBVWvfVicxlAeW1q8Kpcwv0PCvl3Kt9et1xrzicibibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了和 Mikolov 等人（2013 年）的标号统一起来，h 必须被 vwI 替换，v′wi 需被 v′wO 替换，以及 vw~ij 需被 v′wi 替换。另外，和 Mikolov 的 NEG 的目标相反，我们 a) 在整个词汇集中优化目标，b) 最小化负对数可能性而非最大正对数可能性（如上文所说），和 c) 已经把&amp;nbsp;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibbusiaTad1ibiaFIbahCjHvypCC40a2jjEDcfZic2jHVbjcUqUnE85zz5w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望替换成它的蒙特卡洛近似。对于更多推导 NEG 的信息，敬请参见 Goldberg 和 Levy 的笔记 [22]。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经看到 NEG 只是在 k=|V| 而且 Q 是一个均等分布时和 NCE 相等价。在其它情况下，NEG 只是和 NCE 近似，也就是说它并没有直接优化正确词的可能性，这是语言建模的关键。虽然 NEG 可能因此对于学习词嵌入非常有用，但是它不能保证渐近一致性（asymptotic consistency），这使得它不适合语言建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然 Devlin 等人 [23] 提出的自标准化技术（self-normalisation）不是一个基于样本的方法，它为语言模型的自身标准化技术更直观，我们待会儿会介绍。我们之前提到的把 NCE 的损失函数的分母 Z(c) 设为 1，这个函数实质上自标准化了。这是一个有用的性质，因为它允许我们跳过标准化项 Z(c) 的昂贵计算。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆，我们的损失函数 Jθ 最小化我们的训练数据的所有词 wi 的负对数可能性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c49zZn4bAesL95KxKJib0TmtbFtjdfxyBic6e3cQkF7GFZQZm1oLJKDwQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们像之前一样可以把 softmax 分解成和：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibeynoia42hvCmjniauWiabU6mj3dw9wxqDiawynZFpEMYiau15Dx3y2CbNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们可以限制我们的模型，使得它可以让 Z(c)=1 或 logZ(c)=0，那么我们可以避免计算标准化项 Z(c)。Devlin 等人（2014 年）因此提出给损失函数加上一个平方错误惩罚项，来鼓励模型把 Z(c) 保持着尽量接近 0：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c13ibXA4RLeuYbjl9mwYmkSTGbay84ENvOknj5zdVF5AyOocHB5HqIUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyicpvmo3DZaYAVmzrL98g7dHc6ianaAJqSOxrcNLDN1c0iaMl7zpdcbAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 α 允许我们去做模型准确度和平均数自标准化的取舍。我们大体就可以保证 Z(c) 将和我们希望的一样非常接近于 1。在它们的 MT 系统的解码时间内，Devlin 等人（2014 年）接着设置 softmax 的分母为 1，而且仅仅使用分母和它们的惩罚项来计算 P(w|c) ：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cqC78Hnn2gbu3KLkflib9OdR1N14CPdZur9BhQ2NLS1QrKTQuNdTuAGw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们称自标准化加速了大概 15 倍，而相比于常规的非自标准化的神经语言模型，在 BLEU 分数上只有少量的退步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;低频的标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Andreas 和 Klein [11] 提出仅仅标准化一部分训练样本已经足够，且仍能获得和自标准化一样的行为。他们由此提出低频的标准化（Infrequent Normalisation，或 IN），仅仅在惩罚项中取小部分样本，来形成一个基于样本的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们先来把之前的损失函数 Jθ 分解成两个独立的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cZHwZp3t1jwky9WrF9lUx8R6ia7ibt7zGshS1xQwXaDCFx802EjliaaCCA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在在训练数据中，通过仅仅计算一个词的子集 C（它包含词 wj）来在第二项中取小部分样本，从而从上下文 cj 采样（因为 Z(c) 仅依赖于上下文 c）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cObEotOxXgr92yKTQuEEKU024BmiaYQNQF7GjxMYZrFve10oITCCib3Hw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，γ 控制着子集 C 的大小。Andreas 和 Klein（2015 年）提出 IF 综合了 NCE 和自标准化的优点，因为它不用给所有的训练样本计算标准化项（NCE 完全不用计算），但是又像自标准化一样允许去做模型准确度和平均数自标准化的取舍。他们观察到当它只用从十分之一的样本中采样时，它能够加速 10 倍，而没有明显的模型表现损失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;其它方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;至此，我们已经集中讲解了近似或者完全避免 softmax 分母 Z(c) 的计算的方法，因为它就是计算中最昂贵的一项。我们因此不去特别留意&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即倒数第二层 h 和输出词嵌入层 v′w 的点积。Vijayanarasimhan 等人 [12 ] 提出用快速的位置敏感哈希（locality-sensitive hashing）来近似&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，尽管这个方法在测试阶段可以加速模型，但是在训练阶段，这个加速实际上会消失，因为词嵌入必须重新标号，且批（batch）的数量会增加。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;选择哪个方法呢？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看完了最流行的基于 softmax 的和机遇采样的方法，我们展示了对于经典的 softmax 有很多替代品，而且几乎它们所有都在速度上有重要提升，它们大多数也略有模型表现上的不足。所以，这自然引到了一个问题，就是对于某一个特定的任务，哪个方法是最好的方法。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Approach方法&lt;/th&gt;&lt;th&gt;Speed-up factor加速倍数&lt;/th&gt;&lt;th&gt;During training?在训练阶段吗？&lt;/th&gt;&lt;th&gt;During testing?在测试阶段吗？&lt;/th&gt;&lt;th&gt;Performance (small vocab)表现（小词汇集上）&lt;/th&gt;&lt;th&gt;Performance(large vocab)表现（大词汇集上）&lt;/th&gt;&lt;th&gt;Proportion of parameters 参数的采用百分比&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Softmax&lt;br/&gt;Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;1x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Hierarchical Softmax 多层次的softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;25x (50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Differentiated Softmax 微分 Softmax&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&amp;lt; 100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;CNN-Softmax&lt;br/&gt;CNN-Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad - good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;30%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Importance Sampling 重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(19x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Adaptive&lt;br/&gt;Importance Sampling 具有适应性的重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Target Sampling 目标采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Noise Contrastive&lt;br/&gt;Estimation 噪音对比估计方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;8x (45x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Negative Sampling 负采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Self-Normalisation 自标准化方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(15x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;6x (10x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;表格 1：比较语言建模中近似 softmax 的几种方法&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在表格 1 中比较了我们之前介绍的几种方法的表现。加速倍数和表现均来自于 Chen 等人（2015 年）的实验，同时我们在括号中记录了原作者们提出的加速倍数。第三和第四列分别显示了在训练阶段和测试阶段是否达到这个加速倍数。请注意，加速倍数的不同也许是因为没有优化，或者原作者没有用到 GPU，显卡运算常规的 softmax 比一些其它方法更加高效。部分方式没有可用的比较的方法，其表现则大部分参考相似的方法，比如自标准化方法应该和低频标准化相当，重要性采样方法和具有适应性的重要性采样方法与目标采样法相当。Jozefowicz 等人（2016 年）提出的 CNN-Softmax 方法的表现则根据纠正（correction）的大小时好时坏。在所有的方法里，除了 CNN-Softmax 方法中用了明显更少的参数，其它方法仍然需要存储输出词嵌入。微分 Softmax 方法因为能够存储一个稀疏权重矩阵从而减少了参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像一直以来，没有哪个方法对于所有的数据集或者任务来说都是最好的。对于语言建模，常规的 softmax 在小词汇库数据集上仍然有一个非常好的表现，比如说 Penn Treebank，甚至能够在中数据集中也很好，比如 Gigaword，但是在大数据集上则很差，比如 1B Word Benchmark。目标采样方法、多层次 Softmax，和低频标准化方法则在大词汇库上表现更好。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微分 Softmax 方法则大体上对于小词汇库和大词汇库上都表现不错。有趣的是，多层次 Softmax（HS）在小词汇库上表现不好。然而，在所有的方法之中，HS 都是最快的，而且在给定的时间内能够处理最多的训练样本。负采样法则在语言建模任务上表现不佳，却在学习词表示上有非常好的表现，从 word2vec 的成功可以看得出来。请注意，所有的结果都不可尽信：Chen 等人（2015 年）在报告里说到在实际中运用噪音对比估计方法存在困难；Kim 等人（2016 年）用多层次 Softmax 在小词汇库上获得了很好的表现，而重要性采样方法则被 Jozefowicz 等人（2016 年）提出的最先进的语言模型所用在一个大词汇库中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，如果你真的准备去用上述的方法，TensorFlow 实现 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling) 了一些基于采样的方法，也解释&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; (https://www.tensorflow.org/extras/candidate_sampling.pdf) 了其中一部分方法的不同点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;小结&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇对于一些不同近似 softmax 的方法的介绍，不仅可以用于提升和加速词的表示的训练，也对语言建模和机器翻译有所帮助。正如我们看到的，大部分方法都非常相关，且源于同一点：必须找到方法来近似昂贵的 softmax 分母的标准化计算。记住这些方法，我希望你现在能更好地训练且理解你的模型，你甚至可以准备去自学更好的词表示模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如我们看到的，学习词的表示是一个非常广的领域，对于成功有很多相关因素。在之前的博文中，我们学习了流行的模型的建筑，而在这篇博文中，我们着重学习关键部分，softmax 层。在下一篇博文中，我们将介绍 GloVe，一个依赖于模型因素分解，而不是语言建模。我们将把我们的注意力转移到其它的对于成功学习词嵌入起关键作用的一些超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Morin, F., &amp;amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. http://doi.org/10.1002/j.1538-7305.1951.tb01366.x &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from http://arxiv.org/abs/1411.2738 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jean, S., Cho, K., Memisevic, R., &amp;amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from http://www.aclweb.org/anthology/P15-1001 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Andreas, J., &amp;amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vijayanarasimhan, S., Shlens, J., Monga, R., &amp;amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from http://arxiv.org/abs/1412.7479 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Ling, W., Trancoso, I., Dyer, C., &amp;amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from http://arxiv.org/abs/1511.04586 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. http://doi.org/10.1109/TNN.2007.912312 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Gutmann, M., &amp;amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML』12), 1751–1758. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Zoph, B., Vaswani, A., May, J., &amp;amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vaswani, A., Zhao, Y., Fossum, V., &amp;amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from http://arxiv.org/abs/1410.8251 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Goldberg, Y., &amp;amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.』s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp;amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL』2014, 1370–1380.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 人工智能法官：英美科学家开发出用于审讯的计算机程序（附论文）</title>
      <link>http://www.iwgc.cn/link/3204727</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自卫报&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; white-space: normal; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;伦敦大学学院（UCL），谢菲尔德大学和美国宾夕法尼亚大学的最新研究表明，人工智能已经可以分析法律证据与道德问题，进而预测审讯结果。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;em style="font-size: 16px; text-align: justify; white-space: pre-wrap; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="font-size: 16px; text-align: justify; white-space: pre-wrap; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cPEFtYZMq4BgxXibM2uOR3lY7LN0TPbtvQR14ZZR07hlI0ia3dZFI1fjg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-align: justify; font-size: 16px; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;em style="font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;该算法分析了 584 宗有关酷刑和侮辱，审判不公与侵犯隐私的英文案卷。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-align: justify; font-size: 16px; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;em style="font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 20 世纪 60 年代，有科学家曾经预测，电脑将有一天能够预测司法判决的结果。随着近年来计算机科学的发展，人工智能程序在进行高度复杂的任务。它们在电影，电视节目和音乐上「猜你喜欢」时正在变得越来越准确。而现在，一项英美科学家的开创性研究告诉我们，人工智能可以用来预测审讯结果了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伦敦大学学院（UCL），谢菲尔德大学和宾夕法尼亚大学的科学家刚刚发表的最新研究表明，人工智能已经可以分析法律证据与道德问题，进而预测审讯结果。这项研究的成果可以帮助人们提高法庭审判的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能「法官」在分析欧洲人权法院涉及酷刑，虐待，有辱人格和侵犯隐私的案件中做出的判定中有五分之四与人类法官的判决相同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究人员的程序审查了包含 584 个案件的英文数据集，有关「人权公约」中的第 3 条（涉及酷刑或侮辱虐待的案件，250 起），第 6 条（保护公平审判权，80 起），以及第 8 条（隐私和家庭生活，254 起）。选择这些案件的原因是他们是代表基本权利的案件，同时存在大量的公布数据。在研究中，人工智能程序分析所有信息，并提出自己的司法判决。在其中 79% 的案子里，人工智能提出的判决与当时的法庭判决一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nikolaos Aletras 博士是这项研究的领导者，他是伦敦大学学院计算机科学系的研究员。Aletras 说道：「我们不认为人工智能会在司法领域代替法官或者律师。」在这项研究中，为了防止计算机学习出现误差和偏见，「违法」与「不违法」的案例各占 50%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cRngUw574xyfEYP8eexoQ3xr2dL6slpUxicPCJUnzv0T3aPlticIeUw9g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;人工智能「法官」在分析欧洲人权法院涉及酷刑，虐待，有辱人格和侵犯隐私的案件中做出的判定中有五分之四与人类法官的判决相同。（图片：Getty Images）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个项目的开发过程中，研究小组发现欧洲人权法院对于判决更多依赖非法律事实，而不是纯法律依据。这表明，欧洲人权法院的法官们相比「形式主义者」而言，在法律理论上更加的「现实主义」。根据历史研究，其他的高等法院也是如此，例如美国最高法院。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项研究佐证了其他一些有关高等法院判决影响因素研究的观点，Aletras 说道：「目前我们还需要学习更多数据，以进一步的研究改进我们的算法。但我认为它可以成为一个有效的工具，用于判定哪些案件违反了『欧洲人权公约』。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伦敦大学学院的计算机科学家 Vasileios Lampos 博士，同时也是研究报告的共同作者，解释了这项研究的开创性：「之前的同类研究通常根据犯罪本身和政策立场来预测判决，而我们第一次使用法院编写的案卷来进行分析。」这项研究发现，预测法院判决的最可靠的方式是通过分析案卷中所使用的语言以及文本中提到的话题和情景。案文中「情景」部分包括案件中事实背景的信息。通过组合案件所涵盖的「话题」中提取的信息，分析数据中有关人权公约相关三条的「情景」，最终实现 79％的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Lampos 表示，他们的人工智能程序目前缺乏训练数据：「我们认为这项研究的成果将能提高那些高级法院的工作效率，但是目前看来，我们需要收集更多的文件和法院案卷进行测试。在理想状况下，我们使用那些向法院提交的文件来测试和改进我们的算法，而不是公开的判决文本。但目前我们缺乏这样的数据，只能使用法院公开的部分案件摘要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在司法领域，律师们早已将人工智能应用到了工作中，这些程序可以执行一些复杂的任务──例如搜索概念而不是简单的关键字──这可以极大地缩短判断文件与案件是否有关的时间。今年五月，IBM 发布了人工智能律师 Ross，通过使用 Watson 的计算能力为人们提供法律建议。IBM 的这项技术最近已被 Baker＆Hostetler 律师事务所采用，这家律师事务所主要处理破产案件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1APrf4VicMuibQ6biaicYI4cicwVCnz9B1YEuH1ia7W3nBmCRyiaSfqPNCF1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Ross 使用 IBM 的 Watson 人工智能平台（图片：Getty Images）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但目前看来，最高法院的法官们还不能将这种新技术应用在判决上。&lt;/span&gt;&lt;span&gt;Aletras 等人的这项研究已发表在了 PeerJ Computer Science 上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：预测欧洲人权法院的司法裁决：自然语言处理的观点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Aletras, N, Tsarapatsanis, D, Preoţiuc-Pietro, D, Lampos, V&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：自然语言处理和机器学习近期的发展为我们提供了工具，可以建立预测模型用以揭开法庭审判的原理。这项研究可以让律师和法官受益，他们可以利用我们的模型作为工具快速审案，或从已知信息中获取要点帮助判断。本文中，我们第一次系统性地研究了仅通过分析文本内容来预测欧洲人权法院的司法判决。我们制定了一个二元分类任务，在分类器中输入从案例中提取的文本内容，目标输出是关于是否存在违反人权公约的条款的实际判断。文本信息使用连续字序列（contiguous word sequences），即 N-gram 和话题来表示。我们的模型可以准确预测（平均 79% 正确率）法院的判决。我们的实证分析表明，案件的形式事实是最重要的判决因素。这与法律现实主义的理论一致，这表明司法决策受到事实情况的显著影响。我们同时发现，事件的话题内容是这种分类任务的另一个重要特征，我们通过定性分析进一步探讨了这种关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 约翰·马尔科夫：犯罪技术将与人工智能一同进化</title>
      <link>http://www.iwgc.cn/link/3204728</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自纽约时报&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：JOHN MARKOFF&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;人工智能技术的飞速发展为人类社会带来了方方面面的便捷，但会加持犯罪的各种手段。通过人工智能程序，鼠标轻轻一点就能轻易实现视频和音频窃听的犯罪技术不会太远。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象一下年迈的母亲打电话向你求助，她忘记了银行卡密码。但真实的情况是她不是你的妈妈，电话另一端的声音只是听起来像她。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那只是计算机合成的声音，人工智能的一大绝招已经在电话中被用来伪装身份。&lt;/span&gt;&lt;span&gt;这样的情景目前尚属科幻，但未来，它可能成为一种犯罪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伪装技术的大范围实现需要的这种软件正在快速发展。例如 DeepMind，Alphabet 旗下以 AlphaGo 闻名的公司，这个程序已经在围棋大赛中击败了几个世界顶级选手。最近，DeepMind 宣布已经开发出一种新程序可以模仿任何人类的声音，这种声音听起来比之前的任何一套文本语音系统生成的声音都要自然，并将现有的自然语音技术与人类语音的差距缩小了 50%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，讽刺的是今年计算机安全产业的年总收入达到了 750 亿美元，并开始讨论机器学习和模式识别技术将会如何改善计算机安全眼下的悲剧状态。但是还有一个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「人们还没搞明白网络犯罪已经变得自动化而且成指数级扩张，」一位执法机构的顾问和《未来的罪行》一书作者 Marc Goodman 说到。他补充道，「这不是 Matthew Broderick 在地下室里搞黑客攻击那么简单了，」参考 1983 年的电影《战争游戏》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年年初，美国国家情报局局长 James R. Clapper 曾警告过关于先进人工智能技术的恶意使用。在其年度安全审查中，Clapper 先生强调了虽然人工智能系统能带来便捷，但它们也扩大了在线世界的漏洞。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算机犯罪技术的不断尖端化在网络攻击工具的进化中可见一斑，比如，Goodman 所说的，广泛使用的臭名昭著的恶意程序 Blackshades。该程序作者是瑞典人，去年在美国被定罪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个程序在计算机地下市场上卖的很火，其功能可以被形容为「盒子里的刑事特权（criminal franchise in a box）」Goodman 说。它能让不懂技术的用户在计算机上部署一个勒索程序或者通过鼠标点击来进行视频或者音频窃听。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些工具的下一代产品将会加上机器学习技术，而该技术原先是由人工智能研究者用来改善机器视觉、语音理解、语音合成和自然语言理解的。一些计算机安全研究者相信数字犯罪在 5 年前就已经开始尝试使用人工智能技术了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;颠覆互联网无所不在的验证码可以显示这一点——全自动区分计算机和人类的图灵测试。这是卡耐基梅隆大学的研究者 2003 年发明的 challenge-and-response puzzle，用来阻止自动程序窃取在线帐户的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;普通的人工智能研究者与黑客罪犯一直在部署机器视觉软件，5 年前就颠覆了验证码技术。加州大学圣迭戈分校的计算机安全研究员 Stefan Savage 说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果你两年没改验证码了，它就会被一些机器视觉算法操控，」他说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两人惊讶的是，拖慢恶意人工智能发展速度的东西已经可以以低成本或免费的人力解决。例如，一些网络犯罪已经将破解验证码外包给电子工厂，这里人力解码一个密码的费用只需一丁点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创造力更佳的计算机骗子已经使用网络色情作为奖励发送给那些破解验证码的人，Goodman 说。自由劳动是人工智能任何时候都无法匹敌的一种商品。接下来又会怎样呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;刚踏入犯罪的人可以搭上新技术发展的顺风车。声音识别技术如苹果的 Siri 和微软的小娜现在广泛用于与计算机的互动。同时亚马逊的 Echo 声音控制扬声器和 Facebook 的 Messenger 聊天机器人平台迅速成为在线商务与客户支持的在线通道。通常的情况是，无论何时，当一种像声音识别这样的交流技术进步开始成为主流，犯罪很快就能将它利用上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我会质疑，通过聊天机器人提供客户支持的公司不在不知不觉地承担起一项社会工程，」krebsonsecurity.com 的一位调查记者 Brain Krebs 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;社会工程，说的是促使人们执行某些行动或者泄露信息的行为被看成是计算机网络安全链上最弱的一环。网络犯罪分子已经在利用人类最好的品质——信任和助人为乐——来进行盗窃和间谍行为。创造人工智能化身欺骗网络用户的能力只会将问题恶化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在州政府和政治运动的政治宣传中广泛使用的聊天机器人技术可以反映出这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者已经发明了「计算宣传（computational propaganda）」这一短语来形容 Facebook 和 twitter 等社交媒体上欺骗行为的爆发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最近的一篇研究论文中，牛津互联网研究所的社会学家 Philip N. Howard 和布达佩斯考文纽斯大学（Corvinus University of Budapest）研究员 Bence Kollanyi 描述了在即将到来的「英国退欧」公投中，政治聊天机器人是如何扮演一个有策略的小角色来塑造在线对话的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;犯罪中用上这些软件只是时间的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;独立计算机安全专家 Mark Seiden 说到，&lt;/span&gt;「攻击社会工程的设计中有很多小聪明，但是就我所知，还没人开始使用机器学习来找到最容易上当受骗的那群人，」他停了停又说，「我本来应该回答你的是：我很抱歉，Dave，我现在不能回答这个问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 机器能做噩梦吗？MIT开发出能生成恐怖惊恐图片的深度学习算法</title>
      <link>http://www.iwgc.cn/link/3189608</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#ffffff"&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#7f7f7f"&gt;&lt;span&gt;&lt;b&gt;机器之心编译&lt;/b&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;万圣节就要到了，本着吓死人不偿命的精神，麻省理工学院（MIT）Media Lab 今日上线了一个用人工智能吓人的网站 Nightmare（噩梦）：http://nightmare.mit.edu/#portfolioModal22。在这个网站上，研究者展示了利用人工智能算法生成的恐怖风格的图片，其中包括埃菲尔铁塔等地标建筑和人脸等一些结果。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW62dH7PLqwRcjNwKOmHB4Cp4gqJ3CaZnzAaHsIdldZxiasSJiav1Ifqgg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;地标：美国自由女神像、法国埃菲尔铁塔、日本东京塔&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWvtnepvzBIDpZwGibicRVRT4GkVmrxQahUvKKeeQjjV5iaj6ux7VKibvBIw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;人脸&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，利用深度学习软件生成恐怖图像已经不是什么新闻了。其中著名的有谷歌的 Deep Dream 生成的带有许多眼睛的狗脸的图片；还有前段时间中国出现的 Uber「幽灵车」事件的恐怖司机头像也有人认为是软件生成的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW2GJV4ohA9MJ5hL7HFRfCWnVX5SEpY0faJiac9JU1dRZVenibDHkP7KkQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌的 Deep Dream 生成的狗脸&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWJho6OEbibSv4CE2BLZ211IS5XbtN6B5GNXjr8dqzbVQ1wyjuiaFiaEI4g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Uber「幽灵车」司机&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MIT 还在这个网站上列出了一个恐怖和人工智能交织发展的超短历史：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2000 年前：恐怖的起点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;万圣节传统的神秘起源可以追溯到凯尔特人庆祝的古老异教节日。凯尔特人将这一天作为收获季节的结束和冬季的开端。他们相信这种季节的变换会打开死者世界的大门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1000 年前：人工智能的第一次迹象&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东西方文明都描绘了关于人造实体的传说故事——这些人造的存在能够思考、感知、帮助或伤害他人。在许多故事中，这些「生物」都脱离他们的创造者的控制，并获得了超越任何人预想的知识和能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1816 年：没有夏季的一年&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1816 年的春天经历了历史记录中最奇怪的气象现象：一个永无止境的冬天。这迫使三位作家将自己关在了日内瓦湖畔的豪宅中。玛丽·雪莱、约翰·威廉·波利多里、拜伦勋爵比赛看谁能写出最惊悚的故事。而他们所有人都获胜了。雪莱创造了弗兰肯斯坦；波利多里种下了吸血鬼文化的种子；拜伦则在他的诗作《黑暗（Darkness）》中通过地球上的最后一个人的讲述开启了世界末日惊悚题材的先河。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1840 年：第一个计算机程序诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能和恐怖惊悚开始交汇：拜伦勋爵（现代吸血鬼文学的创始之父）的妻子 Anne Isabella Milbanke 生下了计算机历史上一位先驱爱达·洛夫雷斯（Ada Lovelace）。她编写了世界上第一个机器算法，要知道，当时所谓的计算机器还仅存在于纸面之上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1930 年：恐怖惊悚兴起&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 世纪 30 年代的电影荧幕成为了在黑屋子里面吓人的前所未有的媒介。许多恐怖电影成了人们的消遣，其中包括弗兰肯斯坦、德古拉、木乃伊、隐形人、伦敦狼人……这也催生了一个有创造性的且有利可图的恐怖惊悚片行业。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1956 年：人工智能诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1956 年炎热的夏天，Marvin Minsky 和其它睿智的头脑聚集到了达斯茅斯学院。在一场创造力的爆发中，他们奠定了人工智能成长的基础：开发能够在西洋跳棋上击败人类、进行复杂数学计算……乃至能够生成英语句子等等的程序。有传言说计算机生成的第一个句子是：TRICK OR TREAT？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2016 年：人工智能驱动的恐怖&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几千年来，不同地域、不同宗教和不同文化的人都在创造吓人的方式。恐怖需要引发人内心的情绪才能在人类创造之中保留一席之地。在我们还不清楚人工智能的局限性的今天，这个挑战是尤其重要的：机器能够学会吓人吗？为了这一目标，MIT 推出了 Haunted Faces 和 Haunted Places：计算机通过深度学习算法和邪恶的灵魂生成恐怖惊悚图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，在这个网站上，MIT 还请求网友对他们的软件生成的恐怖图片进行评分。这些评分将作为 MIT 的这个恐怖图片生成模型的进化的训练数据，将使其能够生成越来越恐怖的图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据了解，研究者首先通过鬼屋和末日城市的图片对他们的人工智能算法进行了训练，然后将一些著名地标的图片输入该模型。经过处理之后，该模型能让这些图片带上阴郁的地狱风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW9xHIqMhs4KCsq8tfUrAVvK4J1dq5bfzGfKGJsrvClnZN6cib3ibHziadg/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果 Elon Musk 和 Stephen Hawking 的关于人工智能对人类生存的威胁的警告还不够吓人，MIT 的这个故意吓人的项目可算是做到了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道我们未来的计算机主人会不会使用这种生成的恐怖图片来恐吓我们呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，让我们来认识一下该项目的三位研究开发者：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWUaEoCSHiamfhOicd3vKGXONvJvSbJby58R8wnBYPRL7yWg8YR0S9svtA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part1：基于语言建模的词嵌入模型</title>
      <link>http://www.iwgc.cn/link/3189609</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part1，&lt;span&gt;全面介绍了词嵌入模型，接下来几天机器之心将继续发布 Part2、Part3，希望能对大家了解词嵌入有所帮助。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入简史&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;语言模型的简介&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;经典的自然语言模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W model&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CBOW&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无监督学习词嵌入（word embeddings）在许多自然语言处理的任务中都取得了前所未有的成功，因此它常被视为自然语言处理的万灵药。实际上，在许多自然语言处理架构中，它们确实几乎替代了诸如布朗聚类（Brown clusters）和 LSA 特征等传统型分布式特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 ACL（计算机语言学会）和 EMNLP（在自然语言处理中实证方法会议）的会议论文很大程度都是词嵌入的研究，有些人还认为词嵌入这种嵌入方法比 EMNLP 更加适合的自然语言处理。今年的 ACL 会议有了不仅一个，而是两个的词嵌入模型的研讨会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入之间的语义关系在外行人看起来就像变魔术。深度自然语言处理的讲座常以「国王－男人＋女人≈女王」的幻灯片来做开场白，一篇最近在 Communications of the ACM 的文章向词嵌入模型致敬，并称之为自然语言处理实现突破的主要原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文将会是本系列第一篇全面介绍词嵌入模型的文章，将讨论词嵌入模型的热度是否会持续下去及其原因。在这个介绍里，我们将尝试把在这个领域分散的论文串联起来，强调很多模型、应用和有趣的特征，并将在后续的文章中重点关注多语言环境下的词嵌入模型和词嵌入评估任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这第一篇文章将呈现目前的基于语言建模的词嵌入模型。在我们深度讨论很多的模型时，我们会挖掘它们的优点，希望能够在过去和当前的研究的背景下提供新的见解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于命名方式的简单小结：接下来我们将使用当前热门的「词嵌入（word embeddings）」术语，来指代词语在低维度向量空间的稠密表示。「词嵌入」和「分布式表征（distributed representations）」是两种可互换的表示方法。我们将特别强调「神经词嵌入（neural word embeddings）」，即运用神经网络训练的词嵌入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;词嵌入简史&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从上世纪九十年代开始，向量空间模型就已在分布式语义中得到了应用。当时，许多用于预测连续空间的词表征的模型已经被研究了出来，其中包括隐含语义分析（LSA：Latent Semantic Analysis）和隐狄利克雷分布（LDA：Latent Dirichlet Allocation）。想要详细了解词嵌入背景下的分布式语义的历史的读者可以看看这篇文章：https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2003 年创造了词嵌入这个名词，并且在自然语言模型中将其与模型参数一起联合训练。据了解 Collobert 和 Weston 于 2008 年首次展示了预训练的词嵌入的实际应用。他们里程碑式的论文《A unified architecture for natural language processing》不仅将词嵌入确立成了一种可用于下游任务的有用工具，还引入了现在已经成为了许多方法的基础的神经网络架构。但是让词嵌入最终能流行起来的是 Mikolov 等人在 2013 年创立的 word2vec，这是一个允许无缝训练和使用预训练嵌入的工具套件。在 2014 年，Pennington 发布了一个具有竞争力的预训练的词嵌入集 GloVe，标志着词嵌入已经成为了主流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入是目前无监督学习的成功应用之一。它们最大的好处无疑是它们不需要昂贵的人工标注，而是从未标注的现成大数据集中派生的。然后预训练的词嵌入就可以运用在仅使用少量有标注数据的下游任务中了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然而然地，每个前向传播的神经网络都把在词汇表中的词语当成输入，并把它们表示成低维空间中向量。然后，它们再通过反向传播进行调整，得出词嵌入作为第一层的权重。通常，这称之为「嵌入层（Embedding Layer）」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;产生词嵌入作为副产物的神经网络和 word2vec 这样的以生成词嵌入为特定目标的方法之间的主要区别是它们的计算复杂度。对于一个大的词汇集来说，使用非常高深度的架构来生成词嵌入的计算成本太高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是为什么直到 2013 年词嵌入才进入自然语言处理的舞台。计算复杂度是词嵌入模型的一个关键权衡，也是我们这篇概述中会重复出现的主题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个区别就是训练目标：word2vec 和 GloVe 都是用来生成广泛语义关系的词嵌入模型，这对许多下游任务有用；而这种方式训练的词嵌入对不依赖于这种语义关系的任务并无太多帮助。相反，常规的神经网络对于某个特定任务生成的词嵌入在别的任务往往功能有限。值得注意的是，一个依赖于语言建模这样的语义关系的任务能够生成类似于词嵌入模型的嵌入，这一点我们将在下节探讨。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外说明一点，word2vec 和 Glove 之于自然语言处理，也许就像是 VGGNet 之于计算机视觉，亦即一个普通的权重初始化——它能提供有用特征，而且无需长时间训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比较不同的模型，我们可以设想如下的标准：我们设想一串来自词汇库 V（其大小为|V|）的包含 T 个训练单词的的字符序列 w_1,w_2,w_3,⋯,w_T。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象我们的模型是一段包含 n 个单词的文段。我们将每一个单词与一个 d 维的输入向量 v_w（嵌入层中的同名词嵌入）和一个输出向量 v_w'（另一个词表征，其作用下面很久就会介绍）联系在一起。最终，对于每一个输入 x，我们相对于模型参数θ和模型输出分数 f_θ(x) 来优化目标函数 J_θ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言建模上的一项注意&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入模型和语言模型联系紧密。对语言模型的质量评价基于它们学习 V 词汇库的词语概率分布的能力。事实上，许多最新的词嵌入模型一定程度上尝试预测序列下一个会出现的词。另外，词嵌入模型的评价通常运用困惑度（perplexity）——一个从语言建模借来的基于交叉熵的评价标准。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们进入词嵌入模型的众多细节之前，让我们简单介绍一些语言建模的基础知识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体而言，语言建模力求在给定之前的词语的情况下，计算一个词语 w_t 的出现概率，也就是 p(w_t|w_{t−1},⋯w_}t−n+1})。运用链式法则和马尔可夫假设，我们就可以近似地通过之前出现的 n 个词得到每个词的概率乘积，从而得到整个句子或整篇文章的乘积：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWicOE3T44QVXmJJBnfWKckT1iccqtqAvyPlnZGtbdANbCicBKPGVC9EcMw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基于 n 元的语言模型中，我们可以用一个词的组分的 n 元的频率（frequency）来计算这个词的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW89AoRESTK3jPFEhlHHzu4YUicI6Tguzia2cp51tLMSgaqhMMqiczUXTog/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设置 n=2 产生二元模型，而 n=5 和 Kneser-Ney 则一同呈现平滑的五元模型——平滑的五元模型在语言建模中是公认的的一个强有力基准线。更多的细节，敬请参照斯坦福大学的演讲课件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在神经网络中，我们通过大家熟知的 Softmax 层来计算相同的目标函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWWz7wZibufQfmlfkAtMuAnOadPjsRsWEwfz3F34Wx16l1WdpQEWNFLUg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;内积 h^T v'_{w_t} 计算了词 w_t 的未标准化的对数－概率（log-probability），我们用在词汇库 V 中的所有词的对数－概率之和来把它标准化。h 是它的倒数第二层（见图 1 前向传播网络的隐藏层）的输出向量，而 v'_w 就是词 w 的输出嵌入，亦即在 softmax 层的权重矩阵中的表征。注意虽然 v'_w 可以表征词 w，但它是从输入词嵌入 v_w 独立学习的，因为向量 v'_w 和向量 v_w 的相乘对象是不同的（v_w 和索引向量相乘，v′_w 和 h 相乘）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW90nRVHqppUJW0O2Hgzxibsg4KnpkfznAzWxOicO5WUoJUKK8UdfT2GLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1: 一个自然语言模型（Bengio 等人，2006 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要计算每个词 w 在神经网络输出层的概率。想要高效地做到这一点，我们将 h 和一个权重矩阵相乘，这个权重矩阵每行都是对于在 V 中出现的词 w 所得的 v′_w。我们随后将得到的向量（我们通常称之为 logit，也就是前一层的输出）以及 d=|V| 传入到 softmax 层，softmax 层则把词嵌入「压扁」成一个词汇库 V 里面词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 softmax 层（对比于之前的 n 元计算）仅仅隐式地考虑之前出现的 n 个词。长短时记忆模型（Long Short-term Memory, 英文简称 LSTM），通常用来作自然语言处理模型，将这些词编码成状态 h。我们在下一章将会介绍的 Bengio 的自然语言模型，则是把之前的 n 个词通过一个前向传播层传入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请大家记住这个 softmax 层，许多后续介绍的词嵌入模型都将或多或少地运用它。运用这个 softmax 层，模型将尝试着在每一时刻 t 都最大化正确预测下一词的概率。于是，整个模型尝试最大化整个数据集的平均对数－概率:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWpcHJLCvDnQlKZcE77WSicYKhiafQzE0z1DXda8nHmsPaqr6WDbaW53yA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相似地，运用链式法则，模型的训练目标通常是最大化整个语料库的所有词相对于之前 n 个词的平均对数－概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWusgOAUobrxHnr6orodvhibE4W7OepmxR7NwnXg5zd0YQvM8uw7MicelA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果想在试验阶段从这个语言模型取样词，我们可以在每一时刻 t 贪婪地选择最高概率的词 p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})，或者用定向搜索。举个例子，我们可以用它来生成像运用了 LSTM 作为解码器的 Karpathy 的 Char-CNN 中的任意文本序列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;经典神经语言模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2013 年 [1] 提出的经典神经语言模型包含一个前向传播神经网络，它有一个隐藏层，用来预测文本序列的下一个单词，如图 2 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWfEyQxhGahfuuZq0geAAop5IoibjhibdaeqibXfHhVz28kpKGRnlOAb0QQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2: 经典神经语言模型（Bengio 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型的最大化目标函数就是我们在上文中介绍的典型的神经语言模型的目标（为了简洁，我们忽略规范化（regularization）这一项）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWn1icJveGOrgts4xhvSSibJPX0fpAldGN2koShDuqkmrbibXjyicWSj2uqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;f(w_t , w_{t-1} , \cdots , w_{t-n+1}) 是这个模型的输出，即 softmax 计算出的概率 p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})。n 在这里就是传入这个模型的之前 n 个词的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人首先提出词嵌入模型，它是一个在实数范围 R 内的词特征向量。他们的架构非常经典，是目前各种改进方法的原型。他们原始模型中的基础模块依然能在现在的许多神经网络和其他词嵌入模型中找到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;嵌入层：一个用索引向量和词嵌入矩阵相乘得出的词嵌入层；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;中间层：一个可包含一层或多层的中间层，例如，一个可以将之前出现的 n 个词非线性地组合在一起的全连接层（fully－connected layer）；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Softmax 层：一个最终层，输出词汇库 V 中词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，Bengio 等人发现了目前最先进模型中存在的两个核心问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现 2. 中间层可以由一个 LSTM 替代，这个已被最新神经语言模型使用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现最后的 softmax 层（更确切地说，是标准化项）是神经网络的瓶颈，因为计算 softmax 的计算复杂度与词汇库 V 中词的数量成正比，而个数量通常为成百上千，乃至几百万。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，如何在一个大词汇库中用较低的计算成本计算 softmax，成为了建立神经语言模型和词嵌入模型的一个关键挑战。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Bengio 等人对神经语言模型的的最初探索以后，计算机计算能力和算法还尚不允许在大词汇库上的训练。词嵌入模型的研究因而止步不前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Collobert 和 Weston [4]（因此被称为 C&amp;amp;W）在 2008 年展示了词嵌入模型在一个充分大的数据库中如何向下游任务携带语法和语义，并且提升性能。他们 2011 年的论文充分解释了他们的做法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的解决方法避免了对于 softmax 层的昂贵计算，其方法是采用另一个目标函数：&lt;/span&gt;&lt;span&gt;Collobert 和 Weston 的神经网络输出是正确词序列相对于不正确词序列高出的分数 f_θ，而不是 Bengio 等人的论文中用来最大化基于之前的词出现的下一个词概率的的交叉熵标准。他们为了这个目标函数采用了一个成对排名的标准，如下所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWAEQonuI5CpSO632Pmn7be0eQeRkf9wxDG3Rf3qTE4OU0XG8pBXibW1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型从所有包含 n 个词的窗口 X 中取样得到正确的窗口 x。对于每一个窗口 x，用 V 中的 w 代替 x 的中间词来产生一个不正确的版本 x(w)，而模型的目标就是最大化模型对于正确的窗口和不正确窗口的分数的距离。如图 3 所示，他们的模型架构类似于 Bengio 等人的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW8HspRpB0o7WThGibaVeZbbtN4BoNOMMVcDXCKN86QIlsDbACKyrR1mA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3: 去掉了排名目标的 C&amp;amp;W 的模型（Collobert 等人，2011 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算后的语言模型能够生成具有许多语义关系的词嵌入，例如国家能够聚类在一起，语法上接近的词在向量空间上相邻。他们的排名函数避免了 softmax 的复杂计算，且保持了 Bengio 等人论文中计算同样昂贵的完全相连的中间层（2.）（见图 3 中的 HardTanh 层）。他们对于 130000 个词的模型需要花费 7 周来训练的有一部分原因在于此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们现在介绍当今毫无疑问最为流行的词嵌入模型 word2vec，它源于 Mikolov 等人在 2013 年中两篇论文，且催生了上千篇词嵌入的论文。正因为词嵌入模型是自然语言处理中深度学习的一个关键的模块，word2vec 通常也被归于深度学习。然而严格上来说，word2vec 并不属于深度学习，因为它的架构并非多层，也不像是 C&amp;amp;W 模型一般运用非线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在他们的第一篇文章 [2] 中，Mikolov 等人提出了更低计算成本的学习词嵌入的两个架构。他们的第二篇论文 [3] 通过加入更多的提升了训练速度和准确度的策略来提升了模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些架构提供了对比于 C&amp;amp;W 模型和 Bengio 模型具有如下两大优点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们去掉了昂贵的中间层。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们运用语言模型来更多地考虑上下文。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们等等会讲到，他们的模型之所以成功不仅是因为这些改变，而更是因为某些特定的训练策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们会来看这两个架构：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;连续的词袋（CBOW）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;言模型只能通过观察之前出现的词来进行预测，且对于此类模型的评价只在于它在一个数据集中预测下一个词的能力，训练一个可以准确预测词嵌入的模型则不受此限。Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词，见图 4。他们称这个模型为连续的词袋（continuous bag-of-words，或者 CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWXSGmO8jAuKFwv9zmrvYCplt3EGa7zzYUGHNfQskm1FyDxLmnXaYTAg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4：连续的词袋（Mikolov 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 的目标函数和语言模型仅有着细小差异：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWbYXukF6Eicj6IqicqHRgFonWBLHib9ef3NasDyN9wnCQ4l4RlkMicmG40A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型并没有传入 n 个之前的词，而是在每个时刻 t 接收目标词的前后 n 个词的窗口 w_t。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词，如图 5 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWzfBaiaf9dNJkKtYCspNhMjnHj0TXRYMVB327oc96B6RaHqKjS1v63lQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5：Skip-gram（Mikolov 等人，2013）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;skip-gram 模型的目标因此用目标词前后的各 n 个词的对数──概率之和计算如下的目标：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWnjIBuKqBgQnAibFvxvM3Z24AB5qH8T05e50nw1JnoobMSKDOWQubIIQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更直观地解释 skip-gram 模型是怎样来计算 p(w_{t+j}|w_{t}) 的，让我们先来回顾 softmax 的定义：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWVT2D38N3IOCZzfl2WicwaOyg7zEgQjyvOc4ooZeR8uVAP3McspuxAOQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不计算目标词 w_t 基于前面出现的词的概率，而是计算周围词 w_{t+j} 对于 w_t 的概率。于是，我们可以简单地替换掉这些变量：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWiaNWtVic3M07x51F1hiaVYfLhG0I8CPG2he33YQ5vvs5FibCoV9B3FCKOg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 skip-gram 架构并不包括能够产出中间状态向量 h 的中间层，h 自然地成为对于输入词 w_t 的词嵌入 v_{w_t}。这也是我们为什么想给输入向量 v_w 和输出向量 v′_w 以用不同的表示，因为我们想要将词嵌入和自己相乘。用 v_{w_t} 替换 h，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWqDibSBVj9jFLGxEAZ0X8RNkYvpchHuCRoC7vqibjec11SjJibz7Muk8vg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 Mikolov 论文中的代号和我们的有细微差别，他们标注词语为 w_I，而周围的词为 w_O。如果我们用 w_I 替换 w_t，用 w_O 替换 w_{t+j}，然后根据乘法交换律交换内积的向量位置，我们能够得到和它们论文中一样的公式表示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWHicxcOHpT6EMTyHuBUU4GnxZtQMzY0FsHicc2QqzZHmQ5BhnWcqyl1Ow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一篇博文，我们将要讨论近似昂贵的 softmax 函数的不同方式，以及令 skip-gram 成功关键的训练决策。我们也会介绍 GloVe[5]，一个基于矩阵乘法分解的词嵌入模型，并讨论词嵌入和分布式语义的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;References&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Bengio, Y., Ducharme, R., Vincent, P., &amp;amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. http://doi.org/10.1162/153244303322533223 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. http://doi.org/10.1145/1390156.1390177 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp;amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from http://arxiv.org/abs/1103.0398 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models, 12. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Geoffrey Hinton最新论文：使用快速权重处理最近的过去</title>
      <link>http://www.iwgc.cn/link/3189611</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ArXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWmdMgiakHgSDWrrR1KsllMs0SZRo0CRAicTL5pibUnjfKXadSrw1If8BSw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;直到最近，对人工神经网络的研究在很大程度上还局限于仅有两种类型的变量的系统：代表当前或最近输入的神经活动以及学习获取输入、输出和收益（payoff）之间规律的权重。这种限制并没有什么好的存在理由。突触拥有在许多不同时间尺度上的动态，这表明人工神经网络可能能够受益于变化速度比行为（activity）慢但比标准权重远远更快的变量。这些「快速权重（fast weights）」可以被用于存储最近过去的临时记忆（temporary memories），它们能为过去（past）提供一种在神经角度上可行的实现这种类型的注意（attention）的方式，而且最近研究已经证明过去（past）在序列到序列（seq2seq）模型中非常有用。通过使用快速权重，我们可以避免存储神经活动模式的副本的需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWibYsQe9NyibQdMSNEOTYcWnD8iaITyibjTp9QeBY3D76iaccRSHBy1U7I8Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：快速关联记忆模型（fast associative memory model）；其中，红色线：持续边界条件，灰色线：慢速过渡权重，蓝色线：快速过渡权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWa8rshQMcNGvLF8FyKS0906lZTsCm9u4Il5SV6eU8ZAtlH2aHcyfedA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3：多层快速关联记忆模型（multi-level fast associative memory model）；其中，灰色虚线：更新快速权重和清空隐藏状态，红色线：持续边界条件，灰色实线：慢速过渡权重，蓝色线：快速过渡权重&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>专题 | 脑芯编：窥脑究竟，结网造芯</title>
      <link>http://www.iwgc.cn/link/3189613</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;9月份时，&lt;/em&gt;&lt;em&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=3&amp;amp;sn=e07ccb2264f7d003d6c81bfa6e5a91dd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=3&amp;amp;sn=e07ccb2264f7d003d6c81bfa6e5a91dd&amp;amp;scene=21#wechat_redirect"&gt;机器之心曾发文章宣布「机器之心」和「矽说」将共同推出系列文章「脑芯编」&lt;/a&gt;，揭秘类脑芯片的过去、如今与将来。本文是此专题的第二篇，窥脑究竟，结网造芯。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;〈二〉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几重卷积几重生&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;蜘蛛结网，是为了捕食昆虫；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;蜘蛛侠结网，是为了拯救世界；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;码农 Data Scientist (~ds~) 结网，是为了——&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;换一个角度看世界，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英语叫做： Representation。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你只想知道一个关于神经网络的常识，我认为上面这个单词是最不应该错过的。就像每个学模拟电子学的人，其实归根结底就是学了两个字——放大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;话接上回，我们说到，通过一系列乘累加和非线性激活函数，我们就可以实现一个神经元。而关键的问题就是如何把神经元们连起来。解决这个问题之前，我们先要明白神经网络的作用——通过一系列线性和非线性的变化重新将输入信息映射成为表达事物的本质的简化特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你觉得上面一句的每个字都认识，却不知道他在说什么，那么我们来看一个经典的例子——人类的视觉皮层（Visual Cortex）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;视觉皮层, 一场生物与 AI 的伟大握手&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;码农老师的生物课又来了……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你有没有想过当你看到生命中一个重要的人的时候，比如说基友（码农怎么会有妹纸？），你是看到是他/她的鼻子，眼睛，脸上的痘痘，昨晚熬夜的黑眼圈……但是这些东西最后都只留下了一个映像——我面基了。可是你有没有想过从你看到图像，到你得到的结论，无数的信息都已经没有过滤，你的脑子完成了一次将 4K3D 图像压缩成两个字的过程，到底发生了什么事？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWT15dRSNa56DWETYtTSrwiaLpZutAYVKw9JMr8RsjI0BVs5BjDeLcH6Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个过程就是从信息经过视觉皮层（神经网络？？）的过程。从前到后，他经过了几站：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）始发站——视网膜，比较像是一个电子系统的传感器，用来接收信号；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）快速交流道——LGN，他是将左右眼看到的信号重新编码后传递给视觉皮层，像是一个电子系统中的主控处理器与总线（请原谅我不说 LGN 的中文，因为说了你也记 不住）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（3）第一站——主视觉区 V1，第一层神经网络，司「边界检测（Edge Detection）」一职，这可能是神经元数量最丰富的一个区域；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（4）第二站——次视觉区 V2，第二层神经网络，司「基础特征提取」一职，归纳视觉信号的形状、大小、颜色、频率……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（5）第三站——V3，司「位置「，也是个过渡区，一条线上你有些站你不知道为什么会停~&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（6）第四站——V4/V5（MT）分支，深度神经网络，各司「色彩/运动」；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（6）V4 分支终点站 1——换乘 inferotemporal Cortex，近深度智能 TE 区，司」目标识别「~~~终于终于我认出基友来了，撒花~~&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（7）V5 分支终点站 2——换乘 Parietal Cortex, 进深度智能 MST 区，司「空间运动分析」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视觉皮层可能是目前为止人类认识的最透彻的大脑部分，不过，好像建立在无数的活体实验上。。。即使如此，还是有很多未知的空间亟待生物学家探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWwwja1MeVVSAz7EAFibpTn3almeJ8AfXIVub43z44a7zqyOy4ePX3OSg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道读到这里，对人工智能略有了解的你有没有觉得这堂生物课在哪里见过？先做边界检测，在再做特征提取，在进行分类识别，这不就是大名鼎鼎的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C N N&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LeNet/VGG/AlexNe&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;t&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积，让加速成为一种可能&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实在神经网络领域里，目前为止唯一能算的上前所未有成功的就是 CNN（Convolution Neural Network，卷积神经网络）。最早的 CNN 可以追溯到 98 年 Yann LeCun 的一篇如何识别手写数字的 paper，这里出现了第一个 CNN 的雏形 LeNet：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWbL3Qg3I7hgicawo40gfeIfJ8McqGhfdHUXNHPW0YTEROwS2VibVCK0wg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从结构上来，CNN 继承了视觉皮层中对信号处理「层」的概念，虽然不是那么的 100% 的吻合，但是 CNN 的初级层往往用来做「边界检测」这样的简单的特征提取，而在深度层重新组合初级层的信息成为抽象的再表达（Representation）, 最后交给事件的发生的相关概率归纳出事物的本质。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，一个比较不太准确的趋势是神经元的数量随层的深度逐渐减少，但是单个神经元的粗壮程度（输入数量）随层的深度逐渐增加。视觉皮层也具有相似的规律，V1 的数量多，但是结构比较简单，但到了 V4/V5，链接变得很复杂，但占的区域却比 V1 小的多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，这些都不做电路的人重点。对于硅工们而言 CNN 获得巨大成功的原因在于&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;它极大地节省了神经网络的硬件开销&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使神经元为单位作加速器成为了可能&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（1）CNN 定义了一种更高能效的元操作——卷积核&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于卷积是什么，大家可以去参考一篇《一文读懂卷积神经网络》（广泛地转载于各大公众号间），下图是我目前看到的最形象的卷积描述。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWK7vTTOjapGK5wbWTW6SrYZ0ZNtib4GuVqgPlMt0sv5C8TW9olVtUqUg/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;该图片源自网络，感谢原 gif 作者&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其本质就是对于一个区块，判断和自己系数组成的「基」区块的相似程度，得到的分数越高就越相似。这样，当一种「基区块」被赋予一种特征是，即使用于整张图片的特征提取，他的系数也是固定的，因此大量的系数加载操作可以被省略。同时，一个固定大小的「卷积核」成为了比「乘累加」更高阶、更高效的原子操作，在现代计算机体系结构中，实现越复杂，但操作越固定的加速器，其效率和速度的提升也就越大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（2）Pooling——是垃圾就要扔掉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNN 网络的另一个巨大贡献就是在卷积层和层之间，设置了一个」垃圾箱「，把上一层产生的无效信息都扔掉，避免了超大规模的数据传输和存储。大家把这叫做 Pooling，我又要来吐槽那个中国人给他取了个」池化「的名字，虽然我也找不到更好的名字，但根本无法帮助理解。Pooling 的策略很多，最常见的是 max pooling 就是留个最大的，然后又其他都扔掉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;（3）「乱撸」？(ReLU)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet 后期发展到 AlexNet 后，激活函数也从 sigmoid 变成了 ReLu，他们的图形曲线大概如下所示。用脚趾头也知道，Relu 操作的实现就是把符号位为负置 0 就好了。至于 sigmoid 么，传承自经典机器学习回归理论，是 e 指数的除法操作，编译后简直就是一场噩梦，我们先把他当作一个古老的神话就好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWoK8EnmXGTcAaFuas4H1ER7IUZ7KTYhCW3OdLshGvU8Csj3pMMwYnbQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以上种种硬件实现的简化，加上 CNN 的巨大胜利，都当让硅工们看到了直接从电路角度优化的角度切入人工智能芯片的可能。但是，也发现了一个问题，传统的硬件加速的算法往往是做死的，比如椭圆加密，浮点乘除等等。但是 CNN 的元操作——卷积核——虽然模式固定，但是其每一层的卷积核数量和层数却是纷繁复杂，固定的硬件并不能实现网络的可塑性（structural plasticity）？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那该怎么办？下一次，我们先来回顾下 CPU 的发展历程，看看现代处理器的」形与令「。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「几重卷积几重生」——你看懂神经网络的卷积了？还有「重生」呢~限于篇幅，只能等番外了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就到这里，且听下回分解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;〈三〉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梦里不问形与令&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;世界上有两种管家&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种是 Batman 的 Alfred&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;能服务能做饭能伪装能打架&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;狠起来超人也不是干不过&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种是天朝的大内总管&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;掌印秉笔，啥事不会&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;老大又吩咐了就去传个话&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你脑子里的 CPU 是哪一种？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了神经元，知道了怎么把神经元连成网络，这个系列终于进入了主题——怎么实现神经网络。如果在这个问题上加一个条件，那就是「怎样用芯片实现神经网络的计算」？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在回答这个问题以前，让我们先去拜访两位长者——Alan Turing 和 John Von Neumann，目前大家公认的计算机之父。话说前者才是真的「苟利国家生死以，岂因祸福避趋之」，详见卷福主演的奥斯卡获奖电影《模仿游戏》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWYKQNNibibtjsU0Fr8SPLcEYZRq1G7vOteOgHG7rx1HV5l8vYE4ogjbOA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图灵-冯-诺依曼架构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了表达对大师的尊敬，我起了个很干脆的标题。大师之所以是大师，是因为他们定义了在 80 年前定义了通用计算机的数学模型和体系结构。在这过去的 80 年里，任何试图推翻这些结构的「投机」分子几乎都没什么好下场。但是，总有人希望推翻这个架构。先简单的描述下两位长者干了什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alan Turing 在 1936 年提出了一种具有普适性的逻辑计算模型，证明通过有限状态机完成输入数据的操作，可以实现任意复杂的逻辑运算。图灵机本身描述的场景在现在看来已经没什么意义，但是他第一次完整的定义普适计算机体系机构——一卷很长很长的带子（infinite length tape）通过一个有磁头 (head) 的有限状态表 (finite state table) 进行读取/处理/改写的机器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWZlxUNuDnMicZrjNwgpdYYAZ4l9blJfsNHSbwEClR87PjtNlGIX4JMdg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图灵机：带子、磁头和状态机&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9 年后，Von Neumann 把带子改叫做「Memory」，状态表叫做「CPU」，磁头改叫做「Connection (Bus)」，换了一副图，就有了史称「冯诺依曼架构」的现代计算机体系结构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWd3yyvtt5j4qR7vCmv1lEjx7ajicialS3DzoX6y0gM8L9BmWEIxEmGtLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;教科书上会说这个结构有啥特点，这是让你背的。其实很简单，图灵-冯诺依曼架构最大的特点是把计算任务分为了 2 个部分——数据存储 (memory) 和数据处理 (processor)。处理器几乎不能存数据，存储器几乎不能算数据。两部分用一种连接方式 (bus) 按一定规则通信。泾渭分明的特点让冯诺依曼架构处理事情起来特别有条理，就像「男主外女主内」的家庭角色分配一样，在硬件资源极度受限的情况下，成为了自动化发展的中坚力量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;冯诺依曼架构有一个升级版，叫做哈佛 (Harvard) 架构，把存储空间分为了指令 (instruction) 存储和数据存储，对应不一样的操作。目前的主流嵌入式微处理器基本采用这个架构，但 Anyway 这并不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;冯诺依曼架构在过去的 60 年称霸人间，如果这项专利申请成功的话，这一定是史上最赚钱的专利。可是，冯诺依曼架构在经历了各种法院撕逼后，被判定为一项没有收益人的专利……（Youyou Tu 和青蒿素在这面前简直不值一提）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;成也萧何 – x86 的不可一世&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然冯老爷子在自己的架构下发明了人类第一台计算机，ENIAC 和 EDVAC，但诺依曼的真正崛起还是要归功于 x86。如果你不知道 80x86 是什么，那只能说明我们已经有代沟了，嗯，很深深的代沟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Intel 自 1978 年推出 8086 后，x86 体系架构就一直是电脑（上到服务器，下到平板电脑）核心处理芯片的不二选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWULoPDCciaO6vyyJMfNl4YDVqzmsWWmKaq3JicIToq6yl55tB6GNXXXAg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Intel x86 i7 版图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;顺便做个普及，在冯诺依曼架构下，每个处理器会干的事情是有限制的，通常这个限制叫做指令集。它规定 CPU 的基本操作，没有指令集 (instruction set) 定义的复杂操作可以通过基本操作的组合来完成，比如指令集里没有乘法，那我们可以通过一定数量的加法来完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在冯老爷子的机构里，谁的指令集越大，可以访问的存储空间越大，谁就越牛逼。x86 的指令集从 8086 到 i7 不断扩张与膨胀，最终成为了一个会算双精单精、矢量图像，多核多线程多 Cache 的巨无霸。简单的说，到 2013 年的时候，史上最强 core 已经无所不能了。可是历史不断在重演一幕就是，当绝顶高手号称要独孤求败的时候，不知道哪里窜出来的毛小伙子可能一个起手式就把你撂倒了。圣经里大卫王这么干掉了 Goliath，《倚天屠龙记》里，张无忌这么称霸了光明顶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那谁是 x86 的张无忌呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;移动设备，RISC 的春天&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;独孤求败的 x86 其实有个致命的缺陷——能效，通俗地说就是「做一次」要花费的能量。可是每块肌肉都很发达的 muscleman 总是要比一般人多吃几碗饭吧。我们现在能买到的 i7 即使在省电模式也要消费超过 47W 的功耗。本省 47W 不算什么，但是苹果乔大叔的出现，让 47W 一下子很麻烦。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Iphone/Ipad 和一系列手持的充电设备对瓦级以上的功耗是非常敏感的！x86 的功耗导致它「充电 2 小时使用 5 分钟」的悲惨结局。肌肉男瘦身变成筋肉男的必然的命运。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这时，x86，或者说是 intel 的张无忌出现了—ARM Cortex RISC. 所谓 RSIC 就是精简指令集（Reduced Instruction Set），他能干的事情很有限，但是他的功耗低。X86 在其巅峰时期无数次地战胜过 RISC，以至于 ARM 出现时并有没足够重视他，那时候 Intel 还在和 AMD 抢 64 位 x86 的主导权呢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么无数次败下阵来的 RISC 可以最终成功呢？因为这次，他寻找到了一个 partner——加速器。在移动端的应用设备里，其实也有很对需要强大计算消耗的进程，这是 RISC 本省无法胜任的。但是，实际应用上，往往这些进程是有固定的模式和使用场景的。比如手机在通话时的语音编解码，拍照时的图像 处理（俗称「美颜」）和无线通信是的编解码。对于这样一个经常重复，且模式固定的高通量计算，可以在总线上加入一个专用模块（ASIC）加速，在处理专用任务是 ASIC 的能效又比通用处理器高很多。下图就是 ARM 有名的产品之一 A9，除了 CPU 外，它的浮点与超标量计算（NEON）都被移到了 CPU 外（一般来说，这不能算作加速器）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW1iaicRxC3bflgneKa5sKl1oFZfzYpoJPdMtNvh0ibv5zwbPqsh2f692UA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是开头的那个故事，你每天充的电不够「超人」吃的，与只能换个块头小，但是能够指挥其他人的总管。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;败也萧何 – 冯诺依曼瓶颈&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「泾渭分明，靠总线连」的冯诺依曼架构带来了单核/少核时代计算机的春天，但冯诺依曼架构的致命缺陷——冯诺依曼瓶颈——也悄悄地增长。随着摩尔定律的发展，远距离的搬移大规模数据早已取代了计算本身，成为制约高效计算的重要瓶颈，对于 x86 结构，有太多指令可以直接穿过总线访问存储空间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 RISC+加速器的体系结构里，总线作为「总管」和「内务府」、「上书房」、「御膳房」间的桥梁，更是好不吃紧。当瓶颈出现在通信上时，冯诺依曼架构就体现出了它垂垂老矣的一面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个问题，在实时处理的人工智能场景下显得格外突出，信号从入到出，都是按照是数据流 (Data flow) 的传输模式一帧一帧地来。这一特征在类脑的神经网络实现中就更加明显。如果每一个卷积的系数都要去云深不知处的存储海洋里去寻找，那神经元的处理效率会非常低。简单地说：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谁脑子 TM 的是一半纯记忆&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一半纯分析的呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;脑子么，都是左右开工的。边走边忘，雁过留痕，却也是旧相识，恢复不出来一个一毛一样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，摆在类脑芯面前的路有三条：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）	采用冯诺依曼经典架构，把神经元计算写在指令集里，反正是超人，技多不压身；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）	采用 RISC+神经元/神经网络加速器，给「总管」再开个府呗；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（3）	放弃冯诺依曼架构，完全分布式硬件，像「数据流「一样的风骚走位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这三个选项都有不错的代表，我们慢慢来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梦里不问形与令，你知道计算机形（体系结构）和令（指令集）了么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心发布，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>资源 | 微软官方整理：用于Azure机器学习的免费数据集</title>
      <link>http://www.iwgc.cn/link/3180600</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Microsoft&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lee Scott&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDh403YYkBnSPp4nibgKKgTKBLickrN16xHFp5ibEKAvxa8ibiaKmLaElMQFg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要学习怎么使用微软 Azure 机器学习，最重要的是获取样本数据集和进行实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在微软，我们有大量的样本数据集可用。这些数据集已经在 Azure Cortana Intelligence Gallery 中的样本模型中得到了应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中一些数据集可以通过 Azure Blob 存储获取，所以可以直接链接到 Azure 机器学习实验；而其它的数据集则是以 CSV 格式提供的。下面列出的这些数据集都将提供直接的链接。你可以通过 Import Data 模型在你的实验中使用这些数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些数据中的剩下数据集都列在模块（module）面板中的 Saved Datasets 下；当你在 ML Studio 中打开或创建一个新实验时，你能在实验画布（experiment canvas）的左边看到它们。你可以直接将这些数据集拖拽到实验画布而将它们应用到你自己的实验中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下列出了一些可以免费使用的数据集：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;成年人收入普查二分类数据集&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个 1994 年的普查数据库的子数据集，使用了 16 岁以上的工作年龄的成年人的数据，其带有一个经调整之后大于 100 的收入指数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用人口学信息对人进行分类，以预测一个人年收入是否超过 5 万美元&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Kohavi, R., Becker, B., (1996). UCI Machine Learning Repository Irvine, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机场代码数据集（Airport Codes Dataset）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;美国机场代码&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含每个美国机场，提供了机场 ID 编号和名字，以及机场所在的城市和州。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;汽车价格数据（Automobile price data，原始数据）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;按厂家和车型分类的汽车信息，其中包括价格、气缸数量和 MPG 等特征，以及保险风险评分（insurance risk score）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个风险评分最初是与汽车价格关联的，后来根据实际风险在一个被精算师称为符号化（symboling）的过程中进行了调整。+3 的值表示该汽车是有风险的，而 -3 的值则表示它可能是相当安全的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用回归或多变量分类，根据特征预测风险评分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Schlimmer, J.C. (1987). UCI Machine Learning Repository Irvine, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;自行车租赁 UCI 数据集（Bike Rental UCI dataset）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;UCI 自行车租赁数据集基于来自 Capital Bikeshare 公司的真实数据，该公司在华盛顿特区运营着一个自行车租赁网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含 2011 年和 2012 年每一天和每一小时的数据，总共有 17379 行。每小时租赁自行车数量的范围在 1 到 977 之间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Bill Gates RGB Image&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;已转换成 CSV 数据的公开可用的图像文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用于转换该图像的代码提供在使用 K-均值聚类模型的颜色量化（Color quantization using K-Means clustering model）的详情页面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;献血数据（Blood donation data）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个来自台湾新竹市输血服务中心献血数据库的一个子数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;献血者数据包括献血频率、总献血次数、自上次献血以来的时间和献血量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：目标是通过分类预测献血者是否在 2007 年 3 月献血，其中 1 表示目标区间内的一个献血者，0 表示没有献血者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Yeh, I.C., (2008). UCI Machine Learning Repository , CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;亚马逊网站的书评&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由宾夕法尼亚大学研究者采集（地址：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.cs.jhu.edu/~mdredze/datasets/sentiment/）。-参见论文《Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification》，来自 John Blitzer, Mark Dredze, and Fernando Pereira; 计算语言学协会 (ACL), 2007-&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原来的数据集包含 97.5 万条包含 1、2、3、4、5 评分的书评。这些书评都是用英语写的，截取自 1997-2007 年这个时间段。这个数据集已经被下采样成了 1 万条书评。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌数据（Breast cancer data）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由 Oncology Institute 提供的三个与癌症相关的数据集中的一个，其常常出现在机器学习文献中。结合了来自对大约 300 种组织样本的实验室分析的特征的诊断信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：基于 9 种属性分类癌症类型，其中一些是线性的，一些是按类别划分的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Wohlberg, W.H., Street, W.N., &amp;amp; Mangasarian, O.L. (1995). UCI Machine Learning Repository, CA: 加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌特征（Breast Cancer Features）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了来自 X 射线图像的 10.2 万个可疑区域（候选项）的信息，其中每个区域都用 117 个特征进行了描述。这些特征是专有的，而且它们的含义没有被该数据集的创造者（Siemens Healthcare）揭示出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;乳腺癌信息（Breast Cancer Info）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了来自 X 射线图像的可疑区域的额外信息。每个样本都提供了对应 Breast Cancer Features 数据集行数的信息（如，标签、病人 ID、图像块相对于整张图像的坐标）。每个病人都有很多样本。对于患癌的病人来说，一些样本是积极的，一些样本是消极的。该样本有 10.2 万个样本。这个数据集有偏置的，其中只有 0.6% 的点是积极的，其余都是消极的。该数据集由 Siemens Healthcare 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Appetency Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_small_train_appetency.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Churn Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_small_train_churn.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Dataset Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的数据：http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction%20-%20orange_small_train.data.zip&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含来自法国电信公司 Orange 的 5 万个客户。其中每个客户有 230 个匿名的特征，其中 190 个数值特征和 40 个类别特征。这些特征是非常稀疏的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CRM Upselling Labels Shared&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 KDD Cup 2009 客户关系预测挑战赛的标签：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;http://www.sigkdd.org/site/2009/files/orange_large_train_upselling.labels&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;能效回归数据（Energy Efficiency Regression data）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于 12 种不同的建筑外形收集的模拟能量分布。这些建筑按照 8 个特征进行了区分，比如：玻璃窗面积、玻璃窗面积分布和取向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用回归（regression）或分类（classification）来预测能效等级，其给出的两种响应是有实际价值的。对于多类别分类，响应变量被取舍到了最接近的整数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Xifara, A. &amp;amp; Tsanas, A. (2012). UCI Machine Learning Repository Irvine, CA:加州大学信息与计算机科学学院&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;航班延误数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自美国交通部收集的 TranStats 数据集中的乘客航班正常率数据。该数据集覆盖 2013 年 4 月到 10 月的统计，在上传到 Azure ML Studio 之前，该数据集处理如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该数据集经过过滤只覆盖美国本土的 70 个最繁忙的机场&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;废除了标记显示延误超过 15 分钟的航班&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;转航班数据也被消除&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;选择使用数据目录如下：Year, Month, DayofMonth, DayOfWeek, Carrier, OriginAirportID, DestAirportID, CRSDepTime, DepDelay, DepDel15, CRSArrTime, ArrDelay, ArrDel15, Cancelled&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;美国 2011 年 10 月飞机到达与离开的记录数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测航班延误&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：来自美国交通部的 http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;amp;DB_Short_Name=On-Time.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;森林火灾数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含来自葡萄牙东北部的天气数据，比如温度、湿度指数和风速，结合与森林火灾的记录。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：这是一项很难的回归任务，目的是预测森林火灾焚烧的地区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Cortez, P., &amp;amp; Morais, A. (2008). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[Cortez and Morais, 2007] P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 – Portuguese Conference on Artificial Intelligence, December, Guimarães, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9. 地址：&lt;/span&gt;&lt;span&gt;http://www.dsi.uminho.pt/~pcortez/fires.pdf.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;德国信用卡 UCI 数据集&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;UCI Statlog（德国信用卡）数据集（Statlog+German+Credit+Data)）使用了 german.data 文件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集通过一系列的属性进行表述，根据人进行分类，每个样本表示一个人。此数据集中有 20 个特征，都是数字和类别，以及二元标签（信用风险值）。高信用风险标记为 2，低信用风险标记为 1。将低风险样本误分类为高风险的成本是 1，反之误分类高风险的成本是 5。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;IMDB 电影&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含 Twitter 上评估的有关电影的信息：IMDB 电影 ID、电影名和流派、生产年。该数据集中有 17K 的电影。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;鸢尾花两级数据&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在模式识别文献中，它可能是最知名的数据集。该数据集相对较小，包含来自三个鸢尾属植物分类的每种花瓣测量的 50 个样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：从测量中预测 iris 的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Fisher, R.A. (1988). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;电影 Tweets&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集是 Movie Tweeting 数据集的扩展版本，此数据集有 170K 的电影评估信息，从结构较好的 tweets 中提取。每个示例代表一条 tweet，数据元组：用户、IMDB 电影 ID、评估等级、时间标记、该 tweet 的点赞人数、转推人数。该数据集由 A. Said, S. Dooms, B. Loni and D. Tikk for Recommender Systems Challenge 2014 供用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;汽车MPG数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集是由卡耐基梅陇大学 StatLib 库提供的数据集的修正版本，此数据集曾被 1983 年 American Statistical Association Exposition 使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据列出了每加仑汽油各种类型机动车的消耗情况，同时也包含气缸个数、引擎排放量、马力、总重量和加速这样的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通途：基于 3 个多值离散属性和 5 个连续属性预测节约燃油。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：StatLib, Carnegie Mellon University, (1993). UCI Machine Learning Repository &amp;nbsp;Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Pima 印第安人糖尿病二进制分类数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 National Institute of Diabetes and Digestive and Kidney Diseases 数据集的一个子集。该数据集经过过滤只关注 Pima Indian 遗传的女性病人。数据包括血糖、胰岛素水平、生活方式这样的医疗数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测该主体是否有糖尿病（二分类）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Sigillito, V. (1990). UCI Machine Learning Repository」. Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;餐馆消费者数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一系列关于消费者的元数据，包括人口统计学和喜好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并测试推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Restaurant feature data&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一堆关于餐馆和餐馆特征的元数据，比如食物类型、餐厅风格、位置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并预测推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;餐馆评分数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;包含用户给出的对餐馆的评价，等级从 0 到 2 划分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：使用该数据集，结合其他两个餐饮数据集，可训练并预测推荐系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;钢退火多级数据集（Steel Annealing multi-class）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集包含一系列来自钢材退火实验的记录，数据包含测试钢材类型的物理属性（宽度、厚度、类型（线圈、薄片等））。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测任何二数类属性：硬度或强度，也可用于分析属性间的关联。钢材等级划分遵循一定标准，由 SAE 和其他组织定义。你可以寻求特定的等级，并了解所需要的值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究：Sterling, D. &amp;amp; Buntine, W., (NA). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;望远镜数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;高能量伽马粒子爆发的记录，也带有背景噪声，都使用 Monte Carlo 处理方法模拟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;模拟的目的是改进地表大气 Cherenkov 射线望远镜的准确率，使用统计方法微分想要信号（Cherenkov radiation showers）和背景噪声。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据已经过了预处理，以创建一个以指向相机中心方向为长轴的延长的聚类（elongated cluster）。这个椭圆的特征（通常被称为 Hillas 参数）是可以用于判别（discrimination）的图像参数中的一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用途：预测 shower 表征信号或背景噪声的天气图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意：简单分类准确率对此数据意义不大，因为将背景时间分类为信号要比将信号分类为背景更糟糕。该数据可用来对比 ROC 图应该使用的不同分类器。同时也要注意背景事件（h 代表 hadronic showers）的数量是被低估的，在真实测量中，h 或噪声类代表主要事件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相关研究： Bock, R.K. (1995). UCI Machine Learning Repository Irvine, CA: University of California, School of Information&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;天气数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 NOAA 的每小时地面天气观测（融合了从 2013 年 4 月到 2013 年 10 月的数据）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这份天气 数据包括了机场天气预报站的观测数据，时间从 2013 年 4 月到 10 月。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在上传 Azure ML Studio 之前，数据集要做如下处理：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;气象站 ID 要映射到对应的机场 ID 上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;与忙碌的 70 家机场无关的气象站需要过滤掉&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;日期按年、月、和天分为单独的列&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要选择的列包括：机场 ID、年、月、日、时间、时区、天空状况（skycondition）、能见度、天气类型、干球华氏温度（DryBulbFarenheit）、干球摄氏温度（DryBulbCelsius）、湿球华氏温度（WetBulbFarenheit）、湿球摄氏温度（WetBulbCelsius）、露点华氏温度（DewPointFarenheit）、露点摄氏温度（DewPointCelsius）、相对湿度、风速、风向、ValueForWindCharacter、本站气压（StationPressure）、气压趋向（PressureTendency）、气压变化（PressureChange）、 海平面气压（SeaLevelPressure）、 记录类型（RecordType）、每小时降雨量（HourlyPrecip）、（高度计）Altimeter&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;维基百科标准普尔 500 指数数据集（Wikipedia SP 500 Dataset）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;源自维基百科的基于标准普尔 500 指数中每家公司的文章的数据，以 XML 格式存储。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在将该数据集上传到 Azure ML Studio 之前，需要进行以下处理：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;提取每家特定公司的文本内容&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;移除 wiki 格式&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;移除非字母数字的字符&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;将所有文本转换成小写&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;已知公司类别已被加入&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意有些公司没有找到文章，所以该记录的数量小于 500.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;可以 CSV 格式下载的数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;direct_marketing.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/direct_marketing.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集包含了关于一项直接邮寄活动的客户数据和关于他们的响应的指示。其中每一行代表一个客户。该数据集包含关于用户人口学信息和过去行为的 9 项特征，以及 3 个标签列（访问、转化和支出）。访问（visit）是一个二元行，表示了每次营销活动后客户的访问；转化（conversion）表示客户购买了一些东西；支出（spend）是指花费了多少钱。该数据集由 Kevin Hillstrom 为 MineThatData 电子邮件分析和数据挖掘挑战赛（MineThatData E-Mail Analytics And Data Mining Challenge）提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;lyrl2004_tokens_test.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/lyrl2004_tokens_test.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RCV1-V2 Reuters 新闻数据集中的测试样本的特征。该数据集有 78.1 万条新闻文章以及它们的 ID（该数据集的第一列）。其中每篇文章都已经 tokenized、stopworded 和 stemmed。该数据集由 David. D. Lewis 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;lyrl2004_tokens_train.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/lyrl2004_tokens_train.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RCV1-V2 Reuters 新闻数据集中的训练样本的特征。该数据集有 2.3 万条新闻文章以及它们的 ID（该数据集的第一列）。其中每篇文章都已经 tokenized、stopworded 和 stemmed。该数据集由 David. D. Lewis 提供。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;来自 KDD Cup 1999 知识发现和数据挖掘工具竞赛（KDD Cup 1999 Knowledge Discovery and Data Mining Tools Competition）的数据集。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集可在 Azure Blob 下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://azuremlsampleexperiments.blob.core.windows.net/datasets/network_intrusion_detection.csv，其中包含了训练和测试数据集。训练数据集有大约 12.6 万行和 43 列，其中包含标签；3 列标签性质信息和 40 列数值与字符串/类别特征信息，都可用于训练该模型。测试数据集有大约 2.25 万个测试样本，和训练数据一样有 43 列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;rcv1-v2.topics.qrels.csv (https://azuremlsampleexperiments.blob.core.windows.net/datasets/rcv1-v2.topics.qrels.csv)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 RCV1-V2 新闻数据集中的新闻主题分配。一篇新闻可被分为多个主题。每一行的的格式是 1。该数据集包含 260 万个主题分配，由 David. D. Lewis 共享。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;student_performance.txt&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个数据集来自 KDD Cup 2010 学生表现评估挑战赛（student performance evaluation）。这个数据集已被 Algebra_2008_2009 训练集采用（Stamper, J., Niculescu-Mizil, A., Ritter, S., Gordon, G.J., &amp;amp; Koedinger, K.R.（2010））&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 KDD Cup 2010 教育数据挖掘挑战赛中的 Algebra I 2008-2009 数据集可以在该竞赛的网站中下载：http://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该数据集也可以在 Azure Blob 下载：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://azuremlsampleexperiments.blob.core.windows.net/datasets/student_performance.txt，其中的数据来自于学生辅导系统。其中提供了问题 ID 和简要描述，学生 ID，时间标记，同时还有学生在正确解决问题前的尝试次数。原数据集存储了 890 万条记录，这个数据集减少了取样数量，容量缩小至前 10 万行数据。这份数据每一条目有 23 个不同类型的分项，包括数值、类别和时间戳。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心经授权编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 揭秘微软量子计算研究：拓扑量子计算机</title>
      <link>http://www.iwgc.cn/link/3180601</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Nature&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Elizabeth Gibney&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;微软研究拓扑量子计算已经有十多年了。但是市面上关于其研究的具体内容并不多见。自然杂志采访了微软研究量子结构和计算小组成员 Alex Bocharov，他解释了为什么这家公司在量子计算上会选择一条不同于 IBM、谷歌等竞争对手的路。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一场打造「『通用型』的量子计算机」的竞赛。这种设备可以通过编程实现快速解决传统计算机无法解决的问题。它的出现会对制药、密码等领域带来革命性的变化。世界上多家重要的技术公司都在研究这一挑战，但是微软选择的方式比其对手更加曲折。IBM、谷歌和多家学术实验室选择了相对成熟的硬件，比如超导导线环（loops of superconducting wire），来制作量子比特（qubit）。由于它们具有在同一时间保持在开和关两种状态的混合或叠加态的能力，它们能驱动量子计算机做快速计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是微软却希望能以一种准粒子（quasiparticle）的状态编码量子比特：一种从物质的相互作用中出现的粒子状物体（object）。一些物理学家甚至还不确定微软在做的准粒子（叫非阿贝尔任意子，non-abelian anyons）是否真的存在。但是微软希望利用它们的拓扑性质，这种性质能使量子态对外界干扰具有极强的鲁棒性，来打造所谓的拓扑量子计算机。物质拓扑态的早期理论研究让三位物理学家赢得了今年的诺贝尔奖。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这家公司一直在研发拓扑量子计算，已经有十多年了。现在他们有研究人员为未来的机器编写软件并与学术实验室合作制作设备。Alex Bocharov 是一位数学家和计算机科学家，他微软研究的量子结构和计算组的成员。他对自然讲述了微软的这项研究。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDEWYhn9rUjrGpm9gse4l5P8Oz8OFVEAt10EP3qux38BsVicFuxFQDSmw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Alex Bocharov，&lt;span&gt;微软研究量子结构和计算组成员&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;微软是如何最终决定要做这个拓扑量子比特的？它可能是最难的量子计算硬件的。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们是以人为中心，而不是以问题为中心。和量子计算的领军人物，如 Alexei Kitaev、Daniel Gottesman，最值得注意的是，Michael Freedman，是我们量子计算团队发展的带头人。所以这是 Freedman 自己开拓的视野最终决定了做事情的方式，我们都跟着他。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); text-align: justify;"&gt;&lt;strong&gt;&lt;span&gt;IBM 和谷歌都在使用超导环作为他们的量子比特。你正尝试利用的量子比特是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的量子比特甚至都不是一种物质的东西。但是同样的&lt;/span&gt;&lt;span&gt;物理学家在对撞机中使用的基本粒子不是真正的坚实的物体。我们有的是非阿尔贝任意子（non-abelian anyons），比普通粒子更加模糊。它们是准粒子。被研究的最多的任意子种类出现在非常冷电子链中，而电子链被限制在一个二维表面的边缘。这些任意子的行为既像电子又像它对应的反粒子，它通常以密集的电导峰的形式在一维电子链的两端被观测到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;任意子状的粒子被作为一种独立的物体在 1937 年被首次预测，而且 Kitaev 在 1997 年也说过准粒子可以应用到量子计算机中。但是到了 2012 年，物理学家才首次宣称发现了它们。你可以肯定它们真的存在吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们非常确定这种最简单的物种确实存在。2012 年，荷兰代尔夫特科技大学的 Leo Kouwenhoven 观察到过它们。我不会说有 100% 的确定，但 Kouwenhoven 的观察已经被其他多家实验室再现过。这种激发（excitation）到底是什么并不重要，一旦这种粒子变得可以测量了，它们就可以用来执行计算了。现在的问题是，实验室正在把一些非常复杂的设备放在一起来产生大量的激发（excitation），并尝试开始做计算了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;任意子的开发似乎非常困难。那么相比其他种类的量子比特，使用任意子的优势又在哪里呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在大多数量子系统中，信息被编码到粒子的属性中，与周围环境最轻微的相互作用都会破坏它们的量子态。这意味着他们的操作精确度可能达到了 99.9%，我们成为三个九。在解决现实问题上，我们需要的精确度水平是十个九，所以你需要创造出一个大型阵列的量子比特，能让你来修正这些误差。拓扑量子计算有达到六个或七个九的潜力，这意味着我们不再需要做大量昂贵的误差校正了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;是什么让拓扑量子计算的鲁棒性这么好？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从环境和计算机的其他部分而来的噪音是不可避免的，这可能导致准粒子的位置与强度的波动。但是没问题，因为我们不会将信息编码到准粒子自己身上，但是我们会按顺序交换任意子的位置。我们称之为辫子，因为如果你画出在时间和空间上相邻的任意子对的一个交换序列，那么它们的轨迹线条看起来像辫子。该信息被编码成「拓扑」属性，也就是说，这个系统的集体属性只能跟随宏观运动而不是小波动来变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZYTxIZibal5MKlCHZtQkvDcSibrodBZ8PCCnAnsCQYgxx1fZLmEnmCcfclqZS15zCSQTd0VibXiaeaA/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;辫子的数学理论或许可以作为未来拓扑量子计算机的基础&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;微软已经研究拓扑两个字计算十多年了，这其中所需要大多数量子比特都是假设的。为什么你们会坚持到现在？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是值得的，因为它带来的好处是巨大的，几乎没有坏处。微软是一家经济实力雄厚的公司。如果坐拥 1000 亿美元的现金，你会投资什么呢？比尔盖茨也投资了其他东西——根除疟疾和艾滋病病毒——未来这些研究可能都会需要用到量子计算。比如基因学到目前为止一直借助的是传统计算机，而 100-200 个量子比特计算机可能会给基因学研究带来巨大的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;微软有多少人参与了量子计算研究，你们的投入是多少？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大约在 35 到 40 个人，但是我不想冒昧地谈论资金的问题，也给不出大概的估计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;你们的团队一直在为这种量子计算机开发软件，有什么成果吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到目前为止，我们已经有了一个令人惊讶的成果，创造出了一个更有效的算法，它能减少量子比特相互作用的次数，叫做门（gate），只需要运行必要的计算，而这在传统的计算机上是不可能的。比如，在本世纪初的那几年里，人们认为在量子计算机上计算植物在光合作用中用到的铁氧还蛋白能级（energy level）大约 240 亿年。现在通过理论、实践、工程和仿真相结合，最乐观的估计表明，这项计算可能只需要一小时左右。我们还在继续解决这些问题，并逐步转向更多的应用工作，我们想到了量子化学、量子基因学，以及能在一个小到中等大小的量子计算机上解决的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;这算是占领先机吗? 因为一个可以处理这些问题的量子计算机可能是十年以后的事情。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去的问题是，量子计算机在假设上会不会不传统计算机表现更好这样的问题是不是问题。现在我们不仅想弄清楚它是不是可行的，还有如何实现它？我们需要闯过层层迷雾来解开这些问题，因为我们相信它本身将会成为一个完整的领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 22 Oct 2016 14:00:56 +0800</pubDate>
    </item>
  </channel>
</rss>
