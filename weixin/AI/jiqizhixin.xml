<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>访谈 | 艾伦人工智能研究所CEO Oren Etzioni：深度学习离人类水平的人工智能还差得很远</title>
      <link>http://www.iwgc.cn/link/3222891</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Larry Greenemeier&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Elon Musk 正在全面押注自动驾驶汽车的新计划，他需要强大的人工智能技术来确保特斯拉汽车能够实时理解不同的驾驶情况，并据此实时地做出反应。人工智能正在实现非凡的成就：上周，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719745&amp;amp;idx=2&amp;amp;sn=242eda88fa9a5572e60b43e451a1549b&amp;amp;chksm=871b027fb06c8b693cff17f43553625f008fcb7965582119b651dbbd17e116e35dc801ebeec3&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719745&amp;amp;idx=2&amp;amp;sn=242eda88fa9a5572e60b43e451a1549b&amp;amp;chksm=871b027fb06c8b693cff17f43553625f008fcb7965582119b651dbbd17e116e35dc801ebeec3&amp;amp;scene=21#wechat_redirect"&gt;AlphaGo 计算机程序的创造者报告&lt;/a&gt;了他们的软件已经学会了像一个伦敦本地人一样在纷繁复杂的地铁线路中导航。甚至就连白宫也来凑热闹了，他们在不久前放出了一份报告，旨在帮助美国为机器能像人类一样思考的未来做好准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但已经在研究和尝试解决人工智能的基本问题上工作了几十年的计算机科学家 Oren Etzioni 说：在人们可以或应该将世界交给人工智能接管之前，人工智能还有很长的路要走。Etzioni 目前是艾伦人工智能研究所（AI2）的首席执行官；该组织是由微软的联合创始人 Paul Allen 于 2014 年组建的，该组织的目标是开发人工智能的潜在好处——以及纠正好莱坞乃至其他人工智能研究者鼓吹的人工智能可能威胁人类种族的观念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI2 自己的项目可能并不非常疯狂——比如说他们有一个基于人工智能的学术研究搜索引擎 Semantic Scholar（https://www.semanticscholar.org/）——但他们确实在解决推理（reasoning）等人工智能领域的问题，这将使得这一技术超越 Etzioni 所说的「只在一件事上做得非常好的狭隘的专家」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Scientific American 在纽约最近的一场人工智能会议上对 Etzioni 进行了采访，在采访中他表达了自己对企业过于鼓吹人工智能的当前能力——尤其是被称为深度学习的机器学习技术——的担忧。这个处理过程需要将大型的数据集通过模拟人脑神经网络的网络，以便教会计算机学会自己解决特定的问题，比如识别模式或确定照片中存在的特定物体。Etzioni 还解释了他为什么认为十岁孩童比谷歌 DeepMind 的 AlphaGo 程序更聪明，以及为什么终将需要开发人工智能「卫士」程序来防止人工智能程序变得危险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWyYlpIvclfEsMzcKOjSgwxPqkibXMs4caOmibV3KFLq9D3yN8FTXaibCnw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Oren Etzioni&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是经过编辑整理的采访内容：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;在开发最好的人工智能技术上，人工智能研究者前面还有什么鸿沟吗？&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;一些人已经靠他们自己取得了一点点领先。我们已经在语音识别、自动驾驶汽车（或者说自动驾驶的有限形式）以及当然的围棋等领域取得一些实质性的进展。所有这些都是非常实质的技术成就。但我们该怎样解读它们呢？深度学习无疑是一项很有价值的技术，但在创造人工智能上，我们还有很多其它的问题要解决，其中包括推理（意味着计算机不仅能计算 2+2=4，还能进行理解）和获取可被机器用于创造语境的背景知识。还有另一个例子是自然语言理解。尽管我们已经有 AlphaGo 了，但我们还没有一个能够读懂和完全理解一段话或甚至一句话的程序。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;有人说深度学习是「我们最好的（the best we have）」人工智能技术。那是对深度学习的批评吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;当你有大量进行了标注以使计算机能理解其含义的数据和大量算力，以及需要找到那些数据中的模式的时候，我们可以发现深度学习是无敌的。再次说到 AlphaGo 的例子，该人工智能程序为了学会在不同情形下的正确走子而处理过 3000 万个棋盘局面。还有很多类似的情形——比如放射图像——其中图像需要被标记为有肿瘤或没有肿瘤，一个经过调节的深度学习程序可被用来确定其之前看过的图片中是否有肿瘤。深度学习方面还有很多的工作要做，而且确实，这是前沿的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;那么问题出在哪里？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;问题是对于智能，除了有大量可用来训练程序的情况，还有很多其它情况。比如学生准备 SAT 或 New York Regents exams（大学入学考试）这样的标准考试时所用的数据。他们可不能通过考察之前的 3000 万份标注了「成功」或「不成功」的考试来获得高分。这是一个更为复杂的交互式的学习过程。智能还涉及到从建议、对话的语境或阅读书籍中学习。但是同样地，尽管深度学习领域有这些惊人的进步，我们也还是不能得到一个能做到十岁孩童所能做的事情的程序，即：拿起一本书，读一章，然后回答有关读到的内容的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;为什么人工智能通过标准考试可以成为这项技术的重大进步？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni ：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我们艾伦人工智能研究所实际上已经将其作为一个研究项目了。去年我们宣布设立了一个 50,000 美元的奖给任何开发出了能够通过标准的 8 年级科学考试的人工智能软件的人。来自全世界的 780 支团队用了几个月的时间想要达到这一成就，但没有人能够得到超过 60% 的分数——即使那只是 8 年级考试中的多项选择题。这为我们目前所处的现状提供了一个现实的和定量的评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;能够正确回答问题的人工智能系统是怎样的？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;线索通常在语言之中。最成功的系统使用了经过精心调制的科学文本和其它公共源的信息，然后再使用经过了精心微调的信息检索技术进行搜索以定位每个多项选择题中最好的候选答案。比如，下列物体中最好的电导体是：塑料勺子、木叉子或铁拍子？程序非常善于配对，可以检测到电和铁或导体和铁比塑料和导体远远更常见地共同出现在文档之中。所以有时候程序可以通过这样的捷径找到答案。这差不多也就是孩子们靠已知的信息猜答案的方法了。没有任何系统的得分超过 60%，我可以说这些程序都是在使用统计进行有根据的猜测，而不是对问题进行仔细的推理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;AlphaGo 背后的 DeepMind 团队现在已经有一个可以使用外部记忆系统超越深度学习的人工智能程序了。他们的成果对创造更像人的人工智能有什么影响？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;在推动深度神经网络（模拟人脑设计的人工智能）的发展上，DeepMind 将继续作为领导者。这个特定的贡献是很重要的，但也只是在图结构（比如一个地铁网络）中互连的事实上实现推理的一小步。已有的符号程序就可以轻松地执行这样的任务，但这里的成就（已经发表在 Nature 上）是关于神经网络如何根据样本学习执行任务。整体上看，是 DeepMind 的一大步，却是人类的一小步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;人们可以怎样结合深度学习、计算机视觉和记忆等方法来开发更完整的人工智能？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;这是一个非常吸引人的概念，实际上当我还是华盛顿大学的一位教授时的很多研究都是基于使用互联网作为人工智能系统的数据库的想法。我们开发了一个叫做开放信息提取（open-information extraction）的技术，它索引了 50 亿个网页，并且可以从其中提取句子并将其映射到机器可操作的知识上。该机器在抓取网页和所有句子上能力超强。问题是这些句子是在文本或图片中。我们人类的大脑有非常强大的能力——我们计算机科学家还没有破解这些能力——可以将那些行为映射到推理等等。为什么这种通用数据库和人工智能接口的想法仍然还是科学幻想呢？因为我们还没有搞清楚如何将文本和图像映射成机器可以用来像人类一样进行操作的东西。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;你提到过实现人类水平的人工智能至少还要 25 年的时间。人类水平的人工智能是什么意思？为什么会有这样的时间框架？&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;真正的自然语言理解、人类智能的广度和通用性、我们既能下围棋也能过马路还能制作好看的煎蛋的能力——这种多样性是人类智能的标志，而我们今天做的只是开发在一件小事上做得非常好的狭隘专家。为了得到这个时间框架，我询问了美国人工智能协会（AAAI）的研究者：什么时候我们将实现在广义上和人类一样聪明的计算机系统？没人说那会在未来 10 年内发生，67% 的人说会在接下来 25 年或更往后，25% 的人说「永远不会实现」。他们可能错了吗？可能吧。但你该相信谁，难道是那些紧握行业脉搏的人？还是好莱坞？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;为什么有如此多备受尊敬的科学家和工程师警告说人工智能会加害我们？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我很难推断霍金、Elon Musk 这群人如此讨论人工智能的动机是什么。我猜测可能是黑洞讨论一段时间之后有点无聊了，黑洞是一个慢速发展的主题。我想说的是，在他们以及我极为尊重的比尔盖茨在讨论人工智能会变得邪恶或潜在灾难性后果的时候，他们总是插入「最终」或者「可能」这样的限定符。这我是同意的。如果我们讨论千年以后或无限的未来时，人工智能有可能为人类带来末日吗？当然是有可能的，但我认为这种对未来的讨论不应该分散我们对人工智能与就业、人工智能与武器系统这样真正问题的注意。而且「最终」、「概念上」这样的限定符在转译时往往会丢失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;在人工智能有缺陷的情况下，人们应该担心汽车制造商对自动驾驶汽车不断扩大的兴趣吗？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我对没有方向盘或刹车踏板这样的自动驾驶汽车没有多大兴趣。以我对计算机视觉和人工智能的了解，我对这种汽车相当不舒服。但我是复合系统的粉丝，例如，在你开车瞌睡时它能为你踩刹车。司机加上自动系统要比任何单独一个都更安全。然而这并不简单，将新科技融入到人类生活与工作中不是件容易的事。但是，我不确定融合新科技的解决方案就是让汽车做所有的事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;谷歌、Facebook 和其他科技巨头最近合作建立了 Partnership on Artificial Intelligence to Benefit People and Society (https://www.partnershiponai.org/) 组织为人工智能研究设定最好的道德实践。是不是人工智能技术已经发展到足够的程度了，让他们能进行有意义的对话？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;世界上顶级的科技公司聚到一起想这些事是非常好的思路。我想他们如此做是为了应对人工智能将掌管世界的担心。很多对人工智能的担心完全过分了。即使我们有自动驾驶汽车，也不是说 100 辆车就聚到一起说是要，「拿下白宫」。Elon Musk 这群人提到的风险即使不是百年之后也是数十年之后的事。而且，我们存在一些真正的问题：自动化、数字科技和一般的人工智能真的在影响就业场景，无论是机器人还是其他情景，这才是真正的担心。自动驾驶汽车、卡车将来是会大幅度的改进安全性，但也会影响大量依靠驾驶为生的工人。该新组织应该讨论的另一件事是人工智能可能造成的歧视。如果人工智能技术被用于处理贷款或信用卡应用，他们会以合乎法律与道德的方式来做吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;你如何保证人工智能项目会合乎法律与道德的进行？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;例如你有一家银行，有处理贷款的软件项目，你也无法隐藏在软件的背后。说计算机这么做的并不能成为一个借口。即使计算机项目不使用种族或性别作为明确变量，它也可能有歧视行为。因为计算机项目会接触大量的变量、大量的统计数据，它可能找到邮政编码与其他变量之间的关系，这些关系会构成种族或性别变量的替代。如果它使用这种替代变量影响到决策，就会造成问题，而且人们难以检测或追踪缘由。所以我们提议的思路是人工智能卫士（AI guardian），也就是监测、分析基于人工智能的贷款处理程序的系统，从而保证程序随时间进化时能遵循法律、合乎道德。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;如今有人工智能卫士的存在吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我们正在向社区呼吁开始研究并建立这种东西。我认为如今可能有一些琐碎的存在，但这是目前一个很大的愿景。我们想用人工智能卫士的思路反击一直以来认为人工智能是邪恶的整体力量的普遍场景，就像终结者这样的好莱坞电影传播的那样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part3：word2vec 的秘密配方</title>
      <link>http://www.iwgc.cn/link/3222892</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Terrence L&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;本文是词嵌入系列博客的 Part3，介绍了流行的词嵌入模型全局向量&lt;span&gt;GloVe&lt;/span&gt;。 part2 请点击 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=2&amp;amp;sn=9fedc937d3128462c478ef7911e77687&amp;amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=2&amp;amp;sn=9fedc937d3128462c478ef7911e77687&amp;amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;amp;scene=21#wechat_redirect"&gt;技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法&lt;/a&gt;；Part1请点击 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;技术 | 词嵌入系列博客Part1：基于语言建模的词嵌入模型&lt;/a&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;br/&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWdbIqaJ0f3m5aeLIJyjFUdT9RYgMzn6XQ6Sog9orJTc5AhOOPFHm12Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;全局矢量（GloVe）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入量与分布式语义模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;超参数&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请原谅之前的噱头。这是一篇我很久之前就想要去写的博客。在这篇文章中，我想要强调那些使得 word2vec 成功的秘密成分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我特别要专注于通过神经模型训练的词嵌入与通过传统的分布式语义模型（DSMs）产生的词嵌入之间的联系。通过展示这些组分是如何被转移到 DSMs 中的，我将会证明分布式的方法是丝毫不逊色于流行的词嵌入方法的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然没有什么新的见解，但我感觉传统的方法经常被深度学习的热潮所掩盖，它们之间的相关性应该受到更多关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，这篇博客所依据的文献是 Levy 等人在 2015 年发表的通过词嵌入获得的提升分布式相似性的研究。如果你还没有阅读过，我建议你抓紧搜索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇公开的博客中，我将首先介绍一个流行的词嵌入模型 GloVe，然后我将突出词嵌入模型和分布式予语义方法之间的联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;紧接着，我将会介绍用来衡量不同因素影响的四款模型。之后我会给出除了算法选择之外其他学习词表示中额外因素的概述。最终我将呈现 Levy 等人的建议和结论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;全局矢量（&lt;strong&gt;GloVe&lt;/strong&gt;）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之前发布的那篇博客中，我们已经对流行的词嵌入模型进行了概述。我们遗漏的一个模型便是 GloVe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简而言之，GloVe 希望能明确表明 SNGS 的隐式操作：编码含义作为嵌入空间中的向量偏移——看起来只是一个偶然发现的 word2vec 的副产品——才是 GloVe 的特定目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体来说，GloVe 的作者表明两个词同现概率的比值（而不是它们的同现概率本身）是包含信息并计划作为向量差来编码信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了实现这一目标，他们提出了一种加权最小二乘法的物体 J，旨在最小化两个词的向量点积与它们共现次数的对数之间的差异。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYW5sHHSx6u0hyHW7DrzbBIm9lqBc2bkRBI63Bc65b5MNZCRuvePg68VQ/0?wx_fmt=png"/&gt;&lt;br/&gt;当 wi 和 bi 分别作为词语 i 的词向量和偏差，w~j 和 bj 分别作为词语 j 的文本词向量和偏差，Xij 是在词语 j 的文本中出现词语 i 的次数，而 f 是将相对低的权重分配给稀有和频繁共现的加权函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;共现次数可以被直接编码到词语上下文的共现矩阵中，GloVe 会将这样的矩阵而不是整个语料库作为输入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你想更多地了解 GloVe，最好的参考便是相关的论文或者附属网站&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（http://nlp.stanford.edu/projects/glove/）。除此之外，通过 Gensim 的作者，Quora 问答（https://www.quora.com/How-is-GloVe-different-from-word2vec）或是这篇发布的博客&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html），你可以对 GloVe 及其与 word2vec 的差异有更多的了解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;词嵌入与分布式语义模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入模型，尤其是 word2vec 和 GloVe 变得如此流行的原因是它们的表现似乎一直以来都显著优于 DSMs。许多人将此归因于 Word2vec 的神经架构或是它能预测词语这个事实，这看起来要比只靠共现计数有天然的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将 DSMs 看做计数模型，因为它们通过操作共现矩阵来计算词语的共现次数。相反，神经词嵌入模型可以被看作是一种预测模型，因为它们会去预测周围的词语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2014 年，Baroni 等人表明预测模型几乎在所有的任务中都优于计数模型，从而为词嵌入模型显而易见的优越性提供了一个清晰的证明。这就是终点了吗？并不是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经看到和 GloVe 的差异并不是那么明显：当 GloVe 被 Levy 等人认为是一个预测模型时，它显然是正在分解一个词语上下文共现矩阵，这使其更接近于诸如主成分分析（PCA）和潜在语义分析（LSA）等传统的方法。不止如此，Levy 等人还表示 word2vec 隐晦地分解了词语上下文的 PMI 矩阵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，虽然在表面上 DSM 和词嵌入模型使用不同的算法来学习词语表示——前者计数，后者预测——但从根本上来说，两种类型的模型表现了相同的底层数据统计，即词语间的共现次数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，仍然存在同时也是这篇博客剩下的部分想要回答的一个问题是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么词嵌入模型的表现仍然比几乎拥有相同信息的 DSM 更好？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;继续 Levy 等人 2015 年的成果，我们将分离和鉴定影响神经词嵌入模型的因素并展示它们是如何通过比较以下四个模型来被转移到传统方法中的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;PPMI：PMI 是衡量两个词之间相关性强度的一个常用指标。它是两个词 w 和 c 之间联合概率与边际概率乘积之间的对数比：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWOia5bxY5VCxkoKfhwib0ahN34EjdflZh0icBibz0SbHyRVBhjVrq41hrtg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;PMI(w,c)=logP(w,c)P(w)P(c)。由于词对（w,c）的 PMI(w,c)=log0=−∞从未出现，所以 &amp;nbsp; &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;实际上 PMI 经常被 PPMI 替代，其将负值看作是 0，即：PPMI(w,c)=max(PMI(w,c),0)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;奇异值分解（SVD）：SVD 是最流行的降维方法之一，最初是通过潜在语义分析（LSA）进入自然语言处理（NLP）。SVD 将词文本共现矩阵转化为三阶矩阵 &amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWEPtaqjOxvrzHIOazkc2VaEvS3RGzibS3iaZAxQUE25J0f4aEQUWX7UYg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;的产物，其中 U 和 V 是正交矩阵（即方形矩阵的行和列是正交单位向量），Σ是特征 &amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;值在减弱过程中的对角矩阵。实际上，SVD 经常被用于因式分解由 PPMI 产生的矩&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;阵。一般来说，只有Σ顶端的 d 元素被保存，从而得到：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWKA91ZiaCZyIckSFrmbv1LTibAMWxx8XawYFNdA1Op885Ndrs4ibzEPQVg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;通常被分别用来表示词语和上下文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基于负采样的 Skip-gram 模型也就是 word2vec：要了解更多 skip-gram 结构和负采样可以参考我之前的博客文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;全局矢量（GloVe）已经在上一节中介绍过。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们来看看以下这些超参数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;预处理&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态上下文窗口&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;常用词下采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;删除罕见词语&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联度量&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;转移&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;上下文分布平滑&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;后期处理&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;添加上下文向量&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;特征值加权&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;向量规范化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;预处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Word2vec 引入了三种预处理语料库的方法，这三种方法也可以很容易地应用于 DSM。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动态上下文窗口&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一般来说，在 DSM 中，上下文窗口并未加权，并且具有恒定大小。然而，SGNS 和 GloVe 使用了一种会将更多权重分配给更接近的词语的方案，因为更接近的词通常被认为对词的意义更重要。此外，在 SGNS 中，窗口大小不是固定的，但实际窗口大小是动态的，并且在训练期间在 1 和最大窗口之间均匀采样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;常用词下采样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SGNS 通过概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWzj0badbicGXQHy7tgY8viadtgzbSt038w8E4bsicXb6rtvvvEzTFicl0Hg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来随机去除频率 f 高于某个阈值 t 的词语，从而获得那些非常频繁出现的词语。由于这种下采样是在实际创建窗口之前完成的，因此 SGNS 在实践中所使用的上下文窗口要大于实际所指示的上下文窗口。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;删除罕见词语&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 的预处理中，罕见词语也在创建上下文窗口之前被删除，这进一步增加了上下文窗口的实际大小。虽然 Levy 等人在 2015 年发现这并不会对性能产生什么重大的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;关联度量&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI 已被证明是用于测量词语之间关联程度的有效方式。Levy 和 Goldberg 在 2014 年已经表示 SGNS 对 PMI 矩阵进行隐含地因式分解，因此可以将该公式的两个变化引入到常规 PMI 中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI 转移&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 中，负样本 K 的数量越多，使用的数据就越多，参数的估计也越好。K 影响了有 word2vec 隐性因式分解的 PMI 矩阵偏移，即 k 通过 log k 来转移 PMI 值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们将其转换为常规 PMI，我们获得 Shifted PPMI（SPPMI）：SPPMI(w,c)=max(PMI(w,c)−logk,0)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上下文分布平滑&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 中，根据平滑的一元分布，即提高到α的幂的一元分布来对负样本进行采样，并根据经验将其设置为 34。这会导致频繁的词语被采样的几率要比它们频率所指示的相对较少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以通过将上下文词汇 f（c）的频率同等地提高到α的幂来将其传送到 PMI：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI（w，c）= log p（w，c）p（w）pα（c）其中 pα（c）= f（c）αΣcf（c）α和 f（x）是字 x 的频率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;后期处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类似于预处理，可以使用三种方法来修改由算法产生的词向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;添加上下文向量&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GloVe 的作者建议添加词嵌入和上下文向量以创建最终输出向量，例如： v⃗cat =w⃗cat +c⃗catv→cat = w→cat + c→cat。这增加了一阶相似性项，即 w⋅v。然而，该方法不能应用于 PMI，因为 PMI 产生的是稀疏向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;特征值加权&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SVD 产生以下矩阵：WSVD = Ud·Σd 和 CSVD = Vd。然而，这些矩阵具有不同的性质：CSVD 是标准正交的，而 WSVD 不是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相反，SGNS 更对称。因此，我们可以用可调整的附加参数 pp 来对特征值矩阵Σd 加权，以产生以下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;WSVD = Ud·Σpd。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;向量规范化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们还可以将所有向量归一化为单位长度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy 等人在 2015 年训练了英文维基百科所有转储模型，并基于常用词语的相似性和类比数据集对它们进行了评价。你可以在他们的论文中了解有关实验设置和培训详情的更多信息。我们在下文总结出了最重要的结果和收获。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外收获&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy 等人发现 SVD——而不是词嵌入算法的其中一种——在相似性任务上执行得最好，而 SGNS 在类比数据集上执行得最好。他们还阐明了与其他选择相比，超参数的重要性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 超参数与算法：超参数设置通常比算法选择更重要。没有任意单一的算法能始终胜过其他方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 超参数与更多数据：在更大的语料库上训练对某些任务有帮助。在 6 个例子中有 3 个都能说明，调整超参数更有益。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;揭露之前的观点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了这些见解，我们现在可以揭露一些普遍存在的观点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;嵌入式比分布式方法优秀吗？使用正确的超参数，没有一种方法比另一种方法具有持续的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;GloVe 是否优于 SGNS？SNGS 在所有任务上都胜过 GloVe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;CBOW 是不是很好的 word2vec 配置？CBOW 在任何任务上都比不上 SGNS。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;建议&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后——也是这篇文章中我最喜欢的一部分——我们可以给出一些具体的实际建议：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不要将迁移的 PPMI 与 SVD 一起使用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不要「正确」使用 SVD，即不使用特征向量加权（与使用 p = 0.5 的特征值加权相比性能下降 15 个点）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请使用具有短上下文（窗口大小为 22）的 PPMI 和 SVD。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请使用 SGNS 的许多负样本。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对于所有方法，请始终使用上下文分布平滑（将一元分布提高到α= 0.75 的幂）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 SGNS 作为基准（训练更加稳健，快速和经济）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请尝试在 SGNS 和 GloVe 中添加上下文向量。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些结果与通常假设的情况背道而驰，即词嵌入优于传统方法，并且表明它通常没有什么区别，无论使用词嵌入还是分布式方法 - 重要的是，你调整超参数并使用适当的预处理和后期处理步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 Jurafsky 小组 [5 , 6 ] 的最新论文回应了这些发现，并表明 SVD——而不是 SGNS——通常是当你关心精确词语表达时的首选。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我希望这篇博客对于目前备受关注的，揭示传统分布语义和嵌入模式之间的联系的研究能够有所帮助。正如我们所看到的，分布式语义的知识使得我们可以改进当前的方法并开发现有方法的全新变体。为此，我希望下一次训练词嵌入时，您会将分布式方法纳入考虑范围，或从这些思考中获益。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考文献：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy, O., Goldberg, Y., &amp;amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570 ↩&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Baroni, M., Dinu, G., &amp;amp; Kruszewski, G. (2014). Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. ACL, 238–247. http://doi.org/10.3115/v1/P14-1023&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hamilton, W. L., Clark, K., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Retrieved from http://arxiv.org/abs/1606.02820&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hamilton, W. L., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. arXiv Preprint arXiv:1605.09096.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>首发 | B轮融资3000万美元，Clarifai要让每个人都用上人工智能</title>
      <link>http://www.iwgc.cn/link/3222893</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心中文首发&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;编辑：李泽南、&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;杜夏德&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Clarifai 今日宣布完成 3000 万美元的 B 轮融资。该轮融资由 Menlo Ventures 领投，其他投资方，包括 Union Square Ventures、Lux Capital、高通、Osage University Partners 等&lt;span&gt;均参与过之前轮次的投资&lt;/span&gt;。经过本轮融资，Clarifai 已累计获得了 4125 万美元资金。Clarifai &lt;span&gt;创立于 2013 年，&lt;/span&gt;是一家专注于人工智能图像识别领域的创业公司。自 ImageNet 2013 年竞赛获得图像分类前五名之后，Clarifai 一直处于行业领先地位。现在， Clarifai 的图像识别系统每月分类超过 12 亿个图片和视频。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;Menlo Ventures 常务董事 Matt Murphy 评论道，「我们正处在漫长的人工智能时代的开端。从创建伊始，Clarifai 就将目标定位于让所有人接触到人工智能的力量，他们的队伍始终在为这一愿景而努力着，而 Menlo Ventures 很高兴能为这样的公司提供助力。」&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们对 Clarifai 的信心不仅来源于公司创始人 Zeiler ，他对于成功的渴望驱动着他的团队不断拓展人工智能的新领域；我们也相信除了科技巨头之外，大众化的人工智能也需要这样独立公司的存在。」Lux Captial 高级合伙人 Zavain Dar 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;新的融资将会加速 Clarifai 开发和商务队伍的扩张，标志着公司将会招揽更多高端研究人才，持续快速地发展其人工智能产品。Clarifai 目前已拥有一套独立的人工智能和机器学习平台，可为各种开发者与不同行业提供服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此轮投资消息正值 Clarifai 最新产品推出之际：前不久，公司刚刚发布了自定义训练和视觉搜索。自定义训练将人工智能的能力传递到每个人手中，让每个人都能在数秒内为 Clarifai 的视觉 API 「教授」新概念，无论你是否拥有专业技术。视觉搜索允许使用者通过图片相似性或关键词轻松地编排、访问、或向他人推荐图片或产品，使开发者和企业能够更好地让用户找到他们想要的。这种先进的技术，以往只能在大型科技公司的产品中找到，Clarifai 希望通过他们的 API，让其成为人人都可以使用的工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWNVErgxT5kVzQHsKRVokw7FpVE5hYOBGxo444HjicAIwcmqMLSxBFiafA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;不同于谷歌 Cloud Vison，IBM Watson，微软的通用和领域模型，Clarifai 认为人工智能的未来在于让每个人训练自己的个性化模型&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Clarifai 的视觉识别 API 可以识别超过 11,000 种不同内容的照片和视频，公司同时也提供应用于特定领域的识别工具，包括成人内容（NSFW）识别模型，用于识别可能违规的裸体图像；旅游模型，可以识别旅游有关的概念如「热水浴缸」，「儿童区」，和「室内游泳池」；公司最近还推出了食物模型，将识别提高到了食品原料的程度。Clarifai 有着多元化的客户群体，从《财富》 500 强公司到小型的开发团队都是他们的服务对象，包括 Buzzfeed，Trivago，500px，StyleMePretty 等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Clarifai 是在让所有人都能使用人工智能的理想之下建立的，不论使用者的财力，硬件条件与技术水平如何，Clarifai 可以帮助他们提升自己的产品，改善人们的生活，」Clarifai 创始人和首席执行官 Matt Zeiler 说道。「作为一家独立公司，我们拥有独一无二的优势，我们可以快速决策，保持高速创新，而且始终如一。所有这些都是为了利用数据向合作伙伴提供专属的业务需求。这笔新的资金会加速我们的创新，同时继续我们的使命，我们将创造一个强大的人工智能平台，让人工智能为所有人所用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©机器之心中文首发，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>机器之心招聘：我们带你去未来</title>
      <link>http://www.iwgc.cn/link/3222894</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;qqmusic class="res_iframe qqmusic_iframe js_editor_qqmusic" scrolling="no" frameborder="0" musicid="5050285" mid="0039LBAL1jOfnM" albumurl="/E/p/004HJb7I0kQaEp.jpg" audiourl="http://ws.stream.qqmusic.qq.com/C1000039LBAL1jOfnM.m4a?fromtag=46" music_name="A&amp;nbsp;Thousand&amp;nbsp;Miles" commentid="150771740" singer="David&amp;nbsp;Archuleta&amp;nbsp;-&amp;nbsp;A&amp;nbsp;Thousand&amp;nbsp;Miles" play_length="263000" src="/cgi-bin/readtemplate?t=tmpl/qqmusic_tmpl&amp;amp;singer=David%20Archuleta%20-%20A%20Thousand%20Miles&amp;amp;music_name=A%20Thousand%20Miles"&gt;&lt;/qqmusic&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心是国内领先的前沿科技媒体和产业服务平台，关注人工智能、机器人和神经认知科学，坚持为从业者提供高质量内容和多项产业服务。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体业务方面，机器之心最早在微信公众号平台开始运营，目前已经覆盖了微信、今日头条、百度百家、腾讯内容开放平台等多个大型内容平台，并运营着自己的官方网站。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体之外，机器之心还将依托联想之星Comet Labs的全球资源平台为人工智能领域的参与者提供各项产业服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为生产更多的高质量内容，提供更好的产业服务，我们需要更多的小伙伴加入进来！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线上运营（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司内容产品线上微信、微博、今日头条等渠道的运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.负责上述渠道推广效果分析和经验总结，建立有效运营手段；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.配合线下活动运营进行策划执行工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.配合内容团队进行选题策划和对外推广。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.1-2 年新媒体运营相关经验，具备一定的文字功底，有线下活动组织和策划经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.熟悉微信后台操作，有多渠道沟通经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.善于沟通交流，有一定抗压和创新能力，强责任心和高执行力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线下活动运营（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司线下活动的策划与运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.与内容团队和线上运营共同策划相关选题；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.执行公司商务需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.较强的沟通能力和资源整合能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.执行力强，具备一定的创新能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.1-2 年线下活动组织和策划经验，有活动相关供应商资源者优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;商务总监（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司特定行业客户的商务合作推动与对接，维护与建设积极的客户关系；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.参与制定公司的商务目标、原则、计划和战略，建立完善公司的商务体系及相关制度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.负责重大业务商务谈判的策略制定和执行以及合同的签订；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.负责拓展新的业务渠道，拓展特定行业的典型大客户，并跟踪协调执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，3 年以上工作经验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.良好的沟通、协调和商务谈判能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.具有较强的业务开拓能力、市场洞察力和行业分析能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.有较强责任感和抗压能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;记者（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;工作职责：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责人工智能和前沿技术领域的常规内容生产，撰写人物故事和产业报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.有独立的选题策划和操作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.挖掘国内外人工智能领域的优秀创业公司并进行报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.对国内外人工智能领域的创业者、公司高管、行业专家、科研专家进行深度专访；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.协助其他部门完成相关工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;岗位要求：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.优秀的写作能力，能够驾驭特写、人物故事、常规报道和资讯等各类内容形式；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.对前沿科技充满兴趣和热情，拥有迅速掌握某个特定行业或领域的学习能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对内容有品味，文字功底深厚，执行力强；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.1-2 年科技媒体从业经历， 能够适应新媒体和创业公司的工作节奏，有良好的职业精神和团队意识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;全职编译（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.编译、校对英文文章；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.撰写技术、产品、公司和行业相关文章，撰写分析报告。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.卓越的英语翻译能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.英语或计算机相关专业毕业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对前沿技术有一定了解和热情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;高级分析师（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.人工智能行业数据监测、分析及行业研究报告的撰写；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2. 负责与人工智能领域内各企业的沟通及定期跟踪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，计算机科学、商科、社会学或相关专业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.具备一定的数据分析和洞察能力，对ha数据拥有一定敏锐度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟练使用相关数据分析工具并进行信息搜集整理、图表制作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.良好的文字功底和写作能力和英文阅读能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Web 前端开发（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责机器之心网站部分页面及交互优化修改；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.跟远程后台人员合作，优化整个系统，参与站点维护工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对现有产品的可用性测试和评估提出改进方案，持续优化产品的用户体验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.积极参与工作相关技术研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.拥有良好的口头表达能力、善于学习新的技术；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.年以上 web 前端开发经验，懂 nginx 或者 Apache 操作，有一定的 Linux 系统操作经验，熟悉常用命令；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟悉常用的 js 库，例如 jquery，需掌握 amazeui 敏捷开发框架（bootstrap 亦可）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.熟悉常用的前端 MVC 框架，必须熟悉 gulp、sass；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.具有良好的沟通能力和团队协作能力, 能够与产品经理, 项目经理, 形成良好, 有效的沟通。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;实习生（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.网站、新媒体内容更新；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.翻译、校对英文文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.协助线下活动运营。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.出色的英语阅读和中文写作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.每周能保证 2-3 天坐班；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.积极的学习态度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.计算机科学等理工科专业、英语专业专业优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有意者请将简历发送至：hr@jiqizhixin.com，或添加微信 JuveAlex，zhaoyunfeng1984&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 在语音识别这件事上，汉语比英语早一年超越人类水平（附论文）</title>
      <link>http://www.iwgc.cn/link/3204725</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：吴攀、李亚洲&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;几天前，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;微软语音识别实现了历史性突破&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;，英语的语音转录达到专业速录员水平，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心也独家专访了专访微软首席语音科学家黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt; ，了解到词错率仅 5.9% 背后的「秘密武器」——CNTK。但微软的成果是在英语水平上的，从部分读者留言中我们了解到对汉语语音识别的前沿成果不太了解，这篇文章将向大家介绍国内几家公司在汉语识别上取得的成果（文中提到的论文可点击阅读原文下载）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;10 月 19 日，微软的这条消息发布之后在业内引起了极大的关注。语音识别一直是国内外许多科技公司发展的重要技术之一，微软的此次突破是识别能力在英语水平上第一次超越人类。在消息公开之后，百度首席科学家吴恩达就发推恭贺微软在英语语音识别上的突破，同时也让我们回忆起一年前百度在汉语语音识别上的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cviag8vMQYUdriajb2nmBmvwSBT0tbKoDFMSGlutZ3JdicAPn38IQvCRiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;吴恩达：在 2015 年我们就超越了人类水平的汉语识别；很高兴看到微软在不到一年之后让英语也达到了这一步。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;百度 Deep Speech2，汉语语音识别媲美人类&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 12 月，百度研究院硅谷人工智能实验室（SVAIL）在 arXiv 上发表了一篇论文《Deep Speech 2: End-to-End Speech Recognition in English and Mandarin（Deep Speech 2：端到端的英语和汉语语音识别）》，介绍了百度在语音识别技术的研究成果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9clIVppxH7FBeCf98MwHicHyqadAuqKtGZM8AQcCe00XpXEwcYQBYXN3Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们的研究表明一种端到端的深度学习（end-to-end deep learning）方法既可以被用于识别英语语音，也可以被用于识别汉语语音——这是两种差异极大的语言。因为用神经网络完全替代了人工设计组件的流程，端到端学习让我们可以处理包含噪杂环境、口音和不同语言的许多不同的语音。我们的方法的关键是 HPC（高性能计算）技术的应用，这让我们的系统的速度超过了我们之前系统的 7 倍。因为实现了这样的效率，之前需要耗时几周的实验现在几天就能完成。这让我们可以更快速地迭代以确定更先进的架构和算法。&lt;strong&gt;这让我们的系统在多种情况下可以在标准数据集基准上达到能与人类转录员媲美的水平。&lt;/strong&gt;最后，通过在数据中心的 GPU 上使用一种叫做的 Batch Dispatch 的技术，我们表明我们的系统可以并不昂贵地部署在网络上，并且能在为用户提供大规模服务时实现较低的延迟。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文中提到的 Deep Speech 系统是百度 2014 年宣布的、起初用来改进噪声环境中英语语音识别准确率的系统。在当时发布的博客文章中，百度表示在 2015 年 SVAIL 在改进 Deep Speech 在英语上的表现的同时，也正训练它来转录汉语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当时，百度首席科学家吴恩达说：「SVAIL 已经证明我们的端到端深度学习方法可被用来识别相当不同的语言。我们方法的关键是对高性能计算技术的使用，相比于去年速度提升了 7 倍。因为这种效率，先前花费两周的实验如今几天内就能完成。这使得我们能够更快地迭代。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=f0339hys92j&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别技术已经发展了十多年的时间，这一领域的传统强者一直是谷歌、亚马逊、苹果和微软这些美国科技巨头——据 TechCrunch 统计，美国至少有 26 家公司在开发语音识别技术。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是尽管谷歌这些巨头在语音识别技术上的技术积累和先发优势让后来者似乎难望其项背，但因为一些政策和市场方面的原因，这些巨头的语音识别主要偏向于英语，这给百度在汉语领域实现突出表现提供了机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为中国最大的搜索引擎公司，百度收集了大量汉语（尤其是普通话）的音频数据，这给其 Deep Speech 2 技术成果提供了基本的数据优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过有意思的是，百度的 Deep Speech 2 技术主要是在硅谷的人工智能实验室开发的，其研究科学家（名字可见于论文）大多对汉语并不了解或说得并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这显然并不是问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Deep Speech 2 在汉语上表现非常不错，但其最初实际上并不是为理解汉语训练的。百度美国的人工智能实验室负责人 Adam Coates 说：「我们在英语中开发的这个系统，但因为它是完全深度学习的，基本上是基于数据的，所以我们可以很快地用普通话替代这些数据，从而训练出一个非常强大的普通话引擎。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c0icKPaM9FU1ic1UjIzLkkeuQr75iaAzSd9t52W5wb7MNa3SOGvGaRa82Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于英语和普通话的 Deep Speech 2 系统架构，它们之间唯一的不同是：普通话版本的输出层更大（有 6000 多个汉语字符），而英语版本的只有 29 个字符。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该系统能够识别「混合语音（hybrid speech）」——很多普通话说话人会组合性地使用英语和普通话。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Deep Speech 2 于 2015 年 12 月首次发布时，首席科学家吴恩达表示其识别的精度已经超越了 Google Speech API、wit.ai、微软的 Bing Speech 和苹果的 Dictation 至少 10 个百分点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据百度表示，到今年 2 月份时，Deep Speech 2 的短语识别的词错率已经降到了 3.7%（参考阅读：&lt;span&gt;http://svail.github.io/mandarin/&lt;/span&gt;）！Coates 说 Deep Speech 2 转录某些语音的能力「基本上是超人级的」，能够比普通话母语者更精确地转录较短的查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度在其技术发展上大步迈进，Deep Speech 2 目前已经发展成了什么样还很难说。但一项技术终究要变成产品和服务才能实现价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;科大讯飞的语音识别&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度的 Deep Speech 识别技术是很惊人，但就像前文所说一项技术终究要变成产品和服务才能实现价值，科大讯飞无疑在这方面是做得最好的公司之一。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科大讯飞在自然语言处理上的成就是有目共睹的，在语音识别上的能力从最初到现在也在不断迭代中。2015 年 9 月底，机器之心对&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect"&gt;胡郁的一次专访中&lt;/a&gt;，他就对科大讯飞语音识别技术的发展路线做过清晰的介绍：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞很好地跟随了语音识别的发展历史，深度神经网络由 Geoffrey Hinton 与微软的邓力研究员最先开始做，科大讯飞迅速跟进，成为国内第一个在商用系统里使用深度神经网络的公司。谷歌是最早在全球范围内大规模使用深度神经网络的公司，谷歌的 Voice Search 也在最早开创了用互联网思维做语音识别。在这方面，科大讯飞受到了谷歌的启发，在国内最早把涟漪效应用在了语音识别上面，因此超越了其他平台。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞最初使用隐马尔可夫模型，后面开始在互联网上做，2009 年准备发布一个网页 demo，同年 9 月份安卓发布之后开始转型移动互联网，并于 2010 年 5 月发布了一个可以使用的手机上的 demo；2010 年 10 月份发布了语音输入法和语音云。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;整个过程中最难的地方在于，当你不知道这件事情是否可行时，你能够证明它可行。美国那些公司就是在做这样的事情。而科大讯飞最先领悟到，并最先在国内做的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到今年 10 月份刚好过去一年，科大讯飞的语音识别技术在此期间依然推陈出新，不断进步。去年 12 月 21 日，在北京国家会议中心召开的以「AI 复始，万物更新」为主题的年度发布会上，科大讯飞提出了以前馈型序列记忆网络（FSMN, Feed-forward Sequential Memory Network）为代表的新一代语音识别系统。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cXGicdTiciazC0I6f8l77vqeEoGMRrULEcqV6dyaDwescVk19W9wj4PGDQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在此论文中，我们提出了一种新的神经网络架构，也就是前馈型序列记忆网络（FSMN），在不使用循环前馈的情况下建模时间序列中的 long-term dependency。此次提出的 FSMN 是一个标准的全连接前馈神经网络，在其隐层中配备了一些可学习的记忆块。该记忆块使用一个抽头延时线结构将长语境信息编码进固定大小的表征作为短期记忆机制。我们在数个标准的基准任务上评估了 FSMN，包括语音识别和语言建模。实验结果表明，FSMN 在建模语音或语言这样的序列信号上，极大的超越了卷积循环神经网络，包括 LSTM。此外，由于内在无循环模型架构，FSMN 能更可靠、更快速地学习。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来通过进一步的研究，在 FSMN 的基础之上，科大讯飞再次推出全新的语音识别框架，将语音识别问题重新定义为「看语谱图」的问题，并通过引入图像识别中主流的深度卷积神经网络（CNN, Convolutional Neural Network）实现了对语谱图的全新解析，同时打破了传统深度语音识别系统对 DNN 和 RNN 等网络结构的依赖，最终将识别准确度提高到了新的高度。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来，科大讯飞又推出了全新的深度全序列卷积神经网络（Deep Fully Convolutional Neural Network, DFCNN）语音识别框架，使用大量的卷积层直接对整句语音信号进行建模，更好的表达了语音的长时相关性，比学术界和工业界最好的双向 RNN 语音识别系统识别率提升了 15% 以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBVRabcLJkjdEGhbwAo4ITz6PAOAEYhlPpV44E9gY5mKna244FKmicFA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;DFCNN 的结构图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DFCNN 的结构如图所 示，DFCNN 直接将一句语音转化成一张图像作为输入，即先对每帧语音进行傅里叶变换，再将时间和频率作为图像的两个维度，然后通过非常多的卷积层和池化（pooling）层的组合，对整句语音进行建模，输出单元直接与最终的识别结果（比如音节或者汉字）相对应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;搜狗语音识别&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;纵观整个互联网行业，可以说搜狗作为一家技术型公司，在人工智能领域一直依靠实践来获取更多的经验，从而提升产品使用体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在前几天的锤子手机新品发布会上罗永浩现场演示了科大讯飞的语音输入之后，一些媒体也对科大讯飞和搜狗的输入法的语音输入功能进行了对比，发现两者在语音识别上都有很不错的表现。比如《齐鲁晚报》的对比结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;值得一提的是，得益于创新技术，搜狗还拥有强大的离线语音识别引擎，在没有网络支持的情况下依旧可以做到中文语音识别，以日常语速说话，语音识别仍然能够保持较高的准确率。这一点科大讯飞表现也较为优秀，两者可谓旗鼓相当。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;整体体验下来，搜狗在普通话和英文的语音输入方面表现，与讯飞相比可以说毫不逊色，精准地识别能力基本可以保证使用者无需进行太多修改。此前在搜狗的知音引擎发布会上，搜狗语音交互技术项目负责人王砚峰称「搜狗知音引擎具备包括端到端的语音识别、强大的智能纠错能力、知识整合使用能力以及多轮对话和复杂语义理解能力」，这些都有效保证了搜狗语音输入在识别速度、精准度、自动纠错、结合上下文语意理解纠错方面收获不错的表现。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;八月份，搜狗发布了语音交互引擎——知音，其不仅带来了语音识别准确率和速度的大幅提升，还可以与用户更加自然的交互，支持多轮对话，处理更复杂的用户交互逻辑，等等。知音平台体现出搜狗在人工智能技术领域的长期积累，同时也能从中看出他们的技术基因和产品思维的良好结合。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cfJmmVV0ZbP4CBWibicb3SlSQjNya8q0BtjlBiauIQNo7tvoWpl10YcFkA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;搜狗知音引擎&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;搜狗把语音识别、语义理解、和知识图谱等技术梳理成「知音交互引擎」，这主要是强调两件事情，一是从语音的角度上让机器听的更加准确，这主要是识别率的提升；另一方面是让机器更自然的听懂，这包括在语义和知识图谱方面的发展，其中包括自然语言理解、多轮对话等技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cV8PxINicWJXJ9FEawYWavia1CNzde2ibyot94tV8ibSibIw0tUoFNA2gdvw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;语音识别系统流程：语音信号经过前端信号处理、端点检测等处理后，逐帧提取语音特征，传统的特征类型包括 MFCC、PLP、FBANK 等特征，提取好的特征送至解码器，在声学模型、语言模型以及发音词典的共同指导下，找到最为匹配的词序列作为识别结果输出。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKZW6DiaAocs5146zTGq6G0RBwNr4SuZgnYUOPJTIibRuiauDAaZUYLOSA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;CNN 语音识别系统建模流程&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据搜狗上个月的一篇微信公众号文章写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;在语音及图像识别、自然语言理解等方面，基于多年在深度学习方面的研究，以及搜狗输入法积累的海量数据优势，搜狗语音识别准确率已超 97%，位居第一。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过遗憾的是，搜狗还尚未公布实现这一结果的相关参数的技术信息，所以我们还不清楚这样的结果是否是在一定的限定条件下实现的。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像TechCrunch 统计的美国有 26 家公司开发语音识别技术一样，中国同样有一批专注自然语言处理技术的公司，其中云知声、思必驰等创业公司都在业内受到了极大的关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLnHUIeJZIyticlic6SpxIt7b3dIdAldSU3w3tCgj84DXeHOSzbC6tbEQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上图展示了云知声端到端的语音识别技术。材料显示，云知声语音识别纯中文的 WER 相对下降了 20%，中英混合的 WER 相对下降了 30%。&lt;br/&gt;&lt;br/&gt;在今年 6 月机器之心对云知声 CEO 黄伟（参见：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect"&gt;专访云知声CEO黄伟：如何打造人工智能「云端芯」生态闭环&lt;/a&gt;）的专访中，黄伟就说过 2012 年年底，他们的深度学习系统将当时的识别准确率从 85% 提升到了 91% 。后来随着云知声不断增加训练数据，如今识别准确率已经能达到 97% ，属于业内一流水平，在噪音和口音等情况下性能也比以前更好。&lt;br/&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect"&gt;思必驰的联合创始人兼首席科学家俞凯&lt;/a&gt;是剑桥大学语音博士，上海交大教授。他在剑桥大学待了 10 年，做了 5 年的语音识别方面的研究，后来做对话系统的研究。整体上，思必驰做的是语音对话交互技术的整体解决方案，而不是单纯的语音识别解决方案。因此在场景应用中，思必驰的系统和科大讯飞的系统多有比较，可相互媲美。&lt;br/&gt;&lt;br/&gt;当然，此领域内还有其他公司的存在。这些公司都在努力加速语音识别技术的提升。语音识别领域依然有一系列的难题需要攻克，就像微软首席语音科学家黄学东接受机器之心专访时所说的那样，「理解语义是人工智能下一个需要攻克的难题，要做好语音识别需要更好的语义理解，这是相辅相成的。」&lt;/span&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</title>
      <link>http://www.iwgc.cn/link/3204726</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part2，全面介绍了词嵌入模型， Part1请点击&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" style="font-size: 12px; text-decoration: underline;"&gt;&lt;em&gt;&lt;span&gt;技术 | 词嵌入系列博客Part1：基于语言建模的词嵌入模型&lt;/span&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csHIKEblezwKfQKC9eNPNmd6x1ONG6bMsRjOsJHz6XDqTKEtFx4ev8A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于Softmax的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层次Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微分Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNN-Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于采样（Sampling）的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有适应的重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目标采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;噪音对比估计&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;低频的标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其他方法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;选择哪一种方法？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文是我们词嵌入及其表示系列的第二篇。在上一篇博文里，我们提供了词嵌入模型的概述，并介绍了 Bengio 等人在2003年提出的经典神经语言学习模型、Collobert 和 CWeston 在2008年提出的 C&amp;amp;W 模型，以及Mikolov 在2013年提出的 word2vec 模型。我们发现，设计更好的词嵌入模型的最大挑战，就是如何降低softmax 层的计算复杂度。而且，这也是机器翻译（MT）（Jean等人[10 ]）和语言建模（Jozefowicz等人[6 ]）的共通之处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇博文里，我们将要重点介绍过去几年的研究中 softmax 层的不同近似方法，它们其中的一些被运用在语言建模和机器学习。在下一篇博文里，我们才会介绍别的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了统一以及便于比较，让我们简单重新介绍一下上一篇博文的重点：我们假设训练集是一串包括 T 个训练词的字符序列 w1,w2,w3,⋯,wT ，每一个词来源于大小为 |V| 的词汇库 V。模型大体上考虑n个词的上下文 c。我们将每一个词和一个 d 维度的输入向量（也就是表示层的词嵌入）vw 以及输出向量 v′w（在softmax层的权重矩阵的对于词的表示）联系在一起。最终，我们相对于我们的模型参数 θ 来优化目标函数 Jθ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们一起来回顾一下，softmax 层对于一个词 w 在上下文 c 出现的概率的计算公式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cSM7Vh72kwdkmn1icCfS58XRTmCwEZLEogeNaUybOAPU1Riam40mKjfBQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，h 是倒数第二层的输出向量。注意到和之前提到的一样，我们用c来表示上下文，并且为了简洁，舍弃目标词 wt 的索引 t。计算softmax的复杂度很高，因为计算 h 和 V 里每个词 wi 的输出向量的内积需要通过一部分的分母的求和来获得，从而得到标准化后目标词 w 在上下文 c 的概率。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来我们将会讨论近似 softmax 所采用的不同方法。这些方法可以分成两类：一类是基于 softmax 的方法，另一类则是基于采样的方法。基于 softmax 的方法保证 softmax 层的完好无损，但是修改了它的结构来提升它的效率。基于采样的方法则是完全去掉 softmax 层，优化其它目标函数来近似 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于 softmax 的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层次的 softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这让我们把对一个词出现概率的计算分解成一连串的概率计算，我们将无需对所有词作昂贵的标准化。用 H-Softmax 来替代单一的 softmax 层可以把预测词的任务带来至少50倍的速度提升，因此适用于要求低延迟的任务，比如 Google 的新通讯软件 Allo 的实时沟通功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cTFc71iaicfxRXvC23pN5ewrIYkZRSgAmFvUbaAicGKibLyOt7tpHFDnl1w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：多层词的 softmax&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把常规的 softmax 想成是只有一层深度的树，每个 V 中的词都是一个叶子节点。计算一个词的 softmax的概率则需要标准化所有 |V| 个叶子的概率。反之，如果我们把 softmax 当成一个每个词都是叶子的二叉树，我们只需要从叶子节点开始沿着树的路径走到那个词，而无序考虑其它的词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为一个平衡的二叉树的深度为 log2(|V|)，我们只需要通过计算最多 log2(|V|) 个节点来取得一个词最终的概率。注意到这个概率都已经经过了标准化，因为二叉树中所有叶子节点的概率之和为1，所以形成了一个概率分布。想要粗略地验证它，我们可以推理一个树的根节点（图1中的节点0），它的所有分支必须相加为1。对于每个接下来的节点，概率质量分解给它的分支，直到最终到达叶子节点，也就是词。因为这其中没有损失概率，而且所有词都是叶子，所有词的概率的总和必须为1，所以分层次的 softmax 定义了在 V 上所有词的一个标准化的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体而言，当我们遍历树的时候，我们需要计算每个分支点的左右两个分支来计算这个节点的概率。正因如此，我们要为每个节点指定一个表示。对比于规律的 softmax，对于每个词，我们因此不需要 v'w 的输出词嵌入——反之，我们用给每个节点 n 都指定词嵌入 v′n。因为我们有 |V|−1 个节点，而每一个都拥有一个唯一的表示，H-Softmax 参数都和普通的 softmax 几乎一样。我们现在可以计算给定上下文 c 一个节点 n 的右分支（或左分支）的概率了，方式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAicibJK1LvFZ28em66ZtbEteGvlvOnWicJGceVWRRHWiaKRaiaiaCpTiapNpA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个方法和普通的 softmax 的计算方式几乎一致。现在，我们不计算h和词嵌入 v′w 的点乘积，而是计算每个树节点的h和词嵌入 v′w。另外，我们不计算整个词汇库里所有词的概率分布，我们仅仅输出一个概率，在这个例子中这个概率是 sigmoid 函数的节点 n 的右分支的概率。相反地，左分支的概率是 1−p(right|n,c)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9coiaLqKEFJs9fo63g3FTiaQQ4v5cdAojvAwHKmXSEhbGubm5w2rqlkvBA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：多层次的 softmax计算&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由此易得一个词w在上下文c中的为左右分支的概率之积。举个例子，在上下文「the」、&lt;span&gt;「dog」&lt;/span&gt;、&lt;span&gt;「and」&lt;/span&gt;、&lt;span&gt;「the」&lt;/span&gt;之中，在图 2 中词「cat」的概率可以通过计算从节点1向左，经过节点2转右，再在节点后转右所得的概率来计算。Hugo Lachorelle 在他的课程视频 (https://www.youtube.com/watch?v=B95LTf2rVWM)中提供了一个更详细的介绍。Rong[7] 也很好地解释了这些概念，并且推导了 H-Softmax 的导数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，树的结构十分重要。直观上来说，如果我们让模型在每个节点都来学习二元分类，比如我们可以让相似的路径获得相似的概率，我们的模型应该可以获得更好的表现。基于这一点，Morin 和 Bengio 给树提供 WordNet中的 synsets 的聚类。然而，他们的模型表现却不如常规的softmax。Mnih 和 Hinton [8] 用一个聚类算法来训练树结构来低轨地把词分成两堆，并且他们的模型在一部分的计算中表现和常规的 softmax 相当。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是，只有在我们提前知道想要预测的那个词（以及它的路径）时，我们才能够加速训练。在测试阶段，当我们需要预测最可能出现的词时，尽管缩小了范围，我们仍然需要计算所有词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，我们不需要用&lt;span&gt;「左」和&lt;span&gt;「右」&lt;/span&gt;&lt;/span&gt;来指定子节点，我们可以用一个对应路径的位向量来索引节点。在图 2，如果我们假设用比特 0 来表示向左和比特 1 来表示向右，我们可以用0-1-1来表示一条左－右－右的路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们回顾一下，一个平衡二叉树的路径长度是 log2|V|。如果我们设置 |V|=10000，这就相当于一条大约长度为13.3 的路径长度。相似的，我们可以用平均长度为13.3的路径的位向量来表示每一个词。在信息论中，这指代一串信息长度为 13.3 比特的字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;字的信息内容小结&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先回忆，一个词 w 的信息量（信息熵）I(w)是它的概率的负对数 I(w)=−log2p(w)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在数据集中所有词的熵H就是在一个单词库的所有词的信息熵的期望：H=∑i∈Vp(wi)I(wi)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也可以把一个信息源的信息熵想成是用来编码这部分信息所用的比特数。对于抛掷一枚公平的硬币，每次需要1比特；而对于一个总是输出相同符号的信息源，我们只需要0比特。对于一个平衡二叉树，我们平等对待每一个词，每个词 w 的熵 H 将拥有同样的信息量 I(w)，因为每个词都有同样的概率。在一个 |V|=10000 大小的平衡二叉树中平均的词信息熵 H 就恰好是它的平均路径长度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKR0xvouGYoYt3RoRFNPt3CQyfWOqibfR0ibIDoRSpPeuwZDDibg4NUkmA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们之前讲过，树的结构十分重要。值得注意的是，利用树的结构不但可以获得更好的表现，更可以加速运算：如果我们将更多信息加载进树中，那么更少信息的词将不意味着更短的路径，因为有些词出现频率更高，就可以用更少的信息去编码它。一个 |V|=10,000 长度的信息库的词信息熵大约为 9.16。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，考虑到出现频率，我们可以减少一个词汇库中每个词中的平均比特数。在这个例子中，我们从 13.3 减少到 9.16，相当于加速了 31%。Mikolov 等人 [1]把哈夫曼树运用在多层次 softmax，把更少的比特赋给更常出现的词。比如，「the」这个最常见的英语单词，在树中只需要最少比特数的编码，而第二常见的单词将会被指定第二少比特数的编码，以此类推。虽然我们仍然需要用相同数量的词去编码一个数据集，然而有更高频率出现短的编码，所以平均而言，去给每个词编码只需用更少的比特数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个类似于哈夫曼编码的编码又被称为信息熵编码，因为每个编码词的长度大约是和我们观察到的每个符号的熵成正比。Shannon [5] 在他的实验中建立了英语的信息率的下界，每个字母大约是 0.6 到 1.3 比特；根据每个词的长度为 4.5，这相当于每个单词为2.7到5.85比特。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这个和语言建模（我们在上一篇博文中讲到的那样）联系起来：语言模型的评价标准的困惑度应该是 2H，其中H就是信息熵。一个一元的熵是 9.16，因此它有一个非常高的困惑度 2^9.16=572.0。我们可以将这个值更加具体化，观察一个困惑度为572的模型就像从一个信息源中选择单词，而每个单词有572种选择，每种选择概率相等，且互相独立。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这么说吧：Jozefowicz 等人在 2016 年开发的这个最新的语言模型，在 1B Word Benchmark 中，每个词拥有 24.2 的困惑度。这个模型因此需要大约4.60比特来编码一个词，因为 2^4.60=24.2，非常接近于 Shannon 描述的实验下界。我们能否，以及如何使用这个模型去建立一个更好的多层次 softmax 层？这些问题仍留待我们去探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微分 Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Chen等人[9]介绍了一个经典 softmax 层的变形—— 微分Softmax（Differentiated Softmax，英文缩写为D-Softmax）。D-Softmax 基于不是所有的单词都需要同样多的参数：许多高频词可以拟合许多参数，而非常低频词则只能拟合很少的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了做到这一点，他们不用常规 softmax 层的 d×|V| 大小的包含输出词嵌入 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cCbHjibecAfibMSdDsCe46w79x89D4aibEYKGowQnSBJHExd1Gnsk7ialwA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的稠密矩阵，而用一个稀疏矩阵。然后他们把 v′w 排列成块，根据他们的频率排列，而每块的词嵌入的维度都是 dk。块的数量和他们的向量大小均为可以优化的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdExer2zIaU8T1CH179unBARXsWPnKoV9bxWPMrRgZhResAia55v2CaQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图3：微分 softmax（Chen等人(2015)）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图3中，分区A的词嵌入的维度是 dA（这些是高频词的词嵌入，因为它们被赋予更多的参数），而分区 B 和 C 的词嵌入分别有 dB 和 dC 维度。注意到所有不属于任何分区的区域，也就是图1中的这些没有阴影的区域，都设为 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前的隐藏层 h 都被当成是把每个对应维度的分区的特征串在一起。在图3中的h由大小分别为 dA、dB 和 dB 的分区组成。D-Softmax 不计算矩阵－向量的乘积，而是计算每个分区的乘积和它们在h的分区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为许多词只需要相对来说较少的参数，计算 softmax 的复杂度降低了。对比 H-Softmax，这个加速在测试阶段仍然存在。Chen 等人（2015）发现 D-Softmax 是在测试阶段最快的方法，而且是最准确的模型之一。然而，因为它给低频词赋予了更少的参数，D-Softmax 对于低频词的建模效果并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CNN-Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个对经典 softmax 层的改进受到了 Kim 等人最近的对于通过一个字母层次（character-level）的 CNN 的输入词嵌入 vw 的研究启发。Jozefowicz 等人（2016）建议对输出词嵌入做相同的事情，即通过一个字母层次的CNN。注意到如果我们像在图4中的在输入和输出有一个 CNN，生成输出词嵌入 v′w 的 CNN 必须和生成输入词嵌入 vw 的 CNN 不一样，就像是输入词嵌入和输出词嵌入必须不一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csXJPjEiaclKa88v8e40TNrbw6sibibcMnkPwpmAQmWE3c9GNk054uzm9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图4: CNN-Softmax（Jozefowicz 等人 ，2016年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然这个仍然需要计算常规 softmax 的标准化，这个方法很大程度上减少了模型参数的数量：我们现在不存储d×|V|的词嵌入矩阵，而仅仅是追踪 CNN 的参数。在测试阶段，输出向量 v′w 可以提前计算，所以模型的表现不会受损。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，因为字母都在练习空间中表现，且因为得到的矩阵都倾向于学习一个把字母映射到词的平滑函数，基于字母的模型常常会难以区分拼写相似而意思迥异的词。为了避免这个问题，研究者们加上一个通过每个词学习的连接系数，就能很大程度地减少常规的 softmax 和 CNN-softmax 的表现差异。调整修正项（correction term）的维度，研究者就可以取得模型大小和表现好坏的平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者也强调不需要用 CNN-softmax，而是把前一层h的输出传入一个字母层次的 LSTM，它输出词语的方式可以是一次输出一个字母。因此，每一个时间步中，softmax 输出的并不是词，而是字母的概率分布。然而，他们不能得到和这一层相当的表现效果。Ling 等人[14] 为机器翻译采用一个相似的层，得到了具有竞争力的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于采样的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上文中讨论的这些方法仍然保持着 softmax 大体的结构，而基于采样的方法则完全和 softmax 层无关。它们的方式是近似其它的损失函数的 softmax 的分母的正则化来。然而，基于采样的方法只是在训练时有效——在推断（inference）中，完全的 softmax 仍然需要计算来获得一个正则化后的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了直观上看 softmax 分母在损失函数的影响，我们将会推导我们相对于我们模型 θ 的损失函数 Jθ 的梯度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在训练过程中，我们希望给训练集中的每个词 w 减少模型的交叉熵损失函数（cross-entropy loss）。这就是我们的 softmax 的输出的负对数。如果您不确定 softmax 和 cross-entropy 的关系，请看看Karpathy的解释 (http://cs231n.github.io/linear-classify/#softmax-classifier)。我们的损失函数如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c2s5OvdiblDD98icF2pVmSM6wdkrAlq1RBdFnlPGLyIjuNp9z2IRJ54icA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意在实际操作中，Jθ 就是在整个数据集的所有负对数概率的平均值。为了获得这个推导，我们可以把 Jθ 分解成 ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cc28a98FibJJaqM9Z5ZcK3HEMNtRtn6bNXpnicKy1OcJibMI3iaLuQrHJyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cX5fBZSjYZiaosEaZjj18eIs7LQjxbY21SZ8B0ibQ1ianrmAiclPM5OibXCQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了简洁，并且和 Bengio 和 Senécal [4 , 15]的标号对应起来（注意到第一篇文章中，他们计算了正对数的梯度），我们用 −E(w) 来代替 h⊤v′w 的点积。我们的损失函数于是看起来像这个形式：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9czrTnJnxb1YtXMcPgKx9kNWYJs85PvcEdXXicsF9UnsNvGpexEjM6bwg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了反向传播，我们现在可以计算相对于我们的模型参数 θ 的 Jθ 的梯度 ∇：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibb57T6eIt9RBpXOfYfcyqX50QzZsFT0env3mKaibo3MddDlyKKw9QPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 logx 的梯度是1x，运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBicpdmib8xb0PPWxoarS5NiaBnx7W8ribT4mx1UTgsSibxu1qtWmKWaZhFw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以在和中移动梯度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cIl0KSVjhsIkjq4YibsnQcqic1HHAcIf79mS8VCr3l0c8mtkc62twy6kw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 exp(x) 的梯度就是 exp(x)，再一次运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cuJLJr4kkCOs1xO1SlQNp8vcDKGribAMyicmDPOTnibt6YXGtF4NPFYM7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把它写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cmRm7iayjzaYcu6PyvMEaDW09yaoYW1QkIicKMAJZPCo97XFwWeMFEYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意到：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDXVdfWYA3OCXLOz1AY6znenNyWcYL3cP4Kv61UibFkdz9wL0Wgkya4w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只是 wi 的 softmax 概率 P(wi)（为了简洁，我们忽略它对上下文 c 的依赖）。替换它，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyWdiaqibr7K72GKqdLuoGFUKSphg8cpCVmPzguicqVickjHw9s98FicjTYA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，重新把负系数放到和前面，可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cic7kKz3l2sNOZDk87dXXEYQyo0leyTPgSGDbQiaakLVqzgyznveLtczQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio和Senécal（2003）发现了梯度最终由两部分组成：一个是给目标词 w 的正反馈（上一个式子中的第一项），一个是给其它词 wi 的负反馈，由它们的概率 w（第二项）反映出来。我们可以发现，这个负反馈只是对于所有V中的词 wi 的梯度 E 的&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDicOh0m3Iia8DqYUqxDW1bGaU0oQbmjZvstS0BrmAooJCKTT0jtV0SPQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cJQZ6YAALBzZAYfiaic1rTT8VPuK2TE08A2JicWbz1aFnL3Xx2dhxkia03g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在大多数的基于采样方法的难点是去近似负反馈，让它更容易地去计算，因为我们不想将所有在 V 中的词的概率加起来。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以用蒙特卡洛方法去近似任何概率分布的期待值 E，也就是所有概率分布的随机样本的平均值。如果我们知道网络的分布，即 P(w)，我们就可以从中采样 m 个词 w1,⋯,wm ，然后近似得到如上的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1iaKjjtjgwu7GvJIibul4C6o5NI9pyTTLKS0V8EBRW9YrbibKUYEFcbsw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，为了从概率分布 P 中采样，我们需要计算 P，但这其实是我们最开始想要避免的。于是，我们找到另一个概率分布 Q，我们称之为建议分布（proposal distribution）。Q 应该便于从中采样，而且可以用来作为蒙特卡洛抽样法的基础。我们偏向于选取和 P 相近的概率作为 Q，因为我们希望得到的近似期望能够尽量准确。在语言建模中一个非常直接的选择就是用一元文法分布（unigram distribution）作为 Q 的训练集。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也就是经典的重要性抽样方法（Importance Sampling，英文缩写 IS)：它使用蒙特卡洛抽样，通过一个提议分布Q来近似一个目标概率分布 P。然而，这仍然需要给每个采样的词 w 计算 P(w)。为了避免这个，Bengio 和 Senécal (2003年) 用Liu [16]中提出的有偏估计量（biased estimator）。这个预测函数可以在 P(w) 当成乘积计算时使用，这正是我们要处理的情况，因为每个除法都可以转化成乘法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从根本上来说，我们不必花大代价计算 Pwi 并得到梯度 ∇θE(wi) 的权重，我们只需要利用提议分布Q来得到权重的一个因子。对于一个有偏见的 IS，这个因子就是&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKKOburZqykXNAxdQf9icVJVN3aLgwfm9fBTUibRdibbDIF4DWbmvicJYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cOEicmHnbo25icNKia8pWXTU616hOJ54IFXccicv4fqO6CyPqvicLiar8lQIA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;且&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ccFUe9xS5QmuicPSeRDuwFk04jyCRbViaUmq8kD2Alokmo43yYSQ1qTibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用 r 和 Q 而非 Bengio 和 Senécal (2003年, 2008年) 论文中的 w 和 W 来避免重名。因为我们可以发现，我们仍然计算 softmax 的分母，但是用提议分布 Q 来替换分母的标准化。因此，我们的用以近似期望的有偏差的预测函数如下：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用越少的样本，我们的预测就会越差。另外，我们需要在训练阶段调整我们的样本数量，因为如果样本数量太小，在训练阶段，网络的分布 P 可能和一元分布 Q 分岔，导致整个模型无法收敛。因此，Bengio 和 Senécal 为了防止可能的分岔采用了一个计算有效样本大小的方法。最后，研究者们声称这个方法相对于传统的softmax能够加速19倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自适应性重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 和 Senécal (2008年) 注意到对于重要性采样方法，替换掉越多复杂的概率分布，比如二元（bigram）和三元（trigram）分布，那么接下来在训练阶段，将对于从模型的真实分布 P 来对一元分布 Q 的分岔的对抗将无效，因为n元（n-gram）分布看起来和训练好的神经语言模型的分布挺不一样。作为替代模型，他们提出了一个用在训练阶段调整适应过的 n 元分布来更好地跟随目标分布P。最后，他们根据一些混合方程来插值（interpolate）一个二元分布和一个一元分布，而这些混合方程的参数都是他们用不同频率组的 SGD 训练的，训练目标为最小化目标分布 P 和提议分布 Q 的 Kullback-Leibler 差异。在实验中，他们声称训练效果提高了大约100倍。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目标采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jean 等人（2015年）提出了在机器翻译中运用具有适应性的重要性采样方法。为了让模型更好地适应 GPU 上的并行计算及有限内存，他们将目标单词的数量限制到采样最少必须达到的数量。他们将训练集分区，在每个分区只保留一定数量的单词，形成了总词汇表的一个分集 V′。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这根本上表现了一个独立的提议分布 Qi 可以在训练集的每个分区 i 中运用，这给所有词汇表分集 V′i 内的词赋予同等的概率，而其它的词则概率为0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;噪音对比估计方法&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;噪音对比估计 （NCE）（Gutmann 和 Hyvärinen） [17] 由 Mnih 和 Teh [18] 作为一个比重要性采样方法（IS）更稳定的采样方法提出，因为我们发现 IS 具有一定风险出现建议分布 Q 和分布 P 分岔的情况，而这就是需要优化的地方。不同于之前的方法，NCE 并没有试图直接预测一个词的概率，而是用一个辅助性的损失函数来同时最大化正确词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆 Collobert 和 Weston（2008年）的成对排序标准，它将正数窗口排列在「受损的」窗口之前，这一点我们在上一篇博文已经讲到。NCE 做类似的事：我们训练一个用来从噪音区分目标词的模型。因此，我们可以将预测正确词的任务简化到一个二元分类任务，其中模型试图从噪音样本中区分正确、真实的数据，如图4所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1adCl7nogIcJmZlT6iaictdVDibsHibvcz0WvSUIWCN8uTc71Ctictxo0Ag/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4:噪音对比估计&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于每个词，给定了 n 个在训练集之前出现的词 wt−1,⋯,wt−n+1，我们可以从噪音分布 Q 来生成 k 个噪音样本 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为在 IS 中，我们可以从训练集的一元分布中采样。因为我们需要数据标签来完成我们的二元分类任务，我们指定在上下文 ci 中所有的正确词 wi 为真（y=1），而所有噪音采样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为假（y = 0）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不去计算我们的噪音样本的期望&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9crbV8CnzHCuqRCvbQZYciaSZb81K8vdOmkCCADiaHUib264CuwAHrjJAiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为获得这个期望仍然需要把所有 V 中的词加起来从而预测负标签的标准化的概率，而是再次用蒙特卡洛法近似求平均值：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cYUABqwbDqciaP5mUnGmAbibOib5UeOQ0cIHrAREkby6UgSgtW95l5KPxQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以简化为：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1LaD38BuaEYl5c2wdusjTH2TBDArzibdVMIxAfEtPiaUN65ecBGpaugA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为每个在上下文 c 中正确的单词 wi 生成 k 个噪音样本，我们有效地从两个不同的分布中生成词：正确的词从训练集 Ptrain 的实际分布中采样且依赖于它们的上下文 c，而噪音样本则来自噪音分布 Q。我们因此可以用两个分布的混合模型来表示采样到正样本或负样本的概率，它们基于分别的样本数量来取得权重：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdOuP3fNY2ZVe92AE4XicusmNHaZbeLicw0XJd6eAF5PSuicDxhWicGc66w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据这个混合模型，我们现在可以计算一个样本来自于训练分布 Ptrain 的概率，它就是一个 y 对于 w 和 c 的条件概率：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cvMiaS2C3ZspQOrhOAAZqy6oQqsVicUsPq8JxBLJukcrtT6u0kicicgicUqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它可以简化成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c6zxKO7iboQ6eLdo4G20xOltzNibicAAQw1sZpicBSEyichL0L2WAyN3nPgg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们不知道 Ptrain 的值（正是我们希望计算的），我们用我们的模型 P 的概率来替换 Ptrain：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLibByWY4fXbOmYMZp7kAo6hT3vYXUlzs8XnVeZ6Kghic4L7ibkNiaTcNYg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测噪音样本（y=0）的概率因此就是简单的 P(y=0|w,c)=1−P(y=1|w,c)。请注意计算 P(w|c)，也就是给定它的上下文 c，一个词 w 的概率本质上就是我们对于 softmax 的定义：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cwkssghydynxK27TJQzmTbnZ6mnxtIO6iaSR6ge7e1gbn4qs6vXKbDaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了记号简便和不被混淆，让我们把 softmax 的分母命名为 Z(c)，因为这个分母仅依赖于 h，它从 c 中生成（假定一个固定的 V）。Softmax 于是看起来像这样：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csqnNicNhT0hpxAu6ZicOxxpn0oLs8UD3R2XF3AGWRK23Lzdib7Pyga7mw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mnih 和 Teh（2012 年）和 Vaswani 等人 [20] 实际上将 Z(c) 固定在 1，他们声称这样不会影响模型的表现。这个假设有一个良好的附带效果，那就是可以减少模型的函数数量，同时保证模型可以自己标准化，而不需要依赖特意标准化 Z(c)。确实，Zoph 等人 [19] 发现即使模型学习这个参数，Z(c) 和 1 非常接近，而且具有小的方差。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们在上面的 softmax 算式中可以设 Z(c) 为 1，对于在上下文 c 中的词 w，我们就得到了如下的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以插入上式的这一项来计算 P(y=1|w,c)：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAsQ2v4wTem2aVYqItLdrVXKaic3v4Oex5KkiaVEQu8YFKIwBbLvDj3xw/0?wx_fmt=png"/&gt;&lt;br/&gt;插入这一项到我们的 logistic 回归的目标中，就能得到完整的 NCE 损失函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cl0ribiczQ3gEwcxPTWAjjJ9zZTibyb1vnAb2ZZP7IchESrj0L2CyZT5Sw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意到 NCE 有一个很好的理论上的保证：可以证明当我们增加噪音样本的数量 k 时，NCE 的梯度趋向于 softmax 函数的梯度。Mnih 和 Teh（2012 年）提出 25 个样本就足够使模型表现能够和常规的 softmax 相当，且能够提升 45 倍的运算速度。对于 NCE 的更多信息，Chris Dyer 发表了一些非常好的笔记 [21]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个 NCE 的缺点就是，因为不同的噪音样本来源于对每个训练词 w 的采样，噪音样本和它们的梯度都不能在稠密矩阵中存储，这减少了在 GPU 上使用 NCE 的好处，因为它不能受益于快速稠密矩阵乘法运算。Jozefowicz 等人（2016 年）和 Zoph 等人（2016 年）独立地提出在所有训练词中共享噪音样本，从而 NCE 的梯度可以用稠密矩阵运算来计算，在 GPU 上更加高效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 IS 的相似性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jozefowicz 等人（2016 年）的研究认为 NCE 和 IS 不仅都是基于采样的方法，同时也高度关联。他们发现 NCE 用一个二元分类的任务，IS 可以类似地用一个代理损失函数（也称上界损失函数，surrogate loss function）表示：IS 并不像 NCE 一样用一个 logistic 损失函数去做二元分类任务，而是优化用一个 softmax 和一个交叉熵损失函数做一个多元分类的任务。他们发现当 IS 做多元分类的任务时，它为语言建模提供了一个更好的选择，因为损失函数带来了数据和噪音样本的共同更新，而不是像 NCE 一样的独立更新。事实上，Jozefowicz 等人（2016 年）用 IS 做语言建模，并在 1B Word Benchmark 数据上却取得了优异表现（就像上文所述）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;负采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mikolov 等人（2013）发现并推广的目标——负采样（NEG），可以被看成是 NCE 的一个近似。就像我们之前所说的，NCE 可以被证明是在样本数量 k 增大后对 softmax 损失函数的近似。NCE 的简化算法 NEG 避开了这个保证，因为 NEG 的目标就是学习高质量的词表征，而不同于在测试集上获得低困惑度这一个语言建模的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NEG 也用一个 logistic 损失函数来最小化训练集内的词的负对数可能性。让我们回忆一下，NCE 计算给定上下文 c 一个来自实际的训练概率分布 Ptrain 的词 w 的概率，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibXFmOoB9PA90Cr3ZiaavZeG2ua1ejWBU57oGECs4oePpk7LmejmPXxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 NEG 的主要差别在于 NEG 只是用简化 NEG 的计算来近似这一个概率。因此，它将最昂贵的项 kQ(w) 设为 1，我们就得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ctmfyXQQxokVicmhgzPWTFB12I5CPJssCGVq7ibEibsKvqe0RYSg05sHHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当 k=|V| 而且 Q 是一个均等分布时，kQ(w)=1 刚好为真。在这个情况下，NEG 等价于 NCE。我们设 kQ(w)=1 而不是其它常数的原因可以从重写这个算式来得到，因为 P(y=1|w,c) 可以变型成 sigmoid 函数&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;如果我们将它插入回之前的 logistic 回归函数中，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cUFzzWicd7WsEIdEAwrfYgXvwofgx7WFuztAt5jtpibdCCUFWoGjtgibew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;稍微简化一下，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKbv5qqXtiaibiaRPN8CNXtxhZTuSfMicckRwJC3uuT9nR1NibmFfUXlJx0A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cQbUic1UBuFDUNXblhmdsEu1pE2LqGQEpnicNdbdicKJEQpTRyoaAaXUtg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终得到 NEG 损失函数：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cgO2fzKgP75TWM0zBVWvfVicxlAeW1q8Kpcwv0PCvl3Kt9et1xrzicibibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了和 Mikolov 等人（2013 年）的标号统一起来，h 必须被 vwI 替换，v′wi 需被 v′wO 替换，以及 vw~ij 需被 v′wi 替换。另外，和 Mikolov 的 NEG 的目标相反，我们 a) 在整个词汇集中优化目标，b) 最小化负对数可能性而非最大正对数可能性（如上文所说），和 c) 已经把&amp;nbsp;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibbusiaTad1ibiaFIbahCjHvypCC40a2jjEDcfZic2jHVbjcUqUnE85zz5w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望替换成它的蒙特卡洛近似。对于更多推导 NEG 的信息，敬请参见 Goldberg 和 Levy 的笔记 [22]。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经看到 NEG 只是在 k=|V| 而且 Q 是一个均等分布时和 NCE 相等价。在其它情况下，NEG 只是和 NCE 近似，也就是说它并没有直接优化正确词的可能性，这是语言建模的关键。虽然 NEG 可能因此对于学习词嵌入非常有用，但是它不能保证渐近一致性（asymptotic consistency），这使得它不适合语言建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然 Devlin 等人 [23] 提出的自标准化技术（self-normalisation）不是一个基于样本的方法，它为语言模型的自身标准化技术更直观，我们待会儿会介绍。我们之前提到的把 NCE 的损失函数的分母 Z(c) 设为 1，这个函数实质上自标准化了。这是一个有用的性质，因为它允许我们跳过标准化项 Z(c) 的昂贵计算。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆，我们的损失函数 Jθ 最小化我们的训练数据的所有词 wi 的负对数可能性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c49zZn4bAesL95KxKJib0TmtbFtjdfxyBic6e3cQkF7GFZQZm1oLJKDwQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们像之前一样可以把 softmax 分解成和：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibeynoia42hvCmjniauWiabU6mj3dw9wxqDiawynZFpEMYiau15Dx3y2CbNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们可以限制我们的模型，使得它可以让 Z(c)=1 或 logZ(c)=0，那么我们可以避免计算标准化项 Z(c)。Devlin 等人（2014 年）因此提出给损失函数加上一个平方错误惩罚项，来鼓励模型把 Z(c) 保持着尽量接近 0：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c13ibXA4RLeuYbjl9mwYmkSTGbay84ENvOknj5zdVF5AyOocHB5HqIUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyicpvmo3DZaYAVmzrL98g7dHc6ianaAJqSOxrcNLDN1c0iaMl7zpdcbAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 α 允许我们去做模型准确度和平均数自标准化的取舍。我们大体就可以保证 Z(c) 将和我们希望的一样非常接近于 1。在它们的 MT 系统的解码时间内，Devlin 等人（2014 年）接着设置 softmax 的分母为 1，而且仅仅使用分母和它们的惩罚项来计算 P(w|c) ：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cqC78Hnn2gbu3KLkflib9OdR1N14CPdZur9BhQ2NLS1QrKTQuNdTuAGw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们称自标准化加速了大概 15 倍，而相比于常规的非自标准化的神经语言模型，在 BLEU 分数上只有少量的退步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;低频的标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Andreas 和 Klein [11] 提出仅仅标准化一部分训练样本已经足够，且仍能获得和自标准化一样的行为。他们由此提出低频的标准化（Infrequent Normalisation，或 IN），仅仅在惩罚项中取小部分样本，来形成一个基于样本的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们先来把之前的损失函数 Jθ 分解成两个独立的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cZHwZp3t1jwky9WrF9lUx8R6ia7ibt7zGshS1xQwXaDCFx802EjliaaCCA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在在训练数据中，通过仅仅计算一个词的子集 C（它包含词 wj）来在第二项中取小部分样本，从而从上下文 cj 采样（因为 Z(c) 仅依赖于上下文 c）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cObEotOxXgr92yKTQuEEKU024BmiaYQNQF7GjxMYZrFve10oITCCib3Hw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，γ 控制着子集 C 的大小。Andreas 和 Klein（2015 年）提出 IF 综合了 NCE 和自标准化的优点，因为它不用给所有的训练样本计算标准化项（NCE 完全不用计算），但是又像自标准化一样允许去做模型准确度和平均数自标准化的取舍。他们观察到当它只用从十分之一的样本中采样时，它能够加速 10 倍，而没有明显的模型表现损失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;其它方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;至此，我们已经集中讲解了近似或者完全避免 softmax 分母 Z(c) 的计算的方法，因为它就是计算中最昂贵的一项。我们因此不去特别留意&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即倒数第二层 h 和输出词嵌入层 v′w 的点积。Vijayanarasimhan 等人 [12 ] 提出用快速的位置敏感哈希（locality-sensitive hashing）来近似&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，尽管这个方法在测试阶段可以加速模型，但是在训练阶段，这个加速实际上会消失，因为词嵌入必须重新标号，且批（batch）的数量会增加。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;选择哪个方法呢？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看完了最流行的基于 softmax 的和机遇采样的方法，我们展示了对于经典的 softmax 有很多替代品，而且几乎它们所有都在速度上有重要提升，它们大多数也略有模型表现上的不足。所以，这自然引到了一个问题，就是对于某一个特定的任务，哪个方法是最好的方法。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Approach方法&lt;/th&gt;&lt;th&gt;Speed-up factor加速倍数&lt;/th&gt;&lt;th&gt;During training?在训练阶段吗？&lt;/th&gt;&lt;th&gt;During testing?在测试阶段吗？&lt;/th&gt;&lt;th&gt;Performance (small vocab)表现（小词汇集上）&lt;/th&gt;&lt;th&gt;Performance(large vocab)表现（大词汇集上）&lt;/th&gt;&lt;th&gt;Proportion of parameters 参数的采用百分比&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Softmax&lt;br/&gt;Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;1x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Hierarchical Softmax 多层次的softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;25x (50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Differentiated Softmax 微分 Softmax&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&amp;lt; 100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;CNN-Softmax&lt;br/&gt;CNN-Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad - good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;30%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Importance Sampling 重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(19x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Adaptive&lt;br/&gt;Importance Sampling 具有适应性的重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Target Sampling 目标采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Noise Contrastive&lt;br/&gt;Estimation 噪音对比估计方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;8x (45x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Negative Sampling 负采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Self-Normalisation 自标准化方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(15x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;6x (10x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;表格 1：比较语言建模中近似 softmax 的几种方法&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在表格 1 中比较了我们之前介绍的几种方法的表现。加速倍数和表现均来自于 Chen 等人（2015 年）的实验，同时我们在括号中记录了原作者们提出的加速倍数。第三和第四列分别显示了在训练阶段和测试阶段是否达到这个加速倍数。请注意，加速倍数的不同也许是因为没有优化，或者原作者没有用到 GPU，显卡运算常规的 softmax 比一些其它方法更加高效。部分方式没有可用的比较的方法，其表现则大部分参考相似的方法，比如自标准化方法应该和低频标准化相当，重要性采样方法和具有适应性的重要性采样方法与目标采样法相当。Jozefowicz 等人（2016 年）提出的 CNN-Softmax 方法的表现则根据纠正（correction）的大小时好时坏。在所有的方法里，除了 CNN-Softmax 方法中用了明显更少的参数，其它方法仍然需要存储输出词嵌入。微分 Softmax 方法因为能够存储一个稀疏权重矩阵从而减少了参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像一直以来，没有哪个方法对于所有的数据集或者任务来说都是最好的。对于语言建模，常规的 softmax 在小词汇库数据集上仍然有一个非常好的表现，比如说 Penn Treebank，甚至能够在中数据集中也很好，比如 Gigaword，但是在大数据集上则很差，比如 1B Word Benchmark。目标采样方法、多层次 Softmax，和低频标准化方法则在大词汇库上表现更好。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微分 Softmax 方法则大体上对于小词汇库和大词汇库上都表现不错。有趣的是，多层次 Softmax（HS）在小词汇库上表现不好。然而，在所有的方法之中，HS 都是最快的，而且在给定的时间内能够处理最多的训练样本。负采样法则在语言建模任务上表现不佳，却在学习词表示上有非常好的表现，从 word2vec 的成功可以看得出来。请注意，所有的结果都不可尽信：Chen 等人（2015 年）在报告里说到在实际中运用噪音对比估计方法存在困难；Kim 等人（2016 年）用多层次 Softmax 在小词汇库上获得了很好的表现，而重要性采样方法则被 Jozefowicz 等人（2016 年）提出的最先进的语言模型所用在一个大词汇库中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，如果你真的准备去用上述的方法，TensorFlow 实现 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling) 了一些基于采样的方法，也解释&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; (https://www.tensorflow.org/extras/candidate_sampling.pdf) 了其中一部分方法的不同点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;小结&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇对于一些不同近似 softmax 的方法的介绍，不仅可以用于提升和加速词的表示的训练，也对语言建模和机器翻译有所帮助。正如我们看到的，大部分方法都非常相关，且源于同一点：必须找到方法来近似昂贵的 softmax 分母的标准化计算。记住这些方法，我希望你现在能更好地训练且理解你的模型，你甚至可以准备去自学更好的词表示模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如我们看到的，学习词的表示是一个非常广的领域，对于成功有很多相关因素。在之前的博文中，我们学习了流行的模型的建筑，而在这篇博文中，我们着重学习关键部分，softmax 层。在下一篇博文中，我们将介绍 GloVe，一个依赖于模型因素分解，而不是语言建模。我们将把我们的注意力转移到其它的对于成功学习词嵌入起关键作用的一些超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Morin, F., &amp;amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. http://doi.org/10.1002/j.1538-7305.1951.tb01366.x &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from http://arxiv.org/abs/1411.2738 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jean, S., Cho, K., Memisevic, R., &amp;amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from http://www.aclweb.org/anthology/P15-1001 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Andreas, J., &amp;amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vijayanarasimhan, S., Shlens, J., Monga, R., &amp;amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from http://arxiv.org/abs/1412.7479 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Ling, W., Trancoso, I., Dyer, C., &amp;amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from http://arxiv.org/abs/1511.04586 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. http://doi.org/10.1109/TNN.2007.912312 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Gutmann, M., &amp;amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML』12), 1751–1758. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Zoph, B., Vaswani, A., May, J., &amp;amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vaswani, A., Zhao, Y., Fossum, V., &amp;amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from http://arxiv.org/abs/1410.8251 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Goldberg, Y., &amp;amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.』s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp;amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL』2014, 1370–1380.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 人工智能法官：英美科学家开发出用于审讯的计算机程序（附论文）</title>
      <link>http://www.iwgc.cn/link/3204727</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自卫报&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; white-space: normal; background-color: rgb(255, 255, 255); box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;伦敦大学学院（UCL），谢菲尔德大学和美国宾夕法尼亚大学的最新研究表明，人工智能已经可以分析法律证据与道德问题，进而预测审讯结果。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;em style="font-size: 16px; text-align: justify; white-space: pre-wrap; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="font-size: 16px; text-align: justify; white-space: pre-wrap; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cPEFtYZMq4BgxXibM2uOR3lY7LN0TPbtvQR14ZZR07hlI0ia3dZFI1fjg/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-align: justify; font-size: 16px; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;em style="font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;该算法分析了 584 宗有关酷刑和侮辱，审判不公与侵犯隐私的英文案卷。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-align: justify; font-size: 16px; background-color: rgb(255, 255, 255); color: rgb(136, 136, 136); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;em style="font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 20 世纪 60 年代，有科学家曾经预测，电脑将有一天能够预测司法判决的结果。随着近年来计算机科学的发展，人工智能程序在进行高度复杂的任务。它们在电影，电视节目和音乐上「猜你喜欢」时正在变得越来越准确。而现在，一项英美科学家的开创性研究告诉我们，人工智能可以用来预测审讯结果了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伦敦大学学院（UCL），谢菲尔德大学和宾夕法尼亚大学的科学家刚刚发表的最新研究表明，人工智能已经可以分析法律证据与道德问题，进而预测审讯结果。这项研究的成果可以帮助人们提高法庭审判的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能「法官」在分析欧洲人权法院涉及酷刑，虐待，有辱人格和侵犯隐私的案件中做出的判定中有五分之四与人类法官的判决相同。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究人员的程序审查了包含 584 个案件的英文数据集，有关「人权公约」中的第 3 条（涉及酷刑或侮辱虐待的案件，250 起），第 6 条（保护公平审判权，80 起），以及第 8 条（隐私和家庭生活，254 起）。选择这些案件的原因是他们是代表基本权利的案件，同时存在大量的公布数据。在研究中，人工智能程序分析所有信息，并提出自己的司法判决。在其中 79% 的案子里，人工智能提出的判决与当时的法庭判决一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nikolaos Aletras 博士是这项研究的领导者，他是伦敦大学学院计算机科学系的研究员。Aletras 说道：「我们不认为人工智能会在司法领域代替法官或者律师。」在这项研究中，为了防止计算机学习出现误差和偏见，「违法」与「不违法」的案例各占 50%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cRngUw574xyfEYP8eexoQ3xr2dL6slpUxicPCJUnzv0T3aPlticIeUw9g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;人工智能「法官」在分析欧洲人权法院涉及酷刑，虐待，有辱人格和侵犯隐私的案件中做出的判定中有五分之四与人类法官的判决相同。（图片：Getty Images）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个项目的开发过程中，研究小组发现欧洲人权法院对于判决更多依赖非法律事实，而不是纯法律依据。这表明，欧洲人权法院的法官们相比「形式主义者」而言，在法律理论上更加的「现实主义」。根据历史研究，其他的高等法院也是如此，例如美国最高法院。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项研究佐证了其他一些有关高等法院判决影响因素研究的观点，Aletras 说道：「目前我们还需要学习更多数据，以进一步的研究改进我们的算法。但我认为它可以成为一个有效的工具，用于判定哪些案件违反了『欧洲人权公约』。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伦敦大学学院的计算机科学家 Vasileios Lampos 博士，同时也是研究报告的共同作者，解释了这项研究的开创性：「之前的同类研究通常根据犯罪本身和政策立场来预测判决，而我们第一次使用法院编写的案卷来进行分析。」这项研究发现，预测法院判决的最可靠的方式是通过分析案卷中所使用的语言以及文本中提到的话题和情景。案文中「情景」部分包括案件中事实背景的信息。通过组合案件所涵盖的「话题」中提取的信息，分析数据中有关人权公约相关三条的「情景」，最终实现 79％的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Lampos 表示，他们的人工智能程序目前缺乏训练数据：「我们认为这项研究的成果将能提高那些高级法院的工作效率，但是目前看来，我们需要收集更多的文件和法院案卷进行测试。在理想状况下，我们使用那些向法院提交的文件来测试和改进我们的算法，而不是公开的判决文本。但目前我们缺乏这样的数据，只能使用法院公开的部分案件摘要。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在司法领域，律师们早已将人工智能应用到了工作中，这些程序可以执行一些复杂的任务──例如搜索概念而不是简单的关键字──这可以极大地缩短判断文件与案件是否有关的时间。今年五月，IBM 发布了人工智能律师 Ross，通过使用 Watson 的计算能力为人们提供法律建议。IBM 的这项技术最近已被 Baker＆Hostetler 律师事务所采用，这家律师事务所主要处理破产案件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1APrf4VicMuibQ6biaicYI4cicwVCnz9B1YEuH1ia7W3nBmCRyiaSfqPNCF1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Ross 使用 IBM 的 Watson 人工智能平台（图片：Getty Images）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但目前看来，最高法院的法官们还不能将这种新技术应用在判决上。&lt;/span&gt;&lt;span&gt;Aletras 等人的这项研究已发表在了 PeerJ Computer Science 上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：预测欧洲人权法院的司法裁决：自然语言处理的观点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Aletras, N, Tsarapatsanis, D, Preoţiuc-Pietro, D, Lampos, V&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：自然语言处理和机器学习近期的发展为我们提供了工具，可以建立预测模型用以揭开法庭审判的原理。这项研究可以让律师和法官受益，他们可以利用我们的模型作为工具快速审案，或从已知信息中获取要点帮助判断。本文中，我们第一次系统性地研究了仅通过分析文本内容来预测欧洲人权法院的司法判决。我们制定了一个二元分类任务，在分类器中输入从案例中提取的文本内容，目标输出是关于是否存在违反人权公约的条款的实际判断。文本信息使用连续字序列（contiguous word sequences），即 N-gram 和话题来表示。我们的模型可以准确预测（平均 79% 正确率）法院的判决。我们的实证分析表明，案件的形式事实是最重要的判决因素。这与法律现实主义的理论一致，这表明司法决策受到事实情况的显著影响。我们同时发现，事件的话题内容是这种分类任务的另一个重要特征，我们通过定性分析进一步探讨了这种关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 约翰·马尔科夫：犯罪技术将与人工智能一同进化</title>
      <link>http://www.iwgc.cn/link/3204728</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;选自纽约时报&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：JOHN MARKOFF&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;人工智能技术的飞速发展为人类社会带来了方方面面的便捷，但会加持犯罪的各种手段。通过人工智能程序，鼠标轻轻一点就能轻易实现视频和音频窃听的犯罪技术不会太远。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象一下年迈的母亲打电话向你求助，她忘记了银行卡密码。但真实的情况是她不是你的妈妈，电话另一端的声音只是听起来像她。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那只是计算机合成的声音，人工智能的一大绝招已经在电话中被用来伪装身份。&lt;/span&gt;&lt;span&gt;这样的情景目前尚属科幻，但未来，它可能成为一种犯罪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;伪装技术的大范围实现需要的这种软件正在快速发展。例如 DeepMind，Alphabet 旗下以 AlphaGo 闻名的公司，这个程序已经在围棋大赛中击败了几个世界顶级选手。最近，DeepMind 宣布已经开发出一种新程序可以模仿任何人类的声音，这种声音听起来比之前的任何一套文本语音系统生成的声音都要自然，并将现有的自然语音技术与人类语音的差距缩小了 50%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，讽刺的是今年计算机安全产业的年总收入达到了 750 亿美元，并开始讨论机器学习和模式识别技术将会如何改善计算机安全眼下的悲剧状态。但是还有一个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「人们还没搞明白网络犯罪已经变得自动化而且成指数级扩张，」一位执法机构的顾问和《未来的罪行》一书作者 Marc Goodman 说到。他补充道，「这不是 Matthew Broderick 在地下室里搞黑客攻击那么简单了，」参考 1983 年的电影《战争游戏》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今年年初，美国国家情报局局长 James R. Clapper 曾警告过关于先进人工智能技术的恶意使用。在其年度安全审查中，Clapper 先生强调了虽然人工智能系统能带来便捷，但它们也扩大了在线世界的漏洞。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算机犯罪技术的不断尖端化在网络攻击工具的进化中可见一斑，比如，Goodman 所说的，广泛使用的臭名昭著的恶意程序 Blackshades。该程序作者是瑞典人，去年在美国被定罪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个程序在计算机地下市场上卖的很火，其功能可以被形容为「盒子里的刑事特权（criminal franchise in a box）」Goodman 说。它能让不懂技术的用户在计算机上部署一个勒索程序或者通过鼠标点击来进行视频或者音频窃听。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些工具的下一代产品将会加上机器学习技术，而该技术原先是由人工智能研究者用来改善机器视觉、语音理解、语音合成和自然语言理解的。一些计算机安全研究者相信数字犯罪在 5 年前就已经开始尝试使用人工智能技术了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;颠覆互联网无所不在的验证码可以显示这一点——全自动区分计算机和人类的图灵测试。这是卡耐基梅隆大学的研究者 2003 年发明的 challenge-and-response puzzle，用来阻止自动程序窃取在线帐户的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;普通的人工智能研究者与黑客罪犯一直在部署机器视觉软件，5 年前就颠覆了验证码技术。加州大学圣迭戈分校的计算机安全研究员 Stefan Savage 说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果你两年没改验证码了，它就会被一些机器视觉算法操控，」他说到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两人惊讶的是，拖慢恶意人工智能发展速度的东西已经可以以低成本或免费的人力解决。例如，一些网络犯罪已经将破解验证码外包给电子工厂，这里人力解码一个密码的费用只需一丁点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;创造力更佳的计算机骗子已经使用网络色情作为奖励发送给那些破解验证码的人，Goodman 说。自由劳动是人工智能任何时候都无法匹敌的一种商品。接下来又会怎样呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;刚踏入犯罪的人可以搭上新技术发展的顺风车。声音识别技术如苹果的 Siri 和微软的小娜现在广泛用于与计算机的互动。同时亚马逊的 Echo 声音控制扬声器和 Facebook 的 Messenger 聊天机器人平台迅速成为在线商务与客户支持的在线通道。通常的情况是，无论何时，当一种像声音识别这样的交流技术进步开始成为主流，犯罪很快就能将它利用上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我会质疑，通过聊天机器人提供客户支持的公司不在不知不觉地承担起一项社会工程，」krebsonsecurity.com 的一位调查记者 Brain Krebs 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;社会工程，说的是促使人们执行某些行动或者泄露信息的行为被看成是计算机网络安全链上最弱的一环。网络犯罪分子已经在利用人类最好的品质——信任和助人为乐——来进行盗窃和间谍行为。创造人工智能化身欺骗网络用户的能力只会将问题恶化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在州政府和政治运动的政治宣传中广泛使用的聊天机器人技术可以反映出这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者已经发明了「计算宣传（computational propaganda）」这一短语来形容 Facebook 和 twitter 等社交媒体上欺骗行为的爆发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最近的一篇研究论文中，牛津互联网研究所的社会学家 Philip N. Howard 和布达佩斯考文纽斯大学（Corvinus University of Budapest）研究员 Bence Kollanyi 描述了在即将到来的「英国退欧」公投中，政治聊天机器人是如何扮演一个有策略的小角色来塑造在线对话的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;犯罪中用上这些软件只是时间的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;独立计算机安全专家 Mark Seiden 说到，&lt;/span&gt;「攻击社会工程的设计中有很多小聪明，但是就我所知，还没人开始使用机器学习来找到最容易上当受骗的那群人，」他停了停又说，「我本来应该回答你的是：我很抱歉，Dave，我现在不能回答这个问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 机器能做噩梦吗？MIT开发出能生成恐怖惊恐图片的深度学习算法</title>
      <link>http://www.iwgc.cn/link/3189608</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#ffffff"&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;font color="#7f7f7f"&gt;&lt;span&gt;&lt;b&gt;机器之心编译&lt;/b&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;万圣节就要到了，本着吓死人不偿命的精神，麻省理工学院（MIT）Media Lab 今日上线了一个用人工智能吓人的网站 Nightmare（噩梦）：http://nightmare.mit.edu/#portfolioModal22。在这个网站上，研究者展示了利用人工智能算法生成的恐怖风格的图片，其中包括埃菲尔铁塔等地标建筑和人脸等一些结果。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW62dH7PLqwRcjNwKOmHB4Cp4gqJ3CaZnzAaHsIdldZxiasSJiav1Ifqgg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;地标：美国自由女神像、法国埃菲尔铁塔、日本东京塔&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWvtnepvzBIDpZwGibicRVRT4GkVmrxQahUvKKeeQjjV5iaj6ux7VKibvBIw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;人脸&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，利用深度学习软件生成恐怖图像已经不是什么新闻了。其中著名的有谷歌的 Deep Dream 生成的带有许多眼睛的狗脸的图片；还有前段时间中国出现的 Uber「幽灵车」事件的恐怖司机头像也有人认为是软件生成的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW2GJV4ohA9MJ5hL7HFRfCWnVX5SEpY0faJiac9JU1dRZVenibDHkP7KkQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌的 Deep Dream 生成的狗脸&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWJho6OEbibSv4CE2BLZ211IS5XbtN6B5GNXjr8dqzbVQ1wyjuiaFiaEI4g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Uber「幽灵车」司机&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MIT 还在这个网站上列出了一个恐怖和人工智能交织发展的超短历史：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2000 年前：恐怖的起点&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;万圣节传统的神秘起源可以追溯到凯尔特人庆祝的古老异教节日。凯尔特人将这一天作为收获季节的结束和冬季的开端。他们相信这种季节的变换会打开死者世界的大门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1000 年前：人工智能的第一次迹象&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东西方文明都描绘了关于人造实体的传说故事——这些人造的存在能够思考、感知、帮助或伤害他人。在许多故事中，这些「生物」都脱离他们的创造者的控制，并获得了超越任何人预想的知识和能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1816 年：没有夏季的一年&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1816 年的春天经历了历史记录中最奇怪的气象现象：一个永无止境的冬天。这迫使三位作家将自己关在了日内瓦湖畔的豪宅中。玛丽·雪莱、约翰·威廉·波利多里、拜伦勋爵比赛看谁能写出最惊悚的故事。而他们所有人都获胜了。雪莱创造了弗兰肯斯坦；波利多里种下了吸血鬼文化的种子；拜伦则在他的诗作《黑暗（Darkness）》中通过地球上的最后一个人的讲述开启了世界末日惊悚题材的先河。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1840 年：第一个计算机程序诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能和恐怖惊悚开始交汇：拜伦勋爵（现代吸血鬼文学的创始之父）的妻子 Anne Isabella Milbanke 生下了计算机历史上一位先驱爱达·洛夫雷斯（Ada Lovelace）。她编写了世界上第一个机器算法，要知道，当时所谓的计算机器还仅存在于纸面之上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1930 年：恐怖惊悚兴起&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;20 世纪 30 年代的电影荧幕成为了在黑屋子里面吓人的前所未有的媒介。许多恐怖电影成了人们的消遣，其中包括弗兰肯斯坦、德古拉、木乃伊、隐形人、伦敦狼人……这也催生了一个有创造性的且有利可图的恐怖惊悚片行业。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1956 年：人工智能诞生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1956 年炎热的夏天，Marvin Minsky 和其它睿智的头脑聚集到了达斯茅斯学院。在一场创造力的爆发中，他们奠定了人工智能成长的基础：开发能够在西洋跳棋上击败人类、进行复杂数学计算……乃至能够生成英语句子等等的程序。有传言说计算机生成的第一个句子是：TRICK OR TREAT？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2016 年：人工智能驱动的恐怖&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几千年来，不同地域、不同宗教和不同文化的人都在创造吓人的方式。恐怖需要引发人内心的情绪才能在人类创造之中保留一席之地。在我们还不清楚人工智能的局限性的今天，这个挑战是尤其重要的：机器能够学会吓人吗？为了这一目标，MIT 推出了 Haunted Faces 和 Haunted Places：计算机通过深度学习算法和邪恶的灵魂生成恐怖惊悚图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，在这个网站上，MIT 还请求网友对他们的软件生成的恐怖图片进行评分。这些评分将作为 MIT 的这个恐怖图片生成模型的进化的训练数据，将使其能够生成越来越恐怖的图片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据了解，研究者首先通过鬼屋和末日城市的图片对他们的人工智能算法进行了训练，然后将一些著名地标的图片输入该模型。经过处理之后，该模型能让这些图片带上阴郁的地狱风格。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW9xHIqMhs4KCsq8tfUrAVvK4J1dq5bfzGfKGJsrvClnZN6cib3ibHziadg/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果 Elon Musk 和 Stephen Hawking 的关于人工智能对人类生存的威胁的警告还不够吓人，MIT 的这个故意吓人的项目可算是做到了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道我们未来的计算机主人会不会使用这种生成的恐怖图片来恐吓我们呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，让我们来认识一下该项目的三位研究开发者：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWUaEoCSHiamfhOicd3vKGXONvJvSbJby58R8wnBYPRL7yWg8YR0S9svtA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part1：基于语言建模的词嵌入模型</title>
      <link>http://www.iwgc.cn/link/3189609</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part1，&lt;span&gt;全面介绍了词嵌入模型，接下来几天机器之心将继续发布 Part2、Part3，希望能对大家了解词嵌入有所帮助。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); font-size: 16px; line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入简史&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;语言模型的简介&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;经典的自然语言模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;C&amp;amp;W model&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CBOW&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无监督学习词嵌入（word embeddings）在许多自然语言处理的任务中都取得了前所未有的成功，因此它常被视为自然语言处理的万灵药。实际上，在许多自然语言处理架构中，它们确实几乎替代了诸如布朗聚类（Brown clusters）和 LSA 特征等传统型分布式特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 ACL（计算机语言学会）和 EMNLP（在自然语言处理中实证方法会议）的会议论文很大程度都是词嵌入的研究，有些人还认为词嵌入这种嵌入方法比 EMNLP 更加适合的自然语言处理。今年的 ACL 会议有了不仅一个，而是两个的词嵌入模型的研讨会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入之间的语义关系在外行人看起来就像变魔术。深度自然语言处理的讲座常以「国王－男人＋女人≈女王」的幻灯片来做开场白，一篇最近在 Communications of the ACM 的文章向词嵌入模型致敬，并称之为自然语言处理实现突破的主要原因。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文将会是本系列第一篇全面介绍词嵌入模型的文章，将讨论词嵌入模型的热度是否会持续下去及其原因。在这个介绍里，我们将尝试把在这个领域分散的论文串联起来，强调很多模型、应用和有趣的特征，并将在后续的文章中重点关注多语言环境下的词嵌入模型和词嵌入评估任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这第一篇文章将呈现目前的基于语言建模的词嵌入模型。在我们深度讨论很多的模型时，我们会挖掘它们的优点，希望能够在过去和当前的研究的背景下提供新的见解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于命名方式的简单小结：接下来我们将使用当前热门的「词嵌入（word embeddings）」术语，来指代词语在低维度向量空间的稠密表示。「词嵌入」和「分布式表征（distributed representations）」是两种可互换的表示方法。我们将特别强调「神经词嵌入（neural word embeddings）」，即运用神经网络训练的词嵌入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;词嵌入简史&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从上世纪九十年代开始，向量空间模型就已在分布式语义中得到了应用。当时，许多用于预测连续空间的词表征的模型已经被研究了出来，其中包括隐含语义分析（LSA：Latent Semantic Analysis）和隐狄利克雷分布（LDA：Latent Dirichlet Allocation）。想要详细了解词嵌入背景下的分布式语义的历史的读者可以看看这篇文章：https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2003 年创造了词嵌入这个名词，并且在自然语言模型中将其与模型参数一起联合训练。据了解 Collobert 和 Weston 于 2008 年首次展示了预训练的词嵌入的实际应用。他们里程碑式的论文《A unified architecture for natural language processing》不仅将词嵌入确立成了一种可用于下游任务的有用工具，还引入了现在已经成为了许多方法的基础的神经网络架构。但是让词嵌入最终能流行起来的是 Mikolov 等人在 2013 年创立的 word2vec，这是一个允许无缝训练和使用预训练嵌入的工具套件。在 2014 年，Pennington 发布了一个具有竞争力的预训练的词嵌入集 GloVe，标志着词嵌入已经成为了主流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入是目前无监督学习的成功应用之一。它们最大的好处无疑是它们不需要昂贵的人工标注，而是从未标注的现成大数据集中派生的。然后预训练的词嵌入就可以运用在仅使用少量有标注数据的下游任务中了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;词嵌入模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然而然地，每个前向传播的神经网络都把在词汇表中的词语当成输入，并把它们表示成低维空间中向量。然后，它们再通过反向传播进行调整，得出词嵌入作为第一层的权重。通常，这称之为「嵌入层（Embedding Layer）」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;产生词嵌入作为副产物的神经网络和 word2vec 这样的以生成词嵌入为特定目标的方法之间的主要区别是它们的计算复杂度。对于一个大的词汇集来说，使用非常高深度的架构来生成词嵌入的计算成本太高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是为什么直到 2013 年词嵌入才进入自然语言处理的舞台。计算复杂度是词嵌入模型的一个关键权衡，也是我们这篇概述中会重复出现的主题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个区别就是训练目标：word2vec 和 GloVe 都是用来生成广泛语义关系的词嵌入模型，这对许多下游任务有用；而这种方式训练的词嵌入对不依赖于这种语义关系的任务并无太多帮助。相反，常规的神经网络对于某个特定任务生成的词嵌入在别的任务往往功能有限。值得注意的是，一个依赖于语言建模这样的语义关系的任务能够生成类似于词嵌入模型的嵌入，这一点我们将在下节探讨。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外说明一点，word2vec 和 Glove 之于自然语言处理，也许就像是 VGGNet 之于计算机视觉，亦即一个普通的权重初始化——它能提供有用特征，而且无需长时间训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比较不同的模型，我们可以设想如下的标准：我们设想一串来自词汇库 V（其大小为|V|）的包含 T 个训练单词的的字符序列 w_1,w_2,w_3,⋯,w_T。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象我们的模型是一段包含 n 个单词的文段。我们将每一个单词与一个 d 维的输入向量 v_w（嵌入层中的同名词嵌入）和一个输出向量 v_w'（另一个词表征，其作用下面很久就会介绍）联系在一起。最终，对于每一个输入 x，我们相对于模型参数θ和模型输出分数 f_θ(x) 来优化目标函数 J_θ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言建模上的一项注意&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入模型和语言模型联系紧密。对语言模型的质量评价基于它们学习 V 词汇库的词语概率分布的能力。事实上，许多最新的词嵌入模型一定程度上尝试预测序列下一个会出现的词。另外，词嵌入模型的评价通常运用困惑度（perplexity）——一个从语言建模借来的基于交叉熵的评价标准。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我们进入词嵌入模型的众多细节之前，让我们简单介绍一些语言建模的基础知识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总体而言，语言建模力求在给定之前的词语的情况下，计算一个词语 w_t 的出现概率，也就是 p(w_t|w_{t−1},⋯w_}t−n+1})。运用链式法则和马尔可夫假设，我们就可以近似地通过之前出现的 n 个词得到每个词的概率乘积，从而得到整个句子或整篇文章的乘积：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWicOE3T44QVXmJJBnfWKckT1iccqtqAvyPlnZGtbdANbCicBKPGVC9EcMw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在基于 n 元的语言模型中，我们可以用一个词的组分的 n 元的频率（frequency）来计算这个词的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW89AoRESTK3jPFEhlHHzu4YUicI6Tguzia2cp51tLMSgaqhMMqiczUXTog/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设置 n=2 产生二元模型，而 n=5 和 Kneser-Ney 则一同呈现平滑的五元模型——平滑的五元模型在语言建模中是公认的的一个强有力基准线。更多的细节，敬请参照斯坦福大学的演讲课件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在神经网络中，我们通过大家熟知的 Softmax 层来计算相同的目标函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWWz7wZibufQfmlfkAtMuAnOadPjsRsWEwfz3F34Wx16l1WdpQEWNFLUg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;内积 h^T v'_{w_t} 计算了词 w_t 的未标准化的对数－概率（log-probability），我们用在词汇库 V 中的所有词的对数－概率之和来把它标准化。h 是它的倒数第二层（见图 1 前向传播网络的隐藏层）的输出向量，而 v'_w 就是词 w 的输出嵌入，亦即在 softmax 层的权重矩阵中的表征。注意虽然 v'_w 可以表征词 w，但它是从输入词嵌入 v_w 独立学习的，因为向量 v'_w 和向量 v_w 的相乘对象是不同的（v_w 和索引向量相乘，v′_w 和 h 相乘）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW90nRVHqppUJW0O2Hgzxibsg4KnpkfznAzWxOicO5WUoJUKK8UdfT2GLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1: 一个自然语言模型（Bengio 等人，2006 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要计算每个词 w 在神经网络输出层的概率。想要高效地做到这一点，我们将 h 和一个权重矩阵相乘，这个权重矩阵每行都是对于在 V 中出现的词 w 所得的 v′_w。我们随后将得到的向量（我们通常称之为 logit，也就是前一层的输出）以及 d=|V| 传入到 softmax 层，softmax 层则把词嵌入「压扁」成一个词汇库 V 里面词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 softmax 层（对比于之前的 n 元计算）仅仅隐式地考虑之前出现的 n 个词。长短时记忆模型（Long Short-term Memory, 英文简称 LSTM），通常用来作自然语言处理模型，将这些词编码成状态 h。我们在下一章将会介绍的 Bengio 的自然语言模型，则是把之前的 n 个词通过一个前向传播层传入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请大家记住这个 softmax 层，许多后续介绍的词嵌入模型都将或多或少地运用它。运用这个 softmax 层，模型将尝试着在每一时刻 t 都最大化正确预测下一词的概率。于是，整个模型尝试最大化整个数据集的平均对数－概率:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWpcHJLCvDnQlKZcE77WSicYKhiafQzE0z1DXda8nHmsPaqr6WDbaW53yA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相似地，运用链式法则，模型的训练目标通常是最大化整个语料库的所有词相对于之前 n 个词的平均对数－概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWusgOAUobrxHnr6orodvhibE4W7OepmxR7NwnXg5zd0YQvM8uw7MicelA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果想在试验阶段从这个语言模型取样词，我们可以在每一时刻 t 贪婪地选择最高概率的词 p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})，或者用定向搜索。举个例子，我们可以用它来生成像运用了 LSTM 作为解码器的 Karpathy 的 Char-CNN 中的任意文本序列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;经典神经语言模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人在 2013 年 [1] 提出的经典神经语言模型包含一个前向传播神经网络，它有一个隐藏层，用来预测文本序列的下一个单词，如图 2 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWfEyQxhGahfuuZq0geAAop5IoibjhibdaeqibXfHhVz28kpKGRnlOAb0QQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2: 经典神经语言模型（Bengio 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型的最大化目标函数就是我们在上文中介绍的典型的神经语言模型的目标（为了简洁，我们忽略规范化（regularization）这一项）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWn1icJveGOrgts4xhvSSibJPX0fpAldGN2koShDuqkmrbibXjyicWSj2uqA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;f(w_t , w_{t-1} , \cdots , w_{t-n+1}) 是这个模型的输出，即 softmax 计算出的概率 p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})。n 在这里就是传入这个模型的之前 n 个词的数量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 等人首先提出词嵌入模型，它是一个在实数范围 R 内的词特征向量。他们的架构非常经典，是目前各种改进方法的原型。他们原始模型中的基础模块依然能在现在的许多神经网络和其他词嵌入模型中找到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;嵌入层：一个用索引向量和词嵌入矩阵相乘得出的词嵌入层；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;中间层：一个可包含一层或多层的中间层，例如，一个可以将之前出现的 n 个词非线性地组合在一起的全连接层（fully－connected layer）；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Softmax 层：一个最终层，输出词汇库 V 中词的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，Bengio 等人发现了目前最先进模型中存在的两个核心问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现 2. 中间层可以由一个 LSTM 替代，这个已被最新神经语言模型使用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们发现最后的 softmax 层（更确切地说，是标准化项）是神经网络的瓶颈，因为计算 softmax 的计算复杂度与词汇库 V 中词的数量成正比，而个数量通常为成百上千，乃至几百万。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，如何在一个大词汇库中用较低的计算成本计算 softmax，成为了建立神经语言模型和词嵌入模型的一个关键挑战。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;C&amp;amp;W 模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Bengio 等人对神经语言模型的的最初探索以后，计算机计算能力和算法还尚不允许在大词汇库上的训练。词嵌入模型的研究因而止步不前。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Collobert 和 Weston [4]（因此被称为 C&amp;amp;W）在 2008 年展示了词嵌入模型在一个充分大的数据库中如何向下游任务携带语法和语义，并且提升性能。他们 2011 年的论文充分解释了他们的做法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的解决方法避免了对于 softmax 层的昂贵计算，其方法是采用另一个目标函数：&lt;/span&gt;&lt;span&gt;Collobert 和 Weston 的神经网络输出是正确词序列相对于不正确词序列高出的分数 f_θ，而不是 Bengio 等人的论文中用来最大化基于之前的词出现的下一个词概率的的交叉熵标准。他们为了这个目标函数采用了一个成对排名的标准，如下所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWAEQonuI5CpSO632Pmn7be0eQeRkf9wxDG3Rf3qTE4OU0XG8pBXibW1Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们的模型从所有包含 n 个词的窗口 X 中取样得到正确的窗口 x。对于每一个窗口 x，用 V 中的 w 代替 x 的中间词来产生一个不正确的版本 x(w)，而模型的目标就是最大化模型对于正确的窗口和不正确窗口的分数的距离。如图 3 所示，他们的模型架构类似于 Bengio 等人的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyW8HspRpB0o7WThGibaVeZbbtN4BoNOMMVcDXCKN86QIlsDbACKyrR1mA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3: 去掉了排名目标的 C&amp;amp;W 的模型（Collobert 等人，2011 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;计算后的语言模型能够生成具有许多语义关系的词嵌入，例如国家能够聚类在一起，语法上接近的词在向量空间上相邻。他们的排名函数避免了 softmax 的复杂计算，且保持了 Bengio 等人论文中计算同样昂贵的完全相连的中间层（2.）（见图 3 中的 HardTanh 层）。他们对于 130000 个词的模型需要花费 7 周来训练的有一部分原因在于此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们现在介绍当今毫无疑问最为流行的词嵌入模型 word2vec，它源于 Mikolov 等人在 2013 年中两篇论文，且催生了上千篇词嵌入的论文。正因为词嵌入模型是自然语言处理中深度学习的一个关键的模块，word2vec 通常也被归于深度学习。然而严格上来说，word2vec 并不属于深度学习，因为它的架构并非多层，也不像是 C&amp;amp;W 模型一般运用非线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在他们的第一篇文章 [2] 中，Mikolov 等人提出了更低计算成本的学习词嵌入的两个架构。他们的第二篇论文 [3] 通过加入更多的提升了训练速度和准确度的策略来提升了模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些架构提供了对比于 C&amp;amp;W 模型和 Bengio 模型具有如下两大优点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们去掉了昂贵的中间层。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们运用语言模型来更多地考虑上下文。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们等等会讲到，他们的模型之所以成功不仅是因为这些改变，而更是因为某些特定的训练策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们会来看这两个架构：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;连续的词袋（CBOW）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;言模型只能通过观察之前出现的词来进行预测，且对于此类模型的评价只在于它在一个数据集中预测下一个词的能力，训练一个可以准确预测词嵌入的模型则不受此限。Mikolov 等人运用目标词前面和后面的 n 个词来同时预测这个词，见图 4。他们称这个模型为连续的词袋（continuous bag-of-words，或者 CBOW），因为它用连续空间来表示词，而且这些词的先后顺序并不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWXSGmO8jAuKFwv9zmrvYCplt3EGa7zzYUGHNfQskm1FyDxLmnXaYTAg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4：连续的词袋（Mikolov 等人，2013 年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 的目标函数和语言模型仅有着细小差异：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWbYXukF6Eicj6IqicqHRgFonWBLHib9ef3NasDyN9wnCQ4l4RlkMicmG40A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个模型并没有传入 n 个之前的词，而是在每个时刻 t 接收目标词的前后 n 个词的窗口 w_t。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Skip-gram&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CBOW 可以看作一个具有先知的语言模型，而 skip-gram 模型则完全改变将语言模型的目标：它不像 CBOW 一样从周围的词预测中间的词；恰恰相反，它用中心语去预测周围的词，如图 5 所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWzfBaiaf9dNJkKtYCspNhMjnHj0TXRYMVB327oc96B6RaHqKjS1v63lQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 5：Skip-gram（Mikolov 等人，2013）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;skip-gram 模型的目标因此用目标词前后的各 n 个词的对数──概率之和计算如下的目标：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWnjIBuKqBgQnAibFvxvM3Z24AB5qH8T05e50nw1JnoobMSKDOWQubIIQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更直观地解释 skip-gram 模型是怎样来计算 p(w_{t+j}|w_{t}) 的，让我们先来回顾 softmax 的定义：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWVT2D38N3IOCZzfl2WicwaOyg7zEgQjyvOc4ooZeR8uVAP3McspuxAOQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不计算目标词 w_t 基于前面出现的词的概率，而是计算周围词 w_{t+j} 对于 w_t 的概率。于是，我们可以简单地替换掉这些变量：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWiaNWtVic3M07x51F1hiaVYfLhG0I8CPG2he33YQ5vvs5FibCoV9B3FCKOg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 skip-gram 架构并不包括能够产出中间状态向量 h 的中间层，h 自然地成为对于输入词 w_t 的词嵌入 v_{w_t}。这也是我们为什么想给输入向量 v_w 和输出向量 v′_w 以用不同的表示，因为我们想要将词嵌入和自己相乘。用 v_{w_t} 替换 h，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWqDibSBVj9jFLGxEAZ0X8RNkYvpchHuCRoC7vqibjec11SjJibz7Muk8vg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意 Mikolov 论文中的代号和我们的有细微差别，他们标注词语为 w_I，而周围的词为 w_O。如果我们用 w_I 替换 w_t，用 w_O 替换 w_{t+j}，然后根据乘法交换律交换内积的向量位置，我们能够得到和它们论文中一样的公式表示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW979YOlDwRgcz8Hp09lxbyWHicxcOHpT6EMTyHuBUU4GnxZtQMzY0FsHicc2QqzZHmQ5BhnWcqyl1Ow/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一篇博文，我们将要讨论近似昂贵的 softmax 函数的不同方式，以及令 skip-gram 成功关键的训练决策。我们也会介绍 GloVe[5]，一个基于矩阵乘法分解的词嵌入模型，并讨论词嵌入和分布式语义的关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;References&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Bengio, Y., Ducharme, R., Vincent, P., &amp;amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. http://doi.org/10.1162/153244303322533223 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. http://doi.org/10.1145/1390156.1390177 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp;amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from http://arxiv.org/abs/1103.0398 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models, 12. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 23 Oct 2016 11:21:16 +0800</pubDate>
    </item>
  </channel>
</rss>
