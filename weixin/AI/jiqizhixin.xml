<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 图文并茂的神经网络架构大盘点：从基本原理到衍生关系</title>
      <link>http://www.iwgc.cn/link/2707037</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自THE ASIMOV INSTITUTE&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：FJODOR VAN VEEN&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着新型神经网络架构如雨后春笋般地时不时出现，我们已经很难再跟踪全部网络了。要是一下子看到各种各样的缩写（DCIGN、BiLSTM、DCGAN……），真的会让人有点招架不住。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为此，Fjodor Van Veen 写出了一篇包含了大量架构（主要是神经网络）的盘点性文章，并绘制了直观的示意图进行说明。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIozc8EXjnQRNnwsphcY2wqXu4YvVibcg1JNE4rRe0OKibYra7aQujPCFGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这些架构绘制成节点图的一个问题：它并没有真正展示这些架构的工作方式。比如说，变自编码器（VAE）可能看起来和自编码器（AE）一样，但其训练过程却相当不同。训练好的网络的使用案例之间的差别甚至更大，因为 VAE 是生成器（generator），你可以在其中插入噪声来得到新样本；而 AE 只是简单地将它们的输入映射到其所「记得」的最接近的训练样本。所以必须强调：这篇概览中的不同节点结构并不能反映出这些架构的内在工作方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;列出一份完整的列表实际上是不可能的，因为新架构一直在不断出现。即使已经发表了，我们可能很难找到它们，而且有时候还会不自觉地忽略一些。所以尽管这份清单能为你提供人工智能世界的一些见解，但无论如何请不要认为这份清单是全面的；尤其是当你在这篇文章写出后很久才读到时（注：本文原文发表于 2016 年 9 月 14 日）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于本文中图片所描绘的架构，作者都写了一点非常非常简短的说明。如果你很熟悉其中一些架构，但不熟悉另一些，你可能会觉得这些说明会有用处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;前馈神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（FF 或 FFNN：Feed Forward neural networks）是非常简单的：它们从前向后馈送信息（从输入到输出）。神经网络常被描述为层级形式，其中的层（layer）可能是输入层、隐藏层或输出层。一个单独的层不存在什么连接（connection），而通常相邻的两个层是完全连接的（一个层的每一个神经元都连接到另一个层的每一个神经元）。其中可以说是最简单的实际网络具有两个输入单元和一个输出单元，其可用于对逻辑门进行建模。人们常常通过反向传播（back-propagation）来训练 FFNN，从而让该网络获得配对的数据集——「输入的内容」和「我们想要得到的输出」。这被称为监督学习（supervised learning），其相反的方法被称为无监督学习（unsupervised learning），其中我们只需要给出输入然后让网络自己填补空白。被反向传播的误差（error）常常是输入和输出之间差分（difference）的某种变体（如 MSE 或只是线性差分）。如果该网络有足够的隐藏神经元，那么理论上它总是能够建模出输入和输出之间的关系。实际上它们的使用存在很大的限制，但它们常被用来与其它网络结合以构建新的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;径向基函数&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBF: Radial basis function）网络是使用径向基函数作为激活函数（activation function）的 FFNN。没什么其它的了。但这不意味着它没有用处，但大部分带有其它激活函数的 FFNN 都没有自己的专用名称。这主要是因为人们在正确的时间发明了它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoZK46l9hfANzd0kft6Gpib6ibROQNjdwgJ1Tjz9huxc8Iiba3moVAzP8xw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;霍普菲尔德网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（HN: Hopfield Network）是一种每一个神经元和其它每一个神经元都有连接的网络；它是完全纠缠在一起的意大利面条，其中所有的节点都是全功能的。在训练之前，每一个节点都是输入；在训练过程中，每一个节点都是隐藏；之后它们都是输出。这种网络的训练是：将神经元的值设置成我们想要的模式，从而计算出权重（weight）。之后权重便不再变化。一旦为一种或多种模式进行了训练之后，这种网络总是会收敛成其学习过的一种模式，因为这种网络只能稳定在这些状态。请注意它并不是符合预期的状态（悲伤的是它并不是魔法黑箱）。因为该网络的总「能量（energy）」或「温度（temperature）」在训练过程中会逐渐减小，所以它总会一部分接一部分地稳定下来。每一个神经元都一个可以扩展到这个温度的激活阈值，而如果该神经元的输入总和超过了该阈值，那么输入就会使神经元从两个状态（通常是 -1 或 1，有时候是 0 或 1）之中选择一个。网络的更新可以同步完成，但更常见的是一个接一个更新神经元。如果是一个接一个地更新，就会创建一个公平随机（fair random）的序列来组织哪些单元以哪种顺序更新（公平随机是指所有（n）的选择在每 n 个项中只恰好发生一次）。这样你就能分辨网络何时达到了稳定（收敛完成）：一旦每一单元都被更新后而其中没有任何改变，那么该网络就是稳定的（即退火了的（annealed））。这些网络常被称为联想记忆（associative memory），因为其会收敛到与输入最相似的状态；人类看到半张桌子就能想象出另一半，类似地，如果给这种网络提供半张桌子和一半噪声，那么该网络就能收敛出一张桌子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoQUibSkBabC7ggyvFTvqiaTlD9xjKEiaZ3uBRu4HdGKwvTyIZzKvibJzk5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;马尔可夫链&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（MC：Markov Chain）或离散时间马尔可夫链（DTMC: discrete time Markov Chain）是 BM 和 HN 的某种前辈。可以这样理解：从我目前所处的节点开始，到达我周围任何节点的概率是多少？它们是无记忆的（即马尔可夫特性（Markov Property）），这意味着你所得到的每一个状态都完全依赖于其之前的一个状态。尽管算不上是神经网络，但它们确实类似于神经网络，并提供了 BM 和 HN 的理论基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo9l0nEQvRtRtjx5d0KYZNwt6kjLmXb9U9OaxybNbjo94km4yCSMftJA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BM：Boltzmann machines）和 HN 非常相似，除了：一些神经元被标记为输入神经元，而其它的仍然是「隐藏的」。这些输入神经网络会在整个网络更新结束时变成输出神经元。其开始时是随机权重，然后通过反向传播学习，最近也有人使用对比发散（contrastive divergence）的方法（使用一个马尔可夫链来确定两个信息增益之间的梯度）。和 HN 相比，BM 的神经元有时也有二元激活模式（binary activation patterns），但其它时间它们是随机的：一个单元处在一个特定状态的可能性。BM 的训练和运行过程非常类似于 HN：首先为输入神经元设置特定的钳位值（clamped values），然后该网络就自由了（不需要外力了）。自由了之后这些单元能得到任何值，然后我们在输入和隐藏神经元之间反复来回。它最后会在合适的温度下达到平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoMIHO0icG0JicFyg70RicjFNnZS9N1k1iaeQNgcsFMOs19TYVOxHThNLicCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;受限玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBM：Restricted Boltzmann machines）类似于 BM（这毫不奇怪），所以也类似于 HN。BM 和 RBM 之间的最大不同之处是 RBM 是更受限的，所以也可被更好地使用。它们并不将每一个神经元和其它每一个神经元连接起来，而是只将每组不同的神经元和其它每一组连接起来，所以输入神经元不会直接连接到其它输入神经元，隐藏神经元之间也没有连接。RBM 可以以类似 FFNN 的方式训练，但也有一点不同：不是前向通过数据然后反向传播误差，而是前向通过数据之后再将这些数据反向传回（回到第一层）。在那之后再使用前向和反向传播进行训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo5nHUmWbXBrJfaEvBtEBI42ISBBXZxfXfIdvNribfqcjzsN6vyEc4sRA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（AE：Autoencoders）有一点类似于 FFNN，因为 AE 更像是 FFNN 的一种不用的用例，而非一种根本上不同的架构。自编码器背后的基本思想是自动编码信息，也因此得名。其整个网络有一种沙漏般的形状——其隐藏层比输入层和输出层都小。AE 也是围绕中间层对称的（根据层的数量是奇数或偶数，中间层有 1 层或 2 层）。最小层总是位于中间，这里的信息得到了最大的压缩（该网络的阻塞点（ chokepoint））。中间以上的所有部分被称为编码（encoding）部分，中间以下的所有部分则被称解码（decoding）部分，中间部分则被称为代码（code）。人们可以通过馈送输入以及将误差设置成输入和输出之间的差异的方式，使用反向传播来训练它们。当涉及到权重时， AE 还可以对称式的构建，所以编码权重和解码权重一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo03R1IdiblEmC8DKJ1Npa1AZY5oqniaqlEAzrOZheGia6BHueF8S04kUsA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稀疏自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SAE: Sparse autoencoders）在某种程度上是 AE 的反面。它不是在更少的「空间（space）」或节点中教一个网络表征一些数据，而我们试图在更多空间中编码信息。所以不再是网络在中间收敛然后扩展回输入大小，我们直接消除了中间内容。这些类型的网络可被用于从数据集中提取许多小特征。如果我们以类似于 AE 的方式训练一个 SAE，在几乎所有情况下你都只会得到一个相当无用的恒等网络（输入即是输出，没有任何变换或分解）。为了防止这种情况，我们不反馈输入，而是反馈输入加稀疏驱动器（sparsity driver）。这个稀疏驱动器可以以阈过滤器（threshold filter）的形式，其中只有一个特定的误差会被传播回去和训练，在这次通过过程中其它的误差都将是「无关的」，会被设置为 0。在某种程度上这类似于脉冲神经网络（spiking neural networks），其中并不是所有的神经元在所有时间都在放电（以及为生物合理性给出分数）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoVLich5P7QqGIn8pmWXaQXZwia1SH6JwgUzosoTrl909ARThb8wqZiboKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;变自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（VAE：Variational autoencoders&amp;nbsp;）的架构和 AE 一样，但被「教授」了不同的东西：输入样本的一个近似概率分布。这有点回到本源的感觉，因为它们和 BM 及 RBM 的联系更紧密一点。但它们确实依赖于贝叶斯数学来处理概率推理和独立（probabilistic inference and independence），以及依靠重新参数化（re-parametrisation）来实现这种不同的表征。这种推理和独立部件理解起来很直观，但它们或多或少依赖于复杂的数学。其基础可以归结为：将影响考虑在内。如果某种事物在一个位置发生，而其它地方则发生其它事物，那么它们不一定是相关的。如果它们不相关，那么误差传播应该考虑一下这一点。这是一种有用的方法，因为神经网络是大型的图（graph，从某种角度来看），所以在深入到更深的层时如果排除掉一些节点对其它节点的影响，就会带来帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoLhK2lTuynsA7rPTwRbetepQLqf42v3vl5JoGJ5qGzAUtjGywuOxS9A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;去噪自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DAE: denoising autoencoders）是一种输入中不仅包含数据，也包含噪声（比如使图像更有颗粒感）的自动编码器。但我们以同样的方式计算误差，所以该网络的输出是与不带噪声的原始输入进行比较。这能让网络不会学习细节，而是学习更广泛的特征，因为学习更小的特征往往会被证明是「错误的」，因为更小的特征会不断随噪声变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIom8aNXHMas0LlU5llYQG6KNia03jw5ldRicUvOECuHEsU9oIBXSvciaVyA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度信念网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DBN: deep belief networks ）基本上是 RBM 或 VAE 堆叠起来的架构。事实已经证明这些网络可以堆叠起来高效地训练，其中的每一个 AE 或 REM 只必须编码编码之前的网络即可。这种技术也被称为贪婪训练（greedy training），其中贪婪是指得到局部最优的解决方案，从而得到一个合理的但可能并非最优的答案。DBN 可通过对比发散（contrastive divergence）或反向传播进行训练，以及学习将数据表征为概率模型，就像普通的 RBM 或 VAE 一样。一旦通过无监督学习训练或收敛成了一个（更）稳定的状态，该模型就可被用于生成新数据。如果采用对比发散进行训练，它甚至可以对已有的数据进行分类，因为其神经元已经学会了寻找不同的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIos3J9KTzcMg67bBVZ2estY6AEibksUHvXxwIXrUEnm1KDI9w8rx6brxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（CNN：convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）和其它大多数网络非常不同。它们主要被用于图像处理，但也可应用于音频等其它类型的输入。CNN 的一种典型的用例是让网络对输入的图像进行分类，比如，当输入的图像上有猫时输出「cat」、有狗时输出「dog」。CNN 往往开始带有一个输入「扫描器（scanner）」，其目的是不一次性解析所有的训练数据。比如要输入一张 200×200 像素的图像，你并不需要一个带有 40000 个节点的层。事实上，你只需要创建一个比如说 20×20 的扫描输入层，这样你就可以从该图像的一个 20×20 像素的部分开始输入（通常是从左上角开始）；一旦这个输入完成后（可能是用于训练），你再输入下一个 20×20 像素：将该扫描器向右移 1 个像素。注意人们不会一次性移动 20 个像素（扫描器的宽度），也不是将图像分解成 20×20 的块；相反，而是让扫描器在图像上「爬行」。然后这些输入数据被送入卷积层（convolutional layers），这和普通的层不一样，其中所有的节点并非连接到所有的节点。每一个节点仅将它自己与其近邻的单元连接起来（到底多近取决于具体的实现，但通常不止一点点）。这些卷积层往往会随着网络越来越深而缩小，大部分是按照输入可以轻松整除的因子（所以 20 后面的层可能是 10 ，然后是 5）。这方面常使用 2 的幂，因为它们可以通过 32, 16, 8, 4, 2, 1 这样的定义完全整除。除了这些卷积层，它们常常还有池化层（pooling layer）。池化是一种滤除细节的方法：一种常见的池化技术是最大池化（max pooling）——其中我们取比如 2×2 的像素，然后根据最大量的红色传递这些像素。为了将 CNN 应用到音频上，基本上是输入音频波然后缓慢移动音频片段，一段接一段。CNN 的真实世界实现往往会在末端连接一个 FFNN 以便进一步处理数据，这可以实现高度非线性的抽象。这样的网络被称为 DCNN，但这两者的名字和缩写往往可以混用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoW79bJlXfY5cQPbQLhxBTGBFb64ibLlP9lRQiaAzRtJWJ7N3o2hEv9mYw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;解卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DNN：Deconvolutional neural networks）也被称为逆图形网络（IGN： inverse graphics networks），是反向的卷积神经网络。比如给网络输入一个词「cat」，然后训练它生成一张类似猫的图像（通过将其与真实的猫图片进行比较）。和普通的 CNN 一样，DNN 也能和 FFNN 结合使用，但我们就不给这种网络缩写了。我们也许可以将其称之为深度解卷积神经网络，但你也可以认为当你在 DNN 的前端和后端都接上 FFNN 时，你得到的架构应该有一个新名字。请注意在大多数应用中，人们实际上并不会为该网络送入类似文本的输入，而更多的是一个二元的分类输入向量。比如设 &amp;lt;0, 1&amp;gt; 是猫，&amp;lt;1, 0&amp;gt; 是狗，&amp;lt;1, 1&amp;gt; 是猫和狗。CNN 中常见的池化层往往会被相似的逆向运算替代，主要使用偏差假设（biased assumptions）做插值和外推（interpolation and extrapolation ）（如果一个池化层使用的是最大池化，你可以通过其逆向过程产生特定度更低的新数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo2TF31peYW3w5W7IS2dn7gqJm6hXzJzsSIoAyaxWZNxbfeyDc4Og4bA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度卷积逆向图网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DCIGN：Deep convolutional inverse graphics networks）的名字比较有误导性，因为它们实际是 VAE，但有 CNN 和 DNN 分别作为编码器和解码器。这些网络试图在编码中将特征建模为概率，以便于它能在曾经分别看到猫和狗的情况下，学习产生同时带有猫和狗的图片。类似的，你能给它输入一张带有猫和狗的图片，要求网络去掉图片中的狗，即使之前你未曾做过这样的操作。已有演示表明这些网络也能学习模型图片上的复杂变化，比如改变光源或者 3D 目标的旋转。这些网络往往通过反向传播训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIokRw3rtLTUmiaCI54j51ztiawEOZic5lryoHqwJPwWLRUia9OYMiassnWfHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;生成式对抗网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GAN：Generative adversarial networks）源于不同的网络类型，它们是双胞胎：两个网络一起工作。GAN 包含任意两种网络（尽管通常是 FF 和 CNN），一个网络的任务是生成内容，另一个是用于评判内容。判别网络要么获取训练数据，要么获取来自生成网络的内容。判别网络能够多好地准确预测数据源的程度然后被用来作为生成网络的误差。这创造了一种竞争方式，判别器区别真实数据与生成数据上做得越来越好，而生成器也变得对判别器而言越来越难以预测。这效果很好的部分原因是即使相当复杂的类噪音模式最终也是可预测的，但生成的类似于输入数据的内容更难以学习进行区别。GAN 训练起来相当难，因为不仅要训练两个网络（每个解决各自的问题），两个网络的动态也要平衡好。如果预测或生成相比于对方更好，GAN 收敛不好，因为存在有内在的分歧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoNWSgWHRibWrvmAeYziaxoicLfxG2dOxHOmTNuKo5eB4RmQBD9vtvslIhg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RNN：Recurrent neural networks）是带有时间联结的 FFNN：它们不是无状态的，它们随时间变化在通路与连接之间有联系。神经元不只从前面层中被输入信息，也从来自它们自己的之前的通过中获得信息。这意味着你输入信息和训练网络的顺序很重要：输入「牛奶」然后是「甜饼」与输入「甜饼」然后是「牛奶」相比可能会产生不同的结果。RNN 的一个重大问题是梯度消失（或爆炸）问题，取决于使用的激活函数，信息随时间渐渐损失，就像很深的 FFNN 随深度变化消失信息一样。直观上这看起来不是大问题，因为这些只是权重，不是神经元状态，但随时间变化的权重正是来自过去信息的存储。如果权重达到 0 或 1,000,000 的值，先前的状态就不在具有信息性。RNN 理论上可被用于多个领域，因为大部分的数据形式没有时间线上的变化（也就是不像声音和视频），所以时间决定的权重被用于序列之前的东西，不是多少秒之前发生的内容。大体上，循环网络是发展或完善信息的较好选择，比如 autocompletion（自动完成）任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoibDcrxP15h4Y5ib1cDW8ofB3tJegjF0fmojcLtjxRKN5DJBzsxia6mrww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSTM：Long / short term memory）试图通过引入门（gate）和明显定义的记忆单元对抗梯度消失（爆炸）问题。这个思路受到电路图的启发，而不是生物学上的概念，每个神经元有一个记忆单元和 3 个门：输入、输出、遗忘（ input, output, forget）。这些门的功能是通过禁止或允许其流通确保信息。输入门决定来自上层的信息有多少被该单元存储。输出层在另一端做同样的事，并决定下一层多么了解该细胞的状态。遗忘门看起来像是一个奇怪的东西，但有时被遗忘反而更好。已有实验表明 LSTM 能够学习复杂的序列，比如像莎士比亚一样写作，或者创造交响乐。注意每个门在之前神经元中都有一个权重，所以运行起来需要更多的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo6ush5bNXBSm2XVw0l9dHrJuqjGUPibQQmYZ9VB9NG7FaewoxKk9ma2Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;门循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GRU：Gated Recurrent Units）是 LSTM 的一种轻量级变体。它们有一个门，连线方式也稍微不同：没有输入、输出、遗忘门，它们有一个更新门（update gate）。该更新门既决定来自上个状态的信息保留多少，也决定允许进入多少来自上个层的信息。重置的门函数很像 LSTM 中遗忘门函数，但位置稍有不同。GRU 的门函数总是发出全部状态，它们没有一个输出门。在大多案例中，它们的职能与 LSTM 很相似。最大的不同就是 GRU 更快、更容易运行（但表达力也更弱）。在实践中，可能彼此之间要做出平衡，当你需要具有更大表达力的大型网络时，你可能要考虑性能收益。在一些案例中，额外的表达力可能就不再需要，GRU 就要比 LSTM 好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoKTZV5cN1E3SIbD7fshEFOyvGDQBy2ZwSHmCs2mbZMicyc4ahYvy7nmg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经图灵机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（NTM：Neural Turing machines）可被理解为 LSTM 的抽象化，并试图将神经网络去黑箱化（ un-black-box，让我们洞见里面到底发生了什么。）NTM 中并非直接编码记忆单元到神经元中，里面的记忆是分离的。这种网络试图想将常规数字存储的功效与永久性和神经网络的效率与表达力结合起来。这种网络的思路是有一个可内容寻址的记忆库，神经网络可以直接从中读取并编写。NTM 中的「Turing」来自于图灵完备（Turing complete）：基于它所读取的内容读取、编写和改变状态的能力，意味着它能表达一个通用图灵机可表达的一切事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;双向循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiRNN：Bidirectional recurrent neural networks）&lt;/span&gt;&lt;strong&gt;&lt;span&gt;、双向长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiLSTM：bidirectional long / short term memory networks ）&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;strong&gt;&lt;span&gt;双向门控循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiGRU：bidirectional gated recurrent units）在词表中并未展现，因为它们看起来和各自单向的结构一样。不同的是这些网络不仅连接过去，也连接未来。举个例子，通过一个接一个的输入 fish 这个词训练单向 LSTM 预测 fish，在这里面循环连接随时间记住最后的值。而一个 BiLSTM 在后向通路（backward pass）的序列中就被输入下一个词，给它通向未来的信息。这训练该网络填补空白而非预报信息，也就是在图像中它并非扩展图像的边界，而是可以填补一张图片中的缺失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoq4L9tcXDddYXBicia4pRxUApwVmqskGyAB3UGbWmA0ERFyl1nxIXDO2A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度残差网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DRN：Deep residual networks）是非常深度的 FFNN 网络，有着额外的连接将输入从一层传到后面几层（通常是 2 到 5 层）。DRN 并非是要发现将一些输入（比如一个 5 层网络）映射到输出的解决方案，而是学习将一些输入映射到一些输出 + 输入上。大体上，它在解决方案中增加了一个恒等函数，携带旧的输入作为后面层的新输入。有结果显示，在超过 150 层后，这些网络非常擅长学习模式，这要比常规的 2 到 5 层多得多。然而，有结果证明这些网络本质上只是没有基于具体时间建造的 RNN ，它们总是与没有 gate 的 LSTM 相对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIodzeRf3sxfhibMgeD08QDWnqPPb6vtjS7QTqJfibACPqR9BLEibDQm8AQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回声状态网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（ESN：Echo state networks）是另一种不同类型的网络。它不同于其他网络的原因在于它在不同神经元之间有随机连接（即，不是在层之间整齐连接。），而且它们训练方式也不同。在这种网络中，我们先给予输入，向前推送并对神经元更新一段时间，然后随时间观察输出，而不是像其他网络那样输入信息然后反向传播误差。ESN 的输入和输出层有一些轻微的卷积，因为输入层被用于准备网络，输出层作为随时间展开的激活模式的观测器。在训练过程中，只有观测器和隐藏单元之间连接会被改变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoTqE2dneEzvlWHyuJuhuenQicibmeYgOroneF6sBzbZ3jmzpRjeOTACbw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;液态机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSM：Liquid state machines）看起来与 ESN 非常类似。不同的是，LSM 是脉冲神经网络（spiking neural networks）这一类型的：用阈值函数取代 sigmoid 激活函数，每个神经元也是一个累加记忆细胞。所以当更新神经元的时候，里面的值并不是被设为临近值的总和，也不是增加到它自身上。一旦达到阈值，它将能量释放到其他神经元。这就创造出了一种类似 spiking 的模式——在突然达到阈值的之前什么也不会发生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoJCe2uWVmDXbmxW82tc7XOFgibvcwicugcdvibXiavCL0vVkcfAzTPTd8eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;支持向量机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SVM：Support Vctor Machines）能发现分类问题的最佳解决方案。传统上只能够分类线性可分的数据，比如说发现哪个图像是加菲猫，哪张图片是史努比，不可能有其他输出。在训练过程中，SVM 可被视为在一张图上（2D）标绘所有数据（加菲猫和史努比），并搞清楚如何在这些数据点间画条线。这条线将分割数据，以使得加菲猫在一边，史努比在一边。调整这条线到最佳的方式是边缘位于数据点之间，这条线最大化到两端。分类新数据可通过在这张图上标绘一个点来完成，然后就简单看到这个点位于线的哪边。使用核（kernel）方法，它们可被教授进行 n 维数据的分类。这要在 3D 图上标绘数据点，从而让其可分类史努比、加菲猫、Simon’s cat，甚至分类更多的卡通形象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoxKoWSebXKI5vA2XoDr3IfjagXWxBvHx0xwrCUwia96QEQKicZic2VeibHA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们介绍&lt;strong&gt; Kohonen 网络&lt;/strong&gt;（KN，也称自组织（特征）映射（SOM/SOFM：self organising (feature) map））。KN 利用竞争学习在无监督情况下分类数据。向网络输入信息，然后网络评估那个神经元最匹配该输入信息。然后调整这些神经元以更好地匹配输入，在这个过程中拖带（drag along）着临近神经元。临近神经元能移动多少取决于它们与最好的匹配单元之间的距离。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>业界 | Claudia Perlich Quora 问答集：机器学习能力将成为数据科学家的基本要求</title>
      <link>http://www.iwgc.cn/link/2707038</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Quora&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、孙瑞、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Claudia Perlich 是纽约大学的客座教授，也是 Dstillery 公司的首席科学家，主要的工作是为潜在的品牌客户设计、开发、分析和优化驱动数字广告的机器学习。Claudia Perlich 在业界和学术界都有非常耀眼的成绩。最近获得了 Advertising Research Foundation（ARF）的 Grand Innovation Award，并被选为《纽约商业周刊》年度 40 位 40 岁以下人物名单。她还曾担任过 SIGKDD 2014 大会主席。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1.数据科学家当下面临的最低效的问题是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，我想声明的是我认为这并不是问题：事实上数据科学家 80% 的时间都花在准备数据上。这才是他们的工作！如果你对准备数据不感兴趣，那就不是一个好的数据科学家。任何分析的有效性几乎完全仰仗数据的准备程度，而与你最后选择的算法几乎无关。抱怨数据准备工作就像一个农民抱怨做任何与收成相关的事情，然后把灌溉、施肥、撒种的事交给别人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也可以说，数据准备工作难在原始数据收集上。设计一个收集有用且易于被数据科学消化的数据系统需要高超的技艺。对数据科学家来说，让系统内数据流的过程完全透明化也是需要高技巧的。这个过程需要做抽样、标注数据、匹配等工作。还不包括替换缺失值和过度规划化的工作。为数据科学创造一个有效的数据环境，需包括数据科学而且不能完全被工程工作所占有。数据科学并不总是能规范这种系统对细节的要求来完成一个干净利落的切换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是从更大范围来看，还有更重要的事情需要考虑。我认为到目前为止最大的问题是数据科学解决不相关的问题，浪费了大量时间和精力。原因通常是任何有这个问题的人在表达该问题时都缺乏对数据科学的理解，而且数据科学家最终解决任何他们认为可能是问题的问题，然后找到的方法不一定有帮助（广告常常都非常复杂）。一个典型的类别就是「定义不透彻（underdefined）」的任务：「在这个数据集中找出可操作的 insights！」。不过，大部分数据科学家并不知道要做哪个操作。他们也分不出哪些 insights 是琐碎的，哪些是有趣的？对于哪些跟风行事的人来说，也真没有什么好建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;个人来讲，我觉得有必要提的问题是缺乏数据理解和数据直觉（事实上缺乏这个三个因素常常会让数据科学家很快就下出结论），而且怀疑是最影响效率的限制因素。这些因素影响效率的主要原因是找到正确的答案需要花费很长时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2.你最喜欢的机器学习算法是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;易于上手的逻辑回归（logistic regression，带有很多花里胡哨东西，比如机梯度下降、feature hashing 和 penalties）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我知道在深度学习风靡的时代，这答案有点奇怪。所以我先来讲下缘由：在1995-1998 年这段时间里 ，我用的是神经网络，到了 1998-2002 年这段时间我用的最多的是基于树的方法。从 2002 年开始，逻辑回归（一般的线性模型、包括分位数回归、泊松回归等等）逐渐深得我心。2003 年，我发表一篇机器学习的论文，展示了在 35 个数据集（那时候算是很大了）中三种逻辑回归分析方法与逻辑回归的比较结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长话短说，如果信噪比很高，树往往会赢。但是，如果信噪非常大，那么带有一个 AUC &amp;lt; 0.8 - logistic 的模型就是最好的选择，它总能击败基于树的方法。最终不太令人惊讶：如果信号太弱，高方差模型就完全没用了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以这到底意味着什么？在这种问题上，我倾向于处理低可预测水平的超级噪音。这个问题可以从非常确定性（国际象棋）的角度来考，也可以从非常随机（股票市场）的角度来考虑。（在有数据的情况下）一些问题仅仅是比其他问题更好预测一些。不是一个算法问题而是关于这个世界的概念陈述问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从这个角度来看，我感兴趣的大多数问题非常接近于股票市场。深度学习在另一个方向上效果很好——「这张图中有猫吗？」在一个不确定的世界中，偏置方差权衡（bias variance tradeoff）往往在有更多偏置的情况下是有利的——意味着你会想得到「简单的」非常受限的模型。而这就是逻辑回归的用武之地。我个人发现通过添加复杂的特征而不是尝试限制非常强大（高方差）的模型类别来增强一个简单的线性模型是更容易的。事实上，每个我赢过的数据挖掘比赛（KDD CUP 07–09）我都是用了线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性模型除了性能上的优势，还很稳健，而且往往只需要远远更少的人工处理（好吧，随机梯度下降和 penalties 会让其变得困难一点。当你想要在你不能花费 3 个月长的时间来构建完美的模型的行业进行预测建模时，这是极其重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，线性模型中的发生的情况更有可能得到是可以理解的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3.完全掌握 TensorFlow 需要什么样的数学背景？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这完全取决于你如何定义「掌握」。你是仅仅想用它调节一辆 Formula One 汽车的引擎，还是想要摘得国际汽车大奖赛的奖杯？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，对于灵活使用数学与算法来解决某个问题这一方面，确实是有「掌握」一说的（如果你愿意，你还可以进行算法研究）。这是 Formula One 的技术人员常做的事情。对此，我指的是专业程度极高的数学。我个人不太喜欢做这一块，虽然我也曾学习过大量的高等数学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在应用方面 —— 当你做一个司机的时候，也有「掌握」这一概念。当你可能会使用 TensorFlow 的时候，你有好的灵感吗？你知道如何设计最好的数据表征才能使算法使用更加简单吗？如果 TensorFlow 的某个指标下表现良好，这是否代表它在其它指标下仍效果显著？以上这些例子都是考验你是否掌握 TensorFlow 的场景，而它们对使用者的经验、数据、以及直觉的要求，将远高于对单纯的数学的掌握。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么问题来了：你是想成为 Michael Schumacher（德国著名赛车手），还是想为他工作呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4.机器学习如何影响数字广告？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，在数字广告领域，机器学习无处不在 —— 曾经在 KDD（ACM SIGKDD 国际会议）有很多 ADKDD 研讨会（专注于在线广告）。当前，广告产业的数据日益丰富（虽然我不知是好还是坏），程序化的实时广告的兴起也为机器学习在这一领域的发展提供了充分的机会。作为回顾，你可以抽出一个小时，看一下我近期在 Institute of Advanced Study（高等研究院）的一个演讲：https://www.ias.edu/ideas/2015/perlich-data-video&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多数情况下，机器学习被应用于以下不同的广告产业组成部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;测量与分配（市场混合模型、观测数据的因果模型、用户倾向匹配等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;跨设备关联（基于用户模式、IP 重复等，预测两个设备属于同一个用户的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;意图预测（某个消费者在下个月购买某个新车型的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;广告印象层面的反应预测（点击或完成浏览的概率）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;欺诈检测（分辨机器与人、欺诈链接与点击等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;用户洞察（观测一个在预测用户意图上表现良好的模型 —— 是否能够从中提取一些行为模式，并使用到创新设计中？）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一些广告领域中的机器学习应用是备受争议的。Sweeney 教授的成果显示，机器学习算法会反应潜在的种族歧视，我近期的一些工作也发现，一些预测模型会「倒向信号所在的地方」—— 而在弱指标下这是十分危险的。假设现在我们要预测一个广告的点击量，结果发现预测你非常有可能点击手电筒应用十分简单。这不是因为你对手电筒感兴趣 —— 而是因为你在黑暗中行动笨拙。尽管这一模型在想高点击率优化上表现优异 —— 它只会将广告投放给那些几乎对该产品完全不感兴趣的人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5.随着产业内的机器学习越来越流行，数据科学将如何进化？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要回答这个问题，需要我们先思考一下数据科学与机器学习的关系。对我自己来说，数据科学本身包含了机器学习。从定义上看，机器学习指一个机器从数据中归纳知识的能力 —— 你可以把它称作学习或者推断。没有数据，机器就几乎无法学习。所以，如果有什么区别的话，机器学习在许多产业中的应用扩张，将会成为数据科学重现光彩的催化剂。机器学习的优劣与否，取决于其使用的数据，以及消化算法的能力。我的期望是，不断向基础的机器学习靠拢，将成为数据科学家的基本要求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，对我来说，最重要的数据科学技术之一是评估机器学习的能力。我认为，我们所从事的数据科学并不缺少酷炫的项目和迷人的算法；而我们仍旧有待了解的是事物背后的运作原理以及如何解决不标准的问题。对于机器学习的（学术）观点，我的主要忧虑之一是，目前人们持续地关注在样本表现中的简单结果。基本上 99% 的研究论文都是基于精确度而被采纳的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我从过去 12 年的工作中意识到：在多数应用领域中，学术评估几乎是没用的。在一些随机测试集中表现优秀的模型会变得一无是处。这个话题值得长篇讨论，但简要来说我对以下几点存疑：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人们经常使用的指标（将分类器的精确度定义为判断正确的百分比是罪魁祸首）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;一个模型在最开始预测的往往都是错的这一事实（多数是因为根本没有「对」的东西的数据）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt; 抛开使用背景评估模型 —— 你应当先基于预测采取行动，然后再评估结果是否有改进，而不是直接进行评估&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;样本数过大导致的种种问题 —— 人们基于现有的数据建立模型，而不是他们应有的数据，更加危险的是，人们往往还基于无代表性的样本评估模型。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在带有混合生成分布的对抗情形（adversarial situation）中，模型最终只能识别出「错误的」正类的困境（可以参看我在上文对广告点击量作为指标的评价）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;泄漏 —— 表明某一模型纯粹是某数据集合的衍生物，且该模型的真正表现是非常糟糕的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6.准备数据科学面试时，哪些资源是最好的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个答案的首先最明显的部分是明白你自己准备做什么。一般意义上都是这样，而不只是在数据科学领域。现实情况是今天有很多东西都被叫做数据科学，许多人也自命为数据科学家。所以首先你要知道你到底面试的是什么，那通常并不是很明显。你也许需要在面试之前问上几个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我已经看到数据科学的职位已经分化成了（多少有些夸张）多个类别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找某种分析师，有某种擅长的技能实际上就足够了（你可能甚至不想要这样的工作）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找数据工程师，他们基本上最感兴趣的是你是否跟上了 Scala 编程的最新进展和知道你自己的版本控制方法。网上可能能找到一些标准的准备研讨会，我不知道。（这也不是我想要的工作。）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找能够基于不同的数据大小设计有效的统计分析的人，其中会涉及到一些机器学习、预测建模、聚类，而不会过于在意你是用什么方法完成的（这就开始变得有意思了……）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找深度学习专家……&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们需要一匹独角兽——具备项目管理能力，能将一个业务问题翻译成一个「可解决的」数据驱动的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦你搞清楚了你要做什么，你可能就要重新想想要不要应聘……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我通常在寻找第 3 类的申请者（有时候我也接受第 5 类）。不存在什么捷径「资源」能说服我你有这项工作所需要的能力。不幸的是，这确实涉及到个人的性格和经验。在 Kaggle 竞赛上花一些时间是获取经验的一个好方法。它们有许多数据集可以使用，你也可以从其他参与者那里学习经验。你所接触的数据集越不同，就越好！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，我还需要预测建模的专业知识，并且期望你熟悉来自《Elements of statistical learning》一书的概念。我需要你的能力足够为任何你所需要的事物编程出原型。你需要的技能包括数据拉取（API，SQ）、一些脚本编写的技能（Shell/Perl/Python）、一个建模的优质环境（Python 库/R/独立的实现），一些如何可视化/传达你的发现的想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些发表资源能让你保持敏锐：KDNuggets KDD 大会（申请论文的好集合）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后是一点不太相关的提醒：尽管创业公司很有意思，但你不会想让你的第一份数据科学工作是一家创业公司的数据科学家……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7.你是如何学习机器学习的？学习机器学习时，你最喜欢那本书？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我在学校一开始就在数学和科学学科上表现不错，并不是因为我真正喜欢抽象的事物，而只是我喜欢理解事物和解决难题罢了。所以以数学作为职业就出局了，但对于我想要做什么，我并没有什么强烈的感觉。与此同时我的爸爸则推荐计算机科学作为一个延迟决定的解决方案，然后做点什么我可能会非常擅长的事！他在 92 年时认为计算机很快就将用到各种各样的地方，而且我可以在后面想想我究竟想做什么。（我会对今天的数据科学领域的申请说同样的话。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1995 年，我在科罗拉多大学波尔得分校做了一年交换生，没有任何统计学背景的我迷迷糊糊地参加了人工神经网络的课程，完全不知道我在做什么。从此我和数据结下了不解之缘，剩下的就顺理成章了。在那里，我用德语写了另一篇硕士论文，将汽车的物理模型和该物理模型和被观察到汽车运动之间的残差分析的人工神经网络模型结合到了一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我感觉我仍然很多东西要学（而且也不急着找工作），于是我开始寻找 PhD 项目，通过偶然的机会和朋友的关系，我在 1998 年加入了纽约大学斯特恩商学院信息系统的项目。在我看来，商学院的机器学习成果的优势在于专注解决实际的商业问题，而不是理论上的算法贡献。此外，也更加关注交流和说话的能力，因为我们都是作为下一代教 MBA 学生的教授而培养的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;幸运的是，在 2003 年，学术界还没有完全准备好将机器学习作为商学院的一项研究主题，所以我没有追求一项学术事业，而是加入了 IBM Watson 实验室的预测建模组。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是我的一些「神圣的」引路人列表，按我遇到他们的顺序排列：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Duda, Hard, Stork「Pattern Classification」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bill Greene「Econometrics」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hastie, Tibshirani, Friedman「The Elements of Statistical Learning」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Provost, Fawcett:「Data Mining for Business Intelligence」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也在这里推荐几本轻松的书。一个好的数据科学家应该有一项非常好的技能：和随机性保持良好的关系。对此我推荐 Nassim Taleb 的「Fooled by Randomness」。同样，理解我们人类在处理信息中的偏见也对讲更好的故事而言是非常重要的，才能让我们自己不被数据愚弄。这方面我推荐 Duncan Watts 的一本书「Everything is Obvious Once you know the answer」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;8.作为一个数据科学家，你使用了哪些工具？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是一个相当老派的人，而且因为我的事业在向管理岗位发展，也就更少写代码了，我已经控制好自己不「更新」我的工具集了。另外，因为我们不支持 X 的防火墙后面有非常敏感的数据，所以任何类型的图形界面都是很困难的，我的工作基本上都是通过命令行进行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我常使用 UNIX shell 命令对数据进行操作。其中必然包含：awk, sed, grep, sort, cut, cat, head, tail, uniq……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一层的编译我使用的是 Perl。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拉取数据主要是在我们 Hadoop 上的 Hive 前端上通过 SQL 完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也用过 R、SAS、MATLAB 等工具——目前我大部分是使用 R 处理小事情（创建漂亮的图片等等）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了这些，我的态度是借用和窃取……我没有什么理由要自己实现任何机器学习算法。在这方面很多人都比我做得好。所以我已经积累了很多范围广泛的我发现的任何相关机器学习算法的独立可执行文件（UNIX）：Thorsten Joachims SVM 代码（http://svmlight.joachims.org/ ），Tree 学习的 FEST 库（https://github.com/n17s/fest ），John Langford 的 VowPal Wabbit（https://github.com/JohnLangford/vowpal_wabbit/wiki ）等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有时候我会发现一些我确实需要的工具 API。比如说，我发现了用于 NLP 的 Data Ninja（https://www.dataninja.net/ ）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;9.一个数据科学家需要必备一些特定领域知识吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们对这个问题争论很长时间了。我曾在 2013 年 KDD 的一个小组会议上做了几乎这同一主题的简短报告《The evolution of the expert（http://www.junglelightspeed.com/the-evolution-of-the-expert/ ）》。我过去尝试过许多回答这一问题的方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果你足够聪明，可以成为一个好的数据科学家，那么在大部分情况下，你都可以在一到两个月时间内学会任何你所需要的特定领域的知识。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Kaggle 竞赛已经一遍又一遍地展示了好的机器学习算法胜过专家。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「当聘请数据科学家时，我更感兴趣的是那些在许多行业内的经验比我丰富的人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;或者我就让我的个人履历说话：我曾赢得了 5 次数据挖掘比赛，而我并不是一味乳腺癌、酵母基因组、电信客户关系管理、Netflix 影评或医院管理方面的专家。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所有这些可能都说明这个问题的答案是「No」。事实上，在通常的领域知识的解释上，我也会说「No」。但也有一些非常不同的地方：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我并不需要对一般意义上的领域了解太多，但我需要尽可能理解有关这些数据的创建及其含义的「一切」。这算是领域知识吗？不太算——如果你和一位普通的肿瘤学家交谈，你会发现他或她几乎不能解释你刚发现的 fRMI 数据的细节。你应该与之交谈的人可能应该是理解这些机器和其中的数据处理（比如校准）的技术人员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;10.广告拦截对用户有益吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从我非常个人的观点来看（许多比我聪明的人已经讨论过其经济影响了），短期内它对用户有益——它们可以减少页面加载时间，减少你的移动设备上的数据等等。但事实上内容商会受到伤害：而他们正是用辛劳的汗水为你提供你想要的内容的人。所以从长期来看，我很担心其影响不是人们所想要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果经济的广告模式失败了，因为广告拦截消除了免费内容的收入流，内容上可能会面临这样的选择（通常很糟糕）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;乞求——这就是维基百科正在做的事。问题就在于通过订阅和好心读者的捐助所获得的收入是否足以支撑一个健康的和独立的新闻环境。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更多内置广告渗透进内容之中。尽管 Steve Colbert 可能处在一个过于暗淡的位置上，但我认为这并不是我们想要的趋势。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;放弃独立和有信息的内容的概念，从别人那里复制信息，而不是为记者付钱。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;部分的问题在于现在我们已经习惯了免费的信息甚至娱乐（以及所有用来生产、分发和托管的基础设施）。但不能因为我们能免费得到它们，就意味我们应该这样做。我很乐意为无广告的高质量内容付费！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;11.进入科学技术圈子对于女性来说更难吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以我个人的经验来看，我会说并不会更难——事实上，在这个男性占大多数的技术领域，作为女性还存在一些明显的好处。之前的答案已经指出录取更偏向于女性。事实上，我并不喜欢这样的不平等——我可以看到它的好处，但我也厌恶它在暗示「否则女性就无法成功，因此女性并没有男性那么好。」下面的例外来自我 2014 年给出的关于这一主题的演讲：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一年前，我被要求担任世界上最大也是最负盛名的数据科学会议 SIGKDD 2014 (KDD) 的联席主席。我是该委员会中的唯一女性。很显然，选择我的决定并不是因为我是女性。换句话说，我很高兴这意味着确实有些男人确实在严肃对待性别平等，但另一方面，我有被骗的感觉，因为我并不确定我是否有资格得到它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以当涉及到因为女性是少数派所以要支持女性时，我的感觉很复杂。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我总的看法是：女性的身份给我的事业带来的帮助超过其所带来的伤害。我还没遇见过什么人会让我觉得我的资格是毫无疑问因为我的性别。我从未标榜自己是数据科学领域的女性榜样，也没有哀叹女性的稀少，因为我从来没有担心过我的性别会限制我的能力或成就。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，女性参与到男性居多的领域中还有一个巨大的好处：人们会记得你。你已经和 90% 的典型男性区分开了。我去参加大会的时候，似乎有很多男人都认识我，甚至有时候我根本无法想起见过他们，更不要说记得名字了。事实上我们是很容易被记得的，而被记得是会很有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但被记住是一把双刃剑。一份不是最近的性别研究（http://consumerist.com/2011/01/25/sexy-news-anchors-distract-male-viewers/ ）发现当男性受试者看新闻时，他们可以很好地回忆起长得漂亮的女主持人，但却想不起她们说了什么。而当涉及到有吸引力的男性主持人时，他们可能没法回忆起他领带的颜色，但他们却可以回忆起在中东问题上的最新进展。这并不是因为他们真正相信女主持人读新闻的资格不够，只是生物学在作怪罢了——潜意识在帮倒忙。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以女性在男性主导的行业里确实面临着困境：我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？我们如何确保我们被叫上舞台是因为我们的思想而不是我们的性别？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;12.数据提取（data ingestion）可以自动化吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个大数据的时代，数据提取不得不实现某种程度的自动化——否则其它事情便无从谈起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更有趣的问题是如何最好地实现它的自动化。以及数据准备（data preparation）的哪些阶段可以在消化（digestion）过程中完成。我非常强烈地认为我的数据应该尽可能地「原始（raw）」。所以，你应该，比如说，不要自动化处理缺失数据的方法。我宁愿知道它是缺失的，而不是被系统所取代的。同样，我也更喜欢维护最高粒度的信息——比如说一位客户访问的页面的所有 URL 地址 vs. 只保留主机名（不太理想，但还行） vs. 只保留一些用户目录。从个人的角度来看，有很多理由反对第一种情况——但 hashing 这样的工具可以中和其中一些担忧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们谈谈如何做：在自动化过程方面存在 3 个真正重要的部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果全数据流过大就灵活地采样：如果你每天要处理 500 亿个事件——只是将它们塞进 Hadoop 系统倒还好——但后续操作却很繁琐。相反，除了 Hadoop 备份过程之外，有一个能将有特定价值的事件取出的过程是很好的。详情可参阅我们写的这篇博客文章：http://www.kdnuggets.com/2016/08/automated-data-science-digital-advertising.html&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态历史的注释：拥有所有的事件日志是很不错，但对于预测建模，我通常只需要有能获取实体历史的特征就行了。每一次都加入超过十亿行来创建历史是不可能的。所以部分的提取过程其实是一个注释过程，该过程能为每一个事件附加重要的信息。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有统计测试该评估，看输入数据流的性质是否正在改变，当比如说一些数据源临时变暗时发送警报。一些相关内容请查看这里：http://www.troyraeder.com/papers/kdd12.pdf&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;13.如果想从数据科学家转向数据科学团队经理有哪些挑战？怎么做准备？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这十多年来，我一直在回拒让我管理数据科学团队的要求。而现在，我非常享受和一个团队进行互动、开大脑风暴会议、和某些人一起探究我的想法——但我相当不喜欢告诉别人该做什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人曾见过很多优秀的数据科学家不是很好的管理者——但这可能在任何领域都是一样。许多时候优秀的数据科学家会感到他们事业的唯一进展是开始领导一个团队。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人来看，我发现不自己做数据科学工作的前景是相当艰难的。我也有重大的信任问题——我超级怀疑我自己的结果——所以让其他人来做我的工作是难以接受的。我的忧郁的原因是工作成果的很多方面都依赖于数据准备过程中很多微小的细节。比如说：你删除重复项了吗？样本选择的任何细节都是至关重要的。除了非常了解你的团队的成员的长度和短处，以及知道什么人值得托付怎样的任务——这里要做一点准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为经理，一项非常重要的技能是与不同的业务部门互动，以确认需要做哪些工作，哪些工作优先。那需要了解数据科学的广阔图景以及很好的沟通技巧。作为准备，我建议你尽可能地参与到你目前所从事的项目的界定的过程中，并开始练习如何与企业的利益相关者交流。事实上，任何涉及到向不太技术的观众进行公开演讲的机会都对此会很有帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌、微软合著论文：由知识引导的结构化注意网络</title>
      <link>http://www.iwgc.cn/link/2707040</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoy37z8zSoib7zfs5LbHwQyCHgWhE7V60Swra3d985epfcPGWZpfoicPcg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然语言理解（NLU）是口语对话系统的一个核心组成部分。最近的循环神经网络（RNN）凭借其随时间保存序列信息的强大能力在 NLU 上取得了很好的结果。传统而言，NLU 模块根据话语的扁平结构标记话语的语义槽（semantic slot），其基本 RNN 结构是一个线性链（linear chain）。但是，自然语言展现的语言属性能为更好的理解提供丰富的、结构化的信息。这篇论文介绍了一种全新的模型——由知识引导的结构化注意网络（K-SAN：knowledge-guided structural attention network），这是一种广义的 RNN，再加入了由先前的知识引导的非扁平的网络拓扑。其有两个特点：1）可以从小型训练数据集中获取重要的子结构，让模型可以泛化到之前从未见过的测试数据；2）该模型可以自动找出对预测给定句子的语义标签至关重要的显著子结构，从而可是实现理解性能的提升。在航空旅行信息系统（ATIS）数据基准上的实验表明我们提出的 K-SAN 架构可以使用注意机制（attention mechanism）有效地从子结构中提取出显著的知识，其表现超过了当前最佳的基于神经网络的框架。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;点击「阅读原文」，下载论文↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 量子计算未来布局：这18家公司走在了世界前列</title>
      <link>http://www.iwgc.cn/link/2707042</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CB Insights&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、Cindy、Rick、吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机很快就能真的有用了。世界上最大的几家公司正在尝试让这项技术走出实验室。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoNIiaetXdge0GaR06yia8GFTk50EA4qBTkbc0jpUg0oNfcv4lmcl6PuXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一台量子计算机利用一种叫量子比特（qubit）的亚原子粒子来加速解决复杂的计算。量子计算机近期有望解决包括从优化到量子加密通信在内的许多问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据CB Insight的投资、收购、和合作关系数据，我们列出了 18 家涉足商品化量子计算硬件和软件的企业。它们来自不同的行业，从科技行业巨头到国防承包商再到国家电信公司等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们名单中出现的仅限于那些被报道近三年内在量子计算领域有独特重大技术突破的公司：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;空中客车（Airbus）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;阿里巴巴（Alibaba）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;博思艾伦咨询公司（Booz Allen Hamilton）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;英国电信公司（British Telecommunications）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌（Google）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;惠普（Hewlett Packard）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;IBM&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;英特尔（Intel）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;KPN&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;洛克希德·马丁（Lockheed Martin）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微软（Microsoft）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;三菱（Mitsubishi）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NEC&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;诺基亚（Nokia）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NTT&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Raytheon&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SK电讯（SK Telecom）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;东芝（Toshiba）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.空中客车公司（Airbus）希望量子计算机可以加强航空航天软件&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015年年末，Airbus 集团在威尔士（Wales）的 Newport 成立了一个团队来解决量子计算的问题。Airbus 的 Defence 和 Space 部门主要的任务将会是学习所有有关量子力学的技术，涵盖从密码术到计算。Airbus 不仅打算发展自己的量子计算硬件，还想要将现有的量子设备适用于具体的航空航天问题， 也就是那些需要处理和储存的大量数据（包括卫星流图象的分类和分析）或飞机超耐用材料的创造。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.阿里巴巴打算用量子计算机来开发更安全的电子商务和支持电子商务的数据中心&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 7 月，阿里巴巴的阿里云单位与中国科学院在上海建立了一个研究机构，称为阿里巴巴量子计算实验室（Alibaba Quantum Computing Laboratory）。该实验室的目标是为电子商务和数据中心研究量子安全技术。值得一提的是，同年，中国还发射了世界上第一个量子卫星，旨在发展安全的长距离量子通信。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.博思艾伦咨询公司（Booz Allen Hamilton）想要为政府和商业客户提供量子计算&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Booz Allen Hamilton 是发展中的群体的一部分，为了更好的数据-科学产品和服务，它正在竞争中寻找优势。为此，这家管理咨询公司说，它与政府和商业客户合伙开发实验项目和量子计算原型，并最终通过使用被称为量子退火设备（Quantum Annealers）的特殊设备来解决优化问题 。量子退火设备可以利用量子状态的粒子的自由演化来执行优化问题的计算。Booz Allen Hamilton 感兴趣的领域包技系统与网络优化、车辆路径、物流、工作计划、药品开发、制造业、系统设计和软件中复杂代码的验证&amp;amp;检验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.英国电信（British Telecommunications/BT）正在研究量子计算用以保护其网络上传输的敏感信息&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BT 与东芝公司、爱德华光网络公司（ADVA Optical Networking）和英国国家物理实验室一起合作，研究并实施量子加密，该技术是指光子——光粒子——可用来分配用于保护敏感数据的加密密钥，比如财务或健康记录数据。根据英国知识产权局专利情报组，截至 2013 年， BT 在量子保密通信技术领域优先专利和专利申请者数量方面排名全球第六。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5.谷歌希望通过量子计算来获得更好的人工智能和更好的复杂优化问题的解决方案&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌在量子人工智能实验室（Quantum Artificial Intelligence lab /QuAIL）拥有一台 D-Wave Systems 的量子计算机。该实验室由美国宇航局（NASA）以及位于加利福尼亚州芒廷维尤的美国宇航局艾姆斯研究中心（NASA Ames Research Center）里的大学空间研究协会（Universities Space Research Association）共同承办。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;D-Wave Systems Inc 是世界上第一个商用量子计算机公司——它与谷歌的交易是 D-Wave 历史上最大的一笔。谷歌及其合作伙伴拥有长达七年的最新 D-Wave 机器访问权限，期间新一代 D-Wave 系统将被安装在美国宇航局艾姆斯研究中心设备上以供使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;领导谷歌 QuAIL 工作的 Hartmut Neven 及其团队最近出版了一篇有关其 D-Wave 2X 计算机的论文，它展示了表明该机器的计算执行速度能够比一块经典的计算机芯片快 1 亿倍速的初步测试结果。早在 2013 年，该财团已利用 D-Wave 的机器在 Web 搜索、语音/图像模式识别、规划和行程安排、空中交通管理、机器人外太空任务等应用中进行量子计算的探索，并支持任务控制中心的操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2014 年，为了减少机器学习与人类智能之间的差距——且为了在人工智能的新兴领域中取得领先地位——谷歌开始利用其在 D-Wave 机器上的经验并专注于开发自己的量子硬件。谷歌为此雇佣了圣巴巴拉市加利福尼亚大学的一位物理学教授 John Martinis 及其团队，来建立谷歌的专属量子芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;6.惠普（Hewlett Packard/HP）专注于出售小规模的量子计算机或模拟器，以增加现实世界使用技该术的案例&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;惠普的量子信息处理组（Quantum Information Processing Group）位于英国的布里斯托尔的惠普实验室， 而且它是信息和量子系统实验室（Information and Quantum Systems Laboratory）的一部分。该实验室主要的重点领域包括计算、加密和通信。惠普实验室将它目前正在开发的东西称为「The Machine）」。这款计算机是惠普在建立量子「内存驱动计算」处理器方面的一个尝试，是惠普实验室最大的项目。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;7.IBM 正在致力于制造带有纠错保护的超导电路——保护量子信息免于被称作量子噪声的现象的干扰&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 研究院在其位于纽约约克镇高地的研究中心拥有一个量子计算小组。2015 年 4 月，他们宣布了一种能够检测出比特翻转（bit-flip）和相位翻转（phase-flip）误差的新电路。要想检测并纠正量子计算系统中可能存在的两类误差，这一突破对于克服该挑战来说是非常重要的。同年 12 月，IBM 被授予了一份 IARPA 补助金，以在逻辑量子比特项目（Logical Qubits program）中使用该技术。该项目的目标是主要通过建立一个可以扩展到更高维度的量子电路设计，来克服当前量子系统的局限性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;8.英特尔专注于利用量子计算来在先进制造业、电子工业和更好的系统架构设计中受益&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 9 月，英特尔向代尔夫特工业大学的量子研究所 Qutech 以及荷兰应用研究组织拨了 5000 万美元，用于 10 年合作期的工程支持供给。英特尔 CEO Brian Krzanich 发表了一篇博客，详细描述了公司在量子计算领域的战略利益，以及电子工业和制造业的专业知识在量子计算实践方面的相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;9.KPN 想要更好的后量子密码算法（post quantum cryptographic algorithms）来保护通信安全&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KPN 是瑞士的一家固定电话和移动通信公司，它在其位于海牙和鹿特丹的 KPN 数据中心网络之间实现了端到端的量子密钥分配（Quantum Key Distribution/QKD）。KPN 正与 ID Quantique 合作，后者是一家专门从事量子加密的瑞士公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;10.洛克希德·马丁（Lockheed Martin）通过量子计算改进当前的软件校验/验证等等&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在与南加州大学的合作中，洛克希德·马丁联合设立了USC-洛克希德·马丁公司量子计算中心（QCC）。该中心的研究重点是利用隔热量子计算（adiabatic quantum computing）的力量，在这种计算中，问题可被编码成物理量子系统的最低能态（「最冷」），以寻找带有许多变量的特定问题的最优答案。除此之外，D-Wave Systems 曾在 2015 年宣布与洛克希德·马丁达成了多年协议以将该公司的 512 量子位 D-Wave Two 量子计算机更新成带有 1000 多个量子位的 D-Wave 2X 系统。这是自洛克希德·马丁 2011 年成为 D-Wave 的首位客户以来第二次系统升级。在 2014 年，Aerospace Concepts 公司与洛克希德·马丁 进行了一场合作研究，并最终合作组建了一家新公司 QxBranch——为金融、石油和天然气、航空航天和生物技术行业提供量子计算机产品和服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;11.微软正在为量子计算机创造专用软件和硬件&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的 QuArC 部门成立于 2011 年 12 月，其关注的重点是为可扩展的、容错的量子计算机的使用设计软件架构和算法。该机构值得关注的一项成就是 LIQUi|&amp;gt; ——一种用于量子计算的软件架构和工具套件。微软的 QuArC 组与全世界的许多大学都建立了紧密的合作关系，其中包括代尔夫特理工大学、Nils Bohr 研究所、悉尼大学、普渡大学、马里兰大学、苏黎世联邦理工学院和加州大学圣巴巴拉大学（UCSB）。在 2014 年，微软透露自己在 UCSB 的校园内有一个名叫 Station Q 的小组正在研究拓扑量子计算（topological quantum computing）——旨在改善量子状态的控制设计。在 QuArC 组的软件和算法工作的基础上，Station Q 是微软一项跨世界的工作：将全世界的数学家、计算机科学家、量子物理学家和工程师集合起来构建混合超导/半导体设备，以用于受控环境中的应用，其最终目标是创造一种可扩展的、容错的通用量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;12.三菱电机正在开发加密移动通信的专用量子加密设备&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三菱电机（Mitsubishi Electric）声称其已经开发出了世界上第一个「one-time pad software」——一种用于移动电话的先进加密技术，以确保电话通信保持机密。此外，该公司已经参与到由日本情報通信研究機構主导的一个项目中，以实际应用其技术以测试在量子安全网络上移动通信的可行性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;13. NEC 和富士通希望向其客户提供长距离的量子加密通信&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 9 月，东京大学的纳米量子信息电子研究所（ Institute for Nano Quantum Information Electronics）与富士通和 NEC 集团合作宣布实现了 120 千米长距离安全通信的量子密钥分布，该系统使用了一个单光子发射器系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;14.诺基亚是贝尔实验室的母公司，而贝尔实验室是量子计算算法开发的先驱&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;贝尔实验室的一些研究者是量子计算的先驱，其中包括 Peter Shor（Shor’s Algorithm）和 Luv Grover（Grover’s Algorithm）。贝尔实验室的这个页面（https://www.bell-labs.com/our-research/disciplines/quantum-computingcommunications/ ）上可以看到他们在量子计算上的最新研究活动。诺基亚也与牛津大学和洛克希德·马丁建立了合作来探索量子技术在强化优化和机器学习方面的潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;15.日本电报电话公司（NTT）想要依赖光子处理信息的基于量子的计算机芯片&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NTT 基础研究实验室（Basic Research Laboratories）和 NTT 安全平台实验室（ Secure Platform Laboratories）正在合作探索超冷原子和量子信息处理。在 2014 年，这家公司和来自英国布里斯托大学的研究者开发了一款光学芯片，该芯片可以使用光子来测试量子计算领域的新理论，其目标是减少测试量子理论之前所需的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;16.美国雷神正在探索传感器、计算机、数据安全和图像技术的未来&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;雷神 BBN 科技是一个研发中心，是雷神公司的一部分。该研发中心建立了一个量子信息处理组，专注于使用量子现象进行传感、计算和成像。2012 年，雷神 BBN 获得 IARPA 赞助的量子计算科学项目的 220 美金。雷神的目标是在一个框架中融合量子计算的多个方面，从而更好的管理资源并评估性能。其他合作方包括 NEC、滑铁卢大学、墨尔本大学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;17.韩国 SK 电讯想要在韩国建立一个量子安全加密通信网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2016 年 3 月，SK 电讯宣布它完成 5 个不同国家测试网络进行量子通信的首次展示，覆盖范围共为 256 千米。SK 通信的量子加密系统从 2011 年开始开发，被认为最安全的加密方法之一，使用的是量子物理，而非如今普遍使用的基于数学的加密算法。参与该项目的公司包括 Wooriro Co,Ltd，HFR Inc，韩国安全研究所，电子和电话通讯研究所，首尔大学，KAIST，韩国大学，韩国光州科学技术院，量子信息通信研究协会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;18.东芝在追求量子秘钥分布和安全通信网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东芝的原型量子秘钥分布系统为基于光导纤维计算网络上的密码学应用提供数字秘钥。尤其是东芝在 2015 年宣布来自东芝的生命科学分析中心的基因组数据被提名被一个量子计算系统加密，并被发送到日本东北大学的Medical Megabank Organisation。东芝掌握着世界上最大的量子 IP portfolios 之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 语音识别新里程碑：微软新系统词错率低至6.3%（附论文）</title>
      <link>http://www.iwgc.cn/link/2688676</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Microsoft&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Richard Eckel&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;微软研究者在追求计算机像人类一样理解语音的道路上取得了新的里程碑。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJjNjPliaNXrYLCWIzgh3GPWZfykoj3aCibWSTHtHDOqcLop2xgYguv17Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软首席语音科学家黄学东在产业标准 Switchboard 语音识别任务的最新基准评估中报告出了这样一成果，微软研究者取得了产业中最低的 6.3% 的词错率（WER）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这周二发表的一篇研究论文中，黄学东说：「我们最好的单个系统在 NIST 2000 Switchboard 集上取得了 6.9% 的词错率。我们相信这是取得最好表现的不基于系统结合的单个系统。在 Switchboard 测试数据上，数个声学模型的结合将前沿成果推进到了 6.3% 的词错率。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上周，在旧金山举办的国际语音交流和技术大会 Interspeech 上，IBM 宣称他们取得了 6.6% 的词错率。20 年前，最好的研究系统的词错率是 43%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄说：「这一新的里程碑得益于过去 20 年中由来自不同组织的人工智能社区开发出的各种新技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJOlG27zVBQPTViaMDbuRW8XRLvRicR5q4wnG1829AsTWrwJCS73WnQ4XQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 和微软都提到了深度神经网络时代的到来，其受到了大脑生物处理方法的启发，也成为了语音识别技术进步的关键推动力。计算机科学家已经尝试了数十年去训练计算机系统做图像识别、语音理解这样的任务，但到目前为止，这些系统的准确度仍然不尽如人意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络构建于一系列的计算层之中。今年早些时候，微软研究员利用一个深度残差神经网络（deep residual neural network）系统赢得了 ImageNet 计算机视觉挑战赛，该系统使用了一种新型的交叉层网络连接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJFApmqAsjias8FPVwEibWAEG0IYsdXfvIgcib4RaiajSMnvic8iaQH7KnKeaw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Geoffrey Zweig&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的 Speech &amp;amp; Dialog 研究组的首席研究员兼管理者 Geoffrey Zweig 领头此次的 Switchboard 语音识别工作。他在微软带领新型训练算法、高度优化的卷积和循环神经网络模型、CNTK 这样的工具等内容的开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文：微软 2016 对话语音识别系统（The Microsoft 2016 Conversational Speech Recognition System）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJboMsrKleE606EJcBibpyFLwmIuDLOuANenxkP725frLicA0OXwFWK0og/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们描述了微软的对话语音识别系统，在该系统中我们结合了近期在基于神经网络的声学和语言模型上的进展，推进了在 Switchboard 识别任务上的顶尖成果。受到机器学习集成技术（machine learning ensemble techniques）的启发，该系统使用了一系列卷积和循环神经网络。I-vector 建模和 lattice-free MMI 训练为所有声学模型架构带来了显著的提升。使用了多个前向和反向运行 RNNLM 的语言模型重新计分（Language model rescoring）与基于后验的词系统结合为系统性能带来了 20% 的增益。最好的单个系统使用 ResNet 架构声学模型和 RNNLM rescoring，在 NIST 2000 Switchboard 任务上实现了 6.9% 的词错率。结合系统取得了 6.3% 的词错率，代表了在这一基准任务上对先前成果的改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年，由于对卷积和循环神经网络的精心设计和优化，在降低语音识别错误率上我们已经看到了快速发展。尽管我们对基础架构已经很好地认识一段时间了，但它近期才成为了进行语音识别的最好模型。惊人的是，对声学模型和语言模型而言都是如此。相比于标准的前馈 MLP 或 DNN，这些声学模型有能力对大量带有时间不变性的声学环境建模，而且卷积模型还能应对频率不变性的情况。在语言模型中，循环模型通过对连续词表征（continuous word representations）的归纳能力，在传统的 N-gram 模型上实现了进步。同时，集成学习（ensemble learning）已经在多种神经模型得到了普遍的应用，从而通过减少偏差和方差改进稳健性。在此论文中，我们广泛地使用模型的集成，同时也改进单个组件模型，从而推进在对话电话语音识别（CTS）中的进展，CTS 从上世纪 90 年代就已经成为了检验语音识别任务的一项基准。这一系统的主要特征包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对卷积神经网络和长短期记忆（LSTM）网络这两种基础声学模型架构的集成，每个架构也有多种变体；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 LACE 卷积神经网络中的一个注意机制，其可以有区别地为不同距离的语境赋予权重；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Lattice-free MMI 训练；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在所有模型中使用基于 i-vector 的改编版本；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在前向和反向过程中都运行带有多个循环神经网络语言模型的 language model rescoring；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;融合网络系统组合与最好系统子集搜索的耦合，这正是在有许多候选系统的情况下所需的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该论文在其他部分对该系统进行了详细描述。Section 2 描述了 CNN 和 LSTM 模型。Section 3 描述了我们对 i-vector 改编版的部署。Section 4 展现了 lattice-free MMI 训练过程。语言模型 rescoring 是该系统的一个重大部分，在 Section 5 中有描述。实验结果呈现在 Section 6 中，随后是对相关工作和结论的讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ1I6omFmQMwEgcia2b57iasnc6Cy1NzIibLDPwFKglVpJCzYHE6dvmbB9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 4. 在 eval 2000 set 上的来自 i-vector 和 LFMMI 的性能改进&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ8OcjXl8BK8b2IZ00icLh92BTQbRDApNKibSy6np2RGCl1nsclMETPKGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;表 5. 在 eval 2000 set 上不同声学模型的词错率。除非特别标注，所有的模型都在 2000 小时的数据上进行训练，有 9000 个 senones（聚类的结果）。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>业界 | MIT推出并行计算编程语言Milk：为大数据应用提速</title>
      <link>http://www.iwgc.cn/link/2688677</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Larry Hardesty&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、杜雪&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在今天的计算机芯片中，内存管理（memory management）是基于计算机科学家所称的局部性原理（principle of locality）：如果一个程序需要存储在某个内存位置的一个数据块，它可能也会需要这个数据块附近的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这种假设在大数据的时代已被打破，现在的程序常常需要在许多巨大的数据集上任意离散的少量数据上进行操作。因为从它们的主内存区块中读取数据是今天芯片的主要性能瓶颈，所以不得不进行的更为频繁的读取会极大地减慢程序的执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本周，在并行架构和编译技术国际会议（International Conference on Parallel Architectures and Compilation Techniques）上，来自 MIT 计算机科学和人工智能实验室（CSAIL）的研究者展示了一种名为 Milk 的新型编程语言，该语言可让开发者在需要处理大型数据集中离散数据点的程序中更高效地管理内存。研究人员在一些常用算法上对该语言进行了测试，发现使用这种新语言编写的程序的速度可达到用已有语言所编写的程序的 4 倍。但研究者相信进一步的工作可以让其速度得到更大的提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电气工程和计算机科学教授 Saman Amarasinghe 说，今天的大数据集给已有的内存管理技术带来问题的原因不仅仅是因为它们很大，更多的则是因为它们是稀疏的（sparse）。也就是说，解决方案的规模并不总是与问题的规模成正比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「在社会环境中，我们习惯了应对更小的问题。」Amarasinghe 说，「如果你看看这栋楼（CSAIL）里面的人，你能看到我们全部是互连的。但如果你从整个行星的规模上来看，我的朋友的数量也不会扩大规模。这颗行星上有几十亿人，但我的朋友仍然只有几百人。突然你就有了一个非常稀疏的问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Amarasinghe 说，类似地，比如说有一个有 1000 位客户的在线书店，可能会为其访客提供 20 本最受欢迎的书的清单。然而，这并不意味着，如果这家在线书店的客户达到 100 万了，它就会为其访客提供 20,000 本最受欢迎的书的清单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;本地思考（Thinking locally）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天的计算机芯片并没有为稀疏数据进行优化——其实事实情况正好相反。因为从芯片的主内存区块读取数据的速度很慢，所以现代芯片的内核或处理器中都有自己的「缓存（cache）」——一种相对小的、本地的且高速的内存区块。内核不会一次只从主内存中读取单个数据项，而是会一次读取一整个数据块。而这个数据块是通过局部性原理进行选择的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;局部性原理（principle of locality）的工作方式很容易理解。以图像处理任务为例：如果一个程序的目的是在图像上应用一种视觉滤镜（visual filter）而且一次只能处理图像上的一块，那么当内核请求一个块时，它应该就还会收到其缓存所能容纳的所有相邻的块，这样它就可以一块接一块地处理，而不需要再取更多数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但如果该算法的目标是从在线零售商数据库的 200 万种书中取出仅仅 20 种，那这种方法就不管用了。如果它请求与某一种书相邻的数据，很有可能其相邻的 100 种书的数据都是没有关联的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从主内存中一次只读取一个数据项是非常低效的。「这就像是，每次你想要一勺麦片时，你都需要打开冰箱、打开牛奶盒、倒出一勺牛奶、盖上牛奶盒、将它放回冰箱。」Vladimir Kiriansky 说，他是电气工程和计算机科学的博士生，同时也是这篇新论文的第一作者。Amarasinghe 和 Yunming Zhang 是他的合作者，Zhang 也是一位电气工程和计算机科学的博士生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;批处理（Batch processing）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Milk 只是简单地为 OpenMP 增加了一些命令；OpenMP 是一种 C 或 Fortran 等语言的扩展，可以帮助用来更轻松地为多核处理器编写代码。使用 Milk，程序员可以在任何指令附近插入几行代码，其可以在整个大数据集中迭代，寻找相对较少数量的项。Milk 的编译器（将高级代码转换成低级指令的程序）然后可以据此找到管理内存的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 Milk 程序时，如果一个内核发现它需要一个数据项，它不会发出请求从主内存中读取它（以及其相邻的数据）。它会将该数据项的地址加入到一个本地存储的地址列表中。但这个列表足够长时，该芯片的所有内核都会池化（pool）它们的列表，然后将这些地址按临近排布的形式组合到一起，并将它们重新分配给内核。如此一来，每一个内核都只请求了它知道自己需要的数据项，而且可以高效地检索得到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种描述的层面较高，但实际上的细节会复杂得多。事实上，大部分现代计算机芯片都有多级缓存，一级比一级大，但效率也因此更低。Milk 编译器不仅必须跟踪内存地址列表，还要跟踪存储在这些地址中的数据，而且它常常将这两者在各级缓存之间进行切换。它也必须决定哪些地址应当被保留（因为可能需要被再次访问），哪些应当被丢弃。研究者希望能够进一步提升这种能够编排这种复杂的数据芭蕾舞的算法，从而进一步提升性能表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「今天许多重要的应用都是数据密集型的，但不幸的是，内存和 CPU 之间不断增大的性能鸿沟意味着当前的硬件还没有发挥出它们的全部潜力。」斯坦福大学计算机科学助理教授 Matei Zaharia 说，「Milk 通过优化常见编程架构中的内存访问来帮助解决这一鸿沟。这项成果将关于内存控制器设计的详细知识和关于编译器的知识结合了起来，能为当前的硬件实现良好的优化。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Michael Jordan最新论文：少于单次通过，随机受控的随机梯度方法</title>
      <link>http://www.iwgc.cn/link/2688678</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lihua Lei、Michael I. Jordan&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜雪、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJXYZeFoiaGicaLlOKsFbfBny7eINMgQqgE5zvricr0TziaO5OpSKC3cJLpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们为基于梯度的优化开发并分析了一套程序，我们称之为随机受控的随机梯度（SCSG: stochastically controlled stochastic gradient）。作为 SVRG 算法家族的一员，SCSG 利用梯度在两个级别上估算。与这家族已有的其他算法不同，SCSG 的计算成本和通信成本不需要线性扩展样本量 n；事实上，当目标精确度较低时，这些成本是独立于 n 的。一个在 MNIST 数据集上的 SCSG 的试验评估显示：只需要带有 2.6 MB 内存的日常所用的机器和 8 次磁盘访问，它就能在这个数据集上生成精确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>SyncDaily | 英伟达发布两款基于Pascal的深度学习芯片、以色列发明通过声音和触觉「看见」环境的感官替代设备</title>
      <link>http://www.iwgc.cn/link/2688679</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;英伟达发布 Tesla P4&amp;amp;P40 两款基于 Pascal 架构的深度学习芯片、以色列发明让盲人通过声音和触觉「看见」环境的感官替代设备......机器之心日报，精选一天前沿科技优质内容。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;谷歌新专利：自动驾驶汽车探测警车&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9 月 13 日，NVIDIA（英伟达）在北京国际饭店会议中心召开 GTC China 2016 大会。GTC 是全球最大最权威的 GPU 开发者和行业大会，展示各行业中运用 GPU 技术最重要的创新成果。在会上，NVIDIA 发布了 Tesla P4 和 Tesla P40 两款 Pascal 架构 GPU。本次集成了 72 亿个晶体管的 Tesla P4（2560 个 CUDA 核心）和 120 亿个晶体管的 Tesla P40（3840 个 CUDA 核心）是用来让用户识别和查询语音、图像或文本的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJbfJbQCpiaiaXe5ERHgJ2EXLjluB3PXMuR1Jjj8SSrJen01JXdysOprvA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJtIttohMtibJMNIabd72xzvUr5g6dF2tsG1zjnMZG7dDCPIgCxgviaKKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tesla P4&amp;amp;P40 的性能相当于 40 个 CPU，响应速度是 CPU 解决方案的 45 倍。同时，Pascal 架构能助推深度学习加速 65 倍，最新一代的架构 Pascal 是首个专为深度学习而设计的 GPU。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;人工智能学习侠盗猎车5场景，升级自动驾驶汽车安全系数&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据 MIT 科技评论报道，英特尔实验室与德国达姆施塔特大学合作在 GTA V（Grand Theft Auto V）这款游戏的基础上开发了一款机器学习程序，这款程序能够带来更安全更好的自动驾驶汽车。给这款游戏添上适当的补丁后，进入游戏你就能看见城市驾驶，郊区开车，土路，自行车，利尔喷气式飞机降落在四车道的高速公路这些场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJAPa8cNnpUKVZeYsia573e8J5CK33FYQ1YYlDyTIS228moaqllcsOia8w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而英特尔实验室和达姆施塔特大学的研究人员要想出如何将游戏图像分解成人工智能程序可以识别的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果人工智能了解道路、建筑和人们的方位，它就可以学习如何在这些道路上安全驾驶。而 GTA V 的图形非常先进，人工智能通过学习 GTA V 也许可以把这些经验变成实际知识用在真实世界的驾驶中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;Udacity联合英伟达与奔驰，推出「无人驾驶车」课程&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最新的 TechCrunch Disrupt 2016 大会上，优达学城（Udacity）正式对外发布了「无人驾驶工程师」的纳米学位，这也是全球首门可以在线学习的无人驾驶车工程师培训项目，由优达学城（Udacity）联合梅赛德斯-奔驰、NVIDIA 以及刚被 Uber 收购的 Otto 联合推出，同时滴滴出行将会作为人才雇佣上的合作方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;课程将于 9.14- 27 日接受线上报名，学员将会经过一轮选拔，10 月中旬开始 3 个学期的学习，内容将涉及深度学习、计算机视觉、传感器融合、定位、控制器、汽车动力学、汽车硬件等技术，并且学员将在优达学城自有的真实无人驾驶车上创建并运行自己的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;以色列发明让盲人通过声音和触觉「看见」环境的感官替代设备&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;耶路撒冷希伯来大学知名的大脑与多感官研究实验室最近又出了两个新装备。这些感官替代设备（SSDs）可以通过提供来自声音和触觉的视觉信息来帮助盲人「看见」周围的环境。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，这些装备已经准备好大量投入市场。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个是 EyeCane，用于定位方向的设备，外观很像手电筒，能发出红外线，将距离转化为听觉和触觉的线索，使用户能够感觉到周围五米（16 英尺）内的物体。经过短暂培训后，使用者可以借助 EyeCane 来在简单的环境中估计距离，避开障碍物，导航。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=g0328p0qp55&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个设备是 EyeMusic 是一个应用程序和微型摄像系统，将图像转换成供「大脑进行视觉处理的音景（soundscope）」来传递对象的颜色、形状、和图像位置。经过训练后盲人可以用这个设备来识别字母表中的字母，「看」见动物图片，甚至在一个复杂的视觉景观中找到一个对象或人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=s0328is5bo2&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;Evernote 将要把所有数据搬上谷歌云端&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;全球拥有 2 亿用户的 Evernote 如今正在转移包括用户的 50 亿条笔记在内的所有数据，而目的地是谷歌的云端平台。这家公司也将开始使用谷歌的机器学习的应用程序接口（API）来帮助接入并以多种不同的方式使用这些数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Evernote 将关闭以前的那套基于私人云架构的存储架构。Evernote 的 CTOanirban Kundu 说前两个由谷歌的机器学习 API 替代的是语音-文本的翻译中的语音识别；和自然语言处理用于帮助搜索上下文内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据转移将在今年 10 月开始，大约在年底完工。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编辑整理，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 斯坦福开发机器学习脑机接口，帮助猴子打出了莎士比亚名句</title>
      <link>http://www.iwgc.cn/link/2675473</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自IEEE Spectrum&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Eliza Strickland&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;span&gt;给一只猴子一台打字机，早晚有一天它能打出莎士比亚全集，这是概率定理中的著名例子。最近猴子们不负所望，终于打出了哈姆雷特的那句家喻户晓的名言「To be or not to be. That is the question」。这是斯坦福神经义肢移动实验室的一项最新研究技术，他们在猴子大脑中控制运动的区域里植入了微型电极阵列，并使用了机器学习算法，开发出一个脑机通信接口。猴子们使用这个接口能在一分钟内打出 12 个单词，这是目前最高记录，而且研究员 Paul Nuyukian 说，这项技术还有很大的提升空间。点击&lt;span&gt;&lt;span&gt;「&lt;/span&gt;阅读原文&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;，查看论文链接。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEz8ics76Iyk3Hs21VtvIchXyFYibXoNKrkffqA6r8gpHhkCnTNExdDBgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图像来源：斯坦福大学&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「To be or not to be. That is the question.（存在或灭亡，这是一个问题。）」这句话也是猴子 J 使用大脑植入物来控制计算机光标打出来的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先说清楚，这只猴子不知道它打出来的是莎士比亚的名句，它对哈姆雷特这句著名的独白也没有什么深刻的理解。猴子 J 和它的伙伴猴子 L 都是经过训练后才能使用它们的神经植入体来移动电脑屏幕上的光标，当屏幕上的圈圈变成绿色时，它们就点击这些圈圈。斯坦福大学的研究员们将这些文字放入那些目标中来模拟打字任务。所以要打出哈姆雷特的这句名言，第一个字母「T」圈圈要被点亮，然后是「O」，以此类推。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么要做这些练习？难道这只是让记者抛出「无限猴子定理」的一个借口？&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们一直有这样一个看法：根据概率定理，如果你给一只猴子一台打字机和无限的时间，它最终会随机打出莎士比亚的所有作品。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而这里的情况并不是这样，生物工程师们有一个更加接地气的动机。通过模拟这个打字任务，他们证明了他们的大脑计算机接口可以惠及那些无法正常沟通的人，包括肌萎缩侧索硬化症（ALS）晚期患者，也被称为也被称为 Lou Gehrig 病，这些人最终会全身瘫痪，包括口腔和其他面部肌肉，但思维还是清晰的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项实验创下了一个用意念（mind）打字的新纪录：一只猴子一分钟打出了 12 个单词。「据我们所知，这是目前能达到的最高水平了，」Paul Nuyukian 说，他是斯坦福神经义肢移动实验室（Neural Prosthetics Translational Lab）研究员，也是这项研究相关论文的合作者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=q0328kyui8n&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它的原理是这样的：这些猴子的大脑中位于运动皮质的部分植入了微小的电极阵列，这部分控制着四肢运动。当猴子在训练光标控制任务时，那些电极会测量猴子神经元的电活动，首先会移动它们的胳膊，同时相机会小心地跟踪着这些动作。机器学习算法在数据流中发现模式，并将这些模式转换成猴子的一个意图来移动光标向左，向右，向上，向下。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年大脑打字的记录是一名 ALS 患者创下的，一分钟打出 6 个单词。那项试验的研究者团队更大，其中也有 Nuyujukian，他是 BrainGate 联盟的成员。今年能创下新纪录还是要得益于之前研究中的软件；猴子们使用的系统中装了两个聪明的串联的算法，一个用来解码光标的移动，另一个用来解码猴子要点击哪个字母的意图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nuyujukian 在采访中告诉 IEEE Spectrum，这项技术还有很大的提升空间。目前这些猴子使用接口一次只能选一个字母，他说，但是未来的接口可以借用一些智能手机上的小技术。「我能想象出一个更加聪明的接口，它能自动完成这些单词。」「谷歌和苹果已经做了很多研究工作，试图从我们不精确的手指移动中获取最大化的信息输入，我们可以大大利用这个技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该技术的人类试验已在进行中。在去年的某个大会上，研究者们展示出了一项试验，试验中一名患有 ALS 的女性使用了一台外接安卓平板电脑的光标控制系统来浏览网页、写邮件。这类人类试验说明这些研究者并不是在开什么莎士比亚猴子的玩笑。他们正在尝试帮助那些瘫痪的人能够实现一定程度上自理，赋予他们表达自己思想的能力。但是这些研究者们自己的乐趣并不太多。Nuyujukian 泄露了猴子们早期用这个测试系统打出来的文字，「A banana, a banana, my kingdom for a banana!」以及「a banana by any other name would smell as sweet.」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：一种非人灵长类动物的大脑–计算机打字接口（A Nonhuman Primate Brain–Computer Typing Interface）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;脑机接口（BCI）可以记录大脑活动并将这些活动信息翻译成有用的控制信号。它们能用来帮助瘫痪的人通过控制计算机光标或者机器义肢这类终端执行器恢复部分身体功能。通信神经假体（Communication neural prostheses）是在计算机或移动设备上控制用户界面的脑机接口。我们给两只猕猴植入电极阵列，模拟打字任务，以此展示了一副通信假体。猴子们用了两个目前性能最好的 BCI 解码器，通过一次只提示一个符号/字母的方式，打出了单词和句子。平均来看，使用只与速度相关的二维 BCI 解码器（该解码器根据停留时间（dwell）从符号中进行选择）从一篇报纸文章上复制文本，猴子 J 和猴子 L 的打字速度分别达到了 10.0 个和 7.8 个每分钟（wpm）。有了一个能离散点击关键选项的 BCI 解码器后，打字速度分别上升到 12.0wpm 和 7.8wpm。这是目前使用 BCI 进行沟通能达到的最快速度。然后我们量化比特率和打字速度之间的关系，发这个关系现近似线性：打字速度（以 wpm 计算）是比特率（以字节/秒计算）的三倍。我们还比较了能达到的比特率指标和信息转换速度，并讨论了它们适用于现实世界的打字情境。虽然这项研究还不能模拟单词认知负荷和句子规划（sentence planning）的影响，但它证明了 BIC 作为通信接口的可能性，并代表了对于一个给定 BCI 吞吐量期望能实现的打字速度上限。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心经授权编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 主流深度学习框架对比：看你最适合哪一款？</title>
      <link>http://www.iwgc.cn/link/2675474</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自deeplearning4j.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Aäron van den Oord、Heiga Zen、Sander Dieleman&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;近日，Deeplearning4j 在自己的官方网站发表了一篇对比 Deeplearning4j 与 Torch、Theano、Caffe、TensorFlow 的博客文章，同时 Deeplearning4j 在文章中也对自己的框架进行了较为详细的介绍（多有溢美之词）。机器之心对全文进行了编译，文中观点仅代表原作者立场。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Theano &amp;amp; Ecosystem&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Torch&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Tensorflow&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Caffe&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNTK&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DSSTNE&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Speed&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DL4J: Why the JVM?&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DL4S: Deep Learning in Scala&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Machine-Learning Frameworks&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Further Reading&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Theano 与生态系统&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习领域内的很多学术研究人员依赖于 Theano，这个用 Python 编写的框架可谓是深度学习框架的老祖宗。Theano 像 Numpy 一样，是一个处理多维数组的库。与其他库一起使用，Theano 很适合于数据探索和进行研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Theano 之上，已经有很多的开源的深度库建立起来，包括 Keras、Lasagne 和 Blocks。这些库的建立是为了在 Theano 偶尔的非直觉界面上更简单地使用 API。（截止到 2016 年 3 月，另一个与 Theano 相关的库 Pylearn2 可能即将死亡。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相反，Deeplearning4j 能在 JVM 语言（比如，Java 和 Scala）下将深度学习带入生产环境中，创造出解决方案。Deeplearning4j 意在以一种可拓展的方式在并行 GPU 或 CPU 上将尽可能多的环节自动化，并能在需要的时候与 Hadoop 和 Spark 进行整合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python+Numpy&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）计算图是很好的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）RNN 完美适配计算图&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）原始 Theano 在某种程度上有些低水平&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）高层次 wrappers（Keras，Lasange）减轻了这种痛苦&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）错误信息没有帮助&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）大型模型有较长的编译时间&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比 Torch 更「臃肿」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）对预训练模型支持不佳&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）在 AWS 上有很多 bug&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Torch&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Torch 是一个用 Lua 编写的支持机器学习算法的计算框架。其中的一些版本被 Facebook、Twitter 这样的大型科技公司使用，为内部团队专门化其深度学习平台。Lua 是一种在上世纪 90 年代早期在巴西开发出来的多范式的脚本语言。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Torch 7 虽然强大，却并未被基于 Python 的学术社区和通用语言为 Java 的企业软件工程师普遍使用。Deeplearning4j 使用 Java 编写，这反映了我们对产业和易用性的关注。我们相信可用性的限制给深度学习的广泛使用带来了阻碍。我们认为 Hadoop 和 Spark 这样的开源分布式应该自动具备可扩展性。我们相信一个商业化支撑下的开源框架是保证工具有效并建立一个社区的合适解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）很多容易结合的模块碎片&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）易于编写自己的层类型和在 GPU 上运行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Lua（大部分库代码是 Lua 语言，易于读取）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）大量的预训练模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）Lua（小众）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）你总是需要编写自己的训练代码（更不能即插即用）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）对循环神经网络不太好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）没有商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）糟糕的文档支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌创造 TensorFlow 取代 Theano，其实这两个库相当类似。Theano 的一些创造者，比如 Ian Goodfellow 在去 OpenAI 之前就是在谷歌打造 TensorFlow。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目前，TensorFlow 还不支持所谓的「inline」矩阵运算，但会强迫你按序 copy 一个矩阵，并在其上进行运算。copy 大型矩阵非常耗费成本，相比于其他先进的深度学习工具 TensorFlow 要花费 4 倍的时间。谷歌说他们正在解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;像大部分深度学习框架一样，TensorFlow 在 C/C++ 引擎之上使用 Python API 编写，从而加速其运行。对 Java 和 Scala 社区而言，它并非一个合适的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow 不只是面向深度学习，也有支持强化学习和其它算法的工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌开放 TensorFlow 的目标看起来是想吸引更多的人，共享他们研究人员的代码，标准化软件工程师进行深度学习的方式，并吸引他人对谷歌云服务的兴趣——TensorFlow 针对谷歌云服务进行了优化。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow 并非商业支持下的，而且看起来谷歌也不可能成为支持开源企业软件的企业。它只为研究者提供新工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如同 Theano，TensorFlow 生成一个计算图（比如一系列矩阵运算，像 z=Simoid（x）, 其中 x 和 z 都是矩阵）并进行自动微分。自动微分很重要，因为每次实验一个新的神经网络的时候，你肯定不想手动编写一个反向传播新变体的代码。在谷歌的生态系统中，计算图后来被 Google Brain 使用进行一些繁重工作，但谷歌还未开源其中一些工具。TensorFlow 只是谷歌内部的深度学习解决方案的一半。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;从企业的角度来看，一些公司需要考虑的是他们是否想依赖谷歌的这些工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Caveat：在 TensorFlow 中的所有运算并不都像在 Numpy 中一样。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python+Numpy&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）计算图抽象，如同 Theano&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）比 Theano 更快的编译速度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）进行可视化的 TensorBoard&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）数据和模型并行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比其它框架慢&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比 Torch 更「臃肿」；更神奇；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）预训练模型不多&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）计算图是纯 Python 的，因此更慢&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）无商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）需要退出到 Python 才能加载每个新的训练 batch&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不能进行太大的调整&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）在大型软件项目上，动态键入易出错&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Caffe&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Caffe 是一个知名的、被普遍使用的机器视觉库，其将 Matlab 的快速卷积网接口迁移到了 C 和 C++ 中。Caffe 不面向其他深度学习应用，比如文本、声音或时序数据。如同其他框架一样，Caffe 选择 Python 作为 API。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deeplearning4j 和 Caffe 都能用卷积网络进行图像分类，都展现出了顶尖水平。相比于 Caffe，Deeplearning4j 还提供了任意数量芯片的并行 GPU 支持，以及许多可使得深度学习在多个并行 GPU 集群上运行得更平滑的看起来琐碎的特征。Caffe 主要被用于作为一个托管在其 Model Zoo 网站上的预训练模型的源。Deeplearning4j 正在开发一个能将 Caffe 模型导入到 Spark 的解析器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）在前馈网络和图像处理上较好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）在微调已有网络上较好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）不写任何代码就可训练模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python 接口相当有用&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）需要为新的 GPU 层编写 C++/CUDA&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不擅长循环网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）面对大型网络有点吃力（GoogLeNet，ResNet）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不可扩展&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）无商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CNTK&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNTK 是微软的开源深度学习框架，是「Computational Network Toolkit（计算网络工具包）」的缩写。这个库包括前馈 DNN、卷积网络和循环网络。CNTK 提供一个 C++ 代码上的 Python API。虽然 CNTK 有一个许可证，但它还未有更多的传统许可，比如 ASF2.0，BSD，或 MIT。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;DSSTNE&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;亚马逊的 Deep Scalable Sparse Tensor Network Engine（DSSTNE）是一个为机器学习、深度学习构建模型的库。它是最近才开源的一个深度学习库，在 TensorFlow 和 CNTK 之后才开源。大部分使用 C++ 编写，DSSTNE 似乎很快，尽管它如今没有其它库那样吸引大量关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;(+) 处理稀疏的编码&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;(-) 亚马逊可能不会共享要得到其样本的最好结果所必需的所有信息&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deeplearning4j 使用 ND4J 执行的线性代数计算展现出了在大型矩阵相乘上的至少比 Numpy 快两倍的速度。这也是为什么我们的 Deeplearning4j 被 NASA 喷气推进实验室里的团队采用的原因之一。此外，Deeplearning4j 在多种芯片上的运行已经被优化，包括 x86、CUDA C 的 GPU。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Torch7 和 DL4J 都可并行，但 DL4J 的并行是自动化的。也就是说，我们对工作节点和连接进行了自动化，在 Spark、Hadoop 或者与 Akka 和 AWS 上建立大规模并行的时候，能让用户对库进行分流。Deeplearning4j 最适合于解决特定问题，而且速度很快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;为什么要用 JVM（Java 虚拟机）？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们常被问到我们为什么要为 JVM 实现一个开源深度学习项目，毕竟现在深度学习社区的相当大一部分的重点都是 Python。Python 有很好的句法元素（syntactic elements）让你可以不用创建明确的类就能进行矩阵相加，而 Java 却需要这么做。另外，Python 还有丰富的科学计算环境，带有 Theano 和 Numpy 这样的原生扩展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但 JVM 及其主要语言（Java 和 Scala）也有一些优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，大部分主要的企业和大型政府机构严重依赖于 Java 或基于 JVM 的系统。他们已经进行了巨大的投资，他们可以通过基于 JVM 的人工智能利用这些投资。Java 仍然是企业中使用最广泛的语言。Java 是 Hadoop、ElasticSearch、Hive、Lucene 和 Pig 的语言，而这些工具正好对机器学习问题很有用。另一种 JVM 语言 Scala 也是 Spark 和 Kafka 的编程语言。也就是说，许多正在解决真实世界问题的程序员可以从深度学习中受益，但它们却被一些语言上的障碍隔开了。我们想要使深度学习对许多新的受众更有用，让他们可以直接将其拿来使用。在全世界 1000 万开发者中，他们使用得最多的语言就是 Java。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二，Java 和 Scala 实际上比 Python 更快。任何使用 Python 本身写成的东西——不管其是否依赖 Cython——都会更慢。当然啦，大多数计算成本较高的运算都是使用 C 或 C++ 编写的。（当我们谈论运算时，我们也考虑字符串等涉及到更高层面的机器学习过程的任务。）大多数最初使用 Python 编写的深度学习项目如果要被投入生产，就不得不被重写。Deeplearning4j 可以依靠 JavaCPP 来调用预编译的原生 C++，这能极大地加快训练速度。许多 Python 程序员选择使用 Scala 做深度学习，因为当在一个共享代码基础上与其他人合作时，他们更喜欢静态类型和函数式编程。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三，虽然 Java 缺乏稳健的科学计算库，但把它们编写出来不就好了，这个我们可以通过 ND4J 完成。ND4J 运行在分布式 GPU 或 CPU 上，可以通过 Java 或 Scala API 接入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，Java 是一种安全的、网络的语言，本身就能在 Linux 服务器、Windows 和 OS X 桌面、安卓手机和使用嵌入式 Java 的物联网的低内存传感器上跨平台工作。尽管 Torch 和 Pylearn2 可以通过 C++ 优化，但其优化和维护却比较困难；而 Java 是一种「一次编程，到处使用」的语言，适合那些需要在许多平台上使用深度学习的公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;生态系统&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Java 的普及只能通过其生态系统加强。Hadoop 是用 Java 实现的；Spark 在 Hadoop 的 Yarn 运行时间内运行；Akka 这样的库使得为 Deeplearning4j 构建分布式系统是可行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总的来说，Java 拥有经过了高度检验的基础设施，拥有大量各种各样的应用，而且使用 Java 编写的深度学习网络也能与数据保持紧密，这可以让程序员的工作更简单。Deeplearning4j 可作为一种 YARN 应用而运行和供应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Java 也可以从 Scala、Clojure、Python 和 Ruby 等流行的语言中被原生地使用。通过选择 Java，我们能尽可能最少地排除主要的编程社区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Java 并不如 C/C++ 快，但其比其它语言快得多。而且我们已经构建一种可以通过加入更多节点来进行加速的分布式系统——不管是用 CPU 或是 GPU 都可以。也就是说，如果你想要更快，就加入更多卡吧！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们正在使用 Java 为 DL4J 开发 Numpy 的基本应用（包括 ND-Array）。我们相信 Java 的许多缺点可以被很快地克服，而它的许多优点也还将继续持续一段时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;SCALA&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在开发 Deeplearning4j 和 ND4J 的过程中，我们对 Scala 给予了特别的关注，因为我们相信 Scala 有望变成数据科学的主导语言。使用 Scala API 为 JVM 编写数值计算、向量化和深度学习库能推动该社区向这一目标迈进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要真正理解 DL4J 和其它框架之间的差异，你必须真正试一试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器学习框架&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上面列出的机器学习框架更多是专用框架，而非通用机器学习框架，当然，通用机器学习框架也有很多，下面列出几个主要的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;sci-kit learn：Python 的默认开源机器学习框架&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Apache Mahout：Apache 上的旗舰机器学习框架。Mahout 可用来进行分类、聚类和推荐。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SystemML：IBM 的机器学习框架，可用来执行描述性统计、分类、聚类、回归、矩阵分解和生存分析（Survival Analysis），而且也包含支持向量机。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Microsoft DMTK：微软的分布式机器学习工具包。分布式词嵌入和 LDA。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
  </channel>
</rss>
