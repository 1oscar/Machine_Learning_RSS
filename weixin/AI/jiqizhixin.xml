<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | KDnuggets 官方调查：数据科学家最常用的十种算法</title>
      <link>http://www.iwgc.cn/link/2722289</link>
      <description>&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px;"&gt;&lt;/em&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自kdnuggets&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Terrence L、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px;"&gt;最新一期的 KDnuggets 调查展示了一份数据科学家使用度最高的算法列表，这份列表中包含了很多惊喜，包括最学术的算法和面向产业化的算法。&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;哪些方法/算法是您在过去 12 个月中运用到一个实际的数据科学相关的应用程序中的？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是基于 844 个投票者的结果&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;排名前十的算法以及他们的投票者的比例分布如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8CJkNzYumOt94ERv7VtMNlXRrMYONMVzvsHSPAw8VF1dCib0LAkLjtoA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1 ：数据科学家使用度最高的 10 大算法&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;文末有全部算法的集合列表&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个受访者平均使用 8.1 个算法，这相比于 2011 的相似调查显示的结果有了巨大的增长&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 2011 年关于数据分析/数据挖掘的调查相比，我们注意到最常用的方法仍然是回归、聚类、决策树/Rules 和可视化。相对来说最大的增长是由 (pct2016 /pct2011 - 1) 测定的以下算法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Boosting，从 2011 年的 23.5% 至 2016 年的 32.8％，同比增长 40％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;文本挖掘，从 2011 年的 27.7% 至 2016 年的 35.9％，同比增长 30％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可视化，从 2011 年的 38.3% 至 2016 年的 48.7％，同比增长 27％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;时间序列/序列分析，从 2011 年的 29.6% 至 2016 年的 37.0%，同比增长 25％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;异常/偏差检测，从 2011 年的 16.4% 至 2016 年的 19.5％，同比增长 19％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;集成方法，从 2011 年的 28.3％至 2016 年的 33.6％，同比增长 19％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;支持向量机，从 2011 年的 28.6% 至 2016 年的 33.6％，同比增长 18％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;回归，从 2011 年的 57.9% 至 2016 年的 67.1％，同比增长 16％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最受欢迎算法在 2016 年的调查中有了新的上榜名单：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;K-近邻，46％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;主成分分析，43％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;随机森林，38％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;优化，24％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;神经网络 - 深度学习，19％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;奇异值分解，16％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最大幅下降的有：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联规则，从 2011 年的 28.6% 至 2016 年的 15.3％，同比下降 47％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;隆起造型，从 2011 年的 4.8% 至 2016 年的 3.1％，同比下降 36％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因素分析，从 2011 年的 18.6% 至 2016 年的 14.2％，同比下降 24％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;生存分析，从 2011 年的 9.3% 至 2016 年的 7.9％，同比下降 15％&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下表显示了不同的算法类型的使用：监督算法、无监督算法、元算法，以及职业类型决定的对算法的使用。我们排除 NA（4.5％）和其他（3％）的职业类型。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;职业类型&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;% 投票者比例&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;平均算法使用个数&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;% 监督算法使用度&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;% 无监督算法使用度&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;% 元使用度&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;p&gt;&lt;span&gt;%其他方法使用度&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;p&gt;&lt;span&gt;1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;产业&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;59%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;8.4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;94%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;81%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;55%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;83%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;p&gt;&lt;span&gt;2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;政府/非营利机构&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;4.10%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;9.5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;91%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;89%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;49%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;89%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;p&gt;&lt;span&gt;3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;学生&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;16%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;8.1&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;94%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;76%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;47%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;77%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;p&gt;&lt;span&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;学术界&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;12%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;7.2&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;95%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;81%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;44%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;77%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;p&gt;&lt;span&gt;5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;整体&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;8.3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;94%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;82%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;48%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;p&gt;&lt;span&gt;81%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 1：根据职业类型显示的不同算法使用度&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们注意到，几乎每个人都使用监督学习算法。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;政府和产业业数据科学家比学生和学术研究人员使用更多不同类型的算法，而产业数据科学家们更倾向于使用元算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们根据职业类型分析了前 10 名的算法+深度学习使用情况。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240); word-break: break-all;" width="23"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="71"&gt;&lt;br/&gt;&lt;span&gt;算法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="38"&gt;&lt;br/&gt;&lt;span&gt;产业&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="88"&gt;&lt;br/&gt;&lt;span&gt;政府/非盈利机构&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="58"&gt;&lt;br/&gt;&lt;span&gt;学术界&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="56"&gt;&lt;br/&gt;&lt;span&gt;学生&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="74"&gt;&lt;br/&gt;&lt;span&gt;整体&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;回归&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;71%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;63%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;51%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;64%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;67%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;聚类&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;58%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;63%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;51%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;58%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;57%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;3&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;59%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;63%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;38%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;57%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;55%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;可视化&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;55%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;71%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;28%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;47%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;49%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;5&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;K-近邻法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;46%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;54%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;48%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;47%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;46%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;6&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;主成分分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;43%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;57%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;48%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;40%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;43%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;7&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;统计&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;47%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;49%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;37%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;36%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;43%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;8&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;随机森林&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;40%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;40%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;29%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;36%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;38%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;9&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;时间序列&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;42%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;54%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;26%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;24%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;37%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;10&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;文本挖掘&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;36%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;40%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;33%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;38%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;36%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="10"&gt;&lt;span&gt;11&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="71"&gt;&lt;span&gt;深度学习&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="38"&gt;&lt;span&gt;18%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="88"&gt;&lt;span&gt;9%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="58"&gt;&lt;span&gt;24%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="56"&gt;&lt;span&gt;19%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="74"&gt;&lt;span&gt;19%&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.6;"&gt;&lt;span&gt;表 2：根据职业类型分类的 10 大算法+深度学习使用情况&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更明显的看到差异，我们计算了具体职业分类相比于平均算法使用度的一个算法偏差，即偏差（ALG，类型）=使用（ALG，类型）/使用（ALG，所有的）&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8ia7ER4vmopDKjJicADmiczYOUp3ibPvKd3FRhdVKpBIT3CDVibIxSYWAtnA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：职业对算法的使用偏好&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们注意到，产业数据科学家们更倾向于使用回归、可视化、统计、随机森林和时间序列。政府/非营利更倾向于使用可视化、主成分分析和时间序列。学术研究人员更倾向于使用主成分分析和深度学习。学生普遍使用更少的算法，但多为文本挖掘和深度学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来，我们看看某一具体地域的参与度，表示整体的 KDnuggets 的用户：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;美国/加拿大，40%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;欧洲，32%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;亚洲，18%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;拉丁美洲，5%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;非洲/中东，3.4%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;澳洲/新西兰，2.2%&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于在 2011 年的调查中，我们将产业/政府分在了一组，而将学术研究/学生分在了第二组，并计算了算法对于业界/政府的亲切度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;N(Alg,Ind_Gov) / N(Alg,Aca_Stu) &lt;br/&gt;------------------------------- - 1 &lt;br/&gt;N(Ind_Gov) / N(Aca_Stu)&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此亲切度为 0 的算法表示它在产业/政府和学术研究人员或学生之间的使用情况对等。越高 IG 亲切度表示算法越被产业界普遍使用，反之越接近「学术」。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最「产业」的算法是：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;异常检测，1.61&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;生存分析，1.39&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因子分析，0.83&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;时间序列/序列，0.69&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联规则，0.5&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而 uplifting modeling 又是最「产业的算法」，令人惊讶的发现是，它的使用率极低 - 只有 3.1％ - 是本次调查的算法中最低的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最学术的算法是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;常规神经网络，-0.35&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;朴素贝叶斯，-0.35&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;支持向量机，-0.24&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;深度学习，-0.19&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;EM，-0.17&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下图显示了所有的算法及其产业/学术亲切度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8BmN4KPibXJ4iaUT8FHia5A8zJKllKe3vFayYicrPYlDlp35PhsmTgDzBeA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: center;"&gt;&lt;span&gt;图 3：KDnuggets 投票：最常被数据科学家使用的算法：产业界 VS 学术界&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下表有关于算法的细节、两次调查中使用算法的比例、以及像上面解释的产业亲切度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;表 3：KDnuggets 2016 调查：数据科学家使用的算法&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来的图表展示了算法的细节，按列&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;N：根据使用度排名&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;算法：算法名称，&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;类型：S - 监督，U - 无监督，M - 元，Z - 其他，&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2016 年调查中使用这种算法的调查者比例&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 2011 年调查中使用这种算法的调查者比例&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;变动（％2016 年/2011％ - 1），&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;产业亲切度（如上所述）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="24"&gt;&lt;br/&gt;&lt;span&gt;N&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="96"&gt;&lt;br/&gt;&lt;span&gt;算法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="34"&gt;&lt;br/&gt;&lt;span&gt;类型&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);" width="61"&gt;&lt;br/&gt;&lt;span&gt;2016 年使用度 %&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;span&gt;2011 年使用度 %&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;span&gt;改变度 %&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(240, 240, 240);"&gt;&lt;br/&gt;&lt;span&gt;产业亲和度&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;1&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;回归&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;67%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;58%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;16%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.21&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;聚类&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;U&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;57%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;52%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;8.70%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.05&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;3&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;决策树/Rules&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;55%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;60%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-7.30%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.21&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;可视化&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;49%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;38%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;27%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.44&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;5&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;K-近邻法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;46%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.32&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;6&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;主成分分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;U&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;43%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.02&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;7&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;统计&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;43%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;48%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-11%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;1.39&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;8&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;随机森林&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;38%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.22&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;9&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;时间序列/序列分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;37%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;30%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;25%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.69&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;10&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;文本挖掘&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;36%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;28%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;29.80%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.01&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;11&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;组合方法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;M&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;34%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;28%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;18.90%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.17&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;12&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;支持向量机&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;34%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;29%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;17.60%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.24&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;13&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;Boosting&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;M&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;33%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;23%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;40%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.24&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;14&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;常规神经网络&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;24%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;27%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-10.50%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.35&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;15&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;最优化&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;24%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.07&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;16&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;朴素贝叶斯&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;24%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;22%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;8.90%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.02&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;17&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;Bagging&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;M&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;22%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;20%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;8.80%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.02&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;18&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;偏差检测&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;20%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;16%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;19%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;1.61&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;19&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;神经网络-深度学习&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;19%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.35&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;20&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;奇异值分解&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;U&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;16%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.29&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;21&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;关联规则&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;15%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;29%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-47%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;22&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;图/连接/社会网络分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;15%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;14%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;8%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.08&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;23&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;因素分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;U&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;14%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;19%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-23.80%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.14&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;24&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;贝叶斯网络&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;13%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.1&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;25&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;遗传算法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;8.80%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;9.30%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-6%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;0.83&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;26&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;生存分析&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;7.90%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;9.30%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-14.90%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.15&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;27&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;最大期望&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;U&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;6.60%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.19&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;28&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;其他方法&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;Z&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;4.60%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-0.06&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="12"&gt;&lt;span&gt;29&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="96"&gt;&lt;span&gt;Uplift modeling&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="17"&gt;&lt;span&gt;S&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);" width="61"&gt;&lt;span&gt;3.10%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;4.80%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;-36.10%&lt;/span&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;2.01&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 4：KDnuggets 2016 调查：数据科学家使用的算法&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 17 Sep 2016 16:18:58 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 深度学习能力的拓展，Google Brain讲解注意力模型和增强RNN</title>
      <link>http://www.iwgc.cn/link/2722290</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自distill.pub&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲、孙宇辰、Jianyong Wang&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;本文的作者是 Google Brain 的两位研究者 Chris Olah 和 Shan Carter，重点介绍了注意力和增强循环神经网络，他们认为未来几年这些「增强 RNN（augmented RNN）」将在深度学习能力扩展中发挥重要的作用。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环神经网络（recurrent neural networks）是深度学习的重要组成部分，让神经网络可以处理诸如文本、音频和视频等序列数据。它们可被用来做序列的高层语义理解、序列标记，甚至可以从一个片段生产新的序列！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8n6DWbia7AgWnpMFOkgzuYQqTDcatib0KicqAxqE1ffBic2L62OLOoQqnMg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基本的 RNN 结构难以处理长序列，然而一种特殊的 RNN 变种即「长短时记忆模型（LSTM）」网络可以很好地处理长序列问题。这种模型能力强大，能在翻译、语音识别和图像描述等众多任务中均取得里程碑式的效果。因而，循环神经网络在最近几年已经得到了广泛使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如所发生的一样，我们看到给 RNN 添加新性能的研究工作越来越多。其中有四个特别突出的方向非常激动人心：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic89zXEKTP9lCWibfozJpZ2oewkKBLp6Tq6OUblkU9hcTVZo26CpcQeSlQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些技术都是 RNN 非常有效的扩展，但真正引人注目的是它们可以有效地组合起来，而且似乎正要进入一片更为广阔的天地。此外，它们都依赖于注意力（attention）这样一种同样基础的技术才能有效。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们认为未来几年这些「增强 RNN（augmented RNN）」将在深度学习能力扩展中发挥重要的作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;神经图灵机&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经图灵机（Graves, et al., 2014）是一个 RNN 和一个外部存储库的结合。由于向量（vector）是神经网络的自然语言，所以这个记忆是一个向量数组：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8QiaLQLvib3P17c5VlzhPGTXn7AxheI34KRSYfjmtg6X7Fj5NMzlVvvAQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，读和写是怎么工作的呢？其难点在于我们想让它们可微分（differentiable）。尤其是，我们想让它们对于我们读或写的位置是可微的，以使得我们可以学习读和写的位置。这是棘手的，因为内存地址似乎从根本上就是离散的。神经图灵机（NTM）运用了一种非常聪明的解决方案：在每一步，它们以不同程度在所有地方都进行读和写。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们举一个关于读的例子。RNN 给出一个「注意力分配」来描述我们在所关心的不同记忆位置展开多少，而不是指定一个位置。因此，读操作的结果是一个加权和（weighted sum）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8wIlnqjaAE7Xenw9dhnz0j8S6KBrHkUuTDjyib9CKVsiaedqHp8UgNGGw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同样地，我们以不同程度同时在所有地方执行写操作。同时一个注意力分布描述了我们在每个位置写的多少。我们通过获得一个存储位置中的新的值来实现这一点，这个值是由旧记忆和写入值与由注意力权重决定的两者之间位置的凸组合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8DEJ84JHSULh8m0JcKQIFj0p6BHUHlWaqpNbesyaxiaOCueLrPqPhnIA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，NTMs 如何决定应该关注记忆中的哪些位置呢？实际上，他们使用了两种方法的组合：基于内容的注意力和基于位置的注意力。基于内容的注意力使 NTMs 可以在记忆中查找并关注与那些与查找相匹配的地方，而基于位置的注意力可以实现记忆中的相对运动，从而使 NTM 可以循环。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8KjicPE0CT06KkzVkjjXLIPz6bMd6pYnlmc07jXCiaFoUczPYb7dhSAicg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种读写能力使得 NTMs 可以执行许多简单的算法，超越以前的神经网络。例如，它们可以学习在记忆中存储一个长序列，然后循环它，不断回答指令。当它们做这些时，我们可以看他们读写的位置，以更好地了解他们在做什么：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们也可以学习模仿一个查阅表，甚至学习排序（尽管他们有些作弊）！在另一方面，他们始终不能做数字加法、乘法等许多基本的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic84YnWj0ZZGB4k9LlcAyHFDsqQjBCqOKeFmWpd0D5ESkzdQEpWXOhuCw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从原始的 NTM 论文开始，已经有许多令人振奋的论文探讨相似的方向。神经 GPU（Kaiser &amp;amp; Sutskever，2015）克服 NTM 无法处理数字加法和乘法的问题。Zaremba &amp;amp; Sutskever 在 2016 年采用强化学习来训练 NTMs，而不是原始的可微的读/写。神经随机存取机 (Kurach et al., 2015) 基于指针工作。一些论文已经探讨可微的数据结构，如堆栈和队列 (Grefenstette et al. 2015; Joulin &amp;amp; Mikolov, 2015)。另外，记忆网络 (Weston et al., 2014; Kumar et al., 2015) 是攻克类似问题的另一种方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在某些客观意义上，这些模型可执行许多任务，如学习如何做数字加法，都不是很难。对传统的程序合成领域来说只是小菜一碟。但是神经网络可以做许多其他事情，而像神经图灵机这样的模型似乎已经打破了对它们能力的极大限制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;代码&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一些实现这些模型的开源代码。神经图灵机的开源实现包括 Taehoon Kim (TensorFlow)、Shawn Tan (Theano)、Fumin (Go)、Kai Sheng Tai (Torch)、和 Snip (Lasagne) 做的部署。神经 GPU 公开版的代码是开源的，并放在 TensorFlow 模型库。记忆网络的开源实现包括 Facebook (Torch/Matlab)、YerevaNN (Theano)、和 Taehoon Kim (TensorFlow)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;注意力接口&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当我翻译一个句子时，我会尤其关注于我正在翻译的单词。当我转录一个音频，我会仔细听我正在写的那一段。如果你让我来描述我所坐的房间，我会随时把目光转移到我正在描述的物体上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络可以通过注意力来实现同样的行为——关注所收到信息子集的一部分。例如，一个 RNN 参与另一个网络的输出。在每一个时间步骤，它会关注于另一个 RNN 的不同位置。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们希望注意力是可微的，这样我们就可以学习关注哪里。为了做这个，我们使用了和神经图灵机中一样的 trick：关注所有位置，只是程度不一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8jmOniaO5a0E3Hf7ic1eq0uYsQibOgGuhbnGmMf5KF9lUsIp51ogcSJw7A/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通常是使用基于内容的注意力生成注意力分布。参与的 RNN 会生成一个描述它想关注内容的查询。每一个条目和这个查询做点乘来产生一个分数，这个分数描述这个条目与查询匹配程度。这些分数被输入一个 softmax 来生成注意力分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8QiaLQLvib3P17c5VlzhPGTXn7AxheI34KRSYfjmtg6X7Fj5NMzlVvvAQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RNNs 之间注意力的一个应用是翻译 (Bahdanau, et al. 2014)。一个传统的序列到序列模型需要将整个输入抽象成一个向量，然后将它展开回复出来。注意力规避了这种做法，它让 RNN 沿着它看见的每个单词信息来处理输入，然后让 RNN 生成输出来关注到具有相关性的单词上。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8ZOoB5YBSdEAp4LYzqj5Zic9lRkdkFqjCSYXoDq5ka9EicdELAY64cPQg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Diagram derived from Fig. 3 of Bahdanau, et al. 2014&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种 RNNs 之间的注意力有许多其他应用。它可以用来做语音识别 (Chan, et al. 2015)，使得一个 RNN 处理语音，另一个 RNN 浏览它，使其在生成文本时可以集中在相关的部分上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8ZuVmG5CK3h5ulwabib1VAFLOWiaBem1oCJx6HXrpXO63mDo5KkicMCGLg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种注意力的其他应用包括：文本解析 (Vinyals, et al., 2014)，它使模型在生成解析树时能浏览单词；对话建模 (Vinyals &amp;amp; Le, 2015)，使模型在生成响应时关注于对话的前面部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意力同样可以用在卷积神经网络和 RNN 的接口。它使得 RNN 在每一步可以观察一张图像的不同位置。这种记忆力的一个流行应用就是图片描述（image captioning）。首先，一个卷积网络处理图片提取高层特征。然后一个 RNN 开始运营，生成一段对图像的描述。在生成这个描述的每一个单词时，RNN 关注于图像相关部分的卷积网络解释。如下图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8hIFvBRTvonqmlZzjeiawCFwibVhCic2CgNfrz8TxUJdEnyvKWMhBhibMpg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更广泛地说，当希望与一个在输出具有重复结构的神经网络交互时，注意力接口都可以被采用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们发现，注意力接口已经是一个非常普遍和强大的技术，并且正变得越来越普遍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;自适应计算时间&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;标准 RNN 在每一个时间步骤完成相同的计算量。这看起来不是很直观。一个人当然在问题变得困难的时候思考的更多，不是么？这也限制了 RNN 在 长度为 n 的链表上完成 O(n) 的运算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自适应计算时间（Graves，2016），是让 RNN 在每一步有不同计算量的方式。核心想法很简单：允许 RNN 在每一时间步骤做多个计算步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了让网络学习有多少步骤要做，我们想要步骤的数量是可微分的。我们采用之前用过的技巧完成这项任务：不再是决定运行不连续数量的步骤，而是有一个在运行步骤数量上的注意分布。输出是每个步骤输出的加权求和。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8v3ojKddH9gVuZibrmSAxGHdlMVMYe5LOD6NCveicgqul9R7mVmn59nYw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一些细节在先前的图解中被忽视了。这儿是一个完整的、包含一个时间步骤、三个计算步骤的图解。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic86NJ6k0dYghSwicxBXA7XxUtt9MIJPkDU8qxk7PS2y5Qk9Nbxp80HzBg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里稍有些复杂，所以让我们一步一步解决。在高层次上，我们仍运行着 RNN，并输出状态的加权求和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8VRPgs4hqfqBGia4Xsf2XyCz02Y3APMjdl2HPgYe8JxmibGQpeQfqutPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每一步的权值由「阻止神经元（halting neuron）」所决定。它是一个考察 RNN 状态的 S 型神经元，并产生一个阻止权值，我们可以认为这个权值是我们应该在那个步骤停下来的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8YAGzq6ERSaojsPLSeqYUBor149fOsdic7YZdYxWtlOJIJicshgayhZhg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们对于阻止权重（halting weight）为 1 的有总预算，所以我们顺着顶层跟踪这个预算。当这个值小于 epsilon，我们停止。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8dxWgGyB5IS9TvWmcRFkRxXtbOQNKkCTG8ZTj8f6selMichrJdicsJicuQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于我们是当预算值小于 epsilon 的时候停止，当我们停止时可能会剩余一些阻止预算（halting budget）。我们应该用它做什么呢？技术上，它应被赋予给未来的步骤，但是我们不想计算那些，所以我们将它归属于最后一个步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8qOSegKYOxHD2NGTiay8AZhAOBCib6Q9CK32cyxPT0ju8dlW4l7o6LnOg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当在训练自适应计算时间模型时，有人在成本函数中增加了「考虑成本（ponder cost）」这一术语。它对模型使用的计算量予以处罚。这个值越大，在性能和降低计算时间进行更多地权衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自适应计算时间是非常新的想法，但是我们相信，与其他类似的想法一样，它们都将是非常重要的想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;代码&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今，部署自适应计算时间的唯一一个开源，看起来是 Mark Neumann（https://github.com/DeNeutoy/act-tensorflow）做的。（TensorFlow）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;神经编程器（Neural Programmer）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络在许多任务上表现出色，但是它们也在努力做一些基础事情，例如用普通方法计算很琐碎的算数。如果有一种方式能融合神经网络与普通的编程，并吸收各自最好的东西，那真是太好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经编程器（Neelakantan，et al.，2015）是其中一种方式。为了解决一项任务，它学习创建程序。事实上，在不需要正确的程序样本的情况下，它学习生成这样的程序。它发现如何生产程序，并把它作为完成任务的手段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文中的实际模型回答了关于产生类 SQL 的程序查询表格的问题。然而，这有很多的细节使得这个问题稍有些复杂，所以让我们从想象一个稍简单的模型开始，给定一个算术表达式，并生成一个程序对其进行评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生成程序是一系列的运算。每个运算被定义为在上个运算输出上做运算。所以一个运算可能是例如「在两个步骤前的输出运算和一个步骤前的输出运算相加」这样的事情。这相比与一个有着可被赋值与读取的变量的程序，更像是 Unix 中的管道（pipe）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8ImEnXib4c1u0PPH6rjGytic5zbhHU1UMmaCibXTFnAoD5Hictciaib5oIPXw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;程序挨次通过控制器 RNN 生成一个运算。在每一步，控制器 RNN 输出一个概率分布，决定下一个运算该是什么。例如，我们可能非常确定我们想要在第一个步骤执行加法，然后要有有一个艰难的时间决定第二步我们应该是乘法还是除法，等等下去......&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8BdU2folIVNpXNTWEZKLCzgPwvDGQGsgMuse660haIPVky5enSDWZBw/0?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运算上的结果分布可被评估。不再是在每一步运行单个运算，如今我们采用常见的注意技巧运行所有运算，之后平均所有输出，通过我们运行这些运算的概率对其加权。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8YHUTXV6ibzmK6PubB2CTGGEFQ8qqxsB5qDAoem3GPwfDiaibjTGDjibFKA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只要我们通过该运算能够定义导数，关于概率的程序输出就是可微分的。之后我们就能定义损失，并训练神经网络生成得到正确答案的程序。在这种方式中，神经编程器在没有正确程序样本的情况下学习产生程序。唯一的监督是程序应该得到的答案。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这就是神经编程器的核心观点，但论文中回答的是关于表格的问题，而不是数学表达式的问题。下面是一些额外的灵活技巧：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多类别：神经编程器中的很多预算都是处理类型而不是标量数。一些运算输出表格中选中的列或是选中的单元。只有输出相同类型的会合并在一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基准输入：在给定人口的城市表格情况下，神经编程器需要回答例如「有多少城市人口超过 1000000？」这样的问题。为了使这项任务更容易，一些运算允许网络参考它们正在回答的问题或是类名中的常量。参考通过注意机制以指针网络的形式（Vinyals，et al.，2015）而产生。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经编程器不是唯一让神经网络生成程序的方式。另一个令人愉快的方式是神经编程器——解释器（Neural Programmer-Interpreter，Reed &amp;amp; de Freitas，2015），它能够完成许多非常有趣的任务，但是形式上需要正确程序的监督。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们认为缩小传统编程与神经网络之间的差距是极其重要的。虽然神经编程器显然不是最终的解决方案，但我们认为从它之中能学习到许多重要的思路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;代码&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在看起来没有任何神经编程器的开源部署，但是有一个 Ken Morishita（https://github.com/mokemokechicken/keras_npi）部署的神经编程器——解释器。（Keras）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;巨大的蓝图&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拥有一张纸的人在某些意义上比没有的人要更聪明。会使用数字符号的人可以解决一些问题，反之则不然。使用计算机可以使我们掌握超越自身的令人难以置信的技能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总之，智能很多有趣的形式是人类富有创造力和启发性的直觉与更加脆弱细致的媒介（就像语言和方程式）之间的交互。有时，媒介是物理实体，保存我们的信息，防止我们犯错误，或者处理繁重的计算任务。另一方面，媒介是我们可以控制的大脑里的模型。无论哪种方式，它看起来都是智能的深厚基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近在机器学习的研究结果已经开始呈现这种趋势，将神经网络的直觉与其他事物结合起来。有一种被称为「启发式搜索」的方法。例如，AlphaGo（Silver，et al.，2016）有个关于围棋如何运作的模型，并探索如何在神经网络的直觉指引下完成比赛。相似的，DeepMath（Alemi，et al.，2016）把神经网络作为对处理数学公式的直觉。我们在这篇文章中谈到的「增强递归神经网络」是另一种方式，我们将 RNNs 连接到工程媒介来延伸它们的通用能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与媒介自然交互涉及到采取行动、观察、采取更多行动等一系列操作。这给我们带来一项重大挑战——我们如何学习采取哪种行动？这看起来像是一个强化学习问题，我们将毫无疑问采用那种方式。但强化学习的研究确实正在攻克最难的问题，它的解决方案很难用。而注意力的绝妙支出在于它提供给我们一个更容易的方式，通过部分的在不同程度上采取所有去解决这个问题。在这种方法下，我们能够设计媒介——例如 NTM 存储器——允许分数运算以及可微。强化学习让我们走向单一道路，并尝试从中学习。而注意力会尝试岔路口的每一个方向，并将道路合并到一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意力的一个主要弱点是我们必须完成每步中的每个「行动」。当一个神经图灵机中的记忆量增加时，计算开销会呈线性增长。对此你可以想到一个解决方案，即让你的注意力变得稀疏，这样你就可以只接触到一些记忆。然而这仍然是个挑战，因为你可能希望你的注意力完全基于记忆内容，以使你可以轻易的观察到每一个记忆。我们已经观察到一些可以攻克这个问题的初步尝试，例如 Andrychowicz &amp;amp; kurach 所提出的方法，但看起来还有更多的事情要去做。如果我们确实能做到类似次线性时间注意力工作，那将非常强大！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;增强式递归神经网络，以及注意力的潜在技术，是非常令人激动的。我们期待看到接下来会发生什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参考文献：&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Alemi, A. A., Chollet, F., Irving, G., Szegedy, C., &amp;amp; Urban, J. (2016). DeepMath-Deep Sequence Models for Premise Selection. arXiv preprint arXiv:1606.04442.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Andrychowicz, M., &amp;amp; Kurach, K. (2016). Learning Efficient Algorithms with Hierarchical Attentive Memory. arXiv preprint arXiv:1602.03218.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bahdanau, D., Cho, K., &amp;amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Chan, W., Jaitly, N., Le, Q. V., &amp;amp; Vinyals, O. (2015). Listen, attend and spell. arXiv preprint arXiv:1508.01211.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Graves, A., Wayne, G., &amp;amp; Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Graves, A. (2016). Adaptive Computation Time for Recurrent Neural Networks. arXiv preprint arXiv:1603.08983.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Grefenstette, E., Hermann, K. M., Suleyman, M., &amp;amp; Blunsom, P. (2015). Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems (pp. 1828-1836).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Joulin, A., &amp;amp; Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems (pp. 190-198).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kaiser, Ł., &amp;amp; Sutskever, I. (2015). Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Gulrajani, I. &amp;amp; Socher, R., (2015). Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint arXiv:1506.07285.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kurach, K., Andrychowicz, M., &amp;amp; Sutskever, I. (2015). Neural random-access machines. arXiv preprint arXiv:1511.06392.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Neelakantan, A., Le, Q. V., &amp;amp; Sutskever, I. (2015). Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Olah, C. (2015). Understanding LSTM Networks.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Reed, S., &amp;amp; de Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint arXiv:1511.06279.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M. &amp;amp; Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vinyals, O., Kaiser, Ł., Koo, T., Petrov, S., Sutskever, I., &amp;amp; Hinton, G. (2015). Grammar as a foreign language. In Advances in Neural Information Processing Systems (pp. 2773-2781).Vinyals, O., &amp;amp; Le, Q. (2015). A neural conversational model. arXiv preprint arXiv:1506.05869.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vinyals, O., Fortunato, M., &amp;amp; Jaitly, N. (2015). Pointer networks. In Advances in Neural Information Processing Systems (pp. 2692-2700).&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Weston, J., Chopra, S., &amp;amp; Bordes, A. (2014). Memory networks. arXiv preprint arXiv:1410.3916.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R.S. &amp;amp; Bengio, Y., 2015. (2015). Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2(3), 5.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Zaremba, W., &amp;amp; Sutskever, I. (2015). Reinforcement learning neural Turing machines. arXiv preprint arXiv:1505.00521, 362.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 17 Sep 2016 16:18:58 +0800</pubDate>
    </item>
    <item>
      <title>机器之心x MIT-CHIEF |  脑控接口技术实现儿童注意力提升，开源项目OpenBLAS玩转计算性能升级</title>
      <link>http://www.iwgc.cn/link/2722291</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Chain Zhang, Rita Chen&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt; 编辑：Yina Zhao , Rita Chen&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;麻省理工学院中国创新与创业论坛（简称 MIT-CHIEF) 是美东地区最大的创新创业平台，汇集了美国最尖端的人才和项目，融合了中国和美国的各项优势资源。在刚刚过去的七月里，十六支涵盖医疗健康，新能源，教育及金融等领域的创业团队和 MIT-CHIEF 一起，走访了北京，上海，深圳和成都四大城市和与其相关的创业合作基地，与当地的政府，企事业单位代表进行了卓有成效的合作与交流。机器之心有幸采访到了其中的十一支团队，这是系列采访的第五篇。&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic88GOU84v7J0Jk6UMKCIkLSIOOO4QOnOIqEPX8WQ5wTaNMibwAY0MEd9g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;BrainCo 官方网站：http://www.brainco.tech/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BrainCo 是一家于 2014 年成立于美国波士顿的高科技产品公司。BrianCo 研发的可穿戴产品将涉及学生注意力培训、睡眠管理、智能家居控制、脑交流技术、脑疾病、神经疾病早期预防、疼痛测量等领域。2015 年初，BrianCo 得到天使投资，如今正在全力完成第一款产品，Focus 1，用于注意力提升培训和脑电操控电子设备，是世界上第一款医疗级别的可穿戴脑效率提升专家。机器之心之前有对其进行过专访。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic89wlb9KBOlNRFXiaGEYIxSyC5IYehJ7PsiaWoI5vGTW39h2yapfMu5v6g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;span&gt;&lt;em&gt;&lt;span&gt;PerfXLab 官网网站： http://perfxlab.com/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PerfXLab 是一家计算领域的性能优化解决方案提供商。通过本团队已有的世界领先开源项目 OpenBLAS，以嵌入式和人工智能领域为切入点，提供整套的性能优化框架和服务，帮助人工智能和其他技术计算领域的客户落地，缩短时间，提高效率。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;这一次我们有幸采访到了参与中国行的 BrainCo CMO 张云鹏，首席软件工程师徐睿瑜 和 PerfXLab 的 CEO &amp;amp; Co-Founder 张先轶。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8IwicEu1oXzprxcj6l90Vwsh5ga3q7LZB60CLuH4kqwGDHp7Gfa5hhnw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced : 三位好！首先能否请几位向机器之心的读者简要简绍一下各自的团队和产品的特点？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶: &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;好的！机器之心的读者大家好。我们是一家做计算的公司，产品方向主要是提供人工智能，CV 等领域的高性能解决方案。我们的成员来自中科院，MIT，NVIDIA 和 Intel 等公司。虽然我们目前规模不大，但是人员组成还是相对完备的，有负责硬件架构的，有负责算法和软件优化的，也有研究深度学习的。目前的产品是向嵌入式市场（手机，智能硬件，机器人，无人机等），提供高性能的深度学习解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;徐睿瑜:&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; 大家好！BrainCo 是一家创建于美国马萨诸塞州波士顿的科技产品公司，专注研发脑机接口 (Brain-Machine Interface) 技术。主要的应用方向是通过脑神经反馈训练，为 ADHD 患者治疗提供帮助，提升用户的注意力水平，并且提高学习效率。我们的团队由哈佛大学脑科学中心科学家和 MIT 工程师联合组成，目前核心成员大概二十人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8w9AH22t7bA27ia02p34XjyyygO0qAApBEBbnHqdiabWQQtb7MZrHZX9A/0?wx_fmt=jpeg"/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;BrainCo 团队合影&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced : 之前机器之心对你们的创始人进行过专访，他提到你们新一代的产品 Focus 1. 能否在这里再和我们简要介绍一下这个产品和产品特点？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;好的，BrainCo 的第一代产品是 Focus 1, 是一个集培训操控，教育学习和娱乐教育三位一体的注意力培训设备。孩子通过头环和终端设备，在游戏和课件里互动从而提高注意力水平。家长可以通过手机 App 进行后台管理和培训效果监测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8QGZZkbSmKcYrJvZCeqAa2YOyl0xxVw9ibzN4xnjEq7HYWNyZsax6WLw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced: 什么契机让你们开始这个方面的研发或者尝试呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶:&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; 我们本身就是做高性能计算出身的，领导了全球领先的矩阵计算开源项目 OpenBLAS，也为 OpenCV 贡献了 OpenCL 加速模块。我们看到随着数据越来越大，对处理能力的需求也越来越强烈。特别是最近比较火热的深度学习就对计算能力有比较高的要求，我们的 OpenBLAS 也成为了几个深度学习框架的依赖库之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8T2oA8chcgvgWiaRzjw983JFEjZw9WbgUMUS7635TpmBia0QliauF7Z8lQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;徐睿瑜:&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; 脑控可以实现的功能性其实很多。之所以我们选择从注意力为切入口是基于现在儿童中多动症比例越来越高的现状。根据权威机构统计，美国目前有 6 百万儿童患有多动症 (ADHD), 每年的市场总额达$450 亿，这是一个非常大的市场。同时，现在的药物治疗副作用大，而已有的脑神经反馈训练费用高，周期长，市场缺乏有效的家用解决方案。所以我们想通过自身的技术给家长和儿童提供一个更为高效简单的家用解决方案。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced:那在开发过程中有没有遇到什么困难？你们认为在这个领域你们自身最大的优势是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶: &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;目前人工智能这个领域很热，除了各大巨头的投入，国内外也出现了很多家相关的创业公司。如何找准自己公司的定位，保持技术的领先性是我们的挑战，也可以说是机遇。技术公司归根到底还是人的竞争，我觉得我们最大的优势就是团队非常优秀，在业界处在顶尖水平。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：就像刚才徐睿瑜提到的，脑控技术的应用范围广泛，这可能很多人看来是个优点，但其实对我们来说，也成为了一个难点。如何聚焦定位产品成为产品研发中反复出现的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced：相信两位都参与到了 MIT-CHIEF 的中国行活动中，能否和我们分享一下在这次活动中的主要收获？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶: &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;之前在 UT Austin 做 Research Fellow 的时候，虽然也有创业的想法，但是不大清楚应该怎么做。今年到了 MIT 后，遇到了 MIT-CHIEF，有一种找到组织的感觉。之前参加过 MIT-CHIEF 中国行的晶泰科技几位创始人给了我很大的帮助，无论是从思路的整理还是 BP 的准备。通过这次中国行，我认识了很多朋友，当然也有很多优秀的创业团队。通过互相的交流，开阔了眼界，学习其他前沿领域的发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;团队中本身也有来自 MIT-CHIEF 小伙伴, 从而了解到这个活动和项目。这次中国行除了融资和落地需求，开展在脑控应用领域的战略合作意向也是我们的意向之一。我们团队中的核心成员都是来自 MIT 和哈佛，MIT-CHIEF 是一个让我们认识优秀人才，并接触到更多优质创业资源的平台，这让我们能找到最棒的成员来实现技术功能。这次走访的城市各有特点，例如软件一定是北京，FinTech 一定是上海周边，智能硬件和制造一定是深圳，每个城市都非常有特色和相应的配套设施。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8Fn6fAHgsZ1GmSXbSibZwDgDpAdxBcW2AOvicN8y0c5Uv1FI76BYhnnVg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced : 谢谢三位。接下来我们会对各自的产品和技术问一些相对深入的问题。首先向问 BrainCo, 可以和我们详细介绍一下新一代产品 (Focus 1) 在哪些方面还可以进一步优化？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏:&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; 从上次采访完之后，我们硬件方面已经基本确定。外型设计方面，团队拥有设计 Apple Watch 和小红点至尊奖的小伙伴帮我们做外形优化，而软件方面现在处于攻坚阶段。我们现在做的 Focuse 1 产品是专注于注意力培训，里面涉及培训的流程和培训的内容。所以，现在主要和注意力培训方面的专家合作，探讨细化培训的流程。内容方面我们希望提供多样化的解决方案，不光有内置的游戏还有课件内容，希望给不同的使用者提供更多的选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8gjxAiciaka4jvyWmxjTyNgl4pqJsan3rkx6Yy6QdkUvfsGsAicEzLS8Zw/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced：那对于 PerfXLab 来说，你们面向的是人工智能或其他技术计算领域的企业，应用非常广泛。如果是对可穿戴领域来说（即 BrainCo 现在的这个领域），你们有没有什么涉猎或者思考？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶: &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我个人认为，AR 或者视觉计算技术可能会把可穿戴做成眼前一亮划时代的产品。一提到可穿戴，我第一个想到的是手环，但是它只是收集数据，没有向用户说明数据使用在哪些方面。而当你结合了数据处理，视觉，语音等概念在里面，我觉得可穿戴的市场会更加爆发起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced：所以说无论是怎么获取的数据，手环也好，脑控穿戴也好，最后的落脚点都是在数据上。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;某种程度上来说是的。BrainCo 是专注研发脑机接口 (Brain-Machine Interface) 技术，而脑际接口技术本身是个底层技术，它其实是个平台。脑际接口技术可以应用于不同的方向，需要有不同流程，也就是软件部分。就拿注意力培训来举例吧，它的软件设计的流程注意力培训和对脑电设计智能家居的设计是完全不一样的。再往上，通过这种应用的方式会收集人们很多的脑电数据。数据在当今社会变得非常重要，不管是做大数据还是做 AI，都需要数据。我们的底层技术会提供一个很大的未来数据库，而从数据库可以进行数据挖掘，能总结和研究人们的脑电行为模式，能更好的反馈怎样应用于技术本身，这才是最有意义的。如果我需要 PerfXLab 提供帮助的话，我想是这方面的。在有了海量数据以后，如何能够把其整合起来，提供更好的技术和市场方面的指导意义。这方面不知道你们能不能做到（笑）？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8ZLOO9A2ZKH5ib8sia0lGu0nclUBlez9qb9KentSiau0e4iadLpNbJDRMZg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;肯定可以。我觉得有两点我们可以帮助。第一点是针对你们的嵌入式设备或者穿戴式设备。以目前的趋势来看，这种设备不仅仅是数据的采集，而是实时就要做处理，做反馈等，这对计算的需求还是不小的。我们现在的深度学习高性能解决方案就是为了应对这种情况。第二点是对于原始数据的处理。各个企业都有源源不断的原始数据，但是可能没办法全部处理或者处理非常慢，有时候要几周或者更长时间才能有结果，这种对于实际过程中已经失去意义了。因为迭代更慢，没法适应市场的需求。而我们能做的是，把你们的处理时间缩短，提高效率，支持更大规模的数据分析处理的场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic81Ce8W77YQBquhhqz5n8mdsYDLqneWfRqnf9axibVtlibQNWDty83T6ZQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Synced :　哈哈，感觉采访结束之后两个团队真可以好好聊一下合作细节。采访最后是我们 Synced Talk 的固定快问快答环节。每一期我们都有固定的主题，这一期我们的主题是可穿戴与智能医疗。那么这里有三个问题，请你根据直观感受作答。第一个问题，你们认为既智能手表之后，什么会成为下一代可穿戴成品的主要载体？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;徐睿瑜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我觉得会是智能头环，或者说类似的可以帮助人们解放双手的脑部可穿戴设备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;下一代最符合人体工学的应该是眼镜。不管是什么样的智能硬件，都要符合人体工学。人体的输入设备在眼睛，而人体的输出设备在手这一块儿。另外，智能手机首先是个输入，然后是输出。输入这方面，越靠近脑袋越好，所以最直观的就是在眼镜上面。AR 的眼镜一旦把屏幕的限制解决了，可以给更多的智能应用更广阔的想象空间。以后的可穿戴产品将不会局限于有限的物理屏幕，而是一个 360 度甚至是 3D 的，如果同时加入手势控制和脑控，这会是给下一代带来新的体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个领域我不是专家，不敢做大的预测。我最基本的想法是偏向于眼镜，因为这是更自然的选择。以现在来看，可能是手表。但智能手表是不是大家追求的东西还尚且是个问题。对于男生带手表，更多是当做「男人的珠宝」，而不是在追求 high-tech 的产品，这其实真正制约了手表可穿戴形式的推广。而智能眼镜附加得感觉更多，可以做的东西也更多，我投眼镜一票。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;对，手表其实是在输出这块儿。如果从输入这一块考虑，眼镜在人体工学这块有更大的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced：作为用户本身，你最希望未来的可穿戴产品给你提供什么功能？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;徐睿瑜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我希望是能识别我的意识，我想让它干什么，它就真的能干什么。我还希望它在外形上、功能上、识别能力上都是个性化的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;智能硬件技术已经到了一个爆发的阶段，就是很多人在做不同技术方面的智能化的东西。把硬件方面加上 WiFi 模块，从而变成可穿戴设备，不管是收集数据，还是更复杂的反馈和操控体验，会产生很多数据，而这些数据很可能对我们生活有很好的指导作用。举个例子，现在大家谈大健康，而智能手环只是个记录器，它收集到的数据真正能提供多少健康方面的数据管理和建议，可能是人们更期待的。就是说，我有了数据之后，数据在没有被处理之前，是没有任何信息量没有任何意义的。我希望在数据爆发硬件爆发的同时，真正把智能方面加上去，让它变得像人一样在思考，和人一样能跟你交流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;对于穿戴设备，我希望可以更多和其他的智能设备互联。比如，你的设备或你的眼镜跟你其他的智能家居设备，如冰箱、电视、微波炉等，整体互联操作，可以一体化提高你的生活舒适度和水平。或者，其他的能提高路面上的安全和辅助的标识等等，这是我希望的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced：作为用户，你有很多选择，每个设备都会给你检测一个数据，你是否信赖这些数据？你觉得这些数据会怎样被使用？你会担心数据透露你的隐私吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;徐睿瑜：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;数据收集起来需要收集各种各样的信息，再通过机器学习，或者数据挖掘的方法，进行数据处理，要呈献给不同用户。比如 BrainCo 要呈现给用户的话，用户只用知道我的注意力提高了多少，我有没有得到改善。但是，同样的数据如果要呈现给做科研的人，呈现的数据必须非常精准。所以不同的数据会有不同的维度和接口，合理控制数据的获取和呈现形式能在某种程度上降低风险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张云鹏：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;人在大数据的年代，每一个人都是产生数据的载体。数据可以用来做什么呢，要看数据源是从哪里来的。如果是监控你行动的数据从而用它来间接推测你的睡眠质量，这肯定是不准确的。在数据的应用当中，包括直接和间接的。一方面，收集数据本身，人和人之间也是有个体差异的。所以在做数据分析的话，你会做一个校对，把你的数据校准到一个标准值范围内。这样你分析的东西才更有意义。继续聊直接和间接的问题，就是你的数据是从哪里来的，我们叫数据的质量。有的数据不是直接数据，直接影响到数据的可靠性。数据的使用必须有规范，用户隐私需要保护，所以数据的收集/传送都需要相对应的加密等级。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;张先轶：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;第一，对于可穿戴设备收集到的数据，我希望用户能有足够的控制权，比如是否能公布，起码让用户选择，我对隐私方面还是挺看重的。第二，数据就是钱，拿国内的微博举例，某些数据获取公司发现使用微博的用户数量在上升，那个时候微博的股价还处在低位，如果把这个联动起来，其实可以提前进入，购买股票，来获得盈利。另外一个例子是，有些微博是有地点的，而有地点的数据很多又是和旅游联系在一起的。你可以看到用户产生的数据，可以挖据的东西非常多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Synced : 谢谢！也希望你们在中国的推广和落地一切顺利！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;想了解更多关于 MIT-CHIEF 中国行的信息，请继续关注机器之心每周末的 Synced Talk 专题采访栏目。MIT-CHIEF 2016 Community 向创业团队正式开放！点击 www.mitchief.org 了解详情！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PerfXLab 公司（&lt;span&gt;http://perfxlab.com/&lt;/span&gt;）Discovering Potential. Driving Performance。目前团队正在招募算法/软件工程师或实习生，地点北京上海，亦可远程工作。要求有 Machine learning, CV 基础，计算背景或者写过 GPU，FPGA 等程序加分。对团队或者职位感兴趣的小伙伴可以发送邮件至 &lt;span&gt;jobs@perfxlab.com&lt;/span&gt;；&lt;span&gt;Github: &lt;/span&gt;&lt;span&gt;https://github.com/PerfXLab&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BrainCo 极客计划（www.brainco.tech）正在进行中。目前团队正在招募 IOS 开发工程师 2 名，安卓开发工程师 1 名。感兴趣的小伙伴请发&lt;strong&gt;送简历到 hr@brainco.tech, 标题为「极客计划+职位+申请人姓名」&lt;/strong&gt;。如果你有适合的人选想向我们推荐，也请发送简历到相同邮箱，并附标题为「极客计划+职位+推荐人姓名」。如果被推荐人被正式录用, 推荐人将获得 1000 美金/人的伯乐奖励。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 17 Sep 2016 16:18:58 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 体验谷歌云平台的三大机器学习API：视觉、语音和自然语言</title>
      <link>http://www.iwgc.cn/link/2722292</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 cloud.google.com&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：&lt;strong&gt;李亚洲、&lt;/strong&gt;杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;最近谷歌在其云机器学习 API 的产品页面上添加了 Try the API box：云视觉 API（Cloud Vision API）语音 API（Speech API）和自然语言 API（Natural Language API）。现在任何人都可以立即用自己的图像、声音和文本上体验谷歌的机器智能了。让我们来看看它是怎么玩的。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8rbEicybBWGo71QvopUicHsvpX1amXIEhlW528NP55ZsD1Z0hOdFuHljQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;尝试云视觉 API&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;云视觉 API 使得开发者能通过将强大的机器学习模型封装进一个容易使用的 REST API 中来理解一张图片的内容。现在来试一试，进入云视觉产品页面，并在 Try the API box 中的下拉或打开任意一张图片。点击「验证码」对话框来证明你不是一个自动化的脚本，然后图像打开。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面这张图显示了这个视觉 API 对一张照片的描述。照片上的南瓜灯是我和儿子在万圣节聚会上雕刻的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic80TcuNiboDWgU5eusNo9PicszmRr6tdTQrUf8dwkSMBABSSCK5gOlaPXw/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用这个 API 的标签识别法，Cloud Vision 能对上传的图像进行图像内容分析。看上去，Cloud Vision 的机器智能非常聪明，不仅能理解这个对象，还能理解语境（「万圣节」、「假期」，「雕刻」）。很棒吧？你也可以点击 JSON 响应选项卡来查看看这张图片分析的原生 JSON 格式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;光学字符识别（OCR）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个 API 的另一个令人印象深刻的功能时光学字符识别（Optical Character Recognition，OCR）。它能识别出图像中多种语言的字符和单词，并将它们提取成字符串，附加上每个单词在图像中的位置。让我们用下面这张图来试试这个功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8icUGIyqoZ9mZ5hKveUjfqiaP49eed6bVribkD8H4CpzxiaS3ibuvV1B6fPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当你在 box 中打开这张图片，并打开文本选项卡时，你就能看到 OCR 的分析结果了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8NNjibUv8kmBouDHLbibuicNx1nBNkyMGHbwjGibkDQiaqr6vsMg5bibTibYJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即便这张图中的单词的角度有点斜，还不太清楚，但 OCR 准确的提取词和他们的位置。它甚至能从参会者的 T 恤上选出「beacon」一词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;清楚图片、地标、Logo 的检测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;云视觉 API 也能检测其他特征，包括常见的地标和公司或产品 logo，都有着极高的准确率。同时，安全搜索检测，成人、暴力内容的检测，医学或假冒图像检测，都已经被用于数个社交内容提供商的产品中。先前，这些社交提供商雇佣大量的职员单个审核、过滤用户上传的有问题的图像。在安全搜索上，这能减少大量的人力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8rvGAvT3QQNCgsP1T7cXMPMzdhJXyqIS1jwibN8VEqQVD2eictlHUroAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;尝试云语音 API&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你是否注意到年轻人使用语音控制智能手机？现代智能手机中支持谷歌搜索和谷歌 Now 的语音识别引擎的背后云语音 API。现在，你可以在自己的应用上使用这一突破性的技术了。例如，一家呼叫中心提供商可以使用云语音 API 将音频数据转换为文本，然后你就可以使用自然语言 API 分析这些文本，我们接下来会对此进行讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;云语音 API 也有一个 Try the API box。进入谷歌产品页面，点击麦克风图标，记录一个超过 30 秒的音频。完成记录后，它将音频上传到 API 并展示结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8dQ51qBsicuZJ3FJgwqjTyu6ZMiaD0jZ4cau99Ysic3M62KhhWB3R2a3ibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你也可以在云语音 API 上尝试除英语外的语言。从列表中选择它所支持的 80 种语言与方言。个人发现，云语音 API 在日语上的表现也相当惊人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;尝试自然语言 API&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多开发者使用简单的关键词或常见表达匹配来处理自然语言文本。换言之，他们将文本作为无结构数据处理，没有关于文本含义的任何线索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用云自然语言 API，强大的机器学习模型揭示数据中的结构和含义，里面有一个易于使用的 REST API。现在你掌握的文本是结构化数据的，有各种特性和元数据。通过处理、分析或查询来自终端客户的文本，你可以增加自己应用的智能度了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面就是自然语言 API 的 Try the API box 了。直接点击分析按钮就能探索文本样本了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic80mfMSbSZVDg1sVn3xcTNSfLbbgK8Cib4QAvibctFKKPsxfK3pwTeuYqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Entities 标签中显示的是实体分析结果。给定缺省样本文本，文本中的 Google 一词被分类为公司名，Mountain View 是个地址。据自然语言 API 显示，Sundar Pichai 是一个名人，Android 是个消费品。在可用的情况下，云自然语言 API 也将回复这些实体的 Wikipedia 链接页面。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Aia9LDNzCHCBDhibwVLSCic8hAHQ5dW40Y0UBwAEN8CJKNc5h6RSOQnpg9N7icTyJemxsmyjKeUsPXA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;情感和语法分析&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对文本情感的分析，点击 Sentiment 标签就能完成。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据云自然语言 API 显示，"Sundar Pichai said in his keynote that users love their new Android phones"一句有着积极的情感。点击 Syntax 标签，你也可以进行语句的语法分析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;JSON 语义分析方法的响应提供了建立该文本的依存句法分析树（dependence parse tree）数据，如上图所示。有了这个功能后，你可以把整句话拆分成几个标记，和每个标记的词性（POS）如名词和动词，以及它们之间的依存关系。现在非结构化数据变成了带有分析（insight）的结构化数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;使用云机器学习 API 开发惊人的 App&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如同该文章显示的那样，使用里面的 Try the API boxes，你可以轻松体验谷歌最新的机器学习技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;云视觉 API 如今普遍可用，语音 API 和自然语言 API 处于测试阶段，任何人都可以对其进行评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 17 Sep 2016 16:18:58 +0800</pubDate>
    </item>
    <item>
      <title>深度 | MIT量子专家Seth Lloyd：量子计算更擅长机器学习，发现传统计算无法发现的数据模式</title>
      <link>http://www.iwgc.cn/link/2715048</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Edge.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、Rui Sun、李亚洲、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;让我们来思考下量子计算的未来，我不知道将来的每一台智能手机里是否都有量子计算机，或者我们是否会拥有量子 app 或者 quapps，从而借助量子计算机使我们的通讯更加安全，并且帮我们找到一些有趣的东西。这是一个很难完成的任务。很可能在计算机和智能手机中将拥有量子微处理器，完成特定的任务。&lt;/span&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;而这也是我们这些设备内部的相关技术发展的方向。如果有来自量子力学的优势，那我们就要采用这些优势，就像光合作用中的能量流动带有量子特性一样，如果量子计算的「怪招」能帮助我们，那就尽管使用一下这种「怪招」好了。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=j03285o919x&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen="" style=" width: 556px;  z-index: 1; "&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;SETH LLOYD，教授，量子力学工程师，MIT&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的重要性——一切自然界行为的通用语言&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，研究的兴趣又重新回到将量子力学和量子信息应用于量子引力学理论（译者注：量子引力，是对引力场进行量子化描述的理论，属于万有理论之一。研究方向主要尝试结合广义相对论与量子力学，为当前的物理学尚未解决的问题。当前主流尝试理论有：超弦理论、循环量子引力理论。引力波的发现，为量子引力理论提供了新的佐证。）上，以及探索宇宙本质的基础理论。事实证明，量子信息会给在苦苦寻求这些问题答案的人们带来很多帮助，例如，你掉进黑洞时会发生什么？如果你掉进一个黑洞，会有任何关于你的信息逃离此黑洞吗？这些就是斯蒂芬霍金等专家研究了几十年的问题。事实证明，量子信息学对我们找到这些问题的答案大有益处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;25 年前，我开始研究量子计算的相关问题，也就是原子、分子、光子和基本粒子如何处理信息。那时全世界研究这个问题的人也不过五六个，而现在有成千上万人。在任何一个快速扩张的领域都会出现各种分支。对于如何理解世界的基础问题的研究依然有很多分支，在其如何处理信息方面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有很多了解自然运行方式的实际问题。例如，过去十几年关于光合作用的研究已经非常清晰——光粒子来自于太阳，然后被叶绿素分子吸收，能量在一片树叶中形成并转移到更多树叶中——这是以一种非常量子力学的方式来进行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们研究量子计算所使用的一些模型也恰好可以用来解释光合作用的原理。事实证明，光合植物、细菌和藻类所使用的量子力学都极其复杂。它们会利用到量子相干性和量子纠缠这样的影响，实现高效的能量传输。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我对此的观点是，如果一点量子「诡计」就可以帮你更快的繁殖，那你一定要使用量子「诡计」。结果显示，植物、细菌和藻类已经使用量子诡计超过了 10 亿年来让它们生存的更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，在量子信息和量子计算领域正在发生的是，量子信息是一切自然界行为的通用语言这一点越来越清晰。几年前，Physics Today（这是美国物理学会物理学家的杂志）里有张插页，这不是一张非常性感的插页，但它拥有物理学的所有部分，包括高能物理学、固定物理学、弦理论、力学物理和纳米物理等。而放在正中央的恰恰是量子信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原因在于，这个插页展示了物理的那些部分会和其他部分产生联系，这个领域的谁和其他领域什么人能对上话。他们把量子信息放在中间是因为每个人都在和量子信息进行对话。因此，研究光合作用的物理化学家突然开始和我这样的研究者开始对话，并且开始一起做植物和细菌的实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在进行人工实验。在我们的案例中，它们是模仿光合作用中高效能量传输机制的人造和细菌造的系统。事实上，借助于量子信息理论，我们已经构建了比以往更加高效、甚至是有史以来最为高效的自然发生系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的进展和迅猛发展——更擅长机器学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此同时，除了所有这些理论发展，我们在开发处理信息的设备方面也取得了巨大进步，比如说在量子计算机方面。D-Wave 开发特定目的的量子计算机，而不是能够破解 NSA 代码，让其从心底感到恐惧的通用量子计算机。当然，如果 NSA 有心脏的话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些系统正在迅猛发展，用户购买它们并尝试在解决困难问题方面是否比传统计算机更快。这个问题尚无定论，我们不知道它们是否比传统计算机快。同时，还有些人在开发超导系统、由原子或离子组成的系统，以及光学系统，他们在建构量子计算机方面做的更好，期待着在未来 5-10 年出现能够解决传统计算机永远无法解决的问题的量子计算机。想到关于量子计算机的新想法是令人激动的时刻。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子模拟计算机是一个旧思路，源自于 Richard Feynman，你可以使用量子计算机模拟其他量子系统。20 年前，我写了第一个关于如何编程量子计算机的算法，从而探索量子系统如何运行。在接下来几年，我们打算弄一些设备，能让我们建立量子力学模拟，也就是在内部黑洞发生了什么。我们可以关注下这些模型能做到什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几年前，我的一些朋友和我使用小型量子计算机模拟时间旅行中发生的事情，因为时间旅行的理论内在是量子力学的。当你把一个光子传送回十亿分之一秒之前，并杀死其前身。恩，我们的实验就测试了这样做会发生什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很幸运没有防止虐待光子协会的存在，因为我们的实验杀死了大量的光子。结果证明，一个光子回到过去杀死自己的前身总是会失败，因为时间旅行的量子理论表明你不能回到过去并做一些自相矛盾的事，比如杀死自己。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算如今最有趣的应用是映射传统计算。如今传统计算中最大的进展是编程机器学习，采用计算机处理大量数据，搞清楚里面的模式。NSA 使用这些监控我们，谷歌也使用这种计算监控我们，亚马逊也是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在机器学习中，生活在大数据时代我们毫无秘密。人类每天生成阿佛加德罗量级的数据。谷歌、亚马逊、微软这样的公司正在处理这样的数据，发现我们生活中的方方面面，从而向我们出售产品。计算机越来越擅长处理数据，发现其中的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子力学系统有着这样的特征，能够生成传统系统难以生成的模式。结果表明量子计算机能检测并识别传统计算机难以检测的模式。比如，如果你有过去 50 年中道琼斯的逐笔交易数据，这就是一个大数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你说，「如果我能经受一定量的损失，或者我想要有一定的回报，我处理这些数据发现对我而言最好的投资组合。」好，用一个相当小的量子计算机（在接下来 5 年或者更长时间将会有这样的计算机），你就可以发现比在传统计算机上得到的更准确的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机通过在更微观的层次上存储和处理信息而运行。例如，如果你有一个电子，你可以让它像 0 一样旋转，你也可以让它像 1 一样自旋，你也可以让它同时 0 和 1 的存在，这是量子计算的主要特征。一个量子比特，qubit，能同时是 0 或 1；这也是为什么量子计算比传统计算强力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去的 20 年或更久的时间内，我的同事和我一直在使用电子、光的例子建立量子计算机。所以，一个电子在电场中像 0 一样的摆动，也可以像 1 一样摆动，也可以同时 0 和 1 一样的摆动。我们已经建立了这样的量子计算机和量子通信系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是一个理论家，所以实验主义者不像我在实验室中那样使用螺丝刀，我会破坏东西。但我与实验主义者紧密合作了超过 20 年，建立这样的设备，他们用小型的，有几个量子位的设备开始，但结果证明要有一把量子位才足够演示量子计算的功效。量子计算机如今变得越来越大，我们有了十几位的，不就将会有 50 个量子位的，然后是 500 个量子位。因为如今我们有建立大规模量子计算机的明显路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统计算机遵循著名的摩尔定律，但它不是自然界遵循的定律。它只是对科技进展的一种观察，每两年计算机组件小一倍，组件的数量就翻倍。量子计算机不遵循摩尔定律。原因是建立量子位，并把它们组合在一起是一个复杂的过程。你是在微观的级别上操作的，很难做到这一点。你不需要要精准的进行控制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一个与常见摩尔定律平行的摩尔定律。事实上，它才符合量子理论。随着时间前行，我们在微观层次控制事物的能力越来越好。控制事物的同样能力让我们能做越来越强力的量子计算机。我们的量子计算机与传统计算机相比如今仍是无用的。我记得我以前有一个 16k 存储的计算机，几年后就是 64 K 了，如今是 100 G 的存储或1个TB的存储。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机仍处于只有少量量子位的阶段——可使用的 10 量子位，很快有 50 量子位，然后 100 量子位。尽管与传统计算机相比不具优势，但因为对特定问题量子计算机比传统计算机更强大，这意味着在接下来 5 到 10 年，一旦我们做到 几百量子位（很快就会发生），我们将能够解决传统计算机无法解决的难题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;解决什么样的难题？比如说一个 500 量子位的计算机将能够分解大型数字的因子、破解密码、打破对 NSA 监控的恐惧。但它也能做一些类似监控的事，比如量子机器学习，发现大量数据中的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 12 月，我在 NIPS 大会上就量子机器学习组织了一个量子会议。我们预期数十人参加这个会议，但最后却有 150 人参与，以至于我无法进入会场。传统机器学习领域的人总是在观望新的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们惊人的发现机器学习难题像是一些事的拓扑学一样，想搞清楚一堆数据中空洞的数量。你知道，拓扑学研究事物是否有孔洞或者缺口或者空洞或者链接组件，这是分析数据的人想要发现的世界的特征。但做这些事的传统算法虽然有效，但只限于小数量的孔洞，因为它们不能处理这样的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比之下，如果有一台小型的量子计算机，甚至是仅有几百个量子比特，那你就能发现复杂的模式和拓扑系统，就像你用传统计算机所永远不会发现的洞、缺口和缝隙。我们已经进一步一个全新的阶段。量子计算的第一个二十年是一些从理论中衍生出的非常有趣的想法，和物理学的其他分支建立联系，接下来将出现各种你很喜欢用的算法，只要你愿意拥有一台已经足够强大到去执行这些算法的量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们处在量子计算机正在变强大的边缘，它们能够进行这些分析，对其他量子系统进行模拟（传统系统无法做到），发现传统计算机无法发现的数据中的模式。我们即将进入一个令人激动的时刻，来迎接量子计算的到来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谁将马上拥有量子计算机？答案是每个人。MIT 有 5-6 个实验室都配备了量子计算机，研究者正尝试着对它们进行扩展使其变得更大。全球有几百只团队正在开发量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些实验室中的量子计算机非常有趣，它们看起来有些不同，取决于各自的用途。超导量子计算机的比特是超导电流的，以顺时针的方向在回路中转一圈——这是零；超导电流以逆时针在回路中转一圈——这是一；一个超电流一次往两个方向转，这很难想想，但确实会发生——这同时是零和一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机内部的设备其实是芯片。通过相对传统的方法将超导电路蚀刻在芯片上。然后，芯片会连接到外部世界进来的电线上，因为超导体必须被放置在氦稀释的冰箱里，绝对零度以上的千分之十五度。它放置在那里就像是一个啤酒桶，在冷却时会滴滴答答作响。然后你使用普通计算机与它进行对话。你用自己的键盘输入，这将信号发送到芯片上，然后芯片处理这些信号，并且通过自己不可思议的量子力学机制得到答案。这些事情目前还有些庞大，仅仅是因为需要被安置在一台稀释冰箱里。你不能把它放到膝盖上，因为它会压扁你。但它们现在已经如果紧凑到你可以把它放在你的办公室里，只要你想。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一台特定功能的量子计算机，量子退火炉（quantum annealer），由 D-Wave 制造。这是一台商用设备，已经有不少人购买了。洛克希德马丁已经购买了 D‑Wave 的计算机，谷歌和 NASA，美国军方也是。他们购买的原因是这些设备很有趣。没人能够准确理解内部原理。它们非常神秘的以量子力学的方式进行计算，因为神秘就是量子力学的特点。用户正在购买这些设备应用于自己的领域，来尝试下是否能够解决哪些传统计算机无法解决的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 D-Wave 设备基于我和我的研究生 Bill Kaminsky 在 2002 年写的几篇论文。这是一种商业化的设备，如今已经有一些人买了。Lockheed Martin 买了一台 D-Wave 计算机，谷歌和 NASA 买了一些，军队也正在购买这种计算机。他们之所以购买时因为这些计算机很有趣。没人真正理解计算机内发生了什么，它们以自己的量子力学方式相当神奇的处理事情，因为保持神秘是量子力学的一个本质。人们买这些设备是想看下能否在上面解决传统设备无法解决的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们没能申请专利，D-Wave 继续前行并建立了这种设备。他们免费的使用了这些东西，为什么不能呢？又没有专利。当他们建立的时候，计算机确实不像预期的那样工作，在整个计算中保持低能态。达到更高的能量水平令人振奋，但它仍要解决该难题。为什么这样？没人知道。我与 D-Wave 的人一起工作过，想搞清楚为什么在不该成功的时候成功了。也就是从此，我为所有东西申请了专利，即使我不知道它们时候有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算的另一个有趣点依赖于量子光学和光。很多年以来，这些设备都很大，因为它们包含一堆的大型激光固定在光学台上，被百万张镜子覆盖。研究生在校准这些设备时要非常谨慎，以便于所有的光束以正确的方式存在。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个领域中有一个惊人的进展，因为基于电话通讯技术，如今人们能够将所有的东西安置在一个芯片上。你将一个足球场大小的光学台缩小到一个芯片上，所有的东西都在上面。然后用硅树脂细线蚀刻该芯片，光子沿着这些线跃升，彼此相融，彼此交互，然后从另一端出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些都是很伟大的设备，玩起来也很有趣。关于这些设备的一件事越来越凸显，过去的五六年，即使某种程度上这些设备很简单（也就是光通过一个芯片），但光子在镜面反弹并彼此互融，这些行为非常的神奇。如果你发送 20 个光子从小端口进入芯片，你问光子从其他 20 个端口出来的概率是多少，传统上这很难计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;没有人知道如何做到。然而，芯片能自动的做到。它能生成没人知道如何在传统计算机上生成的模式。它们有一些我们即使使用最大的传统超级计算机也无法生成的怪异的量子特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种设备的一种可能用途是学习。机器学习的一个共同特点是，如果你有一个可以生成一组特定模式的设备，它也能识别一组相同的模式。现在我们正在进行一项实验，来尝试一下能否让这一切发生。当我们拥有了用其中一个芯片生成的模式，那能否训练另一个芯片去识别那些模式？如果我们能做到这一点，那么我们就已经能训练一个量子设备去识别那些不可能被传统计算机生成或识别的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些模式太诡异了。因为它们不能由任何传统设备生成，顾名思义，它们就像是你以前从未见过的东西。除了制造这些我们不知其形的 funky 模式外，量子计算机也可以做那些普通的机器学习任务，比如识别数据中的大规模模式——我们现在一直使用的日常功能，比如人脸识别、语音识别和字符识别等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你正在投资股票市场，一个非常重要的问题是，是否存在一些隐藏的按照某种模式的「动力」来驱动所有股票？如果你知道那个「动力」的话就可以赚很多钱。一台量子计算机可以比传统计算机更加有效地找到这种模式。一台量子计算机可以处理大量普通任务，即使是只有几百量子比特的小型量子计算机，也能做一些传统计算机做不了的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，就像量子计算机能够发现这些传统计算机无法发现的模式一样，还有很多更加疯狂的事情。就识别功能而言，我不知道这些模式对什么有利，但是它们在涉及加密应用的问题上非常有用。比如用无人可以破解的方式去编码信息。如果你把个人信息与这些没有人可破解的模式放在一起，然后——上帝作证——没有人能够解密你的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算和数字计算机的历史进程对比&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将量子计算的当前状态和过去二十年的进展与数字计算机进行对比是非常有价值的。建造一台数字计算机的想法是 20 世纪 30 年代中期由克劳德·艾尔伍德·香农（他那篇颇有影响的哈佛硕士论文的一部分）和和德国的克兰德·楚泽提出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，当时第一台设备是在第二次世界大战期间开始建造的。直到上世纪 50 年代中期，人们才有了这些巨大且非常昂贵的设备。他们中很少有人会花很多钱去开发它。极少量的比特就要花费大量工作，而且它们常常崩溃。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立一台量子计算机的想法是我在 1993 年提出的。在那之后不久，人们开始建造简单的量子计算机。这是很难的，就跟我们开始建造传统计算机的头 20 年一样难。现在我们正处于这个阶段——拥有一个房间大小的量子计算机，以及照顾它们的穿白大衣的实验室技术人员。它们难以操作，会发生故障，而且只有几十个量子比特；但是毫无疑问，我们正在取得进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，有关计算机有这样一句「如果你建造了它们，它们就会来。」我的许多 MIT 资深同事都曾参与早期的计算机研究，比如马文·明斯基（Marvin Minsky）、鲍勃·盖勒格（Bob Gallager）。当他们告诉我过往的美好岁月时（因为他们喜欢自己的工作），他们遇到的其中一件事情是，计算机科学在 20 世纪 50 年代的起源是非常令人兴奋的。但当拥有了一台可以在上面运行算法的设备，即使它体格庞大而且在今天看来是令人难以置信的脆弱不中用，就立刻出现了巨大变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之后的短短几年内，人们一旦开发出第一台可以运行程序的计算机，这些计算机科学家先驱们——在当时来说甚至都没有被叫做计算机科学——就开发出了许多今天我们所知道的最强大的方法，比如说蒙特卡洛和单纯形法，这些算法如今被我们广泛应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的未来——这是一个激动人心的时刻&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个非常激动人心的时刻，它令人兴奋，因为这些从事理论工作的聪明人突然有了一个可以玩的玩具。他们很快想出了与这个相当昂贵的玩具相关的大量有趣游戏。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算领域现在正处于这样一个阶段。我们有这些玩具——这些复杂、不是那么强大的量子计算机，但我们可以用它来玩游戏。我们可以尝试人们遇到的问题，可以看看会发生什么。人们正在提出一些非常有趣的游戏。因此，对于像我这样的人来说，量子计算非常令人兴奋，因为这个领域里充满了拥有奇思妙想的年轻人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我所认识的一些世上最杰出的青年科学家都被吸引到了这个领域，因为它很有趣：你可以玩有趣的游戏；问题很大；你可以发问有关宇宙本质的问题。比如你可以问：我能否识别出一个潦草的 5 或 7？然后你可以与人合作并说：「嘿，我有主意了，我们可以试试吗？」你走在麻省理工学院的走廊上，而有人说：「是的，我们可以尝试一下。让我们看看会发生什么。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;仅仅从智力游戏的角度来看，现在的量子计算领域是一个非常令人愉快的地方。对我来说它很伟大，因为我可以与这些比我聪明得多的怪咖们共事，而这也是一个很大的乐趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们来思考下量子计算的未来，我不知道将来的每一台智能手机里是否都有量子计算机，或者我们是否会拥有量子 app 或者 quapps，从而借助量子计算机使我们的通讯更加安全，并帮我们找到一些有趣的东西。这是一个很难完成的任务。很可能我们的计算机和智能手机中将拥有量子微处理器，完成特定的任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中的原因很简单：无论如何这都是我们设备内部的实际技术的前进方向。如果量子力学能够带来什么好处的话，我们就会利用它们，正好同能量在光合作用中流动所采用的一种量子力学的方式相同。如果量子「把戏」中有什么好处的话，那么就是它本身。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术不是中立的。事实上技术的主要用途之一，只是让富裕而强大的公司利用技术来利用普通人，这是一种自然的经济运作方式。其他一群人失去了他们的工作。有关技术的一件非常自然的事情是，它令一些工作做起来更高效、更容易的同时，也意味着做这些工作的人最终会有更多的工作，因为你的雇主会让你做更多的事情。然后就会产生一群失业者。技术可以使事情变得更有效，但它不一定会使我们的生活变得更容易或更美好。事实上它往往使我们工作更加困难，这是我所反对的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我发现最简单的是告诉人们进展的真相，而这对他们也有好处。有一些财富五百强公司正在大力投资量子计算。IBM 、微软、谷歌、英特尔等，日本的 NEC 已经向量子计算投了一大笔钱，相当多的公司都决定去投资这一领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当他们问我，「我们是否很快就能有一台真的能生产出来并出售的量子计算机？」我说，「好吧，也许并不。」尽管我们现在离这个目标更近了。事实上随着一些新的技术进展，特别是超导量子计算和光学量子计算，我们很有可能将拥有人们能生产出来并出售的量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌、IBM、微软或英特尔这样的公司有一个很好的理由去投资量子计算。这是一项拥有极大前景的技术，即使是在当下，它也不是那个包含在日常的智能手机中的东西。这个理由与计算的一般特性有关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当人们第一次建立起这些体育馆大小的巨型计算机并将其置于体育馆中时，他们对于这些计算机的用处没有丝毫线索。他们在想，「哦，我们会用它来分析类似炮弹轨迹、材料属性这样的东西。」但我们在过去的几十年里都经历过的一件事是，计算机已经做到了那些你从来不会认为它们能做的事情。此外信息处理技术以一种无人预料的方式下爆发了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在仅仅谈论计算机是没有意义的，因为一切都是计算。你的智能手机是一个非常强大的计算机。你的汽车发动机包含 20 至 50 个微处理器，它们一直在进行计算，而这是实现更高燃料效率和污染控制的秘籍，诸如此类。它也被证明是污染控制欺诈的手段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大量设备中有着计算，也存在着信息处理。你碰触的所有东西几乎都可以以一种复杂的方式处理信息，这如今变得很常见。如果你的公司是做信息处理的，那知道接下来的进展就非常重要了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的军备竞赛&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 长期投资于量子计算。刚开始时，也就是 20 多年前，他们就开始强力投资量子计算。这是因为他们有两个该领域的建立者 Rolf Landauer 和 Charlie Bennett 帮助他们开发量子计算。那时发生惊人的事情是件很明显的事情。即使他们没有投资 10 亿美元，也每年投资千百万美元做量子计算研究。我不知道具体投资是多少。结果是，他们有世界上最好的、最聪明的人为他们工作，研究这个话题，这些人知道发生了什么，用自己建立的量子计算机搞事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对其他公司而言也是如此。这些公司是年轻工作者极棒的工作地方，也是产出新思路的主要场所。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 也与量子信息处理有及其紧密的关系。但由于 DARPA 的本质，它们的项目经理总是想要来个全垒打。而且，从一开始就很明显，量子计算是一个有全垒打潜力的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是首个政府拨款的量子计算项目的首席投资者，这是 DARPA 1994 年的一个项目。Jeff Kimble 带领这个团队。DARPA 当时意识到量子计算是他们需要关注的事情。事实上，在过去 20 年中，DARPA 已经投资了量子计算的不同方面的多个项目，其中的很多项目都是成功的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算中的很多基础进展都以各种形式受到过 DARPA 的资助。虽然不知道 DARPA 的老大怎么想，但我认为这是一件好事。而且一个特定项目最终开发出的东西经常与他们一开始想要做的不同。但结果证明，这些项目出来的一些衍生成果非常强大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 是第一个认识到量子力学在光合作用（photosynthesis）中扮演重要角色的基金资助机构。他们创造了第一个资助研究光合作用和能量转换中量子相干性和量子纠缠这样的特殊影响的项目。这是一个很成功的项目，得到了美好的成果。我正在研究项目的一些衍生品是在能量转换上要比自然中更加高效的人造和细菌造的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 染指了很多研究，在量子研究上也上下其手。它开发了量子计算的很多基础思路。IARPA 衍生于 DARPA，它也是量子信息处理前沿的主要投资者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为它成长的太快，因为它在众多领域都有所影响，如今在量子计算和量子信息处理方面有很多的子领域。有许多技术员正在建立量子计算机，其中有很多卓越的人才。在超导量子计算机领域的人才有：被谷歌聘请的 John Martinis；我在 MIT 的同事 Will Oliver；在 Delft 的团队。然后，也有很多人在极力关注新型量子算法这样的疯狂想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我在 MIT 的同事 Soctt Aaronson（Shtetl-Optimized 上的知名博主）有着非凡的想法。他和他的同事正在绘制你可以在量子计算机上解决的一系列问题，这是一项很棒的工作。要建一个量子计算机，其中最成功、最强力的设备是离子阱。你可以采用一堆离子、原子，剥去其中的电子，使用陷阱诱捕它们，然后用激光摧毁它们。我在马里兰大学的同事 Chris Monroe 是该领域的先驱。因斯布鲁克大学的 Rainer Blatt 也在这上面做出了惊人的成就。MIT 的 Ike Chuang 在建立这种设备上也做出了大量的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于量子信息我最喜欢的部分是我们所说的狂野的东西，也就是「Hey，让我们了解下宇宙如何产生的，从量子信息的角度想下它是如何组合在一起的。」考虑下量子引力，依据量子信息没人能理解量子引力这种东西，这也是我已经做了 15 或 20 年的东西，而且如今也有不少人在研究它。这相当有趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大神级人物包括 Caltech 的 John Preskill 和 MacArthur 奖获得者 Alexei Kitaev。他们两个都是很棒的人，在这个领域都取得了巨大成果。如同我所说的，量子信息领域的一大特色就是这些高素质的年轻研究员。刚被斯坦福聘请的 Patrick Hayden 在研究量子力学和量子引力的问题。Brian Swingle 在还是 MIT 的研究生时就想出了量子引力和量子信息之间其中的一个主要连接，仅靠自己他就做到了这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;中国是量子信息竞赛中的后来者，大约是 4 年前开始的，他们在清华建立了一个研究量子计算的机构。这个机构很优秀，他们也在做很伟大的事。中国做量子信息的也有一些很优秀的实验者，比如潘建伟。新加坡国立大学在量子信息处理上也有很多惊人的项目，他们也是该领域的领军队伍之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;日本量子信息领域也有很多优秀的研究员。比如我的同事 Yasunobu Nakamura 和东京科技研究所的团队。NEC 在这里也有一个很厉害的团队，有很多优秀的人。研究量子计算的最大团队集中在加拿大滑铁卢的量子计算研究所。在这里，黑莓的创始人 Mike Lazaridis 捐赠了百万美元作为用来创造优秀的研究团队的种子基金。加拿大的这个研究所的领头者是 Raymond Laflamme。这是现在世界上最大的量子计算研究的集中地。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在欧洲，有很多非常棒的量子团队。维也纳大学就有一群非常出色的人，Anton Zeilinger 和 Philip Walther 在那里待了很长一段时间；在牛津的 Vlatko Vedral 和剑桥的 Richard Jozsa 的领导下，这两个学校也有很多很棒的项目；还有牛津的 David Deutsch，他正是这一领域的创建者。你很难见到他，因为他总是在夜晚出没。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这几年，我与 David Deutsch 交谈过多次，其中有一段特别有趣的对话。当我在 MIT 准备一个会议议程的时候，David 在视频链接里出现了：我坐在大会议室的第一排，正对着 40 英尺的大屏幕，而 David 那 40 英尺高的脑袋就在屏幕上对着我讲话。会议室里没有任何其他人，我们也仅仅是讨论一些物理问题，但那感觉简直就像在与《绿野仙踪》里的巫师对话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;David 是一个非常聪明的人，也是一个思想家。他第一个意识到量子计算机可以做到一些传统计算机无法做到的事情，而这是在 80 年代中期发生的事情。他花了很多时间寻找量子计算机能够在某件事上表现得更好的例子。他有这方面的直觉，并且随后提出了量子计算机的正式概念，但是在五年多的时间里，他没有找到能够证明量子计算机能表现得更好的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当他最终有所斩获的时候，他向众人展示了，在一个传统计算机需要两到三步才能解决的问题上，量子计算机只需要一步。这确实是一个进步，但那个问题本身不是大家关心的问题。尽管如此，他并没有放弃，而是一直在寻找更好的证据。最终，凭借着自己的才华与意志，他终于让整个学术界意识到了量子计算机的重要性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后，其他人开始研究这一领域，并且产生了许多更加有用的想法。量子理论完全改变了寻找函数周期的问题，随后 Peter Shor 借此提出了著名的因子分解与密码破解算法。接着，这一领域就开始了科研竞赛。在过去的二十年，自量子计算的复兴以来，自 Shor 在 1994 年提出算法以来，量子计算已经从几个人的规模，壮大成了拥有几千人、多方向的领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在这整个时间段里，David 不改初心，仍旧在他认为最重要的事情上不断钻研。在刚过去的十年间，他在他自己称为「量子构造理论」领域进行研究。根据我的不完全理解，他试图从基于量子计算理论中，衍生出现实世界最本质的东西。祝愿他最后能够成功。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 专访MIT CSAIL实验室首位女性主管：从7大领域谱写计算的未来</title>
      <link>http://www.iwgc.cn/link/2715049</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Forbes&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Peter High&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick R&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px;"&gt;Daniela Rus 是麻省理工学院计算机科学与人工智能实验室（Computer Science and Artificial Intelligence Laboratory/CSAIL）首位女性主任，同时也是 MIT （Andrew and Erna Viterbi）工程学院的电气工程与计算机科学专业教授，并获得了 2002 年的美国麦克阿瑟学者奖 。&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以研究范围和成员资格作为衡量标准，MIT 的计算机科学与人工智能实验室（CSAIL）是最大的校内实验室。超过 250 家公司经由 CSAIL 孵化，包括 Akami、iRobot、3Com 和 Meraki。CSAIL 的研究分为 7 大重点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;计算生物学&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图像与视觉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言与学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;计算理论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器人技术&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;系统&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Daniela Rus 是麻省理工学院计算机科学与人工智能实验室（Computer Science and Artificial Intelligence Laboratory/CSAIL）主任，同时是 MIT （Andrew and Erna Viterbi）工程学院的电气工程与计算机科学专业教授，并获得了 2002 年的美国麦克阿瑟学者奖 （MacArthur Fellow）。她是 CSAIL 的第一位女性领导，这一特质被她用来帮助鼓励其他女性追随她的步伐，进入到实验室所重视的领域。从她的岗位来看，她已经能够见证并影响一些正推动着当前数字革命的技术的上升趋势，所有这些内容在我们的这次访谈中都有涉及。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Peter High：你可以介绍下你的实验室的背景吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Daniela Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;50 多年来，从第一个分时（time-sharing）系统和第一个计算机密码，到公共密钥加密（public key encryption）和自由软件（free-software）运动，CSAIL 的研究已经推动了计算的边界，并在数字革命中扮演了一个重要的角色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXQicp9a6UAKubCcTY5EfXbXOLqyeA1hMWicRVObWKMA7BSWgFnvVZiaXILPK9t2SKXUcV4BuFDSGuw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;MIT CSAIL 主任 Daniela Rus&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 拥有超过一千名成员，其中包括五百个博士生和博士后，这使它成为 MIT 最大的跨学科研究实验室。成员们涵盖了从像我一样的机器人专家，到数据安全、计算生物学、软件设计和预测分析学专业的专家。这种兴趣的不同组合让实验室可进行重要的跨学科研究，我们相信这种研究将对全球产生重大影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 启动的第一个项目是「Project MAC」，该项目认为两个人可以同时使用同一台计算机，而该机器大概有一个房间那么大。这令我感到吃惊，我们在短短的 50 年间，从多人共用一台机器的幻想走向了一个计算不可或缺的世界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:CSAIL 的目标是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的目标是谱写计算的未来。我们想用计算机科学来处理医疗和教育等领域的重大挑战，从创造更好的医疗诊断工具，到开发零事故汽车，以及鼓励儿童学习编程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 的创建者们着手实现了多用户同时计算，并将它看做是人类能够使用机器来增强自身智力的第一步。这就是 CSAIL 一直以来所追求的目标。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，计算方面最紧迫的问题总是在发生变化。计算机已经缩小到了口袋大小，如今它们存在于我们的手机、汽车、电视机——甚至洗衣机里！新的挑战轮番上演，思考着如何使计算机变得更好、更强、更能干。我们还希望利用计算去解决世界所面临的重要问题，例如在医疗保健、教育和隐私方面。我们努力把科幻小说的世界推向科学，继而推向现实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;High:你如何安排实验室的轻重缓急？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们会问这样一个问题：「我们如何开发出能够进行智能推理且交互起来更直观的计算机，让它们可以做繁重的家务，每天开车送我们去上班？我们的计算系统应该采用什么样的原则和模型，使它们能够运行得更快、更好、更安全、更容易、更有效率？我们是否能够增强自己对数学计算的认识，从而可以应用它来解决现实世界的问题，比如改善我们的数据安全性？」这些问题似乎令人生畏，但其影响深远，为解决世界所面临的问题和挑战开辟了新的途径。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有 50 多个研究小组致力于数百个不同项目，但他们都有一个共同的主题，即发现新的方法以使计算机变得更聪明、更易使用、更安全和更有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们思考计算领域的最重要的长期挑战，并组织各项资源去架构它，必要时会伸向其他学科。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，我们已经着重在自己的特色专业课题方面发起了行动计划。我们的网络安全计划汇集了密码学家、加密研究员和加密政策专家，共同致力于网络攻击的预防和恢复。我们的大数据计划寻求于利用那些对于现有工具来说处理起来过于庞大、快速或困难的海量信息。我们正致力于系统、人工智能、计算、医疗保健和自主性方面的一些新举措。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不断地重新评估事情的优先级，确保将精力集中在那些我们认为计算机科学可以发挥作用的最重要的挑战上。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:麻省理工学院有这么多的实验室。你是如何与他们合作的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我积极地与 CSAIL 和 MIT 的许多研究人员共事，这些大学教授的研究课题范围包括3D打印、海洋机器人、计算折叠和机械工程等。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管许多这些合作伙伴关系是我事先便知晓的，然而最令人兴奋的合作是来自于午餐时与坐我旁边的教授进行谈话。每天与研究人员一起工作是一种乐趣，他们的想法非凡且时而有些出神，他们天生拥有好奇心并总是寻求看问题的新方式。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也与其他实验室有着非常多的接触，围绕着研究所的最高优先级课题进行合作。比如我们正与医学工程与科学研究所（Institute for Medical Engineering &amp;amp; Science/IMES）建立一个合作关系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;High:你是如何进入计算机科学领域的？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我出生在罗马尼亚，父母是计算机科学家和物理学家，这无疑提早给了我得天独厚的科学环境。我是 Jules Verne 的头号粉丝，而且尤其喜欢《海底两万里》。我还看了《迷失太空》的重播，喜爱电脑天才 Will Robinson 和他的 B9 机器人。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我父亲在我高中时鼓励我学习编程，并教我布尔代数。从那时起，我知道自己想要在大学学习计算机科学，然后去康奈尔研究生院。当我的博导 John Hopcroft 谈到计算机科学的宏伟愿景——它们如何能够利用这些方程和算法来让物理机器做所有这些人类不能或不想做的事——时，我就决定将机器人作为我的研究方向。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：你是 CSAIL 的前辈以及第一位女性主管。鉴于技术领域的性别差距，你是否将你的晋升和奖励看作是一个其他女性可追随的模型？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;作为两个女儿的母亲，我很清楚要确保女孩获得接触科学和技术的每一个机会是多么重要。我试图为我的女儿树立一个好榜样，鼓励她们去追随激情，并让她们知道，如果你投入到工作中就能够追求生活中你想得到的任何东西。我的工作部分是由一个信念驱动，我认为每个人都应该知道如何利用计算去解决问题。多年来人们一直在谈论增加计算机的可访问性，以最终使每个孩子都有一台笔记本电脑。但我发现，机器人对儿童的吸引力甚至比计算机「更」大 ，所以我相信机器人比其他任何东西都更能帮助儿童学习几何、物理、编程和许多其他东西。技术素养同阅读写作和数学一样重要，因为它是我们周围的一切。我们想让儿童从小就对机器人感兴趣和感到兴奋，使他们成为未来的创新者。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:你认为什么方法最能将更多的女孩和女性吸引到 STEM 项目中去？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;简言之，我们需要从幼儿园开始把计算思维作为所有成绩中的一个强制性主题。我们还需要找到教授原则的方法，还需要为学生提供动手的项目，从而向他们展示计算如何能够「控制」他们。我通常喜欢把计算视为一个——毫不夸张地说——超级强权，它让你以全新的方式去探索世界和与之互动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于越来越多的工作需要以一个更复杂的方式来使用计算机，我们迫切需要填补编程基础教学方面的知识差距。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开发这些能力不只是「学习编程」，而是可以这样说：「利用编程来学习」。它不是仅仅把这些技能作为目的本身，而是在于学习这些东西如何令你以不同的方式去看待这个世界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经尽力在尝试的一个方法是通过活动来检查我们的一些项目，比如我们一年一度的「Hour of Code」，这个活动向大约 200 名当地公立学校的学生开放。能够向这些学生展示编程是多么的使人兴奋且有趣，这是一件令人高兴的事情。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：机器人是 CSAIL 所重点关注的关键领域。你和你的团队如何为各种高级机器人应用程序划分优先处理机会？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的总体目标是推动自主性科学。我们致力于开发的技术，能够使得单个机器人或机器人团队在无人监督的情况下合作完成复杂任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以看到这些领域出现的一些最令人兴奋的进展，像是软体机器人（soft robotics）、人机互动、自动驾驶、3D打印，以及致力于开发出能够更好地解决问题的机器人的领域。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如软体机器人，一般比「硬」机器人更安全且更有弹性。它们在某些任务中甚至会更有效。其灵巧的结构使它们能够更容易地改变方向或挤进狭窄的空间。如果它们击中了某个东西则不太可能将其打破，而它们的内部程序甚至会利用这些碰撞来获得周围环境的信息。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，3D 打印可以让机器人变得更便宜，并对那些没有昂贵制造技术的人而言更易于生产。我们已经展示了使用简单的家用材料，比如纸张和塑料，就能够生产出几乎能从打印机里走出来的功能型机器人。我想只需几年时间，世界就会变成这样——机器人像今天的智能手机一样普通——–那时你将能够走进一个当地的「robo-Kinko’s」去订购一个你自己的机器人，用以处理家庭或办公室的特殊任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：你和你的团队认为你们的工作在商业上的应用可以达到何种程度？你们与私企之间有合作吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的实验室自 1963 年创建以来，从 CSAIL 剥离出来的公司已超过 250 家，包括 Akamai、iRobot、3Com 和Meraki。CSAIL 拥有一些贡献美国商业图景的特殊东西——即需要深入研究并长期探索的可在现今产生影响的技术。我们非常喜欢培养这种思考创新的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;企业家也像我们的设备一样需要（人际）网络。他们需要导师来帮助他们弥合研究与创业之间的距离。他们需要具备操作经验的人。他们需要资本。我们在 2012 年 推出了一个创业计划倡议，来确保使我们的学生企业家成功的先决条件得到落实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们非常清楚我们的技术能够如何积极地影响世界。通过 CSAIL 联盟计划（CSAIL Alliance Program），我们与许多公司合作，共同探讨我们的研究能够如何直接地改善人们的生活。我们在大数据、网络安全和无线方面也有跨领域行动计划，我们会与行业赞助商合作，将我们的研究导向我们认为将对消费者最为有利的特定方向上。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在更大规模的项目中我们还有其他的特定行业合作者。例如今年秋天，我们启动了丰田- CSAIL 联合研究中心（ Toyota-CSAIL Joint Research Center），它将专注于开发自动驾驶技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体说来，我们正致力于开发高度自动化的汽车，总有一天它会既充当全权掌控的「受雇司机」，又作为可以让人类保持控制的同时仍然能够介入事故预防的「守护天使」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们很高兴能够继续与业界合作，找出将我们的学术成果用于实践的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>一周论文| Word2Vec 作者Tomas Mikolov 的三篇代表作</title>
      <link>http://www.iwgc.cn/link/2715050</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Paper Weekly&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;微信公众号：paperweekly&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;引&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Word2Vec从提出至今，已经成为了深度学习在自然语言处理中的基础部件，大大小小、形形色色的DL模型在表示词、短语、句子、段落等文本要素时都需要用word2vec来做word-level的embedding。Word2Vec的作者Tomas Mikolov是一位产出多篇高质量paper的学者，从RNNLM、Word2Vec再到最近流行的FastText都与他息息相关。一个人对同一个问题的研究可能会持续很多年，而每一年的研究成果都可能会给同行带来新的启发，本期的PaperWeekly将会分享其中三篇代表作，分别是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、Efficient Estimation of Word Representation in Vector Space, 2013&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、Distributed Representations of Sentences and Documents, 2014&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、Enriching Word Vectors with Subword Information, 2016&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Efficient Estimation of Word Representation in Vector Space&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;1&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Google Inc., Mountain View, CA&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Word Representation, Word Embedding, Neural Network, Syntactic Similarity, and Semantic Similarity&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201309&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何在一个大型数据集上快速、准确地学习出词表示？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;传统的NNLM模型包含四层，即输入层、映射层、隐含层和输出层，计算复杂度很大程度上依赖于映射层到隐含层之间的计算，而且需要指定上下文的长度。RNNLM模型被提出用来改进NNLM模型，去掉了映射层，只有输入层、隐含层和输出层，计算复杂度来源于上一层的隐含层到下一层隐含层之间的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文提出的两个模型CBOW (Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)结合了上面两个模型的特点，都是只有三层，即输入层、映射层和输出层。CBOW模型与NNLM模型类似，用上下文的词向量作为输入，映射层在所有的词间共享，输出层为一个分类器，目标是使当前词的概率最大。Skip-gram模型与CBOW的输入跟输出恰好相反，输入层为当前词向量，输出层是使得上下文的预测概率最大，如下图所示。训练采用SGD。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxclmeuYV7dHrN0EeyVPmsvLJoTsdHQ0vj9HBI1gddia91XicU3qzDuvmg/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Code:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;C++代码&lt;/a&gt;&lt;br/&gt;&lt;span&gt;Dataset:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;SemEval-2012&lt;/a&gt;&lt;span&gt;,用来评估语义相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Bengio[1]在2003年就提出了language model的思路，同样是三层（输入层，隐含层和输出层）用上下文的词向量来预测中间词，但是计算复杂度较高，对于较大的数据集运行效率低；实验中也发现将上下文的n-gram出现的频率结合进去会提高性能，这个优点体现在CBOW和Skip-gram模型的输出层中，用hierarchical softmax（with huffman trees）来计算词概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文的实验结果显示CBOW比NNLM在syntactic和semantic上的预测都要好，而Skip-gram在semantic上的性能要优于CBOW，但是其计算速度要低于CBOW。结果显示用较大的数据集和较少的epoch，可以取得较好的效果，并且在速度上有所提升。与LSI和LDA相比，word2vec利用了词的上下文，语义信息更加丰富。基于word2vec，出现了phrase2vec, sentence2vec和doc2vec，仿佛一下子进入了embedding的世界。NLP的这些思想也在用于recommendation等方面，并且与image结合，将image跟text之间进行转换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;Distributed Representations of Sentences and Documents&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;2&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Quoc V. Le, Tomas Mikolov&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Google Inc, Mountain View, CA&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;sentence representation&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;ICML 2014&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;基于word2vec的思路，如何表示sentence和document？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxBFVXDWWha857By9iaCFuL19VpdBCriaO2OhAvYRZSA514af29nT2IyEg/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用one-hot的表示方法作为网络的输入，乘以词矩阵W，然后将得到的每个向量通过平均或者拼接的方法得到整个句子的表示，最后根据任务要求做一分类，而这过程中得到的W就是词向量矩阵，基本上还是word2vec的思路。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来是段落的向量表示方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxTbbjibNTtzzz7CibV1MHML99DNWOs8VUwLiaE6KbNwvibdkia8PDcJhtD3w/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;依旧是相同的方法，只是在这里加上了一个段落矩阵，用以表示每个段落，当这些词输入第i个段落时，通过段落id就可以从这个矩阵中得到相对应的段落表示方法。需要说明的是，在相同的段落中，段落的表示是相同的。文中这样表示的动机就是段落矩阵D可以作为一个memory记住在词的context中遗失的东西，相当于增加了一个额外的信息。这样经过训练之后，我们的就得到了段落表示D，当然这个段落就可以是一段或者一篇文章。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后一种就是没有词序的段落向量表示方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxMwc2yC1w3VZe5ynQgzreTjKlYKk6xxAD46iccWZuwkaDYkpePicduXEw/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从图中就可以感觉到这个方法明显和skip-gram非常相似，这里只是把重点放在了段落的表示中，通过段落的表示，来预测相应的context 词的表示。最后我们依然可以得到段落矩阵D，这样就可以对段落进行向量化表示了。但是输入起码是句子级别的表示，而输出则是词的向量表示，因此个人比较怀疑这种方法的合理性。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;这篇文章是word2vec的方法提出一年后提出的方法，因此本文并没有使用目前非常流行的word2vec的训练方法来训练词向量，而是利用word2vec的思路，提出了一种更加简单的网络结构来训练任意长度的文本表示方法。这样一方面好训练，另一方面减少了参数，避免模型过拟合。优点就是在训练paragraph vector的时候加入了一个paragraph matrix，这样在训练过程中保留了一部分段落或者文档信息。这点在目前看来也是有一定优势的。但是目前深度学习发展迅速，可以处理非常大的计算量，同时word2vec以及其变种被应用得非常普遍，因此该文章提出的方法思路大于模型，思路我们可以借鉴，模型就不具有优势了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;Enriching Word Vectors with Subword Information&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;3&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Facebook AI Research&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Word embedding, morphological, character n-gram&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201607&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何解决word2vec方法中罕见词效果不佳的问题，以及如何提升词形态丰富语言的性能？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实验分为三个部分，分别是（1）计算两个词之间的语义相似度，与人类标注的相似度进行相关性比较；（2）与word2vec一样的词类比实验；（3）与其他考虑morphology的方法比较。结果是本文方法在语言形态丰富的语言（土耳其语，法语等）及小数据集上表现优异，与预期一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;源码公布在Facebook的fastText项目中：&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;https://github.com/facebookresearch/fastText&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;&lt;br/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;利用语言形态学来改进nlp的研究源远流长，本文提及的许多关于character-level和morphology的有趣工作值得参考。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;文章中提出的思路对于morphologically rich languages（例如土耳其语，词缀的使用极为普遍而有趣）来说十分有意义。词缀作为字母与单词之间的中层单位，本身具有一定的语义信息。通过充分利用这种中层语义来表征罕见词汇，直观上讲思路十分合理，也是应用了compositionality的思想。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用形态学改进word embedding的工作十分丰富，但中文NLP似乎很难利用这一思路。其实个人感觉中文中也有类似于词缀的单位，比如偏旁部首等等，只不过不像使用字母系统的语言那样容易处理。期待今后也有闪光的工作出现在中文环境中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;从Word2Vec到FastText，从word representation到sentence classification，Tomas Mikolov的工作影响了很多人。虽然有个别模型和实验结果曾遭受质疑，但终究瑕不掩瑜。word2vec对NLP的研究起到了极大地推动作用，其实不仅仅是在NLP领域中，在其他很多领域中都可以看到word2vec的思想和作用，也正是从word2vec开始，这个世界变得都被vector化了，person2vec，sentence2vec，paragraph2vec，anything2vec，world2vec。以上为本期Paperweekly的主要内容，感谢&lt;strong&gt;memray&lt;/strong&gt;、&lt;strong&gt;zhkun&lt;/strong&gt;、&lt;strong&gt;gcyydxf&lt;/strong&gt;、&lt;strong&gt;jell&lt;/strong&gt;四位同学的整理。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib6Sw6RA7ddKj6ODvvgQgvOqQRBn3u9vwibdJz3FwdX0kKHNZBPf2sKgUhpMBByltT0t9NRH3zianGg/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：PaperWeekly（http://weibo.com/u/2678093863 ）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;知乎专栏：PaperWeekly（https://zhuanlan.zhihu.com/paperweekly ）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 图文并茂的神经网络架构大盘点：从基本原理到衍生关系</title>
      <link>http://www.iwgc.cn/link/2707037</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自THE ASIMOV INSTITUTE&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：FJODOR VAN VEEN&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着新型神经网络架构如雨后春笋般地时不时出现，我们已经很难再跟踪全部网络了。要是一下子看到各种各样的缩写（DCIGN、BiLSTM、DCGAN……），真的会让人有点招架不住。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为此，Fjodor Van Veen 写出了一篇包含了大量架构（主要是神经网络）的盘点性文章，并绘制了直观的示意图进行说明。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIozc8EXjnQRNnwsphcY2wqXu4YvVibcg1JNE4rRe0OKibYra7aQujPCFGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这些架构绘制成节点图的一个问题：它并没有真正展示这些架构的工作方式。比如说，变自编码器（VAE）可能看起来和自编码器（AE）一样，但其训练过程却相当不同。训练好的网络的使用案例之间的差别甚至更大，因为 VAE 是生成器（generator），你可以在其中插入噪声来得到新样本；而 AE 只是简单地将它们的输入映射到其所「记得」的最接近的训练样本。所以必须强调：这篇概览中的不同节点结构并不能反映出这些架构的内在工作方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;列出一份完整的列表实际上是不可能的，因为新架构一直在不断出现。即使已经发表了，我们可能很难找到它们，而且有时候还会不自觉地忽略一些。所以尽管这份清单能为你提供人工智能世界的一些见解，但无论如何请不要认为这份清单是全面的；尤其是当你在这篇文章写出后很久才读到时（注：本文原文发表于 2016 年 9 月 14 日）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于本文中图片所描绘的架构，作者都写了一点非常非常简短的说明。如果你很熟悉其中一些架构，但不熟悉另一些，你可能会觉得这些说明会有用处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;前馈神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（FF 或 FFNN：Feed Forward neural networks）是非常简单的：它们从前向后馈送信息（从输入到输出）。神经网络常被描述为层级形式，其中的层（layer）可能是输入层、隐藏层或输出层。一个单独的层不存在什么连接（connection），而通常相邻的两个层是完全连接的（一个层的每一个神经元都连接到另一个层的每一个神经元）。其中可以说是最简单的实际网络具有两个输入单元和一个输出单元，其可用于对逻辑门进行建模。人们常常通过反向传播（back-propagation）来训练 FFNN，从而让该网络获得配对的数据集——「输入的内容」和「我们想要得到的输出」。这被称为监督学习（supervised learning），其相反的方法被称为无监督学习（unsupervised learning），其中我们只需要给出输入然后让网络自己填补空白。被反向传播的误差（error）常常是输入和输出之间差分（difference）的某种变体（如 MSE 或只是线性差分）。如果该网络有足够的隐藏神经元，那么理论上它总是能够建模出输入和输出之间的关系。实际上它们的使用存在很大的限制，但它们常被用来与其它网络结合以构建新的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;径向基函数&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBF: Radial basis function）网络是使用径向基函数作为激活函数（activation function）的 FFNN。没什么其它的了。但这不意味着它没有用处，但大部分带有其它激活函数的 FFNN 都没有自己的专用名称。这主要是因为人们在正确的时间发明了它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoZK46l9hfANzd0kft6Gpib6ibROQNjdwgJ1Tjz9huxc8Iiba3moVAzP8xw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;霍普菲尔德网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（HN: Hopfield Network）是一种每一个神经元和其它每一个神经元都有连接的网络；它是完全纠缠在一起的意大利面条，其中所有的节点都是全功能的。在训练之前，每一个节点都是输入；在训练过程中，每一个节点都是隐藏；之后它们都是输出。这种网络的训练是：将神经元的值设置成我们想要的模式，从而计算出权重（weight）。之后权重便不再变化。一旦为一种或多种模式进行了训练之后，这种网络总是会收敛成其学习过的一种模式，因为这种网络只能稳定在这些状态。请注意它并不是符合预期的状态（悲伤的是它并不是魔法黑箱）。因为该网络的总「能量（energy）」或「温度（temperature）」在训练过程中会逐渐减小，所以它总会一部分接一部分地稳定下来。每一个神经元都一个可以扩展到这个温度的激活阈值，而如果该神经元的输入总和超过了该阈值，那么输入就会使神经元从两个状态（通常是 -1 或 1，有时候是 0 或 1）之中选择一个。网络的更新可以同步完成，但更常见的是一个接一个更新神经元。如果是一个接一个地更新，就会创建一个公平随机（fair random）的序列来组织哪些单元以哪种顺序更新（公平随机是指所有（n）的选择在每 n 个项中只恰好发生一次）。这样你就能分辨网络何时达到了稳定（收敛完成）：一旦每一单元都被更新后而其中没有任何改变，那么该网络就是稳定的（即退火了的（annealed））。这些网络常被称为联想记忆（associative memory），因为其会收敛到与输入最相似的状态；人类看到半张桌子就能想象出另一半，类似地，如果给这种网络提供半张桌子和一半噪声，那么该网络就能收敛出一张桌子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoQUibSkBabC7ggyvFTvqiaTlD9xjKEiaZ3uBRu4HdGKwvTyIZzKvibJzk5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;马尔可夫链&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（MC：Markov Chain）或离散时间马尔可夫链（DTMC: discrete time Markov Chain）是 BM 和 HN 的某种前辈。可以这样理解：从我目前所处的节点开始，到达我周围任何节点的概率是多少？它们是无记忆的（即马尔可夫特性（Markov Property）），这意味着你所得到的每一个状态都完全依赖于其之前的一个状态。尽管算不上是神经网络，但它们确实类似于神经网络，并提供了 BM 和 HN 的理论基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo9l0nEQvRtRtjx5d0KYZNwt6kjLmXb9U9OaxybNbjo94km4yCSMftJA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BM：Boltzmann machines）和 HN 非常相似，除了：一些神经元被标记为输入神经元，而其它的仍然是「隐藏的」。这些输入神经网络会在整个网络更新结束时变成输出神经元。其开始时是随机权重，然后通过反向传播学习，最近也有人使用对比发散（contrastive divergence）的方法（使用一个马尔可夫链来确定两个信息增益之间的梯度）。和 HN 相比，BM 的神经元有时也有二元激活模式（binary activation patterns），但其它时间它们是随机的：一个单元处在一个特定状态的可能性。BM 的训练和运行过程非常类似于 HN：首先为输入神经元设置特定的钳位值（clamped values），然后该网络就自由了（不需要外力了）。自由了之后这些单元能得到任何值，然后我们在输入和隐藏神经元之间反复来回。它最后会在合适的温度下达到平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoMIHO0icG0JicFyg70RicjFNnZS9N1k1iaeQNgcsFMOs19TYVOxHThNLicCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;受限玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBM：Restricted Boltzmann machines）类似于 BM（这毫不奇怪），所以也类似于 HN。BM 和 RBM 之间的最大不同之处是 RBM 是更受限的，所以也可被更好地使用。它们并不将每一个神经元和其它每一个神经元连接起来，而是只将每组不同的神经元和其它每一组连接起来，所以输入神经元不会直接连接到其它输入神经元，隐藏神经元之间也没有连接。RBM 可以以类似 FFNN 的方式训练，但也有一点不同：不是前向通过数据然后反向传播误差，而是前向通过数据之后再将这些数据反向传回（回到第一层）。在那之后再使用前向和反向传播进行训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo5nHUmWbXBrJfaEvBtEBI42ISBBXZxfXfIdvNribfqcjzsN6vyEc4sRA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（AE：Autoencoders）有一点类似于 FFNN，因为 AE 更像是 FFNN 的一种不用的用例，而非一种根本上不同的架构。自编码器背后的基本思想是自动编码信息，也因此得名。其整个网络有一种沙漏般的形状——其隐藏层比输入层和输出层都小。AE 也是围绕中间层对称的（根据层的数量是奇数或偶数，中间层有 1 层或 2 层）。最小层总是位于中间，这里的信息得到了最大的压缩（该网络的阻塞点（ chokepoint））。中间以上的所有部分被称为编码（encoding）部分，中间以下的所有部分则被称解码（decoding）部分，中间部分则被称为代码（code）。人们可以通过馈送输入以及将误差设置成输入和输出之间的差异的方式，使用反向传播来训练它们。当涉及到权重时， AE 还可以对称式的构建，所以编码权重和解码权重一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo03R1IdiblEmC8DKJ1Npa1AZY5oqniaqlEAzrOZheGia6BHueF8S04kUsA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稀疏自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SAE: Sparse autoencoders）在某种程度上是 AE 的反面。它不是在更少的「空间（space）」或节点中教一个网络表征一些数据，而我们试图在更多空间中编码信息。所以不再是网络在中间收敛然后扩展回输入大小，我们直接消除了中间内容。这些类型的网络可被用于从数据集中提取许多小特征。如果我们以类似于 AE 的方式训练一个 SAE，在几乎所有情况下你都只会得到一个相当无用的恒等网络（输入即是输出，没有任何变换或分解）。为了防止这种情况，我们不反馈输入，而是反馈输入加稀疏驱动器（sparsity driver）。这个稀疏驱动器可以以阈过滤器（threshold filter）的形式，其中只有一个特定的误差会被传播回去和训练，在这次通过过程中其它的误差都将是「无关的」，会被设置为 0。在某种程度上这类似于脉冲神经网络（spiking neural networks），其中并不是所有的神经元在所有时间都在放电（以及为生物合理性给出分数）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoVLich5P7QqGIn8pmWXaQXZwia1SH6JwgUzosoTrl909ARThb8wqZiboKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;变自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（VAE：Variational autoencoders&amp;nbsp;）的架构和 AE 一样，但被「教授」了不同的东西：输入样本的一个近似概率分布。这有点回到本源的感觉，因为它们和 BM 及 RBM 的联系更紧密一点。但它们确实依赖于贝叶斯数学来处理概率推理和独立（probabilistic inference and independence），以及依靠重新参数化（re-parametrisation）来实现这种不同的表征。这种推理和独立部件理解起来很直观，但它们或多或少依赖于复杂的数学。其基础可以归结为：将影响考虑在内。如果某种事物在一个位置发生，而其它地方则发生其它事物，那么它们不一定是相关的。如果它们不相关，那么误差传播应该考虑一下这一点。这是一种有用的方法，因为神经网络是大型的图（graph，从某种角度来看），所以在深入到更深的层时如果排除掉一些节点对其它节点的影响，就会带来帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoLhK2lTuynsA7rPTwRbetepQLqf42v3vl5JoGJ5qGzAUtjGywuOxS9A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;去噪自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DAE: denoising autoencoders）是一种输入中不仅包含数据，也包含噪声（比如使图像更有颗粒感）的自动编码器。但我们以同样的方式计算误差，所以该网络的输出是与不带噪声的原始输入进行比较。这能让网络不会学习细节，而是学习更广泛的特征，因为学习更小的特征往往会被证明是「错误的」，因为更小的特征会不断随噪声变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIom8aNXHMas0LlU5llYQG6KNia03jw5ldRicUvOECuHEsU9oIBXSvciaVyA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度信念网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DBN: deep belief networks ）基本上是 RBM 或 VAE 堆叠起来的架构。事实已经证明这些网络可以堆叠起来高效地训练，其中的每一个 AE 或 REM 只必须编码编码之前的网络即可。这种技术也被称为贪婪训练（greedy training），其中贪婪是指得到局部最优的解决方案，从而得到一个合理的但可能并非最优的答案。DBN 可通过对比发散（contrastive divergence）或反向传播进行训练，以及学习将数据表征为概率模型，就像普通的 RBM 或 VAE 一样。一旦通过无监督学习训练或收敛成了一个（更）稳定的状态，该模型就可被用于生成新数据。如果采用对比发散进行训练，它甚至可以对已有的数据进行分类，因为其神经元已经学会了寻找不同的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIos3J9KTzcMg67bBVZ2estY6AEibksUHvXxwIXrUEnm1KDI9w8rx6brxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（CNN：convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）和其它大多数网络非常不同。它们主要被用于图像处理，但也可应用于音频等其它类型的输入。CNN 的一种典型的用例是让网络对输入的图像进行分类，比如，当输入的图像上有猫时输出「cat」、有狗时输出「dog」。CNN 往往开始带有一个输入「扫描器（scanner）」，其目的是不一次性解析所有的训练数据。比如要输入一张 200×200 像素的图像，你并不需要一个带有 40000 个节点的层。事实上，你只需要创建一个比如说 20×20 的扫描输入层，这样你就可以从该图像的一个 20×20 像素的部分开始输入（通常是从左上角开始）；一旦这个输入完成后（可能是用于训练），你再输入下一个 20×20 像素：将该扫描器向右移 1 个像素。注意人们不会一次性移动 20 个像素（扫描器的宽度），也不是将图像分解成 20×20 的块；相反，而是让扫描器在图像上「爬行」。然后这些输入数据被送入卷积层（convolutional layers），这和普通的层不一样，其中所有的节点并非连接到所有的节点。每一个节点仅将它自己与其近邻的单元连接起来（到底多近取决于具体的实现，但通常不止一点点）。这些卷积层往往会随着网络越来越深而缩小，大部分是按照输入可以轻松整除的因子（所以 20 后面的层可能是 10 ，然后是 5）。这方面常使用 2 的幂，因为它们可以通过 32, 16, 8, 4, 2, 1 这样的定义完全整除。除了这些卷积层，它们常常还有池化层（pooling layer）。池化是一种滤除细节的方法：一种常见的池化技术是最大池化（max pooling）——其中我们取比如 2×2 的像素，然后根据最大量的红色传递这些像素。为了将 CNN 应用到音频上，基本上是输入音频波然后缓慢移动音频片段，一段接一段。CNN 的真实世界实现往往会在末端连接一个 FFNN 以便进一步处理数据，这可以实现高度非线性的抽象。这样的网络被称为 DCNN，但这两者的名字和缩写往往可以混用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoW79bJlXfY5cQPbQLhxBTGBFb64ibLlP9lRQiaAzRtJWJ7N3o2hEv9mYw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;解卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DNN：Deconvolutional neural networks）也被称为逆图形网络（IGN： inverse graphics networks），是反向的卷积神经网络。比如给网络输入一个词「cat」，然后训练它生成一张类似猫的图像（通过将其与真实的猫图片进行比较）。和普通的 CNN 一样，DNN 也能和 FFNN 结合使用，但我们就不给这种网络缩写了。我们也许可以将其称之为深度解卷积神经网络，但你也可以认为当你在 DNN 的前端和后端都接上 FFNN 时，你得到的架构应该有一个新名字。请注意在大多数应用中，人们实际上并不会为该网络送入类似文本的输入，而更多的是一个二元的分类输入向量。比如设 &amp;lt;0, 1&amp;gt; 是猫，&amp;lt;1, 0&amp;gt; 是狗，&amp;lt;1, 1&amp;gt; 是猫和狗。CNN 中常见的池化层往往会被相似的逆向运算替代，主要使用偏差假设（biased assumptions）做插值和外推（interpolation and extrapolation ）（如果一个池化层使用的是最大池化，你可以通过其逆向过程产生特定度更低的新数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo2TF31peYW3w5W7IS2dn7gqJm6hXzJzsSIoAyaxWZNxbfeyDc4Og4bA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度卷积逆向图网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DCIGN：Deep convolutional inverse graphics networks）的名字比较有误导性，因为它们实际是 VAE，但有 CNN 和 DNN 分别作为编码器和解码器。这些网络试图在编码中将特征建模为概率，以便于它能在曾经分别看到猫和狗的情况下，学习产生同时带有猫和狗的图片。类似的，你能给它输入一张带有猫和狗的图片，要求网络去掉图片中的狗，即使之前你未曾做过这样的操作。已有演示表明这些网络也能学习模型图片上的复杂变化，比如改变光源或者 3D 目标的旋转。这些网络往往通过反向传播训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIokRw3rtLTUmiaCI54j51ztiawEOZic5lryoHqwJPwWLRUia9OYMiassnWfHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;生成式对抗网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GAN：Generative adversarial networks）源于不同的网络类型，它们是双胞胎：两个网络一起工作。GAN 包含任意两种网络（尽管通常是 FF 和 CNN），一个网络的任务是生成内容，另一个是用于评判内容。判别网络要么获取训练数据，要么获取来自生成网络的内容。判别网络能够多好地准确预测数据源的程度然后被用来作为生成网络的误差。这创造了一种竞争方式，判别器区别真实数据与生成数据上做得越来越好，而生成器也变得对判别器而言越来越难以预测。这效果很好的部分原因是即使相当复杂的类噪音模式最终也是可预测的，但生成的类似于输入数据的内容更难以学习进行区别。GAN 训练起来相当难，因为不仅要训练两个网络（每个解决各自的问题），两个网络的动态也要平衡好。如果预测或生成相比于对方更好，GAN 收敛不好，因为存在有内在的分歧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoNWSgWHRibWrvmAeYziaxoicLfxG2dOxHOmTNuKo5eB4RmQBD9vtvslIhg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RNN：Recurrent neural networks）是带有时间联结的 FFNN：它们不是无状态的，它们随时间变化在通路与连接之间有联系。神经元不只从前面层中被输入信息，也从来自它们自己的之前的通过中获得信息。这意味着你输入信息和训练网络的顺序很重要：输入「牛奶」然后是「甜饼」与输入「甜饼」然后是「牛奶」相比可能会产生不同的结果。RNN 的一个重大问题是梯度消失（或爆炸）问题，取决于使用的激活函数，信息随时间渐渐损失，就像很深的 FFNN 随深度变化消失信息一样。直观上这看起来不是大问题，因为这些只是权重，不是神经元状态，但随时间变化的权重正是来自过去信息的存储。如果权重达到 0 或 1,000,000 的值，先前的状态就不在具有信息性。RNN 理论上可被用于多个领域，因为大部分的数据形式没有时间线上的变化（也就是不像声音和视频），所以时间决定的权重被用于序列之前的东西，不是多少秒之前发生的内容。大体上，循环网络是发展或完善信息的较好选择，比如 autocompletion（自动完成）任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoibDcrxP15h4Y5ib1cDW8ofB3tJegjF0fmojcLtjxRKN5DJBzsxia6mrww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSTM：Long / short term memory）试图通过引入门（gate）和明显定义的记忆单元对抗梯度消失（爆炸）问题。这个思路受到电路图的启发，而不是生物学上的概念，每个神经元有一个记忆单元和 3 个门：输入、输出、遗忘（ input, output, forget）。这些门的功能是通过禁止或允许其流通确保信息。输入门决定来自上层的信息有多少被该单元存储。输出层在另一端做同样的事，并决定下一层多么了解该细胞的状态。遗忘门看起来像是一个奇怪的东西，但有时被遗忘反而更好。已有实验表明 LSTM 能够学习复杂的序列，比如像莎士比亚一样写作，或者创造交响乐。注意每个门在之前神经元中都有一个权重，所以运行起来需要更多的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo6ush5bNXBSm2XVw0l9dHrJuqjGUPibQQmYZ9VB9NG7FaewoxKk9ma2Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;门循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GRU：Gated Recurrent Units）是 LSTM 的一种轻量级变体。它们有一个门，连线方式也稍微不同：没有输入、输出、遗忘门，它们有一个更新门（update gate）。该更新门既决定来自上个状态的信息保留多少，也决定允许进入多少来自上个层的信息。重置的门函数很像 LSTM 中遗忘门函数，但位置稍有不同。GRU 的门函数总是发出全部状态，它们没有一个输出门。在大多案例中，它们的职能与 LSTM 很相似。最大的不同就是 GRU 更快、更容易运行（但表达力也更弱）。在实践中，可能彼此之间要做出平衡，当你需要具有更大表达力的大型网络时，你可能要考虑性能收益。在一些案例中，额外的表达力可能就不再需要，GRU 就要比 LSTM 好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoKTZV5cN1E3SIbD7fshEFOyvGDQBy2ZwSHmCs2mbZMicyc4ahYvy7nmg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经图灵机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（NTM：Neural Turing machines）可被理解为 LSTM 的抽象化，并试图将神经网络去黑箱化（ un-black-box，让我们洞见里面到底发生了什么。）NTM 中并非直接编码记忆单元到神经元中，里面的记忆是分离的。这种网络试图想将常规数字存储的功效与永久性和神经网络的效率与表达力结合起来。这种网络的思路是有一个可内容寻址的记忆库，神经网络可以直接从中读取并编写。NTM 中的「Turing」来自于图灵完备（Turing complete）：基于它所读取的内容读取、编写和改变状态的能力，意味着它能表达一个通用图灵机可表达的一切事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;双向循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiRNN：Bidirectional recurrent neural networks）&lt;/span&gt;&lt;strong&gt;&lt;span&gt;、双向长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiLSTM：bidirectional long / short term memory networks ）&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;strong&gt;&lt;span&gt;双向门控循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiGRU：bidirectional gated recurrent units）在词表中并未展现，因为它们看起来和各自单向的结构一样。不同的是这些网络不仅连接过去，也连接未来。举个例子，通过一个接一个的输入 fish 这个词训练单向 LSTM 预测 fish，在这里面循环连接随时间记住最后的值。而一个 BiLSTM 在后向通路（backward pass）的序列中就被输入下一个词，给它通向未来的信息。这训练该网络填补空白而非预报信息，也就是在图像中它并非扩展图像的边界，而是可以填补一张图片中的缺失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoq4L9tcXDddYXBicia4pRxUApwVmqskGyAB3UGbWmA0ERFyl1nxIXDO2A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度残差网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DRN：Deep residual networks）是非常深度的 FFNN 网络，有着额外的连接将输入从一层传到后面几层（通常是 2 到 5 层）。DRN 并非是要发现将一些输入（比如一个 5 层网络）映射到输出的解决方案，而是学习将一些输入映射到一些输出 + 输入上。大体上，它在解决方案中增加了一个恒等函数，携带旧的输入作为后面层的新输入。有结果显示，在超过 150 层后，这些网络非常擅长学习模式，这要比常规的 2 到 5 层多得多。然而，有结果证明这些网络本质上只是没有基于具体时间建造的 RNN ，它们总是与没有 gate 的 LSTM 相对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIodzeRf3sxfhibMgeD08QDWnqPPb6vtjS7QTqJfibACPqR9BLEibDQm8AQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回声状态网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（ESN：Echo state networks）是另一种不同类型的网络。它不同于其他网络的原因在于它在不同神经元之间有随机连接（即，不是在层之间整齐连接。），而且它们训练方式也不同。在这种网络中，我们先给予输入，向前推送并对神经元更新一段时间，然后随时间观察输出，而不是像其他网络那样输入信息然后反向传播误差。ESN 的输入和输出层有一些轻微的卷积，因为输入层被用于准备网络，输出层作为随时间展开的激活模式的观测器。在训练过程中，只有观测器和隐藏单元之间连接会被改变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoTqE2dneEzvlWHyuJuhuenQicibmeYgOroneF6sBzbZ3jmzpRjeOTACbw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;液态机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSM：Liquid state machines）看起来与 ESN 非常类似。不同的是，LSM 是脉冲神经网络（spiking neural networks）这一类型的：用阈值函数取代 sigmoid 激活函数，每个神经元也是一个累加记忆细胞。所以当更新神经元的时候，里面的值并不是被设为临近值的总和，也不是增加到它自身上。一旦达到阈值，它将能量释放到其他神经元。这就创造出了一种类似 spiking 的模式——在突然达到阈值的之前什么也不会发生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoJCe2uWVmDXbmxW82tc7XOFgibvcwicugcdvibXiavCL0vVkcfAzTPTd8eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;支持向量机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SVM：Support Vctor Machines）能发现分类问题的最佳解决方案。传统上只能够分类线性可分的数据，比如说发现哪个图像是加菲猫，哪张图片是史努比，不可能有其他输出。在训练过程中，SVM 可被视为在一张图上（2D）标绘所有数据（加菲猫和史努比），并搞清楚如何在这些数据点间画条线。这条线将分割数据，以使得加菲猫在一边，史努比在一边。调整这条线到最佳的方式是边缘位于数据点之间，这条线最大化到两端。分类新数据可通过在这张图上标绘一个点来完成，然后就简单看到这个点位于线的哪边。使用核（kernel）方法，它们可被教授进行 n 维数据的分类。这要在 3D 图上标绘数据点，从而让其可分类史努比、加菲猫、Simon’s cat，甚至分类更多的卡通形象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoxKoWSebXKI5vA2XoDr3IfjagXWxBvHx0xwrCUwia96QEQKicZic2VeibHA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们介绍&lt;strong&gt; Kohonen 网络&lt;/strong&gt;（KN，也称自组织（特征）映射（SOM/SOFM：self organising (feature) map））。KN 利用竞争学习在无监督情况下分类数据。向网络输入信息，然后网络评估那个神经元最匹配该输入信息。然后调整这些神经元以更好地匹配输入，在这个过程中拖带（drag along）着临近神经元。临近神经元能移动多少取决于它们与最好的匹配单元之间的距离。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>业界 | Claudia Perlich Quora 问答集：机器学习能力将成为数据科学家的基本要求</title>
      <link>http://www.iwgc.cn/link/2707038</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Quora&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、孙瑞、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Claudia Perlich 是纽约大学的客座教授，也是 Dstillery 公司的首席科学家，主要的工作是为潜在的品牌客户设计、开发、分析和优化驱动数字广告的机器学习。Claudia Perlich 在业界和学术界都有非常耀眼的成绩。最近获得了 Advertising Research Foundation（ARF）的 Grand Innovation Award，并被选为《纽约商业周刊》年度 40 位 40 岁以下人物名单。她还曾担任过 SIGKDD 2014 大会主席。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1.数据科学家当下面临的最低效的问题是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，我想声明的是我认为这并不是问题：事实上数据科学家 80% 的时间都花在准备数据上。这才是他们的工作！如果你对准备数据不感兴趣，那就不是一个好的数据科学家。任何分析的有效性几乎完全仰仗数据的准备程度，而与你最后选择的算法几乎无关。抱怨数据准备工作就像一个农民抱怨做任何与收成相关的事情，然后把灌溉、施肥、撒种的事交给别人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也可以说，数据准备工作难在原始数据收集上。设计一个收集有用且易于被数据科学消化的数据系统需要高超的技艺。对数据科学家来说，让系统内数据流的过程完全透明化也是需要高技巧的。这个过程需要做抽样、标注数据、匹配等工作。还不包括替换缺失值和过度规划化的工作。为数据科学创造一个有效的数据环境，需包括数据科学而且不能完全被工程工作所占有。数据科学并不总是能规范这种系统对细节的要求来完成一个干净利落的切换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是从更大范围来看，还有更重要的事情需要考虑。我认为到目前为止最大的问题是数据科学解决不相关的问题，浪费了大量时间和精力。原因通常是任何有这个问题的人在表达该问题时都缺乏对数据科学的理解，而且数据科学家最终解决任何他们认为可能是问题的问题，然后找到的方法不一定有帮助（广告常常都非常复杂）。一个典型的类别就是「定义不透彻（underdefined）」的任务：「在这个数据集中找出可操作的 insights！」。不过，大部分数据科学家并不知道要做哪个操作。他们也分不出哪些 insights 是琐碎的，哪些是有趣的？对于哪些跟风行事的人来说，也真没有什么好建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;个人来讲，我觉得有必要提的问题是缺乏数据理解和数据直觉（事实上缺乏这个三个因素常常会让数据科学家很快就下出结论），而且怀疑是最影响效率的限制因素。这些因素影响效率的主要原因是找到正确的答案需要花费很长时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2.你最喜欢的机器学习算法是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;易于上手的逻辑回归（logistic regression，带有很多花里胡哨东西，比如机梯度下降、feature hashing 和 penalties）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我知道在深度学习风靡的时代，这答案有点奇怪。所以我先来讲下缘由：在1995-1998 年这段时间里 ，我用的是神经网络，到了 1998-2002 年这段时间我用的最多的是基于树的方法。从 2002 年开始，逻辑回归（一般的线性模型、包括分位数回归、泊松回归等等）逐渐深得我心。2003 年，我发表一篇机器学习的论文，展示了在 35 个数据集（那时候算是很大了）中三种逻辑回归分析方法与逻辑回归的比较结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长话短说，如果信噪比很高，树往往会赢。但是，如果信噪非常大，那么带有一个 AUC &amp;lt; 0.8 - logistic 的模型就是最好的选择，它总能击败基于树的方法。最终不太令人惊讶：如果信号太弱，高方差模型就完全没用了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以这到底意味着什么？在这种问题上，我倾向于处理低可预测水平的超级噪音。这个问题可以从非常确定性（国际象棋）的角度来考，也可以从非常随机（股票市场）的角度来考虑。（在有数据的情况下）一些问题仅仅是比其他问题更好预测一些。不是一个算法问题而是关于这个世界的概念陈述问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从这个角度来看，我感兴趣的大多数问题非常接近于股票市场。深度学习在另一个方向上效果很好——「这张图中有猫吗？」在一个不确定的世界中，偏置方差权衡（bias variance tradeoff）往往在有更多偏置的情况下是有利的——意味着你会想得到「简单的」非常受限的模型。而这就是逻辑回归的用武之地。我个人发现通过添加复杂的特征而不是尝试限制非常强大（高方差）的模型类别来增强一个简单的线性模型是更容易的。事实上，每个我赢过的数据挖掘比赛（KDD CUP 07–09）我都是用了线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性模型除了性能上的优势，还很稳健，而且往往只需要远远更少的人工处理（好吧，随机梯度下降和 penalties 会让其变得困难一点。当你想要在你不能花费 3 个月长的时间来构建完美的模型的行业进行预测建模时，这是极其重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，线性模型中的发生的情况更有可能得到是可以理解的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3.完全掌握 TensorFlow 需要什么样的数学背景？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这完全取决于你如何定义「掌握」。你是仅仅想用它调节一辆 Formula One 汽车的引擎，还是想要摘得国际汽车大奖赛的奖杯？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，对于灵活使用数学与算法来解决某个问题这一方面，确实是有「掌握」一说的（如果你愿意，你还可以进行算法研究）。这是 Formula One 的技术人员常做的事情。对此，我指的是专业程度极高的数学。我个人不太喜欢做这一块，虽然我也曾学习过大量的高等数学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在应用方面 —— 当你做一个司机的时候，也有「掌握」这一概念。当你可能会使用 TensorFlow 的时候，你有好的灵感吗？你知道如何设计最好的数据表征才能使算法使用更加简单吗？如果 TensorFlow 的某个指标下表现良好，这是否代表它在其它指标下仍效果显著？以上这些例子都是考验你是否掌握 TensorFlow 的场景，而它们对使用者的经验、数据、以及直觉的要求，将远高于对单纯的数学的掌握。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么问题来了：你是想成为 Michael Schumacher（德国著名赛车手），还是想为他工作呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4.机器学习如何影响数字广告？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，在数字广告领域，机器学习无处不在 —— 曾经在 KDD（ACM SIGKDD 国际会议）有很多 ADKDD 研讨会（专注于在线广告）。当前，广告产业的数据日益丰富（虽然我不知是好还是坏），程序化的实时广告的兴起也为机器学习在这一领域的发展提供了充分的机会。作为回顾，你可以抽出一个小时，看一下我近期在 Institute of Advanced Study（高等研究院）的一个演讲：https://www.ias.edu/ideas/2015/perlich-data-video&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多数情况下，机器学习被应用于以下不同的广告产业组成部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;测量与分配（市场混合模型、观测数据的因果模型、用户倾向匹配等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;跨设备关联（基于用户模式、IP 重复等，预测两个设备属于同一个用户的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;意图预测（某个消费者在下个月购买某个新车型的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;广告印象层面的反应预测（点击或完成浏览的概率）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;欺诈检测（分辨机器与人、欺诈链接与点击等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;用户洞察（观测一个在预测用户意图上表现良好的模型 —— 是否能够从中提取一些行为模式，并使用到创新设计中？）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一些广告领域中的机器学习应用是备受争议的。Sweeney 教授的成果显示，机器学习算法会反应潜在的种族歧视，我近期的一些工作也发现，一些预测模型会「倒向信号所在的地方」—— 而在弱指标下这是十分危险的。假设现在我们要预测一个广告的点击量，结果发现预测你非常有可能点击手电筒应用十分简单。这不是因为你对手电筒感兴趣 —— 而是因为你在黑暗中行动笨拙。尽管这一模型在想高点击率优化上表现优异 —— 它只会将广告投放给那些几乎对该产品完全不感兴趣的人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5.随着产业内的机器学习越来越流行，数据科学将如何进化？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要回答这个问题，需要我们先思考一下数据科学与机器学习的关系。对我自己来说，数据科学本身包含了机器学习。从定义上看，机器学习指一个机器从数据中归纳知识的能力 —— 你可以把它称作学习或者推断。没有数据，机器就几乎无法学习。所以，如果有什么区别的话，机器学习在许多产业中的应用扩张，将会成为数据科学重现光彩的催化剂。机器学习的优劣与否，取决于其使用的数据，以及消化算法的能力。我的期望是，不断向基础的机器学习靠拢，将成为数据科学家的基本要求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，对我来说，最重要的数据科学技术之一是评估机器学习的能力。我认为，我们所从事的数据科学并不缺少酷炫的项目和迷人的算法；而我们仍旧有待了解的是事物背后的运作原理以及如何解决不标准的问题。对于机器学习的（学术）观点，我的主要忧虑之一是，目前人们持续地关注在样本表现中的简单结果。基本上 99% 的研究论文都是基于精确度而被采纳的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我从过去 12 年的工作中意识到：在多数应用领域中，学术评估几乎是没用的。在一些随机测试集中表现优秀的模型会变得一无是处。这个话题值得长篇讨论，但简要来说我对以下几点存疑：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人们经常使用的指标（将分类器的精确度定义为判断正确的百分比是罪魁祸首）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;一个模型在最开始预测的往往都是错的这一事实（多数是因为根本没有「对」的东西的数据）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt; 抛开使用背景评估模型 —— 你应当先基于预测采取行动，然后再评估结果是否有改进，而不是直接进行评估&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;样本数过大导致的种种问题 —— 人们基于现有的数据建立模型，而不是他们应有的数据，更加危险的是，人们往往还基于无代表性的样本评估模型。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在带有混合生成分布的对抗情形（adversarial situation）中，模型最终只能识别出「错误的」正类的困境（可以参看我在上文对广告点击量作为指标的评价）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;泄漏 —— 表明某一模型纯粹是某数据集合的衍生物，且该模型的真正表现是非常糟糕的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6.准备数据科学面试时，哪些资源是最好的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个答案的首先最明显的部分是明白你自己准备做什么。一般意义上都是这样，而不只是在数据科学领域。现实情况是今天有很多东西都被叫做数据科学，许多人也自命为数据科学家。所以首先你要知道你到底面试的是什么，那通常并不是很明显。你也许需要在面试之前问上几个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我已经看到数据科学的职位已经分化成了（多少有些夸张）多个类别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找某种分析师，有某种擅长的技能实际上就足够了（你可能甚至不想要这样的工作）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找数据工程师，他们基本上最感兴趣的是你是否跟上了 Scala 编程的最新进展和知道你自己的版本控制方法。网上可能能找到一些标准的准备研讨会，我不知道。（这也不是我想要的工作。）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找能够基于不同的数据大小设计有效的统计分析的人，其中会涉及到一些机器学习、预测建模、聚类，而不会过于在意你是用什么方法完成的（这就开始变得有意思了……）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找深度学习专家……&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们需要一匹独角兽——具备项目管理能力，能将一个业务问题翻译成一个「可解决的」数据驱动的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦你搞清楚了你要做什么，你可能就要重新想想要不要应聘……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我通常在寻找第 3 类的申请者（有时候我也接受第 5 类）。不存在什么捷径「资源」能说服我你有这项工作所需要的能力。不幸的是，这确实涉及到个人的性格和经验。在 Kaggle 竞赛上花一些时间是获取经验的一个好方法。它们有许多数据集可以使用，你也可以从其他参与者那里学习经验。你所接触的数据集越不同，就越好！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，我还需要预测建模的专业知识，并且期望你熟悉来自《Elements of statistical learning》一书的概念。我需要你的能力足够为任何你所需要的事物编程出原型。你需要的技能包括数据拉取（API，SQ）、一些脚本编写的技能（Shell/Perl/Python）、一个建模的优质环境（Python 库/R/独立的实现），一些如何可视化/传达你的发现的想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些发表资源能让你保持敏锐：KDNuggets KDD 大会（申请论文的好集合）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后是一点不太相关的提醒：尽管创业公司很有意思，但你不会想让你的第一份数据科学工作是一家创业公司的数据科学家……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7.你是如何学习机器学习的？学习机器学习时，你最喜欢那本书？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我在学校一开始就在数学和科学学科上表现不错，并不是因为我真正喜欢抽象的事物，而只是我喜欢理解事物和解决难题罢了。所以以数学作为职业就出局了，但对于我想要做什么，我并没有什么强烈的感觉。与此同时我的爸爸则推荐计算机科学作为一个延迟决定的解决方案，然后做点什么我可能会非常擅长的事！他在 92 年时认为计算机很快就将用到各种各样的地方，而且我可以在后面想想我究竟想做什么。（我会对今天的数据科学领域的申请说同样的话。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1995 年，我在科罗拉多大学波尔得分校做了一年交换生，没有任何统计学背景的我迷迷糊糊地参加了人工神经网络的课程，完全不知道我在做什么。从此我和数据结下了不解之缘，剩下的就顺理成章了。在那里，我用德语写了另一篇硕士论文，将汽车的物理模型和该物理模型和被观察到汽车运动之间的残差分析的人工神经网络模型结合到了一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我感觉我仍然很多东西要学（而且也不急着找工作），于是我开始寻找 PhD 项目，通过偶然的机会和朋友的关系，我在 1998 年加入了纽约大学斯特恩商学院信息系统的项目。在我看来，商学院的机器学习成果的优势在于专注解决实际的商业问题，而不是理论上的算法贡献。此外，也更加关注交流和说话的能力，因为我们都是作为下一代教 MBA 学生的教授而培养的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;幸运的是，在 2003 年，学术界还没有完全准备好将机器学习作为商学院的一项研究主题，所以我没有追求一项学术事业，而是加入了 IBM Watson 实验室的预测建模组。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是我的一些「神圣的」引路人列表，按我遇到他们的顺序排列：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Duda, Hard, Stork「Pattern Classification」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bill Greene「Econometrics」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hastie, Tibshirani, Friedman「The Elements of Statistical Learning」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Provost, Fawcett:「Data Mining for Business Intelligence」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也在这里推荐几本轻松的书。一个好的数据科学家应该有一项非常好的技能：和随机性保持良好的关系。对此我推荐 Nassim Taleb 的「Fooled by Randomness」。同样，理解我们人类在处理信息中的偏见也对讲更好的故事而言是非常重要的，才能让我们自己不被数据愚弄。这方面我推荐 Duncan Watts 的一本书「Everything is Obvious Once you know the answer」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;8.作为一个数据科学家，你使用了哪些工具？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是一个相当老派的人，而且因为我的事业在向管理岗位发展，也就更少写代码了，我已经控制好自己不「更新」我的工具集了。另外，因为我们不支持 X 的防火墙后面有非常敏感的数据，所以任何类型的图形界面都是很困难的，我的工作基本上都是通过命令行进行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我常使用 UNIX shell 命令对数据进行操作。其中必然包含：awk, sed, grep, sort, cut, cat, head, tail, uniq……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一层的编译我使用的是 Perl。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拉取数据主要是在我们 Hadoop 上的 Hive 前端上通过 SQL 完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也用过 R、SAS、MATLAB 等工具——目前我大部分是使用 R 处理小事情（创建漂亮的图片等等）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了这些，我的态度是借用和窃取……我没有什么理由要自己实现任何机器学习算法。在这方面很多人都比我做得好。所以我已经积累了很多范围广泛的我发现的任何相关机器学习算法的独立可执行文件（UNIX）：Thorsten Joachims SVM 代码（http://svmlight.joachims.org/ ），Tree 学习的 FEST 库（https://github.com/n17s/fest ），John Langford 的 VowPal Wabbit（https://github.com/JohnLangford/vowpal_wabbit/wiki ）等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有时候我会发现一些我确实需要的工具 API。比如说，我发现了用于 NLP 的 Data Ninja（https://www.dataninja.net/ ）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;9.一个数据科学家需要必备一些特定领域知识吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们对这个问题争论很长时间了。我曾在 2013 年 KDD 的一个小组会议上做了几乎这同一主题的简短报告《The evolution of the expert（http://www.junglelightspeed.com/the-evolution-of-the-expert/ ）》。我过去尝试过许多回答这一问题的方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果你足够聪明，可以成为一个好的数据科学家，那么在大部分情况下，你都可以在一到两个月时间内学会任何你所需要的特定领域的知识。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Kaggle 竞赛已经一遍又一遍地展示了好的机器学习算法胜过专家。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「当聘请数据科学家时，我更感兴趣的是那些在许多行业内的经验比我丰富的人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;或者我就让我的个人履历说话：我曾赢得了 5 次数据挖掘比赛，而我并不是一味乳腺癌、酵母基因组、电信客户关系管理、Netflix 影评或医院管理方面的专家。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所有这些可能都说明这个问题的答案是「No」。事实上，在通常的领域知识的解释上，我也会说「No」。但也有一些非常不同的地方：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我并不需要对一般意义上的领域了解太多，但我需要尽可能理解有关这些数据的创建及其含义的「一切」。这算是领域知识吗？不太算——如果你和一位普通的肿瘤学家交谈，你会发现他或她几乎不能解释你刚发现的 fRMI 数据的细节。你应该与之交谈的人可能应该是理解这些机器和其中的数据处理（比如校准）的技术人员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;10.广告拦截对用户有益吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从我非常个人的观点来看（许多比我聪明的人已经讨论过其经济影响了），短期内它对用户有益——它们可以减少页面加载时间，减少你的移动设备上的数据等等。但事实上内容商会受到伤害：而他们正是用辛劳的汗水为你提供你想要的内容的人。所以从长期来看，我很担心其影响不是人们所想要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果经济的广告模式失败了，因为广告拦截消除了免费内容的收入流，内容上可能会面临这样的选择（通常很糟糕）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;乞求——这就是维基百科正在做的事。问题就在于通过订阅和好心读者的捐助所获得的收入是否足以支撑一个健康的和独立的新闻环境。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更多内置广告渗透进内容之中。尽管 Steve Colbert 可能处在一个过于暗淡的位置上，但我认为这并不是我们想要的趋势。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;放弃独立和有信息的内容的概念，从别人那里复制信息，而不是为记者付钱。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;部分的问题在于现在我们已经习惯了免费的信息甚至娱乐（以及所有用来生产、分发和托管的基础设施）。但不能因为我们能免费得到它们，就意味我们应该这样做。我很乐意为无广告的高质量内容付费！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;11.进入科学技术圈子对于女性来说更难吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以我个人的经验来看，我会说并不会更难——事实上，在这个男性占大多数的技术领域，作为女性还存在一些明显的好处。之前的答案已经指出录取更偏向于女性。事实上，我并不喜欢这样的不平等——我可以看到它的好处，但我也厌恶它在暗示「否则女性就无法成功，因此女性并没有男性那么好。」下面的例外来自我 2014 年给出的关于这一主题的演讲：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一年前，我被要求担任世界上最大也是最负盛名的数据科学会议 SIGKDD 2014 (KDD) 的联席主席。我是该委员会中的唯一女性。很显然，选择我的决定并不是因为我是女性。换句话说，我很高兴这意味着确实有些男人确实在严肃对待性别平等，但另一方面，我有被骗的感觉，因为我并不确定我是否有资格得到它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以当涉及到因为女性是少数派所以要支持女性时，我的感觉很复杂。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我总的看法是：女性的身份给我的事业带来的帮助超过其所带来的伤害。我还没遇见过什么人会让我觉得我的资格是毫无疑问因为我的性别。我从未标榜自己是数据科学领域的女性榜样，也没有哀叹女性的稀少，因为我从来没有担心过我的性别会限制我的能力或成就。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，女性参与到男性居多的领域中还有一个巨大的好处：人们会记得你。你已经和 90% 的典型男性区分开了。我去参加大会的时候，似乎有很多男人都认识我，甚至有时候我根本无法想起见过他们，更不要说记得名字了。事实上我们是很容易被记得的，而被记得是会很有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但被记住是一把双刃剑。一份不是最近的性别研究（http://consumerist.com/2011/01/25/sexy-news-anchors-distract-male-viewers/ ）发现当男性受试者看新闻时，他们可以很好地回忆起长得漂亮的女主持人，但却想不起她们说了什么。而当涉及到有吸引力的男性主持人时，他们可能没法回忆起他领带的颜色，但他们却可以回忆起在中东问题上的最新进展。这并不是因为他们真正相信女主持人读新闻的资格不够，只是生物学在作怪罢了——潜意识在帮倒忙。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以女性在男性主导的行业里确实面临着困境：我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？我们如何确保我们被叫上舞台是因为我们的思想而不是我们的性别？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;12.数据提取（data ingestion）可以自动化吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个大数据的时代，数据提取不得不实现某种程度的自动化——否则其它事情便无从谈起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更有趣的问题是如何最好地实现它的自动化。以及数据准备（data preparation）的哪些阶段可以在消化（digestion）过程中完成。我非常强烈地认为我的数据应该尽可能地「原始（raw）」。所以，你应该，比如说，不要自动化处理缺失数据的方法。我宁愿知道它是缺失的，而不是被系统所取代的。同样，我也更喜欢维护最高粒度的信息——比如说一位客户访问的页面的所有 URL 地址 vs. 只保留主机名（不太理想，但还行） vs. 只保留一些用户目录。从个人的角度来看，有很多理由反对第一种情况——但 hashing 这样的工具可以中和其中一些担忧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们谈谈如何做：在自动化过程方面存在 3 个真正重要的部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果全数据流过大就灵活地采样：如果你每天要处理 500 亿个事件——只是将它们塞进 Hadoop 系统倒还好——但后续操作却很繁琐。相反，除了 Hadoop 备份过程之外，有一个能将有特定价值的事件取出的过程是很好的。详情可参阅我们写的这篇博客文章：http://www.kdnuggets.com/2016/08/automated-data-science-digital-advertising.html&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态历史的注释：拥有所有的事件日志是很不错，但对于预测建模，我通常只需要有能获取实体历史的特征就行了。每一次都加入超过十亿行来创建历史是不可能的。所以部分的提取过程其实是一个注释过程，该过程能为每一个事件附加重要的信息。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有统计测试该评估，看输入数据流的性质是否正在改变，当比如说一些数据源临时变暗时发送警报。一些相关内容请查看这里：http://www.troyraeder.com/papers/kdd12.pdf&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;13.如果想从数据科学家转向数据科学团队经理有哪些挑战？怎么做准备？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这十多年来，我一直在回拒让我管理数据科学团队的要求。而现在，我非常享受和一个团队进行互动、开大脑风暴会议、和某些人一起探究我的想法——但我相当不喜欢告诉别人该做什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人曾见过很多优秀的数据科学家不是很好的管理者——但这可能在任何领域都是一样。许多时候优秀的数据科学家会感到他们事业的唯一进展是开始领导一个团队。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人来看，我发现不自己做数据科学工作的前景是相当艰难的。我也有重大的信任问题——我超级怀疑我自己的结果——所以让其他人来做我的工作是难以接受的。我的忧郁的原因是工作成果的很多方面都依赖于数据准备过程中很多微小的细节。比如说：你删除重复项了吗？样本选择的任何细节都是至关重要的。除了非常了解你的团队的成员的长度和短处，以及知道什么人值得托付怎样的任务——这里要做一点准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为经理，一项非常重要的技能是与不同的业务部门互动，以确认需要做哪些工作，哪些工作优先。那需要了解数据科学的广阔图景以及很好的沟通技巧。作为准备，我建议你尽可能地参与到你目前所从事的项目的界定的过程中，并开始练习如何与企业的利益相关者交流。事实上，任何涉及到向不太技术的观众进行公开演讲的机会都对此会很有帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌、微软合著论文：由知识引导的结构化注意网络</title>
      <link>http://www.iwgc.cn/link/2707040</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoy37z8zSoib7zfs5LbHwQyCHgWhE7V60Swra3d985epfcPGWZpfoicPcg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然语言理解（NLU）是口语对话系统的一个核心组成部分。最近的循环神经网络（RNN）凭借其随时间保存序列信息的强大能力在 NLU 上取得了很好的结果。传统而言，NLU 模块根据话语的扁平结构标记话语的语义槽（semantic slot），其基本 RNN 结构是一个线性链（linear chain）。但是，自然语言展现的语言属性能为更好的理解提供丰富的、结构化的信息。这篇论文介绍了一种全新的模型——由知识引导的结构化注意网络（K-SAN：knowledge-guided structural attention network），这是一种广义的 RNN，再加入了由先前的知识引导的非扁平的网络拓扑。其有两个特点：1）可以从小型训练数据集中获取重要的子结构，让模型可以泛化到之前从未见过的测试数据；2）该模型可以自动找出对预测给定句子的语义标签至关重要的显著子结构，从而可是实现理解性能的提升。在航空旅行信息系统（ATIS）数据基准上的实验表明我们提出的 K-SAN 架构可以使用注意机制（attention mechanism）有效地从子结构中提取出显著的知识，其表现超过了当前最佳的基于神经网络的框架。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;点击「阅读原文」，下载论文↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
  </channel>
</rss>
