<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | 数据和算法像人一样有偏见，你还愿意让人工智能帮你投票吗？</title>
      <link>http://www.iwgc.cn/link/3399371</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自World Economic Forum&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南、曹瑞&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="text-align: justify; color: rgb(136, 136, 136);"&gt;&lt;span&gt;2016 美国大选将至，一些研究者和从业者也趁着这股热潮推出了一些基于数据预测大选结果的人工智能程序，但就像人类自己一样，它们所支持的总统候选人也都不一样（一些俄罗斯人开发的一个人工智能程序会选择特朗普当总统 :O）。未来，如果算法成为了我们日常生活的管家，我们可以让算法来帮助我们选出总统吗？&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;想象一下 2020 年的普通一天，人工智能助手唤你起床，为你端上已准备好的早餐，都是你最喜欢的食物。在晨跑中，播放器会自动播放符合你喜好的最新歌曲。上班路上，电子助手会根据你过去的阅读品味，自动向你推送新闻以供阅读。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你阅览着新闻，注意到总统选举马上就要来了，人工智能参考了你过去的政治看法和本州其他选民的意见，向你推荐了一位民主党候选人。你的手机上，一条弹出信息询问你是否需要 AI 助手帮你准备投票所需文件，你点击「同意」，然后关掉屏幕，继续自己的生活。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人工智能：呆板的数据机器&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI 个人助手在几年前已经走进现实，对于我们来说，把履行公民义务的重任交与它们还是显得有些不合适——即使人工智能几乎总是知道在特定的时刻给我们最好的建议。通过足量的数据学习，人工智能可以为每个人提供准确的，个性化的建议，甚至比你最亲密朋友的建议更完美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alphabet 董事长埃里克·施密特坚信，人工智能的发展会让每个人都会变得更聪明，更有能力，更为成功。人工智能已经展现出了巨大潜力，有希望帮助解决人类社会面临的各种复杂挑战，如气候变暖，人口增长和人类发展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而机器展现出的潜力也带来了担忧。有调查显示，34% 的人表示自己害怕人工智能，而 24% 的人认为人工智能会对社会造成负面影响。相比较未知的恐惧，人工智能对于数据的依赖带来了现实的隐患，GWI 的研究表明，63% 的民众担心他们的个人信息被科技公司滥用。最近 Oxford Internet Institute 的研究显示，人们对于让人工智能助手打理自己生活的方式持谨慎态度，特别是当这些助理提出自己的建议，却又不告诉你它推理过程的时候。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这里，我们没有必要混淆数学与魔法。人工智能并不是在你手机里生活的神秘生物。但我们往往会忘记，人工智能一直在读取我们的个人资料，通过复杂的数学模型，自动推断我们的兴趣、位置、习惯、财务和健康。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;开发者的角色&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当前关于算法与人类的很多讨论都围绕着设计者在算法中的作用——人工智能创造者的潜在意识和偏差是否会被编码进帮我们做出决定的算法中。很多人担心开发者的个人偏见会被带入算法，其中一点点微妙的歧视就会让部分人群的利益受到侵害——也许还有更坏的结果，科技平台会演变成弱势群体难以逾越的门槛。即使算法和写算法的人没有偏见，没有人能够保证训练算法的数据中一切都是平等的，现实世界本身存在着偏见，数据集中的内容也会对人工智能框架产生影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;持这一观点的决策者和专家们经常误解人工智能算法出错的原因。他们不断指责开发者，却忽略了自我学习系统的局限性。将错误推给别人是一种自然反应，特别是在你无法理解这些技术时。算法的偏差很少来自于开发它们的工程师。事实上，在大部分情况下，问题的根源出自训练算法的数据，这才是构建未来人工智能社会所要担心的真正危险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;算法决定论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回想一下机器学习到底是怎么工作的，通过应用统计学技术，我们可以开发自动识别数据中特征的算法。为了达到这个目的，系统需要经过巨大数据集的训练，训练模型的数据越多，预测准确率越高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在个性化数字应用中，这些统计学习技术被用来建立算法，为用户提供个性化服务，计算机阅读了我们的使用模式、品味、偏好、人格特征和社交图谱，随后建立起对于人类的数字观感。计算机形成的社交身份并不基于你的个性或选择，相反，这种虚拟身份来自于你的可统计数据点，和它们的机器解释。这种代替，无论多么复杂，都是人工智能对人类的不完美数字表达。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人工智能只能查找历史数据，为用户所需做出建议。这就是为什么今年 8 月，一个视觉识别神经网络通过 1400 万张照片的训练后预测唐纳德·特朗普将会赢得本届美国总统大选。鉴于这个数据集中并没有女性美国总统，AI 可能判断性别是识别模型的相关特征。但即使排除这一点，如果让这个俄罗斯人训练的人工智能投票的话，它肯定会投特朗普。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuJNEAvGv5NRwWHicibDZweMHAlvrox2VYj1kckWjNETR3GXk45A1eKMcg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这样的推论会导致越来越僵化的推荐系统，它倾向于不断强化现有的看法，就像社交网络中的反射效应一般。「个性化」使每个人都被贴上了标签，让现实生活和网络世界互相割裂。计算机不断地推荐「你喜欢的」内容，用户获得的信息在不知不觉中被算法误导，人类或许在人工智能真正觉醒之前就已深陷其中了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;动态的人生&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的身份是动态的，复杂而充满矛盾的。根据我们的社会背景，我们总会拥有者几个不同的身份，这意味着我们需要用到几种不同的 AI 助理——在学校或工作中的，在酒吧或教堂里的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了通常的自我介绍，我们在网络中可能也需要以不同的身份展现自我，和不同的群体展开互动。我们不希望自己在社交网络中被随意查看，我们也不希望自己在寻找新奇事物时，还要担心朋友和家人的窥视。如果我们想要试试不同的社会身份，会发生什么？4Chan 创始人 Chris Poole 说道：「这不是你在和谁分享的问题，这有关你与他人分享什么样的内容。身份就像一个棱镜，别人通过它来看你会呈现无数不同的面貌。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;区分不同的自我表达阶层，绘制不同社交环境下的身份，对于人工智能而言是一个巨大挑战。很多时候，人类面临的问题不在于算法设计——我们连自己是什么都还没弄清楚。但人工智能助手总会给我们一个答案：关于过去的我们。身份的变化在这样的环境中变得越来越难，我们的生活习惯和信念被自我强化的循环锁定，算法构建的《土拨鼠日》出现了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在日常生活中越依赖于个性化算法，我们的个性就会越被计算所磨灭，我们所读，我们所见，我们生活的方式都将被机器所决定。通过专注于现状，接管探索信息和偶遇陌生人的渠道，用过去发生过的事情试图再一次讨好自己，这就是算法决定论的世界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当过去照进未来，人类赖以生存的自发性，开放与进取变得逐渐稀缺。温斯顿·丘吉尔曾经的话变成了这样：我们塑造了算法，然后，算法塑造了我们。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuYJGwhR0KoMia0kGcPp8GEIz7ueE3Lsaic7iamj2ib8UgXbfiajpibtbG8p2A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;如何阻止未来&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在今天，现实世界中的人工智能应用已经融入到了日常生活的方方面面——而人们对这一科技的兴趣也是越发浓厚。但是有两个主要的挑战正让未来变得难以触及。从科技进步的角度来讲，不同应用之间的数据交换上缺乏互通性标准，而具备这一点能够防止彻底的个性化。要是想要真正有用的话，机器学习系统需要更多的个人数据——而这些数据现在都被孤立地分散在一些有竞争力的科技公司的专业数据库当中。那些掌握数据的公司就掌握了权利。一些公司，最著名的比如说像 Apple 和 Viv，已经开始通过与第三方服务结合的实验来扩大自己的势力范围。最近，一些最大的科技公司宣布了与人工智能研究的主要合作，这样就可以将益处带给大多数人，而不仅仅是少数人。这将会对今后建立对人工智能的普遍信任至关重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从社会的角度来看，人类似乎对人工智能的急速发展有一种莫名的反感。人们担心会失去对人工智能助手的控制。信任是我们控制能力的一种直接表现。试图对生产力进行一些微小的改进，却要赌上关系和名誉，大多数人都不愿意这样做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，在早期，人工智能助手的行为方式可能并不是它的人类制造者所期望的。有先例证明，一些失败的人工智能实验会减少对弱人工智能（narrow AI）解决方案和聊天机器人（conversational bots）的信任。Facebook、微软和谷歌纷纷在 2016 年建立了它们的机器人平台，但过早呈现在人们面前的人工智能科技，因其有限的功能、应用和定制化让用户大失所望。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一直困扰我们的恐惧——人工智能科技的后果，也因为很多科幻小说中所描述的有意识、暴戾的人工智能统治世界的反乌托邦场景而加剧。但是我们所面对的未来，既不会像是人工智慧网络「天网」（Skynet），也不会像乔治·奥威尔的《1984》里一样：而更可能会像是《美丽新世界》（A Brave New World）中所描述的一个享乐主义的社会，在那里，科技的地位仍然是需要为普遍的幸福和自我放纵所服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu2HNia0xTsL27niczVuXYIDgHaaAvL8coPibtn5TXRLUhbHwH8GksVYDJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;未来导向型机制&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科技发展的脚步从未停滞，但希望仍在。2016 年全球杰出青年社区（Global Shapers Community）的年度调查显示，在年轻人眼中，人工智能已经成为了主要的科技发展趋势。此外，21% 的调查对象表示他们支持人形机器人的权利，而且在东南亚，支持的呼声尤为高涨。年轻人们似乎对于人工智能在我们日常生活中所扮演的角色持非常乐观的态度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在欧洲，欧盟的《一般数据保护条例》（General Data Protection Regulation，简称 GDPR）让用户有机会要求对基于分析的算法决策进行解释，限制了绝对形式的算法决策。该条例有望于 2018 年 5 月之前在所有欧盟国家实施。这样的机制能够限制资料搜集，强调了人类可解释性（human Interpretability）在算法设计中不容忽视的重要性。但是，这是否会对一些大型科技公司现行的算法实践带来主要的变化，还尤未可知。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每天都有关于我们每个人的成千上万个算法决策——从 Netflix 的电影推荐、Facebook 上的好友建议，到保险风险评估和信用评分。就各方面而言，人们自己应该有责任对关于自己的算法决策进行跟踪和仔细审查，或者说我们可能需要将此编码到他们使用的数字平台设计当中？责任是非常重要的一点，准确来说是因为在大范围内进行估量和实施是非常困难的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，在一头栽进这个未知的领域之前，我们需要回答一个问题：我们想让人类和人工智能之间的关系成为什么样子？反思这些问题，我们才会设计出非决策性的、透明并且有责任感的算法，这些算法能够辨别出个体当中复杂、发展和多方面的本质。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 07 Nov 2016 14:44:05 +0800</pubDate>
    </item>
    <item>
      <title>基础 | 机器学习入门必备：如何用Python从头实现感知器算法</title>
      <link>http://www.iwgc.cn/link/3399372</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自machinelearningmastery&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Terrence L、武竞、Xavier Massa&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;感知器算法是最简单的人工神经网络形式之一。感知器是一个单神经元的模型，可以用于两个类别的分类问题，也能为以后开发更大的神经网络奠定基础。在本教程中，你将了解到如何利用 Python 从头开始实现感知器算法。&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在完成本教程后，你将学会：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何训练感知器的网络权重&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何利用感知器做出预测&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何对于现实世界的分类问题实现感知器算法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们开始吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节简要介绍了感知器算法和 Sonar 数据集，我们将会在后面应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;感知器算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;感知器的灵感来自于被称为神经元的单个神经细胞的信息处理过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经元通过其树突接受输入信号，并将电信号向下传递到细胞体内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过类似的方式，感知器从训练数据的样本中接受输入信号，训练数据被加权并在称为激活（activation）的线性方程中进行组合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuR58uk06pl1hs3qIsAAqyAXl4jJS9TUgu9XqAy9jQ0S7cNUGkuhLGUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，使用诸如阶跃传递函数（step transfer function）的传递函数将激活变换为输出值或预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu2cXKNFv1vfsAtD4syvAIKOYHBnn6d0m00jAS9oz8LlBOZYKsWRg8Zg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以这种方式，感知器是用于具有两个类（0 和 1）的问题的分类算法，其中可以使用线性方程来分离这两个类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它与以类似方式进行预测的线性回归和 logistic 回归密切相关（例如输入的加权和）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;感知器算法的权重必须使用随机梯度下降算法从训练数据中估计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;随机梯度下降&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度下降是通过跟随成本函数（cost function）的梯度来最小化函数的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这涉及了解成本的形式以及导数，使得从给定的点你可以知道梯度并且可以在该方向上移动，比如下坡到最小值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在机器学习中，我们可以使用一种技术来评估和更新称为随机梯度下降的每次迭代的权重，以最小化我们的训练数据模型的误差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种优化算法的工作方式是每次向模型显示每个训练实例。模型对训练实例进行预测，计算误差并更新模型以便减少下一预测的误差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该过程可以用于在模型中找到能使训练数据上模型误差最小的权重集合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于感知器算法，每次迭代，权重（w）使用以下等式更新：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuCR15qgicJ3zgQ2CzAgmSkoK19p5ssuTGnsLwNnSukqJCtLHG52meDOQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中w是正在被优化的权重，learning_rate是必须配置的学习速率（例如 0.01），（expected - predicted）是在归因于权重的训练数据上的模型的预测误差，x是输入值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;S&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;onar 数据集&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将在本教程中使用的数据集是 Sonar 数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个描述了声呐啾啾叫声并返回不同服务的试探的数据集。60 个输入变量是在不同角度的返回强度。这是一个二元分类问题，需要一个模型来区分金属圆柱体和岩石。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它是一个很好理解的数据集。所有的变量是连续的，通常在 0 到 1 的范围内。因此，我们不必对输入数据进行归一化，这通常是使用感知器算法的一个好地方。输出变量是字符串「M」（表示矿 mine）和「R」（表示岩石 rock），我们需要将其转换为整数 1 和 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过在数据集（M 或 Mines）中预测具有最多观测值的类，零规则算法（Zero Rule Algorithm）可以实现 53％的精度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以在 UCI Machine Learning repository：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)）&lt;/span&gt;&lt;span&gt;中了解有关此数据集的更多信息。你也可以免费下载数据集，并将其放在工作目录中，文件名为 sonar.all-data.csv。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;教程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个教程分为三个部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.作出预测&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.训练网络权重&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.将 Sonar 数据集建模&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些步骤将带给你实现和应用感知器算法到你自己的分类预测建模问题的基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1. 作预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一步是开发一个可以进行预测的函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这将会需要在随机梯度下降中的候选权重值的评估以及在模型被最终确定之后，我们希望开始对测试数据或新数据进行预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是一个名为 predict() 的函数，用于预测给定一组权重的行的输出值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个权重始终是偏差，因为它是独立的，不负责特定的输入值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuMrXeLicnjIkPD4dweYXbr8a7jz0tRU6JQtZzSPJM45hlE1axSzznqNw/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以设计一个小数据集来测试我们的预测函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IucnkwaNuPyMzA1etI2PQuQq2bS27VanfQ2VbGX4Th2crFRJ3FuKialSw/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也可以使用之前准备好的权重来为这个数据集做预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将所有这些集中起来，我们就可以测试我们的 predict() 函数了，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu6YiaMuicXM9pSxLIZ4KAscevJicrCNgMA34sBzpBjnz5rxicyib0FmEA6Ng/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该函数有两个输入值 X1、X2 和三个权重参数 bias、w1 及 w2。该问题的激活函数有如下的形式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuzBpTY21RHjUKGjw27bFc9mdjnct7qtFdERT6FVm9I5XSDggdm1XxTA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;或者，我们能够手动地选择权重值：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iuj6OicNIlSzibiaJ4gNfCb6nFqb16SMkJQdf9K667f8pR7JNQzpQdEjpBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行这个函数，我们将会得到与期望输出值 (**y**) 相符合的预测值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu26ylV03nTicsSeIibHqv7iaiaY8pgSLk3kuAXKVuqbPysqYJjEqicPL04Dg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们已经准备好使用随机梯度下降法（SGD）来最优化我们的权重值。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2. 训练神经权重&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以使用 SGD，来估计出对于训练集的权重值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SGD 有两个参数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;学习率（Learning Rate）：用来限制每次更新中权重项修正值的大小。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;迭代次数（Epochs）：在训练集上训练同时更新权重项的次数。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两个参数，和训练集一起，都将会是预测函数的输入参数。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个函数中，我们需要运行三个循环：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 对于每次迭代进行循环；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 对于一次迭代中，训练集的每一行进行循环；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 对于每一行中，每一个值进行循环。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如你所见，我们在每一次迭代中，对训练集每一行中每一个权值都进行更新。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们基于现有情况模型预测的「误差」，来对权重值进行更新。误差，是由候选权值计算出来的预测值与（数据集中的）期望值的差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对每一个输入属性，都有一个权重值，并且这些权_重值都连续更新_。如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuAUaJJPicgMicUkWX5MF8ibzp4ich0J9nMRdY5p5sBS8wZrq6lhHkxHjGdw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;偏差项以一种相似的方式更新，不过因为它本身就不与特定的输入值有关，因而在式子中没有输入值的项。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu63iarVI5iaR7MVP9IQN2mhhSuc0ribCoQR9YlJ6lOSfREJQaViboGpsfzg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，我们把所有的内容组合到一起。如下所示，在 train_weights() 函数中，它使用 SGD 方法，计算对于给定训练集的权重值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IunAMYCGjib2ru5eSHzTRib6zBsGDdlf6LMnx5pzCqDY4xSbvIGsD5BZLw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如你所见，我们也在每次迭代中，记录下了平方误差之和（这始终是一个正值）。因而我们能在外循环的每次迭代中，print 一些有用的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也可以在我们上面创建的小规模数据集上，对该函数进行测试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IueN9D4GWkC01Xht9OicCCR62qlGUkYQsSswrgFiaPOs9e4TT8m3RgOjdA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将使用 0.1 的学习率和 5 次迭代，也就是把参数在训练集上更新五次，来训练这个模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;运行这个案例，它将会在每一次迭代结束后，显示出该次迭代后的平方误差和，并在完成所有迭代后，显示最后的权重集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuiaPeXF9F8yEg9KQ79AQObDNwmsn4h46icGh3xnGwWaM8XP4S8gnjUqkQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以看到，这个算法很快学会了「解决」这个问题。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在我们来试试看，如何在一个实际的数据集上应用这个算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 对声纳数据集进行建模&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这一节中，我们将使用 SGD 方法，对一个声纳数据集，训练一个感知器模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该例子中，我们假定，在当前的工作目录下，有一名为&lt;strong&gt;sonar.all-data.csv&lt;/strong&gt; 的文件，存储着该数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先该数据集被载入。数据集中字符串格式的数据被转换为数值型，同时输出值从字符串被转换了 0 或 1 的两个整数值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过&lt;strong&gt; load_csv(), str_column_to_float()及str_column_to_int() &lt;/strong&gt;三个函数，我们实现了对数据集的读取及预处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们使用「k 倍交叉验证法」（k-fold cross validation）来对学习后的模型在未知数据集上的表现进行评估。也就是说，我们需要建立 k 个模型并估计各模型的平均误差。分类准确性将被用于模型的评估工作中。这些工作在 &lt;strong&gt;cross_validation_split(), accuracy_metric() 及 evaluate_algorithm() &lt;/strong&gt;函数中被完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们将会使用上面设置的&lt;strong&gt; predict()&lt;/strong&gt; 和&lt;strong&gt; train_weights()&lt;/strong&gt;函数来训练该模型。同时，我们将会用一个新函数&lt;strong&gt; perceptron() &lt;/strong&gt;来将它们组合在一起。如下是完整的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuBsLw6loicYRnj1tu2icZsicHpzNzkOg8UQMiaXHIRjZjMDwm7ZfickCa2jQ/0?wx_fmt=png"/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuuaXykSWI0a7wmb1ch9xpLmqnog2icNTv1mFeicZJYuDJZ95siaP0zVUbw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IupcDG0JWt0OcqaEK0DP12o6gUU1AY05amW7V5DJnGxBG5jJQrCTzUDg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在交叉验证中，我们取 k 为 3——也就是对每一块数据，都有 208/3 约 70 个记录，会在每次迭代中被用于计算。我们取 0.1 的学习率及 500 的训练迭代次数，来训练模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你可以尝试你自己的参数，并且看看你的结果能否战胜我的分数。运行这个例子，将会显示对 3 倍交叉验证中每一块的分数，以及平均的分类正确率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以看到，这个正确率约为 73%，高于由仅考虑主要类的零规则算法（Zero Rule Algorithm）得到的 50% 的正确率基准值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iu66FVpT5THHaNaPYibpuT0p7bQp98MlESvqBiby0FFKVrR256ZfMxAx8g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;拓展&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一节列举了关于该入门指导的拓展内容，你可以考虑深入探究其中的一些内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;调试样例参数。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;尝试着去调整包括学习率、迭代次数乃至于数据预处理的方法，以使模型在该数据集上，得到更高的分数。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;批量化进行随机梯度下降。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;修改随机梯度下降的算法，使得我们能记录下每一次迭代的更新值，并且在迭代结束的时候再基于记录的更新值来更新权重值。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;额外的分类问题。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;将文中提到的技巧，应用于 UCI 机器学习数据集中其他数据。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;回顾&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在该教程中，你学习了如何从零开始，用 Python 实现，基于随机梯度下降的感知器算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你学会了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何对一个二元分类问题进行预测。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何使用随机梯度下降，对一系列的权重值进行最优化。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如何将这个技巧，应用到一个实际的分类预测模型。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 07 Nov 2016 14:44:05 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌ICLR 2017论文提出超大规模的神经网络：稀疏门控专家混合层（附论文）</title>
      <link>http://www.iwgc.cn/link/3399373</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ICLR2017&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IuV0x4q713CXvCrUDcPnX6znymZSFvbg27yzdd9dQiaP6YHVocadEcv5g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：神经网络吸收信息的能力受限于其参数的数量。在这篇论文中，我们提出一种新类型的层——稀疏门控专家混合层（Sparsely-Gated Mixture-of-Experts(MoE)），它能够在仅需增加一点计算的基础上被用于有效提升模型的能力。这种层包含了多达数万个前向的子网络（feed-forward sub-networks，被称为专家（expert）），总共包含了多达数百亿个参数。一个可训练的门网络（gating network）可以确定这些专家的稀疏组合以用于每一个样本。我们将这种 MoE 应用到了语言建模任务上——在这种任务中，模型能力对吸收训练语料库中可用的大量世界知识而言是至关重要的。我们提出了将 MoE 层注入堆叠 LSTM（stacked LSTM）的新型语言模型架构，得到的模型的可用参数数量可比其它模型多几个数量级。在语言建模和机器翻译基准上，我们在更低的成本上实现了可与当前最佳表现媲美或更好的结果，其中包括在 1 Billion Word Language Modeling Benchmark 上测得的 29.9 的困惑度（perplexity），以及在 WMT』14 En to Fr（英法翻译）和 En to De（英德翻译）上分别得到了 40.56 和 26.03 的 BLEU 分数。 &amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2Iuic44HqiaPwSJj32u5pU9YBYu3jJo3I6D6hITribTRaZhtfN1O62alyMiaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1：一个嵌入在语言模型中的专家混合（MoE/Mixture of Experts）层。在这里例子中，其稀疏门函数（sparse gating function）可以选择两个专家（expert）来执行计算。它们的输出由该门网络（gating network）的输出进行调制。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 07 Nov 2016 14:44:05 +0800</pubDate>
    </item>
    <item>
      <title>历史 | 神灵庇佑的11月：图灵机、Firefox 和Windows的诞生时刻</title>
      <link>http://www.iwgc.cn/link/3399374</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Fortune&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、蒋思源&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在世界的不同地方，人们都有幸运时间的说法，不管那是按生肖计算还是要看星座。11 月上半月无疑是计算机科学领域的幸运月。自 1936 年以来，就像有计算机之神的庇佑一样，计算机领域内的很多具有历史里程碑意义的大事件都集中发生在 11 月的上半月，其中包括：阿兰·图灵提出了现代计算机的前身「图灵机」、松下发布第一款手持式计算机、微软宣布开发 Windows 操作系统、IBM 宣布实现磁盘小型化的突破性技术、火狐推出 1.0 版本……这个 11 月，让我们简单回顾一下那些塑造了我们今日的数字生活的 11 月。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicsPeIvLZsUJgazZo1ias2IusqbFC0pc925icqgBhTtia5rx531x0Ae3JA9K72EpjDibtcDodHnqcBicUA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;在 2015 年 3 月香港的一场拍卖预览会上，来自阿兰·图灵的笔记本中的一页在他的肖像前展示&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1991 年 11 月 8 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一天，历史上第一次图灵测试在波士顿计算机博物馆举行。Thinking Software 的总裁 Joseph Weintraub 用他的软件骗过了 10 位裁判员中的 5 位，赢得了这场比赛。他的软件可以像人那样展开异想天开的对话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于 Weintraub 的软件，Mark Halpern 在「图灵测试的麻烦」中写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这款程序在测试过程中被称为终结者 5（T5），它发表的言论比大多数计算机驱动的终端说的话更有趣（这不算夸赞），但对于这些程序来说，就是完美的标准。它主要靠的是「非定向疗法」的策略，多年前该策略由 Joseph Weizenbaum 与 Kenneth Colby 提出，在这个策略中程序可以从输入的数据中挑出一小段（例如，「I wish I knew more about X」中的 X），将这一小段插入预先录制好的回复（「Why are you interested in X?」）中 ... 值得注意的是，五名裁判发现 T5 是人类，包括两名把人类说话当成计算机的裁判。总体上来说，裁判的表现让我们得出一个不太好的总结，T5 无法深入到持续的对话中，缺乏人类常识，它们无法与人类分享酸甜苦乐，与它们聊天的感觉跟照着镜子和自己说话一样，终究无法驱逐孤独的感觉。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是 Weintraub 的软件（T5）与裁判之间的「异想天开的对话片段」：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;裁判 3：告诉我你理想中完美世界是什么样的？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;T5： 想法总比现实多，你是一个心不在焉的教授吗？你自己是怎么想象一个完美世界的？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;裁判 2：Hi，你住在哪里？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;T5： 你耳朵没问题吧？有时我真希望自己就是一个程序分析师... 请继续，我发现这个太有趣了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上世纪 50 年代，阿兰·图灵在其论文「计算机器和智能（Computing Machinery and Intelligence）」中提出「模拟游戏」时，他引用了著名的「洛芙莱斯夫人异议（Lady Lovelace』s Objection）」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们所了解的关于 Babbage 分析机的绝大部分细节都来自于 Lady Lovelace 的回忆录（1842）。她在回忆录中写道，「分析机不会自命为任何东西，它可以做任何我们知道如何命令它执行的事情。」Hartree 曾引用过这句话，他补充道：「这并不是说不可能做出一台可『自我思考』以的电子设备，或者从生物学角度上说，人们可以建立一个条件反射，作为'学习'的基础。根据最近的技术进展来看，无论理论上是否成立，它都是刺激 一个让人兴奋的刺激问题。但是机器似乎不太可能立即就掌握这项技能。」&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我非常同意 Hartree。人们会注意到他的意思并不是机器无法掌握这个技能。分析机是一个通用的数码计算机，所以，如果它的存储量和速度能满足要求，它就可以通过适当的编程来模拟出来。或许伯爵夫人和 Babbage 没有想到这些。他们也没必要在任何情况下都对外声称自己做过的所有事情... 罗浮莱斯夫人异议的另一个版本中说到，一个机器「永远不会做出真正的新东西。」就像那句谚语中说的「太阳底下无新事」。谁敢肯定自己的「原创作品」不是经过接受教育之后产生的成果，或否定它受到一些众所周知的准则影响呢？还有一个更好的版本是，机器从来不会「给我们惊喜」。这种说法更加具挑衅，可以直接反驳。机器经常给我惊喜，大部分是因为我没做好充分的计算来决定让机器做什么，或者就是因为即便我做好了计算，也是匆忙草率的，风险很大。或许我该对我自己说，「我想这里的电压应该和那里的一样，不管怎样，让我们假设它是一样的。」我常常出错，这是很自然的事情，而且结果让我惊喜常常是因为实验结束的时候已经忘了假设。我的讲座主题总是关于我那些邪恶的方法，但是当我要明我所经历的惊喜时，请不要怀疑我。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;Bringsjord、Bello 和 Ferruci &lt;/span&gt;在《Creativity, the Turing Test, and the (Better) Lovelace Test》中写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;不幸的是，打造能够通过（图灵测试）的计算系统的尝试……已经转到浅显的符号操作上，这些操作不管是怎么设计的，都是用来骗人的。这种系统的人类创造者很了解他们只是在尝试欺骗那些与他们的系统进行互动的人，让他们相信这些系统真正是有心智的。而这样做的根本问题是：图灵测试的结构是为了培养骗子。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几个月后，Nuance Communications 赞助了第一轮的威诺格拉德模式挑战赛（Winograd Schema Challenge），这是图灵测试的一个替代选择。其结果是：机器在代词解析（pronoun resolution）上达到了 58.33% 的正确率，相对而言，人类的准确率是 90.9%。即便如此，谷歌的「人工智能机器（artificial intelligence machine）」还是因为变得「极其愤怒（exasperated）」并「打断了人类询问者」的对话而成为了新闻头条：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2004 年 11 月 9 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Firefox 1.0 发布。Firefox（火狐）得名于生活在喜马拉雅山东部和中国西南部的一种动物——红狐。它在发布首日就获得了 100 万的下载量，10 天下载量达到了 1000 万，而在一年之后 Firefox 1.5 发布之前，其下载量已经超过了 1 亿。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1981 年 11 月 10 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;松下推出了 The Link 手持式计算机（不知道可不可以简称「手机」？），它带有一块键盘但没有屏幕。它可以被连接到一台电视机或通过电话拨号连接到一台主机计算机。它的尺寸为 9」x4」，重 21 盎司（大约 595 克），售价 600 美元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1983 年 11 月 10 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软宣布 Windows——「一个窗口管理器和图形设备界面」，并表示其将在四月份将软件交付给经销商（尽管像 Windows 这样的产品是很难预料的，可能需要更长的时间）。Martin Campbell-Kelly 和 William Aspray 在《Computer: A History of the Information Machine》中写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Microsoft Windows 是最近出现的用于 PC 的新操作系统。微软在 1981 年 9 月开始研发一个图形用户界面项目，在此不久之前盖茨拜访了苹果公司的史蒂夫·乔布斯，并看到了正在开发之中的 Macintosh 原型计算机。这个微软项目被命名为 Interface Manager，但是一场名为「让我们的名字基本上能定义这种通用范例」的市场营销项目中，他们将其名字改成了 Windows。他们估计这个系统需要 6 个程序员开发几年的时间。事实证明他们严重低估了。当 Windows 的版本 1 在 1985 年 10 月发布时……据估计该程序包含了 110,000 条指令，用了 8 0 位程序员几年的时间才完成。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1997 年 11 月 11 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 发布了第一个带有巨磁阻（Giant Magnetoresistive (GMR)）磁头的大容量个人计算机磁盘驱动器，这使磁盘驱动器能够进一步小型化。2007 年的诺贝尔物理学奖就颁给了在 1988 年为巨磁电阻效应（GMR effect）做出巨大贡献的 Albert Fert 和 Peter Grünberg&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1936 年 11 月 12 日&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;阿兰·图灵（Alan Turing）将他的论文《论可计算性数字，一种判定问题的应用》（On Computable Numbers, with an Application to the Entscheidungsproblem）提交给伦敦数学学会（London Mathematical Society）。在论文中，图灵描述了一种后来被称之为「图灵机（Turing Machine）」的通用机器，这是一种理想化的计算设备，它能够执行任何数学计算并表达成算法。历史学家 Thomas Haigh 反对现在日益流行的「图灵发明了现代计算机」说法，他说「事实上，图灵没有发明计算机（Communications of the ACM, January 2014）」。下面是 Haigh 的具体观点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们迫切相信 20 世纪 40 年代的计算机运动是由人们对通用图灵机的渴望催生的，从更广泛的角度来看，这种信念反映出我们更愿意看见理论计算机科学能够驱动整体计算的发展。如果认为是图灵创立了计算机科学，那么对计算机科学本身也是一种过度简化，这样一来我们也能肯定地说他发明了计算机。在这种观点下，计算机仅仅是通用图灵机的基本理论思想的一种实现过程，因为它是通用的并且能交换地存储数据和指令... 然而关注历史上的计算机，将其作为逻辑思维的体现，却忽视其发明者在发明计算机时面临的有限资源和对未验证技术所作出的权衡，这种做法本身就剥离了理解计算机历史和发展所需要的信息。电子工程特别是内存技术的发展创造了一个良好的氛围，在这个氛围下考虑以电子方式储存指令的高速电子计算才有意义。反过来，关于设计这些机器的最佳方法的想法也驱动了计算机组件技术和工程方法的发展。通用图灵机自从 20 世纪 50 年代以来就对计算机理论学家充满了吸引力并明确地向前发展，因为它脱离了复杂的计算机功能结构，也脱离了可计算性与设计和工程的解耦问题。这不论在技术上还是社会上都对计算理论家们有很大的作用。然而矛盾的是世界好像对寻找计算机在数学概念上的准确表达充满了兴趣，因为这样就可以避免那些建造并运行一台真正计算机所需要面对的各种各样问题。在计算理论学家眼里，软硬件是可以互换的，但对具有历史眼光的人来说并不这样。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们总是渴望将技术创新的起源归根于「科学」而不是「工程（engineering）」，这种渴望超越了人们对于从杂乱中提取抽象总结的欲望。好像一切都与科学的威望有关，科学有很高的社会地位，而工程恰恰相反。1915 年，三元高真空管的发明成就了第一通越洋电话，为了庆祝这一成功，美国电话电报公司发了一条广告，宣称这是「科学的胜利」，&lt;span&gt;「&lt;/span&gt;不是工程的胜利」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 07 Nov 2016 14:44:05 +0800</pubDate>
    </item>
    <item>
      <title>重磅论文 | 如何通过机器学习解读唇语？DeepMind要通过LipNet帮助机器「看」懂别人说的话</title>
      <link>http://www.iwgc.cn/link/3388468</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自oxml.co.uk&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;还记得经典科幻电影《2001 太空漫游》中的飞船主控计算机 Hall 吗？它具有依靠阅读说话人的嘴唇运动理解其所表达的内容的能力，这种能力也在推动那个幻想故事的情节发展中起到了至关重要的作用。近日，牛津大学、Google DeepMind 和加拿大高等研究院（CIFAR）联合发布了一篇同样具有重要价值的论文，介绍了利用机器学习实现的句子层面的自动唇读技术 LipNet。该技术将自动唇读技术的前沿水平推进到了前所未有的高度。原论文可点击文末「阅读原文」下载。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=b0343vh7eug&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorwjjJeTsCUpuYZcGOZbIBbSX324tGOcoImJRQ8rBamLuRSquI3icEZNg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;唇读（lipreading）是指根据说话人的嘴唇运动解码出文本的任务。传统的方法是将该问题分成两步解决：设计或学习视觉特征、以及预测。最近的深度唇读方法是可以端到端训练的（Wand et al., 2016; Chung &amp;amp; Zisserman, 2016a）。但是，所有已经存在的方法都只能执行单个词的分类，而不是句子层面的序列预测。研究已经表明，人类在更长的话语上的唇读表现会更好（Easton &amp;amp; Basala, 1982），这说明了在不明确的通信信道中获取时间背景的特征的重要性。受到这一观察的激励，我们提出了 LipNet——一种可以将可变长度的视频序列映射成文本的模型，其使用了时空卷积、一个 LSTM 循环网络和联结主义的时间分类损失（connectionist temporal classification loss），该模型完全是以端到端的形式训练的。我们充分利用我们的知识，LipNet 是第一个句子层面的唇读模型，其使用了一个单端到端的独立于说话人的深度模型来同时地学习时空视觉特征（spatiotemporal visual features）和一个序列模型。在 GRID 语料库上，LipNet 实现了 93.4% 的准确度，超过了经验丰富的人类唇读者和之前的 79.6% 的最佳准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1 引言&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;唇读在人类的交流和语音理解中发挥了很关键的作用，这被称为「麦格克效应（McGurk effect）」（McGurk &amp;amp; MacDonald, 1976），说的是当一个音素在一个人的说话视频中的配音是某个人说的另一个不同的音素时，听话人会感知到第三个不同的音素。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;唇读对人类来说是一项众所周知的艰难任务。除了嘴唇和有时候的舌头和牙齿，大多数唇读信号都是隐晦的，难以在没有语境的情况下分辨（Fisher, 1968; Woodward &amp;amp; Barber, 1960）。比如说，Fisher (1968) 为 23 个初始辅音音素的列表给出了 5 类视觉音素（visual phoneme，被称为 viseme），它们常常会在人们观察说话人的嘴唇时被混淆在一起。许多这些混淆都是非对称的，人们所观察到的最终辅音音素是相似的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以说，人类的唇读表现是很差的。听觉受损的人在有 30 个单音节词的有限子集上的准确度仅有 17±12%，在 30 个复合词上也只有 21±11%（Easton &amp;amp; Basala, 1982）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，实现唇读的自动化是一个很重要的目标。机器读唇器（machine lipreaders）有很大的实用潜力，比如可以应用于改进助听器、公共空间的静音听写、秘密对话、嘈杂环境中的语音识别、生物特征识别和默片电影处理。机器唇读是很困难的，因为需要从视频中提取时空特征（因为位置（position）和运动（motion）都很重要）。最近的深度学习方法试图通过端到端的方式提取这些特征。但是，所有的已有工作都只是执行单个词的分类，而非句子层面的序列预测（sentence-level sequence prediction）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇论文中，我们提出了 LipNet。就我们所知，这是第一个句子层面的唇读模型。就像现代的基于深度学习的自动语音识别（ASR）一样，LipNet 是以端到端的方式训练的，从而可以做出独立于说话人的句子层面的预测。我们的模型在字符层面上运行，使用了时空卷积神经网络（STCNN）、LSTM 和联结主义时间分类损失（CTC）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在仅有的一个公开的句子层面的数据集 GRID 语料库（Cooke et al., 2006）上的实验结果表明 LipNet 能达到 93.4% 的句子层面的词准确度。与此对应的，之前在这个任务上的独立于说话人的词分类版本的最佳结果是 79.6%（Wand et al., 2016）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们还将 LipNet 的表现和听觉受损的会读唇的人的表现进行了比较。平均来看，他们可以达到 52.3% 的准确度，LipNet 在相同句子上的表现是这个成绩的 1.78 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，通过应用显著性可视化技术（saliency visualisation techniques (Zeiler &amp;amp; Fergus, 2014; Simonyan et al., 2013)），我们解读了 LipNet 的学习行为，发现该模型会关注视频中在语音上重要的区域。此外，通过在音素层面上计算视觉音素（viseme）内和视觉音素间的混淆矩阵（confusion matrix），我们发现 LipNet 少量错误中的几乎所有都发生在视觉音素中，因为语境有时候不足以用于消除歧义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2 相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节介绍了其它在自动唇读研究上的工作，包含了自动唇读、使用深度学习进行分类、语音识别中的序列预测、唇读数据集四个方面。但由于篇幅限制，机器之心未对此节进行编译，详情请查看原论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorwWGEqaxNdxy4icC0DOfObvhRk3pmYpMuJcocNr7eltzHzW8Bj41eC4w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;表 1：现有的唇读数据集和对应数据集上已被报告出来的最佳准确度。Size 这一栏是指作者训练时所用的话语的数量。尽管 GRID 语料库包含了整个句子，但 Wand et al. (2016) 只考虑了更简单的预测单独的词的情况。LipNet 预测的是句子，因此可以利用时间语境来实现更高的准确度。短语层面的方法被当作简单的分类看待。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3 LipNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LipNet 是一种用于唇读的神经网络架构，其可以将不同长度的视频帧序列映射成文本序列，而且可以通过端到端的形式训练。在本节中，我们将描述 LipNet 的构建模块和架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.1 时空卷积&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积神经网络（CNN）包含了可在一张图像进行空间运算的堆叠的卷积（stacked convolutions），其可用于提升以图像为输入的目标识别等计算机视觉任务的表现（Krizhevsky et al., 2012）。一个从 C 信道到 C' 信道的基本 2D 卷积层（没有偏置（bias），以单位步长）的计算：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorYgIncWu9eiahbgK7HPDBgVuQojW450OlVOQqjE1QhBBdop92YQyy35w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于输入 x 和权重：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYToraXolMA2J5CictGpN0prakcHdHXrlO5oyD1Qkg0zs2qMR11NplwRJkuw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中我们定义当 i,j 在范围之外时，xcij=0.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;时空卷积神经网络（STCNN）可以通过在时间和空间维度上进行卷积运算来处理视频数据：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorvib5eaibsMlk0CQ6VmkoRKSjSJX9UCdpJyibjWmVvtgbW7H8IAPwmRhLw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.2 长短期记忆&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长短期记忆（LSTM）（Hochreiter &amp;amp; Schmidhuber, 1997）是一类在早期的循环神经网络（RNN）上改进的 RNN，其加入了单元（cell）和门（gate）以在更多的时间步骤上传播信息和学习控制这些信息流。我们使用了带有遗忘门（forget gates）的标准 LSTM 形式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorTsialkTZaW7LKhomtfR37dGjy9u3Wmh8iaMrxRDccqf9KBV1tCvPPdsw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 z := {z1, . . . , zT } 是 LSTM 的输入序列，是指元素之间的乘法（element-wise multiplication）, sigm(r) = 1/(1 + exp(−r))。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们使用了 Graves &amp;amp; Schmidhuber (2005) 介绍的双向 LSTM（Bi-LSTM）：一个 LSTM 映射&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTor3DvleogNmia67cic3SDEblejpdp7OzibhCyFURO6pFNZHdwksQC2BmDZA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;，另一个是&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTore5pAnibTT6cY9YFq417X03Wfpicqy8xYzI175RDR1MQCNJcIQhHHk49Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;，然后&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorQQghC6grTIBpW9GMibSpiaicbUfpEHRGhiaia2O4YF0nxNrT5MFzAjemsfw/0?wx_fmt=png"/&gt;&lt;br/&gt;，该 Bi-LSTM 可确保 ht 在所有的 t' 上都依赖于 zt'。为了参数化一个在序列上的分布，在时间步骤 t，让 p(ut|z) = softmax(mlp(ht;Wmlp))，其中 mlp 是一个权重为 Wmlp 的前向网络。然后我们可以将长度 T 的序列上的分布定义为&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorT6Nbo7icWZDl8gxz299ugUS2m1bGhDuvuMg57I7ZiaGNzYLvyaRcJh5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;，其中 T 由该 LSTM 的输入 z 确定。在 LipNet 中，z 是该 STCNN 的输出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.3 联结主义的时间分类&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;联结主义的时间分类损失（onnectionist temporal classification (CTC) loss）（Graves et al., 2006）已经在现代的语音识别领域得到了广泛的应用，因为这让我们不再需要将训练数据中的输入和目标输出对齐（Amodei et al., 2015; Graves &amp;amp; Jaitly, 2014; Maas et al., 2015）。给定一个在 token 类（词汇）上输出一个离散分布序列的模型——该 token 类使用了一个特殊的「空白（blank）」token 进行增强，CTC 通过在所有定义为等价一个序列的序列上进行边缘化而计算该序列的概率。这可以移除对对齐（alignment）的需求，还同时能解决可变长度的序列。用 V 表示该模型在其输出（词汇）的单个时间步骤上进行分类的 token 集，而空白增强过的词汇&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYToriafUPNzMtz2bE0dXqm2JVT9VVRlgUibd6P5neSNM1PsxQwGBEk0wxHhw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中空格符号表示 CTC 的空白。定义函数 B : V˜ ∗ → V ∗，给定 V˜ 上的一个字符串，删除相邻的重复字符并移除空白 token。对于一个标签序列 y ∈ V ∗，CTC 定义&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTor5pq6uP2wWuXOWJDYLKL0E7xvS711B61WCgibXlkt0RB2IoVY8xLgFWg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 T 是该序列模型中时间步骤的数量。比如，如果 T=3，CTC 定义字符串「am」的概率为&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorcPynj8zWyJThbtEavQ1Y9PXMdpkS2KPS7dn5ZmgvA2Bw3PlmoiaiaS5w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个和可以通过动态编程（dynamic programming）有效地计算出来，让我们可以执行最大似然（maximum likelihood）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTor4kuAbicannF6IahRgicQRfJG3Htib8ZN8TpaZkf5Ks3cbwF8me2uF8OLg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 1：LipNet 架构。一个 T 帧的序列被用作输入，被一个 3 层的 STCNN 处理，其中每一层后面都有一个空间池化层（spatial max-pooling layer）。提取出的特征是时间上上采样（up-sample）的，并会被一个 Bi-LSTM 处理；LSTM 输出的每一个时间步骤会由一个 2 层前向网络和一个 softmax 处理。这个端到端的模型是用 CTC 训练的。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.4 LipNet 架构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图 1 给出了 LipNet 的架构，其始于 3×（时空卷积、信道上的 dropout、空间最大池化），后面跟随时间维度中的上采样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为人类每秒钟大约能发出 7 个音素，而且因为 LipNet 是在字符层面上工作的，所以我们总结得到：每秒输出 25 个 token（视频的平均帧率）对 CTC 来说太受限了。时间上采样（temporal up-sampling）允许在字符输出之间有更多的空格。当许多词有完全相同的连续字符时，这个问题会加剧，因为他们之间需要一个 CTC 空白。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后，该时间上采样后面跟随一个 Bi-LSTM。该 Bi-LSTM 对 STCNN 输出的有效进一步会聚是至关重要的。最后在每一个时间步骤上应用一个前向网络，后面跟随一个使用了 CTC 空白和 CTC 损失在词汇上增强了的 softmax。所有的层都是用了修正线性单元（ReLU）激活函数。超参数等更多细节可参阅附录 A 的表 3.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4 唇读评估&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这一节，我们将在 GRID 上评估 LipNet。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.1 数据增强&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预处理（Preprocessing）:GRID 语料库包含 34 个主题，每一个主题包含了 1000 个句子。说话人 21 的视频缺失，其它还有一些有所损坏或空白，最后剩下了 32839 个可用视频。我们使用了两个男性说话人（1 和 2）与两个女性说话人（20 和 22）进行评估（3986 个视频），剩下的都用于训练（28853 个视频）。所有的视频都长 3 秒，帧率为 25 fps. 这些视频使用 DLib 面部检测器和带有 68 个 landmark 的 iBug 面部形状预测器进行了处理。使用这些 landmark，我们应用了一个放射变换（affine transformation）来提取每帧中以嘴为中心的 100×50 像素大小的区域。我们将整个训练集上对 RGB 信道进行了标准化以具备零均值和单位方差。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;增强（Augmentation）：我们使用简单的变换来增强数据集以减少过拟合，得到了多 15.6 倍的训练数据。首先，我们在正常的和水平镜像的图像序列上进行训练。然后，因为该数据集提供了每个句子视频中的词开始和结束时间，所以我们使用单独的词的视频片段作为额外的训练实例增强了句子层面的训练数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.2 基线&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了评估 LipNet，我们将其表现和三位懂得读唇的听觉受损者以及两个由最近的最佳成果启发的 ablation model（Chung &amp;amp; Zisserman, 2016a; Wand et al., 2016）的表现进行了比较。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;听觉受损者：这个基线是由牛津学生残疾人社区（Oxford Students』 Disability Community）的三位成员得到的。在被介绍了 GRID 语料库的语法之后，他们从训练数据集中观察了 10 分钟带有注释的视频，然后再从评估数据集中注释了 300 个随机视频。当不确定时，他们可以选择觉得最有可能的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Baseline-LSTM：使用句子层面的 LipNet 配置，我们复制了之前 GRID 语料库当时（Wand et al., 2016）的模型架构。参看附录 A 了解更多实现细节。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Baseline-2D：基于 LipNet 架构，我们使用仅空间的卷积替代了 STCNN，这类似于 Chung &amp;amp; Zisserman (2016a) 的那些。值得一提的是，和我们用 LipNet 观察到的结果相反，Chung &amp;amp; Zisserman (2016a) 报告他们的 STCNN 在他们的两个数据集上比他们的 2D 架构的性能差分别 14% 和 31%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.3 性能评估&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorruMykGBuc46KvG19LYnQ2bBEmWu9RFCu2ncml5v7RicMWnmL6xqjdJA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;表 2：LipNet 相比于基线的性能&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;表 2 总结了相比于基线的性能。根据文献，人类唇读者的准确率大约是 20%（Easton &amp;amp; Basala, 1982; Hilder et al., 2009）。如预料的一样，GRID 语料库中固定的句子结构和每个位置有限的词子集有助于对语境的使用，能提升表现。这三位听觉受损者的词错率（WER）分别为 57.3%、50.4% 和 35.5%，平均词错率为 47.7%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.4 学到的表征&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这一节中，我们从语音学的角度分析了 LipNet 的学习到的表征。首先，我们创造了显著性可视化（saliency visualisations (Simonyan et al., 2013; Zeiler &amp;amp; Fergus, 2014)）来说明 LipNet 所学的重点区域。特别地，我们向该模型送入了一个输入，并贪婪地解码了一个输出序列，得出了一个 CTC 对齐&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorh1udLfCvsBtfWc414YL2ha4GoGgzYe5lVxSX1s6tq9PHC9YtK9ZrDA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（遵循 3.2 和 3.3 节的符号）。然后，我们计算了&amp;nbsp;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorgnicY6NaBpDtIdtnbv5gbpd0icHMmNiclS5EyGk3jEiagILEU8a078KpVw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的梯度，并考虑了输入视频帧序列，但和 Simonyan et al. (2013) 不一样，我们使用了有引导的反向传播（guided backpropagation (Springenberg et al., 2014)）。第二，我们训练 LipNet 预测的是 ARPAbet 音素，而不是字符，这样可以使用视觉音素（viseme）内和视觉音素间的混淆矩阵（confusion matrix）来分析视觉音素。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.4.1 显著性地图（Saliency Maps）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们应用显著性可视化技术（saliency visualisation techniques）来解读 LipNet 学习到的行为，结果表明该模型会重点关注视频中在语音方面重要的区域。特别地，在图 2 中，我们基于 Ashby (2013) 为说话人 25 的词 please 和 lay 分析了两个显著性可视化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorLtzJEwibDFpAEFpUfxPichI2PMyP8KlqCNQ998RC7Fx7jnalZtWS96icw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 2：词 (a) please 和 (b) lay 的显著性地图，由向输入的反向传播产生，展示了 LipNet 学会关注的地方。图中的转录由贪婪 CTC 解码（greedy CTC decoding）给出。CTC 空白由空格符号表示。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.4.2 视觉音素（viseme）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据 DeLand（1931）和 Fisher（1968），Alexander Graham Bell 首次假设给定说话人的多音素可被视觉地识别。这在后来得到了证实，这也带来了视觉音素的概念，即一个音素的视觉对应（Woodward &amp;amp; Barber, 1960; Fisher, 1968）。为了我们的分析，我们使用了 Neti et al. (2000) 中音素到视觉音素的映射，将视觉音素聚类成了以下类别：Lip-rounding based vowels (V)、Alveolar-semivowels (A),、Alveolar-fricatives (B)、Alveolar (C)、Palato-alveolar (D)、Bilabial (E), Dental (F)、Labio-dental (G) 和 Velar (H)。完整映射可参看附录 A 中的表 4. GRID 包含了 ARPAbet 的 39 个音素中的 31 个。我们计算了音素之间的混淆矩阵（confusion matrix），然后按照 Neti et al. (2000) 将音素分组成了视觉音素聚类。图 3 表示了 3 个最容易混淆的视觉音素类别，以及视觉音素类别之间的混淆。完整的音素混淆矩阵参看附录 B 图 4.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorpjJIBvKYzpvoOESW6TeHjRxU4FUehdVEsVk79TGXPGaEqwzibSqLemw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 3：视觉音素内和视觉音素间的混淆矩阵，描绘了 3 个最容易混淆的类别，以及视觉音素聚类之间的混淆。颜色进行了行规范化（row-normalised）以强调误差。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5. 结论&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们提出了 LipNet，它是第一个将深度学习应用于模型的端到端学习的模型，可以将说话者的嘴唇的图像帧序列映射到整个句子上。这个端到端的模型在预测句子前不再需要将视频拆分成词。LipNet 需要的既不是人工编入的时空视觉特征，也不是一个单独训练的序列模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们的实证评估表明了 时空特征提取和高效的时间聚集（temporal aggregation）的重要性，确认了 Easton 和 Basala 在 1982 年提出的假说（1982）。此外，LipNet 大大超越了人类的读唇水平的基线，比人类水平高出 7.2 倍，WER 达到了 6.6%，比现在 GRID 数据集中最好的词水平（Wand 等人，2016）还要低 3 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然 LipNet 在实证上取得了成功，Amodei 等人在 2015 年发表的深度语音识别论文显示，只有更多的数据才能让表现提升。在未来的研究中，我们希望通过将 LipNet 应用到更大的数据集中来证明这一点，如由 Chung 和 Zisserman 等人在 2016 年收集的这种数据集的句子水平变体（sentence-level variant）。像默写这样的应用只能使用视频数据。然而，为了扩展 LipNet 的潜在应用，我们能将这种方法应用到一种联合训练的视听语音识别模型上，其中视觉输入会在嘈杂的环境中提升鲁棒性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;致谢、参考文献及附录（略）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 06 Nov 2016 16:45:06 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | Nature发布计算和理论神经科学特刊：剖析机器学习推动下的神经科学进展</title>
      <link>http://www.iwgc.cn/link/3388469</link>
      <description>&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Nature Neuroscience&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：蒋思源、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Nature Neuroscience 近日推出了一个关注应用于神经科学的计算驱动的和理论驱动的方法（computation- and theory-driven approaches）的特刊，介绍了许多生物物理模型和机械式的模型。相关论文可点击「阅读原文」查看。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;神经科学研究的进展无法脱离数据收集，同时也需复杂的方法将这些数据组装和合成到更广泛的框架中。理论神经科学，加上必要的计算技术，能确保我们的努力不仅仅是大规模的收集工作。本期，&lt;em&gt;Nature Neuroscience&lt;/em&gt; 呈现了一批论文综述与观点讨论，包含了一系列当下该领域突出的思考，从神经回路和网络到认知评估和精神疾病。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经机制的深刻见解都是来源于基于理论的研究。描述动作电位传播的 Hodgkin-Huxley 模型，Hebbian-based 的可塑性规则，Barlow 的有效编码假说和 Marr 的三层分析模型都是强有力的证明。然而尽管取得了这些成就，但生物学仍然存在实验和理论之间的脱节，这些在物理学上简直无法想象。技术的进步使得生物学在现代神经科学理论与实验的分离更有先见之明。正是这种大量数据生成的能力需要敏锐的实验设计和聚焦问题以获得理解。由于在没有理论基础的条件下就对这一问题的概念化，实验神经科学不过就是多个观察的简单堆叠，就像宇宙建立在海龟背上那种传说中的模型毫无意义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然神经科学出版了一期关于计算神经科学的特刊，之前已经出版过三期的特刊。第一期是关于这一领域几十年主流的历史评论中不被看好但非常重要的内容。接下来两个合刊分别介绍了一些综述与主要的研究论文。现在我们处在一个临界点，即该领域已经成熟到可以做一期新的特刊，由综述和观点还有一篇评论构成，强调了了计算和理论神经科学近期的一些进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实验创新几乎已经蔓延到神经科学的每一个分支，而每个问题上都有了更多的数据。在《理论神经科学的关键时刻，概念与技术的进步》（Conceptual and technical advances define a key moment for theoretical neuroscience）论文中，Churchland 和 Abbott 推荐一个称之为大帐篷政策（big tent approach）的方法，计算和理论进入了从原始数据分析到建立详细的机制和生物物理模型所有阶段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里有一个详细的生物物理建模的例子，围绕带有相等的兴奋性和抑制性输入的网络建模，实验性证据也支持这种突触平衡（synaptic balance）的存在。在《高效的代码与均衡网络》（Efficient codes and balanced networks）论文中，Denève 和 Machens 论述了平衡网络模型的最新进展，他们特别强调这种网络能够支持高效的编码。大多数网络模型的实例化是以连续时间系统为背景的，然而我们知道神经元主要是是通过离散的尖峰脉冲传递信息的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在《建立尖峰神经模型的功能网络》（Building functional networks of spiking model neurons）的论文中，Abbott, DePasquale 和 Memmesheimer 讨论了当前桥接模拟-数字鸿沟的方法。我们总希望能逆向运行而不是施加一个网络结构来匹配当前的尖峰输出。人口记录和成像现在已经很平常了，但有可能推断出这种生理机制和可变性的来源吗？在《神经相关的状态依从性机制》（The mechanics of state-dependent neural correlations）中，Doiron 和同事们论述了跟踪大脑状态中的神经关联变化如何能暗示潜在的因果要素。但考虑记忆理论的时候，不同的神经状态视图就变得很重要了，也就是我们该如何如何构建并且维持稳定的表征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Chaudhuri 和 Fiete 在《记忆的计算法则》（Computational principles of memory）论文中关注记忆系统所必要的法则。他们论述了相关的网络构架，潜在的生物物理过程，噪音的鲁棒性（robustness to noise），信息能力和编码策略，并承认其可以和计算机科学相媲美。计算机科学，特别是用计算机视觉技术构建的目标识别系统，又兜回来告诉我们如何建立人类视觉感知的模型。在《利用目标驱动的深度学习模型来理解感觉皮层》（Using goal-driven deep learning models to understand sensory cortex）论文的观点中，Yamins 和 DiCarlo 介绍了目标驱动的深层神经网络在解释感官处理上成功的一些原因，并提出可以做出类似的进展来超越感知系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经系统层级上升后，我们就可以做出感知上（perceptual）或者形式更加公开的决定。在《信心与确定：不同目标的不同概率量》（Confidence and certainty: distinct probabilistic quantities for different goals）这篇论文中，Pouget，Drugowitsch 和 Kepecs 提出了决策确定性（decision certainty）与决策信心（decision confidence）之间的绝对区分。当大脑在嘈杂的环境下工作时（「当」指的是「在任何时候」），大脑必须计算出概率分布。作者认为，确定性应该是指所有概率决策变量的编码，而信心应被具体定义为一个决策正确的概率。也许只有时间才能决定是否应该对它们的建议有信心。有了更好的感知模型后，当系统出现故障时，行为和决策制定也能潜在地让我们诊断和修改对策。Huys，Maia 和 Frank 在 Computational psychiatry as a bridge from neuroscience to clinical applications 这篇论文中对计算精神病学的新兴领域做了一个综述，详细介绍了机器学习计算应用以及基于理论的机器模型这两种方式在疾病分类和治疗上综合运用的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本期杂志中涉及的话题涉及的范围很广，大多数重点的内容都是发表在其他期刊上的文章。事实上，我们期刊上神经计算文章的数量最近几年一直在稳步增长。从谷歌搜索的 Google Trends 上看，对计算神经科学的普遍兴趣得到了初步支持（也许是确认偏误（conformation bias）？）过去 5 年中「computational neuroscience」的搜索量稳步增长，其增长可以在同一时期「光遗传学」的搜索频率也有类似的增长中得到佐证。计算神经科学增长或许不会迎来拐点，因为对该领域主题特定的期刊（如计算神经科学期刊。Journal of Computational Neuroscience）以及博士课程的搜索也会将这个词汇的搜索频率推向在峰值。有趣的是，到目前为止，「认知神经科学」的搜索数量远远超过了计算神经科学，但是搜索频率也会有季节性波动，每年秋天大学新生入学时搜索频率会波动性增长。现在每所大学都应该让计算成为神经生物学课程的核心部分。好处是在这 5 年的时间里，我们不再需要通过证明计算与神经科学的相关性来吸引大家对神经计算研究的关注。最大的希望是能促使神经计算在生物学有更好的理解，以及实现更好疾病治疗。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 06 Nov 2016 16:45:06 +0800</pubDate>
    </item>
    <item>
      <title>入门 |  智能时代每个人都应该了解：什么是深度学习？</title>
      <link>http://www.iwgc.cn/link/3388470</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自algorithmia&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：武竞&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文通过介绍人工智能、深度学习和机器学习三者之间的关系来阐明深度学习及其重要性。传统的机器学习智能处理一定量数据，而对于深度学习来说，数据越多，深度学习的技术表现越好。此外该文还介绍了深度学习的几大框架以及优秀在线课程和书籍。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要理解深度学习是什么，我们首先需要了解深度学习与机器学习、神经网络和人工智能之间的关系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;表示这种关系的最好方法是将它们用同心圆可视化：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9RD7kdS9x2m8NcicfRrYToryEzTxr2aibJ55JnZFImXwlBXDEZw7t4ia6jCEgIz0Rs8VpCxCicaZIHwg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最外面的环是人工智能（使用计算机推理）。里面的一层是机器学习。人工神经网络和深度学习在最中心。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广义地说，深度学习是人工神经网络的一个更平易近人的名称。深度学习的「深」是指网络的深度。而一个人工神经网络也可以非常浅。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络的灵感来自大脑皮层的结构。最基本的层次是感知器（perceptron），用数学表示的神经元。与大脑皮层中的结构一样，神经网络可以有几层相互连接的感知器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一层是输入层。该层中的每个节点传入一个输入，然后将节点的输出作为输入传递给下一层中的每个节点。在同一层中的节点之间通常没有连接，最后一层输出处理后的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们称中间部分为隐藏层。这些神经元没有与外部的连接（如：输入和输出），并且只由上一层的节点激活。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTorWiaWOcicwucWH7DjmG1HicaIwfWsQjdmlUNLq5AzoQr5FnUQvfa5iabaOg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;来源：Michael A. Nielsen,「Neural Networks and Deep Learning」&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们认为，深层学习是通过学习神经网络，并利用多层抽象来解决模式识别问题的技术。在 20 世纪 80 年代，由于计算成本和数据量的限制，大多数神经网络只有一层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习被认为是人工智能的一个分支，而深度学习是一种特殊的机器学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习包含了计算机智能，它事先并不知道答案。相反，程序通过运行训练数据来验证其尝试，并根据是否成功相应地修改其方法。机器学习涉及多个学科，从软件工程和计算机科学到统计方法和线性代数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有两大类机器学习方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有监督学习&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;无监督学习&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在有监督学习中，机器学习算法使用有标签的数据集来训练规则。这需要大量的数据和时间，因为数据需要手工标记。有监督学习是分类和回归问题的绝佳选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如，假设我们在运营一家公司，并希望了解奖金对员工留存率的影响。如果我们有历史数据——即员工奖金金额和任期——我们可以使用有监督机器学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;无监督学习没有任何预定义或相应的答案。无监督学习的目标是找出数据中隐含的模式。它通常用于聚类和关联的任务，如按行为将客户分组。亚马逊的「购买此商品的客户也买了...」建议是一类关联任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然有监督学习可能是有用的，但我们经常不得不诉诸无监督学习。深度学习已被证明是一种有效的无监督学习技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;为什么深度学习很重要？&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9RD7kdS9x2m8NcicfRrYTor2vHiaPEzf8qu82zENY6eMuNvYOkDt23K3qfiaH1ZPptNpCV2zmOicafCQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长期以来，计算机就有识别图像中特征的技术。但结果并不总是好的。机器视觉一直是深度学习的主要受益者。使用深度学习的机器视觉在许多图像识别的任务上可以与人类媲美。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook 通过使用深度学习在照片面部识别领域取得了巨大的成功。这不仅仅是一个微小的改进，而是一个转折：「当被问及两个不熟悉的面部照片是否显示同一个人时，人类有 97.53％的正确率。由 Facebook 研究人员开发的新软件可以在相同的挑战下有 97.25％ 的正确率，无论照明的变化或者图片中的人是否直接面对相机。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别是另一个受深度学习影响的领域。口头表达极其丰富并且语义模棱两可。百度——中国领先的搜索引擎之一——已经开发了一种语音识别系统，能比人类更快更准确地在手机上生成文本，不管是英语还是普通话。特别令人着迷的是，概括这两种语言不需要额外的设计工作：「过去，人们将中文和英文看作两种截然不同的语言，因此需要设计非常不同的特征，」百度首席科学家 Andrew Ng 说，「现在的学习算法普适性强，你仅仅需要让计算机学习就行。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌现在正使用深度学习来优化公司数据中心的能源消耗。他们将冷却能源需求降低了 40％。这意味着公司的电能使用效率提高了 15％，节省了数亿美元。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度学习微服务（Deep Learning Microservices）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是一些使用深入学习的微服务的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图片标签（Illustration Tagger）。这是应用 Illustr2Vec 的一个例子，这个微服务可以为图片打上安全性、存疑性、评分、版权以及一般类别的标签，以了解图像中的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepFilter 是一种将图像应用于艺术滤镜的图片风格转换服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;年龄分类器使用面部检测来确定照片中的人的年龄。Places 365 分类器使用预训练的 CNN 并基于 Places 图像数据集（B. Zhou, et al., 2016）来识别图像中特定的位置，例如庭院、药店、酒店房间、冰川、山脉等。最后是 InceptionNet，可以使用谷歌的 TensorFlow 直接应用。它输入一个图像（如一辆汽车），并返回前 5 个与图像相关的预测类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;开源深度学习框架&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9RD7kdS9x2m8NcicfRrYToryGdCianliclVfbhYWibG3W9NpTZSZcqy1OXpo3atrt5fcF9NNGVA0K35A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习可以通过一些开源项目来实现。一些最流行的技术包括（但不限于）：Deeplearning4j（DL4j），Theano，Torch，TensorFlow 和 Caffe。决定使用哪个框架的因素有：他们的技术目标，还有是否为低级别、是否作为学术研究或是否以应用程序为导向。以下是每个的概述：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DL4J:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于 JVM&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分布式&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与 Hadoop 和 Spark 集成&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Theano:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在学术界很受欢迎&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相对的入门级&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有 Python 和 Numpy 接口&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Torch:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于 Lua&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Facebook 和 Twitter 使用的内部版本&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;包含预训练模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌撰写的 Theano 继承版本&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有 Python 和 Numpy 接口&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;高度多线程&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于某些问题集可能运行速度稍慢&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Caffe:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;非通用。专注于机器视觉问题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 C++中实现，速度非常快&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不容易扩展&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有 Python 接口&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度学习在线课程&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌和 Udacity 合作开发了免费在线深度学习课程（https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009），这也是 Udacity 机器学习工程师 Nano 学位的构成部分。该课程面向经验丰富的软件开发人员，针对希望在机器学习或其子专业有所专长的人员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个选择是非常受欢迎的 Andrew Ng 关于机器学习课程（https://www.coursera.org/learn/machine-learning），由 Coursera 和 Stanford 主办。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度学习相关书籍&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然许多深度学习课程对学习者的教育背景要求相当高，但 Grokking 深度学习（Grokking Deep Learning）这本书并非如此。用他们的话说：「如果你高中数学及格，并能熟练使用 Python，我就可以教你深度学习。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一本流行的书是 Deep Learning Book，内容正如其名。这是一本自下而上内容丰富的书，因为它涵盖了深度学习所需的所有数学知识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习简要（Deep Learning Simplified）是一个很棒的 YouTube 视频系列，将深度学习分解成日常的词语和概念。下面是该系列第一个视频：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=b0343ceu60x&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 06 Nov 2016 16:45:06 +0800</pubDate>
    </item>
    <item>
      <title>开源| 华为开源streamDM：用于Spark Streaming的数据挖掘软件</title>
      <link>http://www.iwgc.cn/link/3388471</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自huawei-noah.github.io&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px; text-align: justify; white-space: pre-wrap;"&gt;华为诺亚方舟实验室开源 &lt;/em&gt;stream DM ，是一种使用 Spark Streaming 挖掘大数据的开源软件。Stream DM 是 Apache Software License v2.0 许可下的开源软件。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;大数据流学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大数据流学习（Big Data stream learning）比批量或离线学习更富有挑战性，因为数据在流动的过程中不太可能保持同一种分布。而且，数据流中的每一个样本只能被处理一次，否则它们就需要占用内存进行总结，同时该学习算法也必须非常高效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Spark Streaming&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Spark Streaming（https://spark.apache.org/streaming/） 是核心 Spark API 的一个扩展，它能让多个源的数据流处理成为可能。Spark 是一个可扩展可编程的框架，用于大规模分布式数据集（也称为弹性分布式数据集（RDD））处理。Spark Streaming 接收输入的数据流后将数据分批，再由 Spark 引擎处理，生成结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Spark Streaming 数据被编成一个 DStreams 序列，内在地表示成一个 RDD 序列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;包含以下方法：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在第一次开放的 StreamDM 中，我们部署了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SGD Learner (http://huawei-noah.github.io/streamDM/docs/SGD.html) 和 Perceptron (http://huawei-noah.github.io/streamDM/docs/SGD.html#perceptron)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Naive Bayes (http://huawei-noah.github.io/streamDM/docs/NB.html)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CluStream (http://huawei-noah.github.io/streamDM/docs/CluStream.html)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hoeffding Decision Trees (http://huawei-noah.github.io/streamDM/docs/HDT.html)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bagging (http://huawei-noah.github.io/streamDM/docs/Bagging.html)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Stream KM++ (http://huawei-noah.github.io/streamDM/docs/StreamKM.html)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们部署了以下数据生成器&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（http://huawei-noah.github.io/streamDM/docs/generators.html）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HyperplaneGenerator&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RandomTreeGenerator&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RandomRBFGenerator&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RandomRBFEventsGenerator&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们部署了 SampleDataWriter：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（http://huawei-noah.github.io/streamDM/docs/SampleDataWriter.html），它可以调取数据生成器创建样本数据用于模拟和测试。&lt;/span&gt;&lt;span&gt;后面我们将计划开放：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;分类：随机森林&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;回归：Hoeffding 回归树，Bagging，随机森林&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;聚类：Clustree, DenStream&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Frequent Itemset Miner：IncMine, IncSecMine&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;下一步&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了快速介绍一下 StreamDM 的运行，请打开 Getting Started （http://huawei-noah.github.io/streamDM/docs/GettingStarted.html）文件。StreamDM Programming Guide (http://huawei-noah.github.io/streamDM/docs/Programming.html) 展示了 StreamDM 的细节。完整的 API 文档，可以参考这里：http://huawei-noah.github.io/streamDM/api/index.html。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 06 Nov 2016 16:45:06 +0800</pubDate>
    </item>
    <item>
      <title>重磅论文 | 解析深度卷积神经网络的14种设计模式（附下载）</title>
      <link>http://www.iwgc.cn/link/3377505</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、武竞、李泽南、蒋思源、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这篇论文的作者是来自美国海军研究实验室的 Leslie N. Smith 和来自美国马里兰大学的 Nicholay Topin，他们在本论文中总结了深度卷积神经网络的 14 种设计模式；其中包括：1. 架构结构遵循应用；2. 扩增路径；3. 努力实现简洁；4. 增加对称性；5. 金字塔形状；6. 用训练数据覆盖问题空间；7. 过训练；8. 增量特征构造；9. 规范层输入；10. 可用资源决定网络深度；11. 转换输入；12. 求和连接；13. 下采样过渡；14. 用于竞争的 MaxOut。该论文已被提交到了 ICLR 2017。论文原文可点击文末「阅读原文」下载。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习领域近来的研究已经产出了大量的新架构。与此同时，也有越来越多的团队在将深度学习应用到新的应用和问题上。这些团队中的许多都可能是由没有经验的深度学习实践者构成的，他们可能会对让人眼花缭乱的架构选择感到困惑，因此会选择去使用一个更古老的架构，如 AlexNet。在这里，我们尝试挖掘近来深度学习研究中包含的集体知识（collective knowledge）以发现设计神经网络架构的基本原理，从而帮助弥合这一差距。此外，我们还描述了几种架构创新，其中包括 Fractal of FractalNet、Stagewise Boosting Networks 和 Taylor Series Networks（我们的 Caffe 代码和 prototxt 文件将会在被 ICLR 接受后公开）。我们希望这项初步的工作能够激励进一步的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.引言&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近，关于新型神经网络架构的文章已经出现了很多，特别是关于残差网络（Residual Network）的，比如 He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b)。这促使我们在一个更高的层面上来看待这些架构——将这些架构看作是普遍设计原理的潜在来源。这是相当重要的，因为现在有许多没有经验的实践者在想办法将深度学习应用到不同的新应用上。缺乏指导会导致深度学习实践新手忽视最新的研究而选择 AlexNet（或一些类似的标准架构），不管其是否合适他们的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种研究的极大丰富也是一个机会：可以确认能为特定背景的应用带来好处的元素。我们提出了一些基本的问题：深度网络设计的普遍原理是否存在？这些原理可以从深度学习的集体知识（collective knowledge）中挖掘出来吗？哪些架构选择在哪些特定的背景（context）中效果最好？哪些架构或部分架构看起来很简洁优美？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设计模式（design pattern）的概念最早由 Christopher Alexander (Alexander (1979)) 引入到建筑物和城镇的结构设计上。Alexander 写道：一种永恒的架构可以一直存在，这种质量可以通过基于普遍原理进行设计而实现。这种设计模式的基础是它们能在给定的背景中解决力量的冲突，并实现类似于自然生态平衡那样的均衡。设计模式既是高度特定的（使得它们可以很清楚地遵循），也是灵活的（让它们可被适配到不同的环境和情景中）。受 Alexander 的工作的启发，「gang of four」（Gamma et al. (1995)）将设计模式的概念应用到了面向对象的软件的架构设计上。这本经典的计算机科学书籍描述了 23 种可以用来解决软件设计中普遍存在的问题的模式，例如「需求总是在改变」。我们受到了之前这些在架构上的工作的启发，决定阐释神经网络架构的可能设计模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设计模式可以提供普遍性的指导原则，在这里我们首先要定义用于神经网络架构的设计模式。整体而言，要为所有的神经网络和所有的应用定义设计原理是一项巨大的任务，所以我们将这篇论文的范围限制在了卷积神经网络（CNN）及其基本的图像分类应用上。但是，我们认识到架构必须依赖于具备我们的第一设计模式的应用——设计模式 1：架构结构遵循应用；但相关的细节留待未来解决。此外，这些原理让我们可以发现已有研究中的一些缺陷和阐释全新的架构特征，比如 freeze-drop-path（参见 4.1 节）。这里阐述的经验法则可能对有经验的和新手的实践者都有价值。另外，我们真心希望这项初步的研究能够成为其它研究的垫脚石，能帮助其他人发现和分享其它深度学习设计模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.相关工作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本节介绍和总结了其它一些神经网络架构上的相关研究工作，但由于篇幅限制，机器之心未对此节进行编译，详情请查看原论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.设计模式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就我们所知，提供合适架构选择的指导与理解的文献资料很少。《Neural Networks: Tricks of the Trade》(Orr &amp;amp; Muller, ¨ 2003) 这本书包含了网络模型推荐，但没有参考过去几年的大量研究。与这项工作最接近的可能是 Szegedy et al. (2015b)，作者在其中描述了几种基于他们自己的经验的设计原理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们仔细审阅了文献以提取出它们的共性并将它们的设计归结成了基本的元素——这些元素也许可被认为是设计模式。在审阅文献的过程中，我们似乎很清楚一些设计似乎是简洁优雅的，而另一些则没那么简洁优雅。在这里，我们将首先描述一些高层面的设计模式，然后再提出一些更为详细的设计模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.1 高层面的架构设计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些研究者已经指出 ImageNet 挑战赛 (Russakovsky et al., 2015) 的获胜者在不断使用越来越深度的网络（参见：Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan &amp;amp; Zisserman (2014), He et al. (2015)）。另外在 ImageNet 挑战赛上很明显的一点是：将通过网络的路径的数量倍增是最近的一个趋势；如果看一看 AlexNet 到 Inception 到 ResNets 的演变，就能明显看到这个趋势。比如说，Veit et al. (2016) 表明 ResNets 可被看作是带有不同长度的网络的指数集合（exponential ensemble）。这引出了设计模式 2：扩增路径。开发者可以通过将多个分支包含在架构中来实现。最近的例子包括 FractalNet (Larsson et al. 2016)、Xception (Chollet 2016) 和决策森林卷积网络（Decision Forest Convolutional Networks (Ioannou et al. 2016)）。我们甚至可以更进一步预测今年的 ImageNet 获胜者也还会增加他们的架构中的分支数量，而不是继续增加深度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科学家们已经拥抱简洁性/简约性（simplicity/parsimony）几个世纪了。简洁性的例子可参考论文「Striving for Simplicity」(Springenberg et al. 2014)，其使用更少类型的单元实现了当时最佳的结果。我们将其加为设计模式 3：努力实现简洁——使用更少类型的层以保持网络尽可能简单。我们还在 FractalNet (Larsson et al. 2016) 设计中注意到了一种特定程度的简洁性，我们将其归功于其结构的对称性。架构的对称性（architectural symmetry）通常被看作是美丽和质量的标志，所以我们在这里得到了设计模式 4：增加对称性。除了对称性以外，FractalNets 还遵循了「扩增路径」设计模式，所以它是我们第 4 节的实验的基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了理解相关的力量，考察权衡是设计模式的关键元素。一种基本的权衡是最大化表征的力量 vs 冗余的和非区分的信息的压缩。这普遍存在于所有卷积神经网络中，从数据到最后的卷积层，激活（activation）被下采样（downsample）并且信道数量增加。一个例子是深度金字塔残差网络（Deep Pyramidal Residual Networks (Han et al. (2016))）。这让我们得到了设计模式 5：金字塔形状，其中在整个架构中应该有一次整体的平滑的下采样，而且该下采样应该与信道数量的增长结合起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习中另一个重要的权衡是：训练精度 vs 网络泛化到其从未见过的案例的能力。泛化的能力是深度神经网络的一个很重要的性质。一种提升泛化的方法是设计模式 6：用训练数据覆盖问题空间（Ratner et al. 2016, Hu et al. 2016, Wong et al. 2016, Johnson-Roberson et al. 2016）。这让训练精度可以直接提升测试精度。此外，正则化（regularization）常被用于提升泛化。正则化包括 dropout (Srivastava et al. 2014a) 和 drop-path (Huang et al. 2016b) 等方法。正如 Srivastava et al. 2014b 指出的那样，dropout 可通过向架构中注入噪声来提升泛化能力。我们将在训练过程使用正则化技术和谨慎的噪声注入可以提升泛化（Srivastava et al. 2014b, Gulcehre et al. 2016）的结论归结为设计模式 7：过训练（over-training）。过训练包含网络在一个更艰难的问题上训练的任何训练方法——该问题的难度超过了必要，因此在更容易的推理情况中的表现可以得到提升。除了正则化方法，设计模式 7 还包括有噪声的数据的使用（Rasmus et al. 2015, Krause et al. 2015, Pezeshki et al. 2015）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.2 细节上的架构设计&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多更成功的架构的一个共同点是使每个层更容易完成任务。使用极深层网络（very deep network）就是这样的例子，因为任何单个层只需要递增地修改输入。这部分解释了残差网络（residual network）的成功，因为在极深层网络中，每层的输出可能与输入相似，因此将输入代替层的输出能使层更容易完成任务。这也是扩增路径设计模式背后的一部分动机，但是使每个层简化任务的想法超越了这一概念。设计模式 8 ：增量特征构造（Incremental Feature Construction）的一个例子是在 ResNets 中使用短距离跳跃（skip）。最近的一篇论文（Alain &amp;amp; Bengio (2016)）证明在深度 128 的网络中使用长度为 64 跳越会导致网络的第一部分不训练，并且导致不变化的权重，这是需要避免的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设计模式 9：规范层输入（Normalize layer inputs）是另一个简化层任务的方法：使层输入标准化。已经显示，层输入的标准化能改善训练结果和提高准确性，但是潜在机理并不清楚（Ioffe &amp;amp; Szegedy 2015, Ba et al. 2016, Salimans &amp;amp; Kingma 2016）。Batch 标准化的论文（Ioffe &amp;amp; Szegedy 2015）将提高归因于解决内部协变量偏移问题，而流标准化（streaming normalization）的作者（Liao et al. 2016）认为这也许是其它原因造成的。我们认为标准化使所有输入样本更加平等，就好像它们通过单位转换进行缩放一样，这使得反向传播（back-propagation）训练更有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些研究，如 Wide ResNets（Zagoruyko &amp;amp; Komodakis 2016），显示增加信道（channel）的数量提高了性能，但是多余的信道会产生额外的代价。许多基准数据集的输入数据有 3 个通道（即颜色 RGB）。几乎是普遍现象，CNN 的第一层的输出增加了信道的数量。设计模式 11：转换输入。增加信道的几个例子 / ImageNet 的第一层输出的数量分别为 AlexNet (96)，Inception (32)，VGG (224)，以及 ResNets (64)。直观上讲，第一层中信道数量从 3 增加是合理的，因为它允许以多种方式检查输入数据，但是不清楚使用多少个过滤器。另一个是成本与精确度的权衡。成本包括网络中的参数的数量，这直接反映在训练的计算量和存储成本中。增加信道数量会增加成本，这导致设计模式 10：可用资源决定网络深度。除了在下采样（down-sampling）时使输出数量加倍（见设计模式 13），根据内存、计算资源和期望的精确度来选择第一层的深度。深度学习的计算开销很高，每个从业者必须平衡这些成本与其应用程序的性能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.2.1 分支连接：串联、求和/平均与 Maxout&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当存在多个分支时，有三种方法来合并输出：串联、求和（或平均）与 Maxout。目前看来研究人员对它们的看法各不相同，没有哪一种方式更具优势。在本节中，我们提出一些简单的规则来决定如何合并分支。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;求和是合并输出的最常见方法之一：求和/平均将分支间的近似工作分割，最终形成设计模式 12：求和连接（Summation Joining）。求和是残差网络的最佳连接机制。因为它允许网络计算校正项（即残差）而无需整个信号。sum 和 fractal-join（平均）之间的差异最好通过 drop-path 来理解（Huang et al.，2016）。在输入跳跃连接总是存在的残差网络中，求和能使卷积层学习残差（与输入的差）。另一方面，在具有若干分支的网络中，如 FactalNet（Larsson et al.，2016），使用均值是最佳方式，因为随着分支被随机丢弃，它可以保证输出平顺。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些研究者似乎更喜欢串联（concatenation，例如 Szegedy et al，2015）我们相信串联对于增加池化时的输出数量是最有用的，这让我们得到了设计模式 13：下采样过渡（Down-sampling Transition）。这就是说，当池化或使用步幅（stride）超过 1 的下采样时，组合分支的最好方法是串联输出信道，它可以平滑地实现通常以下采样方式实现的信道连接和信道数量增加。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Maxout 已经被用于竞争，如本地竞争网络（Srivastava 等人，2014）和多尺度竞争网络（Liao 与 Carneiro，2015）Maxout 只选择一种激活，形成设计模式 14：MaxOut for Competition。它与求和或平均「合作」的激活方式相反，其中存在「竞争」关系，只有一个「赢家」。例如，当分支由不同大小的核（kernel）组成时，Maxout 可用于尺度的不变性，这类似于最大池化（max pooling）的平移不变性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们认为所有这些连接机制可以同时加入单独网络，不同于典型情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMoZKibvyjic6RNFKY4Pqst8zLTUcMAdvyLva0m0R2oKanPbPYhX4nZfCw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：这是 FractalNet 模块（a）和 FoF 架构（b）。曾表示如下：卷积层粉红色，连接层（如均值）是绿色，池层是黄色，预测层是蓝色。（b）中的灰色模块表示（a）中的 FractalNet 实例。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.1 架构创新&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本论文的重点是阐明基本设计原则，这样做的原因就是帮助我们发现一些架构上的创新，在本节中，这些创新将进一步被描绘出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，我们建议将求和/平均、串联和 maxout 连接机制与单一架构中的不同角色结合起来。接下来，通过增加分支的设计模式 2 来让我们能够大规模修饰 FractalNet 架构的顺序。最后按照我们称之为 Fractal of FractalNet (FoF) 网络，也就是 1b 中展示的分形模式调整模块，而不是按照最大深度来调整。该架构可将深度替换成更大数量的路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.1.1 Freeze-Drop-Path 和 Stagewise Boosting Networks（SBN）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Drop-path 是被 Huang 等引进的（2016b）. 它通过迭代训练随机移除分支路径，就好像这条路径在整个网络中是不存在的。出于对对称性的考虑，我们使用了一个叫 freeze-path 的相反的方法。我们冻结权重来达到零的学习率（learning rate），而不是在训练期间直接移除网络中的分支路径。循环神经网络领域也已经有一种类似的想法被提了出来 (Krueger et al. 2016)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们称结合了 drop-path 和 freeze-path 效用的模型为 freeze-drop-path，这个可以在非随机情况下得到很好的解释。图 1 显示了一个 FractalNet 分形结构。我们从最左侧路径开始训练，并将 drop-path 应用到其他分支上面。这个分支的训练速度会很快，因为相对于整个网络只需要训练少量的参数。随后冻结那条分支路径的权重并激活在原来右边的一条分支路径。最左边的分支也就可以提供一个很好的近似函数，并且下一条分支也能在矫正的基础上运行了。因为下一个分支路径相比前一个包含了更多的层，所以和原来的相比更容易逼近矫正项的近似值，因此这样的分支允许网络获得更大的准确性。这样也就可以继续从左至右来训练整个网络。freeze-drop-path 将最后加入 FoF 架构（图片 1b），这个称之为梯度递增网络（Stagewise Boosting Networks (SBN)），因为它就是类似于梯度递增的（Friedman et al. 2001）。递增神经网络 (boosting neural network；Schwenk &amp;amp; Bengio 2000) 并不是个新概念，但是这个构架就是新的。在 B 部分我们将讨论测试的实施。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.1.2 泰勒级数网络（Taylor Series Netwroks，TSN）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;泰勒级数是一个经典的、众所周知的函数逼近方法。泰勒级数的扩展是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于神经网络也是函数近似，将网络的分支（branch）看成一个泰勒级数展开的项，它可以作为 SBN 的延伸。这意味着，在求和连接单元（summation joining unit）之前使第二分支的结果平方，类似于泰勒展开中的二阶项。类似地，使第三分支立方。我们将它称作「泰勒级数网络」（TSN），并且存在多项式网络的优先级（Livni et al. 2014）和网络中的乘式项（例如 Lin et al. 2015 年的论文）。TSN 与 SBN 类比的实现细节详见附录讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.2 结果&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该章节内的实验主要是验证上面提到的架构创新的验证，但并非完全进行测试。未来会有更完整的评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMXZnhzEGhib28CCoMjV0icy0CichWxZFiaNWHSsWwz7qRNMz4B3ibGFQNRYg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 1：在 CIFAR-10 和 CIFAR-100 上各种架构的测试准确率对比。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMbQeIqKibKIgUdQx91GCQEhhDgicHjlqdoFPv8bzQgjB1UbEytm6FQP2w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：对比原 FractalNet 与用 Concatenation 或 Maxout 替代了一些 fractal-joins 的 FractalNet。同样展示的还有当用平均池化替代了最大池化时的测试准确度。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfM0yS5vLEpsH4qpFrOo7nanbukINFzwNe51Cy9oFHmaN7aSiaKpu5Rs0Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 3：对比原 FractalNet 与用 Concatenation 或 Maxout 替代了一些 fractal-joins 的 FractalNet。同样展示的还有当用平均池化替代了最大池化时的测试准确度。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;表一和图 3 接下来对比 4.1 章节中描述的架构创新的最终测试准确率的结果。最终的结果显示要比 FractalNet 基线差一点，但从 3a 和 3b 图中可以明显看到新架构训练起来要比 FractalNet 更快。FoF 架构最终测试准确率类似于 FractalNet，但 SBN 和 TSN 架构（使用 freeze-drop-path）在学习率下降的时候准确率会落后。这在 CIFAR-100 上要比 CIFAR-10 更加明显，表明这些架构可能更适合带有大量分类的应用。但是，我们也遗留下了对更多合适应用的探索，以后再做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5. 结论&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此论文中，我们描述了通过研究近期深度学习论文中的新架构而发现的卷积神经网络架构的设计模式。我们希望这些设计模式对希望推进前沿结果的有经验的实践者和寻求深度学习新应用的新手都有所帮助。接下来也有许多的潜在工作可以做，一些工作我们也在此论文中有所指明。我们的努力主要限定在进行分类的残差神经网络，但希望这一初步工作能启发其他人进行循环神经网络、深度强化学习架构等等其它网络的架构设计模式的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 05 Nov 2016 14:40:38 +0800</pubDate>
    </item>
    <item>
      <title>产品 | 机缘巧合诞生的讯飞语音输入法，如何累积了 4 亿用户？</title>
      <link>http://www.iwgc.cn/link/3377506</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 10 月 18 日的锤子发布会上，除焦点 M1L 之外，语音输入部分惊艳了不少观众。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;台上的老罗对着手机说出「今天上午，我们一行人从火车站来到了洲际酒店」，被迅速识别转换成文字出现在手机屏幕上。接着，老罗开始「长时间的胡说八道」，讲了一段自己没吃晚饭不舒服、吃药、喝冰水、来不及去医院、直接上发布会的过程。16 秒不间断的高语速大段口语内容，不到 1 秒便准确呈现在屏幕上，现场雷鸣般的掌声和欢呼声久久不能平息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMxbh2TeZWg3qUm2RA8jy7sR6ZInPM1SjicVGU2HaWibdQ3xtUkMkUOCyw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;老罗现场「胡说八道」的内容&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;老罗的现场演示展示出语音输入的便捷、可靠与高效。支持这一切的，正是讯飞输入法的语音输入功能。自 2010 年发布以来，讯飞输入法已累积超 4 亿用户，活跃用户超 1.1 亿。据称，随着深度学习技术的不断突破和应用，其语音识别准确率高于 97%，1 分钟可识别 400 字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 11 月 2 日的讯飞输入法沟通会上，讯飞输入法产品总监翟吉博分享了讯飞输入法背后的故事，包括这是一个最初仅 4 人的「小项目」、涟漪效应为这款输入法带来的提升、以及他们对输入法这一产品的思考。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三个月，四个人&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年 6 月 8 日，苹果发布了拥有「100 多项创新设计」的经典产品 iPhone 4，引发全球排队购机热潮。据称，iPhone4 的全球销量虽次于诺基亚「神机」1100，但总销售量也超过 1 亿大关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过 iPhone 4 屏幕仅为 3.5 英寸。虽说在当时已经算「大屏」，但现在看来也不过是 iPhone7plus 屏幕的二分之一，用全键盘打字时仍有不少困难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然用手指输入文字体验不好，可不可以用语音输入？当时做语音相关工作的翟吉博「基于纯技术的思维，将手写输入、语音识别和拼音放在一起，做出了输入法的 Demo」。虽然自己不以为意，但当时的上司看到成果，认为这个产品应该让更多人使用。于是技术出身的翟吉博，开始了学习了解市场、分析用户需求，成为了一名「产品经理」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMSaDlEs69wEmkljRNLicP2twtyVyPsUxoKLKKnn5tWzgpso2gxLrjhMw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;通过讯飞听见，嘉宾分享的内容可以实时呈现在屏幕上&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年 10 月，在 iPhone4 发售 4 个月后，讯飞输入法正式上线。6 年积累，曾经由 4 人小团队封闭 3 个月打造的产品，已经牢牢占据各大应用商店输入法类下载量第二。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么是讯飞？回想这款输入法出现的时机，虽然 PC 上已有搜狗输入法、百度输入法等相关产品，但移动端市场还处在前期，针对手机端优化的输入法还是空白。「我们认为手机端的输入方式会发生变化，语音交互的比重会越来越大。而且语音输入已经达到可使用的基本门槛，加上对涟漪效应的理解，我们认为通过几年的时间，讯飞输入法可以成熟。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如今的讯飞输入法团队成员，最开始多是热心用户。曾在论坛里吐槽功能不好用、给产品经理提建议的粉丝成为了讯飞输入法的运营经理，机锋论坛里做 ROM 的「大神」正在负责起渠道推广。曾在电脑城卖过光盘、做过网站，因设计输入法皮肤获奖的用户，也成为了讯飞输入法的专职皮肤设计师。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如何获取更多用户？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;满足了使用的基本需求后，如何让更多人使用这款产品？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;经过细致的思考和调研，翟吉博团队发现用户在使用语音输入时有四种需求需要被满足：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先是网络，当时的讯飞输入法需要调用云端极度依赖网络，但移动互联网并不稳定，用户对流量也很敏感；其次是方言，不同方言区的用户的特殊词难以被识别；再其次是个性化语言，不同的人有不同的语言习惯、说话方式和自己的惯用词汇；最后是跨语言交流，让不同语言的人可以通过文字互相了解，方便沟通。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过推出离线版、方言版、学习个人习惯和中英文实时翻译等版本和功能，讯飞输入法不断满足着这些需求。目前讯飞输入法支持包括粤语、东北话、河南话、四川话能在内近 20 种方言，「秃噜皮儿」、「辣子」等名词都能被迅速识别；选择中英文翻译功能，对准话筒说中文，屏幕上会自动翻译为英文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMm1TsaHFtia5CN7SEicPnGWEibhYNFjEWu7Ux9Q0ksAKukNCceQDBukj7w/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;产品总监翟吉博现场展示方言版效果，「巴适」、「马路牙子」都能识别出来&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除此之外，为满足明星粉丝用户的需求，推出了明星皮肤和图片；为满足二次元用户，可以用讯飞输入法上轻松打出颜文字，甚至还有斗图功能……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这大概是对用户最友好的输入法了。作为高依赖度的工具类产品，获得 4 亿累计用户，1.1 亿活跃用户似乎也就不足为奇。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为什么识别得快又准？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;世界上最早的语音识别系统是由 AT&amp;amp;T 贝尔实验室开发的 Audrey，可以识别 10 个英文数字。到了 1960 年代，人工神经网络被引入语音识别，两大突破是线性预测编码（Linear Predictive Coding，LPC) 与动态时间弯折（Dynamic Time Warp），不过大都是基于单词、孤立词或是特例人的研究。上世纪 80 年代末，李开复实现了基于隐马尔科夫模型的大词汇量语音识别系统 Sphinx，才完成了语音识别向随机内容、非特例人的句子识别的转变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;直到 2010 年，深度神经网络技术开始应用于语音识别，识别的效果和速度才得到了跨越式的提升。通过海量训练语料基础上的高精度声学模型和语言模型训练，结合解码引擎工程技术，人工智能技术的加入给语音识别带来全新的发展前景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9H8eXM2kHRdtPPzmmGVKfMd3tajJ4gmMOGlKMjiacYEoQ0mVyUDjDyyO7YPXvMW0SQY0WJtrs1W9Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;主流语音识别系统框架&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过仅有核心技术的提升是不够的，对于深度神经网络来说，真实的数据就是养料和智慧。科大讯飞轮值总裁胡郁曾用「涟漪效应」解释过数据和技术应用的关系：当某一项核心技术刚刚被大众所使用时，就像一滴水滴入水面，水波纹的起伏就是核心技术与用户期望之间的误差。水波纹逐步传播，就像核心技术正在逐步被更多的用户所使用，虽然这时效果还不太好，接触到的用户也不多，但这些用户不知不觉中贡献的经验和数据已经被系统自动学习和更新。当水波纹向外扩散，接下来接触到核心技术的人已经在使用更新过的系统。随着使用的人群越来越多，水波纹扩散的越来越广，大家会发现其实水波的振幅也越来越小，系统的性能也大幅提高。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正是 6 年间用户不断的贡献真实数据，才让讯飞输入法达到了「语音输入通用识别率为 97%，正常的语音输入文字已经不再有很大障碍」的程度，用户体验也在这一过程中逐步提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了语音识别，讯飞输入法的手写识别部分也用到了神经网络和图像识别技术，还可以支持连续书写的文字识别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这样一个「低头时代」，又会有多少人选择语音输入？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;答案可能远比想象的多。讯飞输入法后台数据显示，虽不是主要输入手段，语音输入的用户比例一直在提升，已经接近手写输入的比例。在这个追逐效率的时代，选择语音输入的用户大概会越来越多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以及，如果真的很忙来不及发文字，可以考虑试一试语音输入。毕竟在微信上收五条 60 秒语音的经历，有过一次就不想再有一次啦。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 05 Nov 2016 14:40:38 +0800</pubDate>
    </item>
  </channel>
</rss>
