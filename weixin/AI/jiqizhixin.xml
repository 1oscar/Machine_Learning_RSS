<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 亿万词汇构建神经网络，Facebook提出语言模型训练新算法</title>
      <link>http://www.iwgc.cn/link/3231915</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Facebook Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Edouard Grave、Justin Chiu、Armand Joulin&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、武竞&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Facebook 人工智能研究（FAIR）设计出一种新式的 softmax 函数逼近，专用于 GPU，帮助其在语言模型的基础上通过巨量词汇来有效训练神经网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于在语音识别、机器翻译或语言建模等领域的优异表现，用于序列预测的神经网络最近重新获得关注。然而这些模型都需要巨量的计算，这反而限制了它们的应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在语言建模领域，最近的研究进展用到了海量的大型模型，这些大型模型只能在大型 GPU 集群上训练，并且一次需要几周时间。这些处理密集型工作很棒，也有利于探索大型计算基础设备，但这些计算设备对于学术界来说通常十分昂贵，投入生产也不实际，以至于限制了研究的速度、再生产能力和结果的可用性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;意识到这种计算上的瓶颈，Facebook 人工智能研究（FAIR）设计出一种新式的 softmax 函数逼近，专用于 GPUs，帮助其在语言模型的基础上通过巨量词汇来有效训练神经网络。我们的方法叫做自适应 softmax（adaptive softmax），利用不平衡词分布形成簇（cluster），这种簇能明确地减少对计算复杂度的期望，从而规避对词汇量的线性依赖。这种方法通过利用流行架构的特殊性和矩阵-矩阵向量运算（matrix-matrix vector operations）进一步减少了训练和测试时的计算成本。这使得它特别适合于 GPU，而以往的方法，如分层 softmax，NCE 和重要性采样，都是为标准的 CPU 设计的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FAIR 也开发并正在开源一个名为 torch-rnnlib 的库，这个库允许研究者设计出新的循环模型并在 GPU 上以最小的代价测试这些原型（prototypes）。它也允许通过绑定 torch.cudnn 无缝对接快速基线。几个标准循环网络，如 RNN、LSTM 和 GRU 都已经被部署进来，下面我们将展示如何利用这个库来设计一个新的循环网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些工具和技术后来被一起用来应对标准基准，如 Euro Parl 和 One Billion word，这些都是因需要使用巨大的词汇量而复杂的训练环境，让我们无法在 GPU 上拟合一个大模型和 full softmax。结果显示我们在单一的 GPU 上每秒能处理 12500 个单词，通过标准逼近，大大提升了效率，减少了从试验到结果的时间，同时得到的精确度接近于 full softmax 的精确度。这就能让学界和业界的工程师与研究者都能在短时间内在资源有限的情况下训练出最好的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;利用 torch-rnnlib 建立一个循环模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环模型的定义有很多，我遵循的是这一条：循环网络随着离散时间对变量序列建模。它遵循马尔可夫的属性，其未来的状态只取决于它的现状。最简单的循环模型是 Elman 的循环模型。根据当下的输入变量 x[t] 以及之前的状态，在每个时间步骤 t，就会输出一个 y[t]。更确切的说，Elman 的循环模型可以通过下面的等式来定义：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;h[t] = f(R * h[t-1] + A * x[t]),&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;y[t] = B * h[t]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，h 代表网络（隐藏的）内在的状态，f 是 sigmoid 函数。Elman 之后就有人提出了更复杂的循环模型，如 LSTM、GRU 或者 SCRNN。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;什么是语言模型？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语言建模的目的是在一个给定词典中的一个词序列上学习一个概率分布。该联合分布被定义为给定状态下的词的条件分布的一个乘积。确切地说，一个 T 词序列 w[1],...,w[T] 的概率被给定为的 P(w[1],..., w[T])) = P(w[T]|w[T-1],..., w[1])...P(w[1]).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个问题通常由基于计数统计的非参数模型解决（详见 Goodman, 2001）。最近，基于循环神经网络的参数模型在语言建模上才流行起来（例如，Jozefowicz 等人, 2016，obtained state-of-the-art performance on the 1B word dataset）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;如何用 Torch-rnnlib 建立一个标准的模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们给出了用于建构带有循环连接的三个不同的 API：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1.nn.{RNN, LSTM, GRU} 接口可以用来建构在所有层都带有相同数量隐藏单元的循环网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpOR2AFKnxJYlaWAJmZCKEOCHQfc884T05Wsr0VGyrVCcwaN26DBbZh2xw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2.rnnlib.recurrentnetwork 的接口能用于构建任意形状的循环网络。上一个和这个接口都为你考虑到了节省隐藏状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORFG3iaa76HYUibpohBz3GoOhAewicg2lwvpRFFxRUePf8SmYHYeAiaECQMg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3.nn. SequenceTable 接口能用来将计算像一个『scan』一样链接起来。nn. RecurrentTable 构建器（constructor）仅仅是一个轻量级的封装，能随着时间为你克隆循环模块。然而，要注意的是这是最低层级的接口，你还需要 rnnlib.setupRecurrent(model, initializationfunctions) 来设定循环隐藏状态行为。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORvwF1DZicDGhjbfRCQHkk9QMvfNv9sW5IlmEy7ZvPj2xbbhxPjDWbQNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;建立你自己的循环网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你也可以通过定义一个行为像 cell 那样的函数来创建你自己模型，以及一个类似于这个 cell 状态的初始化函数。&lt;/span&gt;&lt;span&gt;在 rnnlib.cell 中有预定义的 cell，如 LSTM、RNN、和 GRN。下面来看看如何一步步建立一个 RNN：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORNqibQXqHuKibllnNKP6yC2Qhn8HobjVV6A002ZZdbnamduP9bSdxwyfQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;在 GPU 上训练它&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然 torch-rnnlib 连着 nn 模块接口，仅需要在模型上调取：cuda（）就能将它拉进 GPU。rnnlib 的目的是允许用户自由创建新 cell，或者使用快速基线。这样就 OK 了，如果你在上一节中使用第一或第二个 API 来构建循环网络，就可以轻松使用 cudnn 来大大加速你的网络。对于 nn.{RNN, LSTM, GRU} 接口，仅需要用 usecudnn=true 就能调取这个建构器（constructor）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpOR09L1hvicAic8mGCQqHZS8G8mibibofGuLUIJOQsicK9c9dXbop0WcyYm5VQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于第二个 API，仅需要将 rnnlib.Recurrent 替换成 rnnlib.makeCudnnRecurrent 并将 cell 函数改为 cudnnAPI 中已有的一个 cell 串。例如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORm0gfOLXuqqp5ITooun7ibTCRuFjN0YICNNkjHma2KvZX0qF39qQAN7Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终的结果通常是，模型的循环部分至少能提速两倍。要注意的是，不是整个模型提速两倍，尤其是如果大部分计算不是在循环部分中时。例如，如果你的模型中有一个 softmax，比循环部分需要更多的计算，那最终速度可能只提升 1.3 倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORzX2KNAT3z4aXia2HGIVw4bEPibZxAMK8OakbhjUc6SJ1sicsicNqia7icmUA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adaptive-Softmax：为 GPU 定制的 softmax 近似模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当处理大输出空间（如语言模型）时，分类器（classifier）可能是模型的计算瓶颈。过去已经提出了许多解决方案（分层 softmax（hierarchical softmax），噪声对比估计（noise contrastive estimation），差分 softmax（differentiated softmax）），但是它们通常被设计用于标准的 CPU，并且很少充分利用 GPU 的特性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们研究出了一种新的近似方法，叫做自适应 softmax（adaptive softmax）：一个使其计算载荷量（computational budget）适应数据分布的 softmax。它通过更快地访问最常用的类（class），为它们提供更多的资源，来达到优良的近似效果和快速运行时间之间的平衡。更准确地说，它学习了一个 k-通道（k-way）的层次 softmax（hierarchical softmax），它考虑了 GPU 的架构以有效地分配计算。这种计算资源的分配可以使用一个简单的动态规划算法来精确求解。几个技巧可以进一步处理分类器的计算负载问题：我们使用分枝少的树（shallow tree）来避免顺序计算（sequential computation），并且我们为每个 GPU 集群（cluster）确定类（class）的最小值，以避免浪费 GPU 的并行计算能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如表格 1 中显示的，自适应 softmax 几乎可以与完整 softmax（full softmax）的结果相媲美，并且自适应 softmax 速度更快。它也优于 GPU 运行的其他近似模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORz9IejxHa3nrWLqjv2vtK74ziaBHgyx2aibpibaqQHXYlDWax9066xvyaQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表格 1. 使用 Text8 的模型结果比较。ppl 值越小越好。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORwesiajeX3PA4dRwka5STtHxEX7Fic4iaYFlg06HibmnuE7DrSEXBWwtLJg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图. 语言模型在不同 softmax 近似模型上的收敛效果。它建立在 LSTM 模型之上。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;10 亿单词数据集在单个 GPU 上几天内达到了值为 45 的困惑度值&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除自适应 softmax（adatpive softmax）外，在技术细节方面，我们使用相对标准的设置：对于小模型，我们使用层数为 1、2048 个神经元单位的的 LSTM；对于大模型，我们使用层数为 2、2048 个神经元单位的 LSTM。我们使用 L2 正则化（regularization）的权重和 Adagrad 来训练模型。我们使用大小为 128 的批处理（batch），同时设置反向传播（back-propagation）窗口的大小为 20。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于自适应 softmax，我们使用 10 亿单词的训练数据集分布的最佳设置，即 4 个 GPU 集群（cluster），每用一个集群，数据的维数减少 4 倍（更多细节详见论文）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpOR4osX6tZgAj9CFTzROL7YAhqicbAShxhncWe3akXIdLOQwMJXFnibNCeA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表格 2. 使用 10 亿数据集的模型 perplexity 值比较（值越小越好）。注意到 Jozefowicz 等人用了 32 个 GPU 训练，我们只用了 1 个 GPU。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如表格 2 中显示的，我们的小模型在几天内就达到了 43.9 的 perplexity 值。我们的大模型在 6 天内达到了 39.8 的 perplexity 值。目前最好的 perplexity 值（越小越好）是 30.0，由 Jozefowicz 等人在 2016 年达到。这个结果是他们用 3 周的时间，使用了 32 个 GPU 达到的。他们也声称使用 18 个 GPU 训练的更小模型，达到了数值为 44 的 perplexity 值。我们的小模型的速度为 180 毫秒/批，并在一个迭代（epoch）后（迭代时间大约为 14 小时）达到数值为 50 的 perplexity 值。不使用 cuDNN 加速库，小模型的速度为 230 毫秒/批，这比之前只慢了 30%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 26 Oct 2016 11:51:05 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 微软开源Microsoft Cognitive Toolkit深度学习工具包，加入强化学习元素</title>
      <link>http://www.iwgc.cn/link/3231916</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Microsoft Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Allison Linn&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今天，微软宣布发布了 Microsoft Cognitive Toolkit 的更新版本，这是一个用于深度学习的系统，可用于加速 CPU 和 NVIDIA GPU 上的语音和图像识别以及搜索相关性等领域的发展。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;项目地址：https://github.com/microsoft/cntk&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个工具包之前被称为 CNTK，最早是由微软一些想要更快更高效地做自己的研究的计算机科学家开发的。它很快就超越了语音领域并演变成了一个产品，包括一些领先的国际家电制造商和微软的旗舰产品组（flagship product groups）在内的客户依靠它来执行各种各样的深度学习任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软 Artificial Intelligence and Research 部门首席科学家和 Microsoft Cognitive Toolkit 的一位关键架构师 Frank Seide 说：「我们将其从一个研究工具变成了可以用在产品之中的东西。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib9ezqHLusWxfqaFozVbpORUYUibYw9b58SIewX24IbTXaBXKICw6jwhmetNQ0Raa4f5Iw4jKic2esg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Frank Seide&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该工具包的最新版本现已通过一个开源证书发布到了 GitHub 上，其新增功能包括对 Python 和 C++ 编程语言的支持。研究者还可以使用这个新版本开发一种叫做强化学习（reinforcement learning）的人工智能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，该工具包的性能也优于之前的版本。它也比其它工具包更快，尤其是当需要跨多台机器运行大数据集时。为了开发消费者产品和专业产品，这种大规模部署对跨多个 GPU 的深度学习来说是必需的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也是加速研究突破的关键。上周，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect"&gt;微软 Artificial Intelligence and Research 宣布在识别对话上已经达到了人类的水平&lt;/a&gt;。这个团队将实现这一里程碑背后的巨大速度提升归功于了 Microsoft Cognitive Toolkit。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开发了这个微软工具包的团队说其跨多服务器工作的能力是超过其它深度学习工具包的关键优势。当这个微软工具包被用于解决更大型的数据集时，可以实现更优的性能和准确度。Microsoft Cognitive Toolkit 有内置的算法来最小化这种计算的退化（degradation of computation）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「使用 Microsoft Cognitive Toolkit 的一个关键理由是其可以针对大型数据集跨多 GPU 和多机器进行有效地扩展，」微软合作伙伴工程经理 Chris Basoglu 说，他在该工具包的开发中扮演了一个关键的角色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib9ezqHLusWxfqaFozVbpOR7UFoq8bHqGHeAZiaSxLAo1Zcjbeg6CWZm1y6CyNvM2iaTTiamkDicQ6AHQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Chris Basoglu&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Microsoft Cognitive Toolkit 可以轻松处理从相对较小到非常非常大等各种规模的数据集，既可以在一台笔记本上运行，也可以运行在数据中心中的一系列计算机上。它可以运行在使用传统 CPU 或 GPU 的计算机上；GPU 以前主要的用途是处理对图形要求较高的视频游戏，后来人们发现可以用它来非常高效地运行深度学习所需的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Microsoft Cognitive Toolkit 代表着微软和 NVIDIA 紧密合作以为深度学习社区带来进步，」NVIDIA 的 Accelerated Computing Group 总经理 Ian Buck 说，「和以前的版本相比，在扩展到一个 NVIDIA DGX-1™ 中的 8 个 Pascal GPU 之后，其性能几乎提升了两倍。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Microsoft Cognitive Toolkit 是为在多个 GPU 上运行而设计的，包括 Azure 的 GPU 产品，该产品目前还是预览版。该工具包已经经过了优化，可以最好地利用 NVIDIA 硬件和 Azure 产品的网络功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;民主化人工智能及其工具&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当小型创业公司和大型科技企业都看到了深度学习的使用对语音理解和图像识别的可能性时，我们发布了该工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;广义上讲，深度学习是一种需要用到大量数据（被称为训练集）的人工智能技术，从而能教会计算机系统学会识别图像或声音等输入中的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如说，可以用一个包含了各种水果和蔬菜图片的训练集来训练一个深度学习系统，之后该系统能学会自己识别水果或蔬菜的图片。它获得的数据越多，它的表现就会越好；所以每次当它遇到一个新的、长相奇怪的茄子或扭曲的苹果时，它都可以调整自己的算法以使其变得更为准确。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib9ezqHLusWxfqaFozVbpORwibnllspnDiaWMib5EhpZ00A2aictNwtBYm31gzGo3HNAdumHZaPMpuDJw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在使用 Microsoft Cognitive Toolkit 训练语音声学模型中，随着应用更多的数据，它能收敛出更高的准确率。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这类的成果不只是研究的里程碑。由于深度学习的进步，加上计算马力的大幅度增长，我们如今有了像 Skype Translator 这样的消费者产品，能识别语音并提供实时语音翻译。还有 Cortana 数字化助手，能理解语音并帮助你做机票搜索和备忘约会这样的所有事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软首席语音科学家黄学东说，「这就是使用 Microsoft Cognitive Toolkit 民主化人工智能的一个样例。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;更灵活的完成更复杂的任务&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Basoglu 说在他们第一次开发该工具箱的时候，他们发现许多开发者不能或不想写大量代码。所以他们创造出一个自定义系统，能让开发者更简单的配置深度学习系统，不需要额外的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，随着该系统变得越来越流行，他们了解到一些开发者想将自己的 Python 或 C++ 代码与该工具箱的深度学习能力结合起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们也了解到一些研究人员想要使用该工具箱进行强化学习研究。强化学习是代理通过大量试错直接学习做某些事的一种研究领域，比如在房间中找到线路或合成句子。这类研究可能最终引向真正的人工智能，也就是系统能够自己做复杂的决策。新版本的工具箱就给了开发者做强化学习研究的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Microsoft Cognitive Toolkit 最初由语音研究员开发，如今却能被用于多种目的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了给用户提供更好的结果，Bing relevance 团队使用它更好的发现搜索词条相关的隐藏的链接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如，在用户输入「How do you make an apple pie?」的时候，带有深度学习的系统经过训练可自动明白用户在寻找菜谱，即使「recipe」一词并不在搜索词条内。没有这样的系统，这种规则只能手动编程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing relevance 团队的软件开发工程师 Clemens Marschner 说，他们团队和该工具箱的创造者们有着非常紧密的合作，从而更好的让开发者做除了语音之外的深度学习任务。对他们而言，所得的回报就是使用大规模的计算能力快速的获得结果。他说，「没有其他解决方案能让我们这么简单的将学习在 GPU 上扩展到大型数据上。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软也在不断使用 Microsoft Cognitive Toolkit 改进语音识别。语音服务部门的应用科学经理 Yifan Gong 说，他们已经使用该工具开发出了更准确的声学模型，应用到了包括 Windows 和 Skype Translator 在内的产品中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Gong 说他的团队依靠该工具开发新的深度学习架构，包括使用 LSTM 技术来为顾客导出更准确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 26 Oct 2016 11:51:05 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 脑机接口可以将我们的思想直接翻译成文本吗？（附论文）</title>
      <link>http://www.iwgc.cn/link/3231917</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Science Daily&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：曹瑞、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你曾经有没有想过要是有一个设备能够将你的想法解码成真实的语音或是书面文字的话会怎么样呢？或许这会增强现存的一些设备语音接口的性能，可能会引起语言病理学上的变革，尤其是对于那些没有语言和运动能力的「闭锁综合症」患者来说更是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;《人类神经科学前沿》中一篇评论的作者Christian Herff说道：「所以，我不需要去问『Siri，今天的天气怎么样』或者是『Google，我可以去哪儿吃午饭』，只需要想象我在说这些话就可以」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;读取某人的想法或许还只是存在于科幻小说当中，但是科学家已经可以将我们在说话或者是聆听时大脑中的信号进行解码，生成语音。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Herff 和他的合著者Dr. Tanja Schultz在他们的综述当中，比较了在大脑中使用各种脑成像技术捕捉神经信号，并将其解码为文本的利与弊。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些技术包括功能性磁共振成像（MRI）、近红外成像（能够根据神经元的代谢活动检测到神经信号）、脑电图（EEG）和脑磁图（MEG）（能够检测到与语音相对应的神经元的电磁活动）等。特别是一种叫做脑皮层电图描记法（ECoG）的方法，在Herff的研究之中展现出了很大的发展前景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项研究提出了一种大脑到文本（Brain-to-text）系统，这种系统在已经植入了电极网格以便治疗的癫痫病人参与者身上进行了实验。他们将他们面前的屏幕上的文本读出来，同时他们的大脑活动将被记录下来。这就形成了可以将语言音素（或者说是「语音」）与神经信号模式匹配起来的数据库的基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当研究者们将语言和词典的模型都运用到了算法当中时，就可以高度精确地将神经信号解码成文本。Herff说道，「这是我们第一次证明可以在大脑信号当中使用自动语音识别（ASR）技术，将大脑活动进行足够准确解码。但是从现在对植入电极的需求可以看出，这项技术还需要很长的时间才能被运用到日常生活当中」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，这个领域要是想从现在发展到一个可正常工作的思维检测设备要怎么做呢？ Herff说道：「第一个里程碑应该是要将想象的短语从大脑活动中解码，但是我们还有很多的技术问题有待解决。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个研究的结果虽然非常令人激动，但对于这类型的脑机接口研究来说还只是处于初步阶段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文：神经信号的自动语音识别： 一份焦点评述（Automatic Speech Recognition from Neural Signals: A Focused Review）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：语音接口目前已经被广泛接受，并且被运用于很多真实生活中的应用和设备当中，它们已经成为了我们日常生活中的一部分。尽管我们认为语音接口能够生成可以理解的语音，但是由于嘈杂的环境、吵闹的旁观者或者是使用者不能说话（比如说患有闭锁综合症的患者），这种能力就会从可能变成不可能。出于这些原因，想象出说话的场景要比真实说话更加可取。基于想象语言的语音接口不需要听得见的声音就能够进行快速、自然的交流，这让因为各种情况不能说话的人都有了发声的机会。这份焦点评述分析了不同的脑成像技术使用自动语音识别技术来识别神经信号中语音的潜力。我们认为基于代谢过程的方式，比如说功能近红外光谱成像和功能性磁共振成像技术，因其低分辨率不适合神经信号的自动语音识别，但在研究语音过程的神经机制中却非常有用。相反，电生理活动能够快速捕捉语音过程，也更适宜运用到自动语音识别当中。我们的实验结果也证明了这些通过无创测量大脑活动（脑皮层电图描记法）产生的信号在神经数据语音识别上的潜力。作为使用神经信号自动语音识别技术的第一个实例，我们对大脑到文本（Brain-to-text）系统进行了讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib9ezqHLusWxfqaFozVbpORcvLQOClJGbbo602eVT8WdQ3u61TdEOw4vx5JzrPYUttibPTvEna1AjQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1. ECoG 和音频数据同时被记录下来。然后语音解码软件被用于确定声学数据中元音和辅音的时间。然后 ECoG 模型通过计算所有与特定音素（phone）有关联的片段的均值和协方差而为每个音素单独接受训练。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib9ezqHLusWxfqaFozVbpORJp98otFRAbZcvRyNPnMlnabezHfRj6KI7QdJBXr2t47IlaPS9Hel5g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图2. Brain-to-text 系统的解码过程。Broadband gamma power 被提取出来用于 ECoG 数据的短语。然后通过结合 ECoG 音素模型的知识、词典和语言模型解码出最有可能的词序列。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 26 Oct 2016 11:51:05 +0800</pubDate>
    </item>
    <item>
      <title>业界 | IBM推出Watson数据平台，机器学习的力量将无处不在</title>
      <link>http://www.iwgc.cn/link/3231918</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自IBM News&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李泽南&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;IBM 宣称要提供世界最快的数据采集引擎和机器学习服务，加速商务人工智能。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib9ezqHLusWxfqaFozVbpORT7wVbJRvkYjFvz2DWPLkE5JAcbiawe8RDIWudbIdKXfuRnC4kiauy5sw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，IBM 宣布推出 IBM Watson 数据平台，旨在帮助其他公司从数据中获得更有价值的信息。该平台向数据人员提供了世界上最快的数据采集引擎和认知决策，使他们能够在 IBM Cloud 中与他们想要的服务进行协作。IBM 同时启动了 IBM Watson 机器学习服务，通过直观的自助服务界面使机器学习变得简单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大数据是让技术人员梦想成真的基础。随着科技巨头间竞争的不断升级，这项技术已经进入每个人的手中，成为了生产力工具。在大数据普及之后，更多想法和创造性的解决方案将不断涌现。在目前「数据第一」的风潮之下，Watson 数据平台的出现让未来变得触手可及。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「机器学习已经变得前所未有的强大，但是目前的数据专业人员缺乏将之充分利用的能力，」IBM 分析师，高级副总裁 Bob Picciano 说道。「Watson 数据平台应用认知辅助来创建机器学习模型，让数据分析的速度更快。它还提供一个访问机器学习服务和语言的接口，使任何人，从应用开发者到首席数据官，可以无缝协作。我们的平台能让数据更有意义，提出更好的问题，做出更有效的决策。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 副总裁 Rob Thomas 则使用保险公司做了一个简单的例子，向我们解释企业如何通过机器学习变得「更聪明」。「大多数保险公司根据历史数据评价风险，」Thomas 说道。「而我们启用了实时评分系统，使得保险公司编写的每个条款，每个外部因素——所有数据都可以进入模型，同时这个模型实时更新。不同的保险公司会要求不同的保费，一切取决于数据的分析。这是机器学习的本质，也宣告了机器学习的核心技术已能被人们掌握。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于 Apache Spark，Watson 机器学习可以从结构化和非结构化数据以及开放式机器学习库中自动构建模型，同时将模型快速部署，为业务运营助力。其专利的数据科学认知辅助技术根据获得的数据对每个机器学习算法进行评分，为满足需求作出最佳匹配。它还包括业界最全面的算法集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通过协作扩大人工智能在商务领域的影响&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据最近的《哈佛商业评论》调查，80％的组织认为团队无法在共有数据上合作是高效完成业务目标的障碍。数据专业人员在工作时互相独立，使用不同的编程语言，没有相同的关注点，并且不得不将时间浪费在数据收集和清理上。Watson 数据平台有助于解决所有这些问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Watson 数据平台可以使不同专业的数据人员通力合作——如数据科学家，数据工程师，商业分析师和开发者，允许他们在数据集中协同工作，使用他们各自使用的语言，服务和工具。此外，该平台使数据专业人员能够轻松地在整个企业中分享可视化的分析成果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Watson 数据平台利用 IBM 在 Apache Spark，IBM Cloud，认知计算和 The Weather Company 等项目中的成果，并应用了 IBM 研究院开发的多种技术。令数据专业人员有能力：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;以创纪录的速度（大于每秒 1000 亿字节）将大量不同数据导入云中&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;清理，编辑和塑造数据，以方便建模&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在版本维护时根据需要添加和删除协作者&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;将服务拖放到分析薄中，以提高效率，节约时间&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过基于 IBM Cloud 的 Watson 数据平台，公司可以将自己的数据与外部数据结合使用，利用内置管理来解决流程，隐私和法规要求，同时保持对数据的控制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Watson 数据平台正在构建一个不断扩大的技术服务生态系统，使数据专业人员能够自由使用自己喜欢的语言和他们需要的服务。IBM 已将 SQL，Python，R，Java 和 Scala 纳入 Watson 数据平台，同时已发展了二十多个合作伙伴，以扩展平台服务，包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Qubole——使 IBM Data Science Experience 的用户能够使用 Spark 在他们选择的公共云设备上处理数据；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RStudio——支持 R 软件包的开发，集成了 R 的现有工具，包括 Shiny 和用于 Apache Spark，sparklyr 的新 R 接口，可快速将数据科学工作流用于生产；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Keen IO——提供了强大的 API 集合，允许数据科学家收集，分析和可视化任何存在于网络中的事件。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「数据专业人员使用各种不同的工具和语言，他们可以在处理数据时自由选择，无论 R，Python 还是 Scala，——他们同时可以使用多种机器学习和可视化服务，他们可以使用任何已知的方式随时随地处理数据」，IDC 分析和信息管理集团副总裁 Dan Vesset 评论道。」IBM Watson 数据平台为数据专业人员提供了一个开放，强大的生态系统，为人们提供了一个易于使用的平台，鼓励人们协同工作。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目前的服务&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Watson 数据平台将「Project DataWorks」推向市场，并已在 IBM 云的自助服务和企业计划中推出，数据专业人员在已经可以在平台中使用他们需要的工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM Watson 机器学习服务将从 Apache SparkML 开始，未来会继续加入其他算法，并且可以通过 Watson Data Platform 访问，作为 IBM Bluemix 或 z/OS 上的 API。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 26 Oct 2016 11:51:05 +0800</pubDate>
    </item>
    <item>
      <title>访谈 | 艾伦人工智能研究所CEO Oren Etzioni：深度学习离人类水平的人工智能还差得很远</title>
      <link>http://www.iwgc.cn/link/3222891</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Larry Greenemeier&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Elon Musk 正在全面押注自动驾驶汽车的新计划，他需要强大的人工智能技术来确保特斯拉汽车能够实时理解不同的驾驶情况，并据此实时地做出反应。人工智能正在实现非凡的成就：上周，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719745&amp;amp;idx=2&amp;amp;sn=242eda88fa9a5572e60b43e451a1549b&amp;amp;chksm=871b027fb06c8b693cff17f43553625f008fcb7965582119b651dbbd17e116e35dc801ebeec3&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719745&amp;amp;idx=2&amp;amp;sn=242eda88fa9a5572e60b43e451a1549b&amp;amp;chksm=871b027fb06c8b693cff17f43553625f008fcb7965582119b651dbbd17e116e35dc801ebeec3&amp;amp;scene=21#wechat_redirect"&gt;AlphaGo 计算机程序的创造者报告&lt;/a&gt;了他们的软件已经学会了像一个伦敦本地人一样在纷繁复杂的地铁线路中导航。甚至就连白宫也来凑热闹了，他们在不久前放出了一份报告，旨在帮助美国为机器能像人类一样思考的未来做好准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但已经在研究和尝试解决人工智能的基本问题上工作了几十年的计算机科学家 Oren Etzioni 说：在人们可以或应该将世界交给人工智能接管之前，人工智能还有很长的路要走。Etzioni 目前是艾伦人工智能研究所（AI2）的首席执行官；该组织是由微软的联合创始人 Paul Allen 于 2014 年组建的，该组织的目标是开发人工智能的潜在好处——以及纠正好莱坞乃至其他人工智能研究者鼓吹的人工智能可能威胁人类种族的观念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;AI2 自己的项目可能并不非常疯狂——比如说他们有一个基于人工智能的学术研究搜索引擎 Semantic Scholar（https://www.semanticscholar.org/）——但他们确实在解决推理（reasoning）等人工智能领域的问题，这将使得这一技术超越 Etzioni 所说的「只在一件事上做得非常好的狭隘的专家」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Scientific American 在纽约最近的一场人工智能会议上对 Etzioni 进行了采访，在采访中他表达了自己对企业过于鼓吹人工智能的当前能力——尤其是被称为深度学习的机器学习技术——的担忧。这个处理过程需要将大型的数据集通过模拟人脑神经网络的网络，以便教会计算机学会自己解决特定的问题，比如识别模式或确定照片中存在的特定物体。Etzioni 还解释了他为什么认为十岁孩童比谷歌 DeepMind 的 AlphaGo 程序更聪明，以及为什么终将需要开发人工智能「卫士」程序来防止人工智能程序变得危险。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWyYlpIvclfEsMzcKOjSgwxPqkibXMs4caOmibV3KFLq9D3yN8FTXaibCnw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Oren Etzioni&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是经过编辑整理的采访内容：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;在开发最好的人工智能技术上，人工智能研究者前面还有什么鸿沟吗？&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;一些人已经靠他们自己取得了一点点领先。我们已经在语音识别、自动驾驶汽车（或者说自动驾驶的有限形式）以及当然的围棋等领域取得一些实质性的进展。所有这些都是非常实质的技术成就。但我们该怎样解读它们呢？深度学习无疑是一项很有价值的技术，但在创造人工智能上，我们还有很多其它的问题要解决，其中包括推理（意味着计算机不仅能计算 2+2=4，还能进行理解）和获取可被机器用于创造语境的背景知识。还有另一个例子是自然语言理解。尽管我们已经有 AlphaGo 了，但我们还没有一个能够读懂和完全理解一段话或甚至一句话的程序。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;有人说深度学习是「我们最好的（the best we have）」人工智能技术。那是对深度学习的批评吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;当你有大量进行了标注以使计算机能理解其含义的数据和大量算力，以及需要找到那些数据中的模式的时候，我们可以发现深度学习是无敌的。再次说到 AlphaGo 的例子，该人工智能程序为了学会在不同情形下的正确走子而处理过 3000 万个棋盘局面。还有很多类似的情形——比如放射图像——其中图像需要被标记为有肿瘤或没有肿瘤，一个经过调节的深度学习程序可被用来确定其之前看过的图片中是否有肿瘤。深度学习方面还有很多的工作要做，而且确实，这是前沿的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;那么问题出在哪里？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;问题是对于智能，除了有大量可用来训练程序的情况，还有很多其它情况。比如学生准备 SAT 或 New York Regents exams（大学入学考试）这样的标准考试时所用的数据。他们可不能通过考察之前的 3000 万份标注了「成功」或「不成功」的考试来获得高分。这是一个更为复杂的交互式的学习过程。智能还涉及到从建议、对话的语境或阅读书籍中学习。但是同样地，尽管深度学习领域有这些惊人的进步，我们也还是不能得到一个能做到十岁孩童所能做的事情的程序，即：拿起一本书，读一章，然后回答有关读到的内容的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;为什么人工智能通过标准考试可以成为这项技术的重大进步？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni ：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我们艾伦人工智能研究所实际上已经将其作为一个研究项目了。去年我们宣布设立了一个 50,000 美元的奖给任何开发出了能够通过标准的 8 年级科学考试的人工智能软件的人。来自全世界的 780 支团队用了几个月的时间想要达到这一成就，但没有人能够得到超过 60% 的分数——即使那只是 8 年级考试中的多项选择题。这为我们目前所处的现状提供了一个现实的和定量的评估。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;能够正确回答问题的人工智能系统是怎样的？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;线索通常在语言之中。最成功的系统使用了经过精心调制的科学文本和其它公共源的信息，然后再使用经过了精心微调的信息检索技术进行搜索以定位每个多项选择题中最好的候选答案。比如，下列物体中最好的电导体是：塑料勺子、木叉子或铁拍子？程序非常善于配对，可以检测到电和铁或导体和铁比塑料和导体远远更常见地共同出现在文档之中。所以有时候程序可以通过这样的捷径找到答案。这差不多也就是孩子们靠已知的信息猜答案的方法了。没有任何系统的得分超过 60%，我可以说这些程序都是在使用统计进行有根据的猜测，而不是对问题进行仔细的推理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;AlphaGo 背后的 DeepMind 团队现在已经有一个可以使用外部记忆系统超越深度学习的人工智能程序了。他们的成果对创造更像人的人工智能有什么影响？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;在推动深度神经网络（模拟人脑设计的人工智能）的发展上，DeepMind 将继续作为领导者。这个特定的贡献是很重要的，但也只是在图结构（比如一个地铁网络）中互连的事实上实现推理的一小步。已有的符号程序就可以轻松地执行这样的任务，但这里的成就（已经发表在 Nature 上）是关于神经网络如何根据样本学习执行任务。整体上看，是 DeepMind 的一大步，却是人类的一小步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;人们可以怎样结合深度学习、计算机视觉和记忆等方法来开发更完整的人工智能？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;这是一个非常吸引人的概念，实际上当我还是华盛顿大学的一位教授时的很多研究都是基于使用互联网作为人工智能系统的数据库的想法。我们开发了一个叫做开放信息提取（open-information extraction）的技术，它索引了 50 亿个网页，并且可以从其中提取句子并将其映射到机器可操作的知识上。该机器在抓取网页和所有句子上能力超强。问题是这些句子是在文本或图片中。我们人类的大脑有非常强大的能力——我们计算机科学家还没有破解这些能力——可以将那些行为映射到推理等等。为什么这种通用数据库和人工智能接口的想法仍然还是科学幻想呢？因为我们还没有搞清楚如何将文本和图像映射成机器可以用来像人类一样进行操作的东西。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;你提到过实现人类水平的人工智能至少还要 25 年的时间。人类水平的人工智能是什么意思？为什么会有这样的时间框架？&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;真正的自然语言理解、人类智能的广度和通用性、我们既能下围棋也能过马路还能制作好看的煎蛋的能力——这种多样性是人类智能的标志，而我们今天做的只是开发在一件小事上做得非常好的狭隘专家。为了得到这个时间框架，我询问了美国人工智能协会（AAAI）的研究者：什么时候我们将实现在广义上和人类一样聪明的计算机系统？没人说那会在未来 10 年内发生，67% 的人说会在接下来 25 年或更往后，25% 的人说「永远不会实现」。他们可能错了吗？可能吧。但你该相信谁，难道是那些紧握行业脉搏的人？还是好莱坞？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;为什么有如此多备受尊敬的科学家和工程师警告说人工智能会加害我们？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我很难推断霍金、Elon Musk 这群人如此讨论人工智能的动机是什么。我猜测可能是黑洞讨论一段时间之后有点无聊了，黑洞是一个慢速发展的主题。我想说的是，在他们以及我极为尊重的比尔盖茨在讨论人工智能会变得邪恶或潜在灾难性后果的时候，他们总是插入「最终」或者「可能」这样的限定符。这我是同意的。如果我们讨论千年以后或无限的未来时，人工智能有可能为人类带来末日吗？当然是有可能的，但我认为这种对未来的讨论不应该分散我们对人工智能与就业、人工智能与武器系统这样真正问题的注意。而且「最终」、「概念上」这样的限定符在转译时往往会丢失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;在人工智能有缺陷的情况下，人们应该担心汽车制造商对自动驾驶汽车不断扩大的兴趣吗？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我对没有方向盘或刹车踏板这样的自动驾驶汽车没有多大兴趣。以我对计算机视觉和人工智能的了解，我对这种汽车相当不舒服。但我是复合系统的粉丝，例如，在你开车瞌睡时它能为你踩刹车。司机加上自动系统要比任何单独一个都更安全。然而这并不简单，将新科技融入到人类生活与工作中不是件容易的事。但是，我不确定融合新科技的解决方案就是让汽车做所有的事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;谷歌、Facebook 和其他科技巨头最近合作建立了 Partnership on Artificial Intelligence to Benefit People and Society (https://www.partnershiponai.org/) 组织为人工智能研究设定最好的道德实践。是不是人工智能技术已经发展到足够的程度了，让他们能进行有意义的对话？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;世界上顶级的科技公司聚到一起想这些事是非常好的思路。我想他们如此做是为了应对人工智能将掌管世界的担心。很多对人工智能的担心完全过分了。即使我们有自动驾驶汽车，也不是说 100 辆车就聚到一起说是要，「拿下白宫」。Elon Musk 这群人提到的风险即使不是百年之后也是数十年之后的事。而且，我们存在一些真正的问题：自动化、数字科技和一般的人工智能真的在影响就业场景，无论是机器人还是其他情景，这才是真正的担心。自动驾驶汽车、卡车将来是会大幅度的改进安全性，但也会影响大量依靠驾驶为生的工人。该新组织应该讨论的另一件事是人工智能可能造成的歧视。如果人工智能技术被用于处理贷款或信用卡应用，他们会以合乎法律与道德的方式来做吗？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;你如何保证人工智能项目会合乎法律与道德的进行？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;例如你有一家银行，有处理贷款的软件项目，你也无法隐藏在软件的背后。说计算机这么做的并不能成为一个借口。即使计算机项目不使用种族或性别作为明确变量，它也可能有歧视行为。因为计算机项目会接触大量的变量、大量的统计数据，它可能找到邮政编码与其他变量之间的关系，这些关系会构成种族或性别变量的替代。如果它使用这种替代变量影响到决策，就会造成问题，而且人们难以检测或追踪缘由。所以我们提议的思路是人工智能卫士（AI guardian），也就是监测、分析基于人工智能的贷款处理程序的系统，从而保证程序随时间进化时能遵循法律、合乎道德。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;如今有人工智能卫士的存在吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Etzioni：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;我们正在向社区呼吁开始研究并建立这种东西。我认为如今可能有一些琐碎的存在，但这是目前一个很大的愿景。我们想用人工智能卫士的思路反击一直以来认为人工智能是邪恶的整体力量的普遍场景，就像终结者这样的好莱坞电影传播的那样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part3：word2vec 的秘密配方</title>
      <link>http://www.iwgc.cn/link/3222892</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Terrence L&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;本文是词嵌入系列博客的 Part3，介绍了流行的词嵌入模型全局向量&lt;span&gt;GloVe&lt;/span&gt;。 part2 请点击 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=2&amp;amp;sn=9fedc937d3128462c478ef7911e77687&amp;amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650720050&amp;amp;idx=2&amp;amp;sn=9fedc937d3128462c478ef7911e77687&amp;amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;amp;scene=21#wechat_redirect"&gt;技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法&lt;/a&gt;；Part1请点击 &lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;技术 | 词嵌入系列博客Part1：基于语言建模的词嵌入模型&lt;/a&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;br/&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="text-decoration: underline; max-width: 100%; font-size: 12px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWdbIqaJ0f3m5aeLIJyjFUdT9RYgMzn6XQ6Sog9orJTc5AhOOPFHm12Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;全局矢量（GloVe）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;词嵌入量与分布式语义模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;超参数&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;结果&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请原谅之前的噱头。这是一篇我很久之前就想要去写的博客。在这篇文章中，我想要强调那些使得 word2vec 成功的秘密成分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我特别要专注于通过神经模型训练的词嵌入与通过传统的分布式语义模型（DSMs）产生的词嵌入之间的联系。通过展示这些组分是如何被转移到 DSMs 中的，我将会证明分布式的方法是丝毫不逊色于流行的词嵌入方法的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然没有什么新的见解，但我感觉传统的方法经常被深度学习的热潮所掩盖，它们之间的相关性应该受到更多关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，这篇博客所依据的文献是 Levy 等人在 2015 年发表的通过词嵌入获得的提升分布式相似性的研究。如果你还没有阅读过，我建议你抓紧搜索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇公开的博客中，我将首先介绍一个流行的词嵌入模型 GloVe，然后我将突出词嵌入模型和分布式予语义方法之间的联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;紧接着，我将会介绍用来衡量不同因素影响的四款模型。之后我会给出除了算法选择之外其他学习词表示中额外因素的概述。最终我将呈现 Levy 等人的建议和结论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;全局矢量（&lt;strong&gt;GloVe&lt;/strong&gt;）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之前发布的那篇博客中，我们已经对流行的词嵌入模型进行了概述。我们遗漏的一个模型便是 GloVe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简而言之，GloVe 希望能明确表明 SNGS 的隐式操作：编码含义作为嵌入空间中的向量偏移——看起来只是一个偶然发现的 word2vec 的副产品——才是 GloVe 的特定目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体来说，GloVe 的作者表明两个词同现概率的比值（而不是它们的同现概率本身）是包含信息并计划作为向量差来编码信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了实现这一目标，他们提出了一种加权最小二乘法的物体 J，旨在最小化两个词的向量点积与它们共现次数的对数之间的差异。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYW5sHHSx6u0hyHW7DrzbBIm9lqBc2bkRBI63Bc65b5MNZCRuvePg68VQ/0?wx_fmt=png"/&gt;&lt;br/&gt;当 wi 和 bi 分别作为词语 i 的词向量和偏差，w~j 和 bj 分别作为词语 j 的文本词向量和偏差，Xij 是在词语 j 的文本中出现词语 i 的次数，而 f 是将相对低的权重分配给稀有和频繁共现的加权函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;共现次数可以被直接编码到词语上下文的共现矩阵中，GloVe 会将这样的矩阵而不是整个语料库作为输入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你想更多地了解 GloVe，最好的参考便是相关的论文或者附属网站&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（http://nlp.stanford.edu/projects/glove/）。除此之外，通过 Gensim 的作者，Quora 问答（https://www.quora.com/How-is-GloVe-different-from-word2vec）或是这篇发布的博客&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html），你可以对 GloVe 及其与 word2vec 的差异有更多的了解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;词嵌入与分布式语义模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;词嵌入模型，尤其是 word2vec 和 GloVe 变得如此流行的原因是它们的表现似乎一直以来都显著优于 DSMs。许多人将此归因于 Word2vec 的神经架构或是它能预测词语这个事实，这看起来要比只靠共现计数有天然的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以将 DSMs 看做计数模型，因为它们通过操作共现矩阵来计算词语的共现次数。相反，神经词嵌入模型可以被看作是一种预测模型，因为它们会去预测周围的词语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2014 年，Baroni 等人表明预测模型几乎在所有的任务中都优于计数模型，从而为词嵌入模型显而易见的优越性提供了一个清晰的证明。这就是终点了吗？并不是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经看到和 GloVe 的差异并不是那么明显：当 GloVe 被 Levy 等人认为是一个预测模型时，它显然是正在分解一个词语上下文共现矩阵，这使其更接近于诸如主成分分析（PCA）和潜在语义分析（LSA）等传统的方法。不止如此，Levy 等人还表示 word2vec 隐晦地分解了词语上下文的 PMI 矩阵。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，虽然在表面上 DSM 和词嵌入模型使用不同的算法来学习词语表示——前者计数，后者预测——但从根本上来说，两种类型的模型表现了相同的底层数据统计，即词语间的共现次数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，仍然存在同时也是这篇博客剩下的部分想要回答的一个问题是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么词嵌入模型的表现仍然比几乎拥有相同信息的 DSM 更好？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;继续 Levy 等人 2015 年的成果，我们将分离和鉴定影响神经词嵌入模型的因素并展示它们是如何通过比较以下四个模型来被转移到传统方法中的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;PPMI：PMI 是衡量两个词之间相关性强度的一个常用指标。它是两个词 w 和 c 之间联合概率与边际概率乘积之间的对数比：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWOia5bxY5VCxkoKfhwib0ahN34EjdflZh0icBibz0SbHyRVBhjVrq41hrtg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;PMI(w,c)=logP(w,c)P(w)P(c)。由于词对（w,c）的 PMI(w,c)=log0=−∞从未出现，所以 &amp;nbsp; &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;实际上 PMI 经常被 PPMI 替代，其将负值看作是 0，即：PPMI(w,c)=max(PMI(w,c),0)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;奇异值分解（SVD）：SVD 是最流行的降维方法之一，最初是通过潜在语义分析（LSA）进入自然语言处理（NLP）。SVD 将词文本共现矩阵转化为三阶矩阵 &amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWEPtaqjOxvrzHIOazkc2VaEvS3RGzibS3iaZAxQUE25J0f4aEQUWX7UYg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;的产物，其中 U 和 V 是正交矩阵（即方形矩阵的行和列是正交单位向量），Σ是特征 &amp;nbsp; &amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;值在减弱过程中的对角矩阵。实际上，SVD 经常被用于因式分解由 PPMI 产生的矩&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;阵。一般来说，只有Σ顶端的 d 元素被保存，从而得到：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWKA91ZiaCZyIckSFrmbv1LTibAMWxx8XawYFNdA1Op885Ndrs4ibzEPQVg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;通常被分别用来表示词语和上下文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;基于负采样的 Skip-gram 模型也就是 word2vec：要了解更多 skip-gram 结构和负采样可以参考我之前的博客文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;全局矢量（GloVe）已经在上一节中介绍过。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;超参数&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们来看看以下这些超参数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;预处理&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态上下文窗口&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;常用词下采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;删除罕见词语&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关联度量&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;转移&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;上下文分布平滑&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;后期处理&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;添加上下文向量&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;特征值加权&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;向量规范化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;预处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Word2vec 引入了三种预处理语料库的方法，这三种方法也可以很容易地应用于 DSM。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动态上下文窗口&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一般来说，在 DSM 中，上下文窗口并未加权，并且具有恒定大小。然而，SGNS 和 GloVe 使用了一种会将更多权重分配给更接近的词语的方案，因为更接近的词通常被认为对词的意义更重要。此外，在 SGNS 中，窗口大小不是固定的，但实际窗口大小是动态的，并且在训练期间在 1 和最大窗口之间均匀采样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;常用词下采样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SGNS 通过概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWzj0badbicGXQHy7tgY8viadtgzbSt038w8E4bsicXb6rtvvvEzTFicl0Hg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来随机去除频率 f 高于某个阈值 t 的词语，从而获得那些非常频繁出现的词语。由于这种下采样是在实际创建窗口之前完成的，因此 SGNS 在实践中所使用的上下文窗口要大于实际所指示的上下文窗口。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;删除罕见词语&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 的预处理中，罕见词语也在创建上下文窗口之前被删除，这进一步增加了上下文窗口的实际大小。虽然 Levy 等人在 2015 年发现这并不会对性能产生什么重大的影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;关联度量&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI 已被证明是用于测量词语之间关联程度的有效方式。Levy 和 Goldberg 在 2014 年已经表示 SGNS 对 PMI 矩阵进行隐含地因式分解，因此可以将该公式的两个变化引入到常规 PMI 中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI 转移&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 中，负样本 K 的数量越多，使用的数据就越多，参数的估计也越好。K 影响了有 word2vec 隐性因式分解的 PMI 矩阵偏移，即 k 通过 log k 来转移 PMI 值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们将其转换为常规 PMI，我们获得 Shifted PPMI（SPPMI）：SPPMI(w,c)=max(PMI(w,c)−logk,0)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上下文分布平滑&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 SGNS 中，根据平滑的一元分布，即提高到α的幂的一元分布来对负样本进行采样，并根据经验将其设置为 34。这会导致频繁的词语被采样的几率要比它们频率所指示的相对较少。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以通过将上下文词汇 f（c）的频率同等地提高到α的幂来将其传送到 PMI：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PMI（w，c）= log p（w，c）p（w）pα（c）其中 pα（c）= f（c）αΣcf（c）α和 f（x）是字 x 的频率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;后期处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类似于预处理，可以使用三种方法来修改由算法产生的词向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;添加上下文向量&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GloVe 的作者建议添加词嵌入和上下文向量以创建最终输出向量，例如： v⃗cat =w⃗cat +c⃗catv→cat = w→cat + c→cat。这增加了一阶相似性项，即 w⋅v。然而，该方法不能应用于 PMI，因为 PMI 产生的是稀疏向量。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;特征值加权&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;SVD 产生以下矩阵：WSVD = Ud·Σd 和 CSVD = Vd。然而，这些矩阵具有不同的性质：CSVD 是标准正交的，而 WSVD 不是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相反，SGNS 更对称。因此，我们可以用可调整的附加参数 pp 来对特征值矩阵Σd 加权，以产生以下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;WSVD = Ud·Σpd。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;向量规范化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们还可以将所有向量归一化为单位长度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy 等人在 2015 年训练了英文维基百科所有转储模型，并基于常用词语的相似性和类比数据集对它们进行了评价。你可以在他们的论文中了解有关实验设置和培训详情的更多信息。我们在下文总结出了最重要的结果和收获。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;额外收获&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy 等人发现 SVD——而不是词嵌入算法的其中一种——在相似性任务上执行得最好，而 SGNS 在类比数据集上执行得最好。他们还阐明了与其他选择相比，超参数的重要性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 超参数与算法：超参数设置通常比算法选择更重要。没有任意单一的算法能始终胜过其他方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 超参数与更多数据：在更大的语料库上训练对某些任务有帮助。在 6 个例子中有 3 个都能说明，调整超参数更有益。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;揭露之前的观点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了这些见解，我们现在可以揭露一些普遍存在的观点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;嵌入式比分布式方法优秀吗？使用正确的超参数，没有一种方法比另一种方法具有持续的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;GloVe 是否优于 SGNS？SNGS 在所有任务上都胜过 GloVe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;CBOW 是不是很好的 word2vec 配置？CBOW 在任何任务上都比不上 SGNS。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;建议&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后——也是这篇文章中我最喜欢的一部分——我们可以给出一些具体的实际建议：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不要将迁移的 PPMI 与 SVD 一起使用。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;不要「正确」使用 SVD，即不使用特征向量加权（与使用 p = 0.5 的特征值加权相比性能下降 15 个点）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请使用具有短上下文（窗口大小为 22）的 PPMI 和 SVD。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请使用 SGNS 的许多负样本。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对于所有方法，请始终使用上下文分布平滑（将一元分布提高到α= 0.75 的幂）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 SGNS 作为基准（训练更加稳健，快速和经济）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;请尝试在 SGNS 和 GloVe 中添加上下文向量。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些结果与通常假设的情况背道而驰，即词嵌入优于传统方法，并且表明它通常没有什么区别，无论使用词嵌入还是分布式方法 - 重要的是，你调整超参数并使用适当的预处理和后期处理步骤。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自 Jurafsky 小组 [5 , 6 ] 的最新论文回应了这些发现，并表明 SVD——而不是 SGNS——通常是当你关心精确词语表达时的首选。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我希望这篇博客对于目前备受关注的，揭示传统分布语义和嵌入模式之间的联系的研究能够有所帮助。正如我们所看到的，分布式语义的知识使得我们可以改进当前的方法并开发现有方法的全新变体。为此，我希望下一次训练词嵌入时，您会将分布式方法纳入考虑范围，或从这些思考中获益。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考文献：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy, O., Goldberg, Y., &amp;amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570 ↩&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Baroni, M., Dinu, G., &amp;amp; Kruszewski, G. (2014). Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. ACL, 238–247. http://doi.org/10.3115/v1/P14-1023&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hamilton, W. L., Clark, K., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Retrieved from http://arxiv.org/abs/1606.02820&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hamilton, W. L., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. arXiv Preprint arXiv:1605.09096.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>首发 | B轮融资3000万美元，Clarifai要让每个人都用上人工智能</title>
      <link>http://www.iwgc.cn/link/3222893</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心中文首发&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;编辑：李泽南、&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;杜夏德&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Clarifai 今日宣布完成 3000 万美元的 B 轮融资。该轮融资由 Menlo Ventures 领投，其他投资方，包括 Union Square Ventures、Lux Capital、高通、Osage University Partners 等&lt;span&gt;均参与过之前轮次的投资&lt;/span&gt;。经过本轮融资，Clarifai 已累计获得了 4125 万美元资金。Clarifai &lt;span&gt;创立于 2013 年，&lt;/span&gt;是一家专注于人工智能图像识别领域的创业公司。自 ImageNet 2013 年竞赛获得图像分类前五名之后，Clarifai 一直处于行业领先地位。现在， Clarifai 的图像识别系统每月分类超过 12 亿个图片和视频。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;Menlo Ventures 常务董事 Matt Murphy 评论道，「我们正处在漫长的人工智能时代的开端。从创建伊始，Clarifai 就将目标定位于让所有人接触到人工智能的力量，他们的队伍始终在为这一愿景而努力着，而 Menlo Ventures 很高兴能为这样的公司提供助力。」&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们对 Clarifai 的信心不仅来源于公司创始人 Zeiler ，他对于成功的渴望驱动着他的团队不断拓展人工智能的新领域；我们也相信除了科技巨头之外，大众化的人工智能也需要这样独立公司的存在。」Lux Captial 高级合伙人 Zavain Dar 说道。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;新的融资将会加速 Clarifai 开发和商务队伍的扩张，标志着公司将会招揽更多高端研究人才，持续快速地发展其人工智能产品。Clarifai 目前已拥有一套独立的人工智能和机器学习平台，可为各种开发者与不同行业提供服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此轮投资消息正值 Clarifai 最新产品推出之际：前不久，公司刚刚发布了自定义训练和视觉搜索。自定义训练将人工智能的能力传递到每个人手中，让每个人都能在数秒内为 Clarifai 的视觉 API 「教授」新概念，无论你是否拥有专业技术。视觉搜索允许使用者通过图片相似性或关键词轻松地编排、访问、或向他人推荐图片或产品，使开发者和企业能够更好地让用户找到他们想要的。这种先进的技术，以往只能在大型科技公司的产品中找到，Clarifai 希望通过他们的 API，让其成为人人都可以使用的工具。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9LprWu7eD2xib3jhgiaXmPYWNVErgxT5kVzQHsKRVokw7FpVE5hYOBGxo444HjicAIwcmqMLSxBFiafA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;不同于谷歌 Cloud Vison，IBM Watson，微软的通用和领域模型，Clarifai 认为人工智能的未来在于让每个人训练自己的个性化模型&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Clarifai 的视觉识别 API 可以识别超过 11,000 种不同内容的照片和视频，公司同时也提供应用于特定领域的识别工具，包括成人内容（NSFW）识别模型，用于识别可能违规的裸体图像；旅游模型，可以识别旅游有关的概念如「热水浴缸」，「儿童区」，和「室内游泳池」；公司最近还推出了食物模型，将识别提高到了食品原料的程度。Clarifai 有着多元化的客户群体，从《财富》 500 强公司到小型的开发团队都是他们的服务对象，包括 Buzzfeed，Trivago，500px，StyleMePretty 等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Clarifai 是在让所有人都能使用人工智能的理想之下建立的，不论使用者的财力，硬件条件与技术水平如何，Clarifai 可以帮助他们提升自己的产品，改善人们的生活，」Clarifai 创始人和首席执行官 Matt Zeiler 说道。「作为一家独立公司，我们拥有独一无二的优势，我们可以快速决策，保持高速创新，而且始终如一。所有这些都是为了利用数据向合作伙伴提供专属的业务需求。这笔新的资金会加速我们的创新，同时继续我们的使命，我们将创造一个强大的人工智能平台，让人工智能为所有人所用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©机器之心中文首发，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>机器之心招聘：我们带你去未来</title>
      <link>http://www.iwgc.cn/link/3222894</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;qqmusic class="res_iframe qqmusic_iframe js_editor_qqmusic" scrolling="no" frameborder="0" musicid="5050285" mid="0039LBAL1jOfnM" albumurl="/E/p/004HJb7I0kQaEp.jpg" audiourl="http://ws.stream.qqmusic.qq.com/C1000039LBAL1jOfnM.m4a?fromtag=46" music_name="A&amp;nbsp;Thousand&amp;nbsp;Miles" commentid="150771740" singer="David&amp;nbsp;Archuleta&amp;nbsp;-&amp;nbsp;A&amp;nbsp;Thousand&amp;nbsp;Miles" play_length="263000" src="/cgi-bin/readtemplate?t=tmpl/qqmusic_tmpl&amp;amp;singer=David%20Archuleta%20-%20A%20Thousand%20Miles&amp;amp;music_name=A%20Thousand%20Miles"&gt;&lt;/qqmusic&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心是国内领先的前沿科技媒体和产业服务平台，关注人工智能、机器人和神经认知科学，坚持为从业者提供高质量内容和多项产业服务。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体业务方面，机器之心最早在微信公众号平台开始运营，目前已经覆盖了微信、今日头条、百度百家、腾讯内容开放平台等多个大型内容平台，并运营着自己的官方网站。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体之外，机器之心还将依托联想之星Comet Labs的全球资源平台为人工智能领域的参与者提供各项产业服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为生产更多的高质量内容，提供更好的产业服务，我们需要更多的小伙伴加入进来！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线上运营（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司内容产品线上微信、微博、今日头条等渠道的运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.负责上述渠道推广效果分析和经验总结，建立有效运营手段；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.配合线下活动运营进行策划执行工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.配合内容团队进行选题策划和对外推广。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.1-2 年新媒体运营相关经验，具备一定的文字功底，有线下活动组织和策划经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.熟悉微信后台操作，有多渠道沟通经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.善于沟通交流，有一定抗压和创新能力，强责任心和高执行力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线下活动运营（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司线下活动的策划与运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.与内容团队和线上运营共同策划相关选题；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.执行公司商务需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.较强的沟通能力和资源整合能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.执行力强，具备一定的创新能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.1-2 年线下活动组织和策划经验，有活动相关供应商资源者优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;商务总监（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司特定行业客户的商务合作推动与对接，维护与建设积极的客户关系；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.参与制定公司的商务目标、原则、计划和战略，建立完善公司的商务体系及相关制度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.负责重大业务商务谈判的策略制定和执行以及合同的签订；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.负责拓展新的业务渠道，拓展特定行业的典型大客户，并跟踪协调执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，3 年以上工作经验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.良好的沟通、协调和商务谈判能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.具有较强的业务开拓能力、市场洞察力和行业分析能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.有较强责任感和抗压能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;记者（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;工作职责：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责人工智能和前沿技术领域的常规内容生产，撰写人物故事和产业报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.有独立的选题策划和操作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.挖掘国内外人工智能领域的优秀创业公司并进行报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.对国内外人工智能领域的创业者、公司高管、行业专家、科研专家进行深度专访；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.协助其他部门完成相关工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;岗位要求：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.优秀的写作能力，能够驾驭特写、人物故事、常规报道和资讯等各类内容形式；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.对前沿科技充满兴趣和热情，拥有迅速掌握某个特定行业或领域的学习能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对内容有品味，文字功底深厚，执行力强；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.1-2 年科技媒体从业经历， 能够适应新媒体和创业公司的工作节奏，有良好的职业精神和团队意识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;全职编译（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.编译、校对英文文章；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.撰写技术、产品、公司和行业相关文章，撰写分析报告。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.卓越的英语翻译能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.英语或计算机相关专业毕业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对前沿技术有一定了解和热情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;高级分析师（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.人工智能行业数据监测、分析及行业研究报告的撰写；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2. 负责与人工智能领域内各企业的沟通及定期跟踪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，计算机科学、商科、社会学或相关专业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.具备一定的数据分析和洞察能力，对ha数据拥有一定敏锐度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟练使用相关数据分析工具并进行信息搜集整理、图表制作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.良好的文字功底和写作能力和英文阅读能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Web 前端开发（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责机器之心网站部分页面及交互优化修改；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.跟远程后台人员合作，优化整个系统，参与站点维护工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对现有产品的可用性测试和评估提出改进方案，持续优化产品的用户体验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.积极参与工作相关技术研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.拥有良好的口头表达能力、善于学习新的技术；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.年以上 web 前端开发经验，懂 nginx 或者 Apache 操作，有一定的 Linux 系统操作经验，熟悉常用命令；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟悉常用的 js 库，例如 jquery，需掌握 amazeui 敏捷开发框架（bootstrap 亦可）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.熟悉常用的前端 MVC 框架，必须熟悉 gulp、sass；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.具有良好的沟通能力和团队协作能力, 能够与产品经理, 项目经理, 形成良好, 有效的沟通。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;实习生（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.网站、新媒体内容更新；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.翻译、校对英文文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.协助线下活动运营。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.出色的英语阅读和中文写作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.每周能保证 2-3 天坐班；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.积极的学习态度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.计算机科学等理工科专业、英语专业专业优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有意者请将简历发送至：hr@jiqizhixin.com，或添加微信 JuveAlex，zhaoyunfeng1984&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 25 Oct 2016 19:37:26 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 在语音识别这件事上，汉语比英语早一年超越人类水平（附论文）</title>
      <link>http://www.iwgc.cn/link/3204725</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：吴攀、李亚洲&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;几天前，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=1&amp;amp;sn=0c6387d422cf9765b9b10c178d160680&amp;amp;chksm=871b021db06c8b0b0b0447124c5c07818f53a17ba470305049af1d7d49806c87e07307a93811&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;微软语音识别实现了历史性突破&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;，英语的语音转录达到专业速录员水平，&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719843&amp;amp;idx=2&amp;amp;sn=4611f810734df8a72286567e90c65a92&amp;amp;chksm=871b021db06c8b0b283ac93f267d95365392be02b3cde5d2fe6df85b359aa96368f25318e2f5&amp;amp;scene=21#wechat_redirect" style="text-decoration: underline; font-size: 12px; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心也独家专访了专访微软首席语音科学家黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt; ，了解到词错率仅 5.9% 背后的「秘密武器」——CNTK。但微软的成果是在英语水平上的，从部分读者留言中我们了解到对汉语语音识别的前沿成果不太了解，这篇文章将向大家介绍国内几家公司在汉语识别上取得的成果（文中提到的论文可点击阅读原文下载）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;10 月 19 日，微软的这条消息发布之后在业内引起了极大的关注。语音识别一直是国内外许多科技公司发展的重要技术之一，微软的此次突破是识别能力在英语水平上第一次超越人类。在消息公开之后，百度首席科学家吴恩达就发推恭贺微软在英语语音识别上的突破，同时也让我们回忆起一年前百度在汉语语音识别上的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cviag8vMQYUdriajb2nmBmvwSBT0tbKoDFMSGlutZ3JdicAPn38IQvCRiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;吴恩达：在 2015 年我们就超越了人类水平的汉语识别；很高兴看到微软在不到一年之后让英语也达到了这一步。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;百度 Deep Speech2，汉语语音识别媲美人类&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 12 月，百度研究院硅谷人工智能实验室（SVAIL）在 arXiv 上发表了一篇论文《Deep Speech 2: End-to-End Speech Recognition in English and Mandarin（Deep Speech 2：端到端的英语和汉语语音识别）》，介绍了百度在语音识别技术的研究成果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9clIVppxH7FBeCf98MwHicHyqadAuqKtGZM8AQcCe00XpXEwcYQBYXN3Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们的研究表明一种端到端的深度学习（end-to-end deep learning）方法既可以被用于识别英语语音，也可以被用于识别汉语语音——这是两种差异极大的语言。因为用神经网络完全替代了人工设计组件的流程，端到端学习让我们可以处理包含噪杂环境、口音和不同语言的许多不同的语音。我们的方法的关键是 HPC（高性能计算）技术的应用，这让我们的系统的速度超过了我们之前系统的 7 倍。因为实现了这样的效率，之前需要耗时几周的实验现在几天就能完成。这让我们可以更快速地迭代以确定更先进的架构和算法。&lt;strong&gt;这让我们的系统在多种情况下可以在标准数据集基准上达到能与人类转录员媲美的水平。&lt;/strong&gt;最后，通过在数据中心的 GPU 上使用一种叫做的 Batch Dispatch 的技术，我们表明我们的系统可以并不昂贵地部署在网络上，并且能在为用户提供大规模服务时实现较低的延迟。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文中提到的 Deep Speech 系统是百度 2014 年宣布的、起初用来改进噪声环境中英语语音识别准确率的系统。在当时发布的博客文章中，百度表示在 2015 年 SVAIL 在改进 Deep Speech 在英语上的表现的同时，也正训练它来转录汉语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当时，百度首席科学家吴恩达说：「SVAIL 已经证明我们的端到端深度学习方法可被用来识别相当不同的语言。我们方法的关键是对高性能计算技术的使用，相比于去年速度提升了 7 倍。因为这种效率，先前花费两周的实验如今几天内就能完成。这使得我们能够更快地迭代。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=f0339hys92j&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语音识别技术已经发展了十多年的时间，这一领域的传统强者一直是谷歌、亚马逊、苹果和微软这些美国科技巨头——据 TechCrunch 统计，美国至少有 26 家公司在开发语音识别技术。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是尽管谷歌这些巨头在语音识别技术上的技术积累和先发优势让后来者似乎难望其项背，但因为一些政策和市场方面的原因，这些巨头的语音识别主要偏向于英语，这给百度在汉语领域实现突出表现提供了机会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为中国最大的搜索引擎公司，百度收集了大量汉语（尤其是普通话）的音频数据，这给其 Deep Speech 2 技术成果提供了基本的数据优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过有意思的是，百度的 Deep Speech 2 技术主要是在硅谷的人工智能实验室开发的，其研究科学家（名字可见于论文）大多对汉语并不了解或说得并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这显然并不是问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Deep Speech 2 在汉语上表现非常不错，但其最初实际上并不是为理解汉语训练的。百度美国的人工智能实验室负责人 Adam Coates 说：「我们在英语中开发的这个系统，但因为它是完全深度学习的，基本上是基于数据的，所以我们可以很快地用普通话替代这些数据，从而训练出一个非常强大的普通话引擎。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c0icKPaM9FU1ic1UjIzLkkeuQr75iaAzSd9t52W5wb7MNa3SOGvGaRa82Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于英语和普通话的 Deep Speech 2 系统架构，它们之间唯一的不同是：普通话版本的输出层更大（有 6000 多个汉语字符），而英语版本的只有 29 个字符。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该系统能够识别「混合语音（hybrid speech）」——很多普通话说话人会组合性地使用英语和普通话。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Deep Speech 2 于 2015 年 12 月首次发布时，首席科学家吴恩达表示其识别的精度已经超越了 Google Speech API、wit.ai、微软的 Bing Speech 和苹果的 Dictation 至少 10 个百分点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据百度表示，到今年 2 月份时，Deep Speech 2 的短语识别的词错率已经降到了 3.7%（参考阅读：&lt;span&gt;http://svail.github.io/mandarin/&lt;/span&gt;）！Coates 说 Deep Speech 2 转录某些语音的能力「基本上是超人级的」，能够比普通话母语者更精确地转录较短的查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度在其技术发展上大步迈进，Deep Speech 2 目前已经发展成了什么样还很难说。但一项技术终究要变成产品和服务才能实现价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;科大讯飞的语音识别&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;百度的 Deep Speech 识别技术是很惊人，但就像前文所说一项技术终究要变成产品和服务才能实现价值，科大讯飞无疑在这方面是做得最好的公司之一。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;科大讯飞在自然语言处理上的成就是有目共睹的，在语音识别上的能力从最初到现在也在不断迭代中。2015 年 9 月底，机器之心对&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=209680426&amp;amp;idx=1&amp;amp;sn=25626f224a84380b4902b077397fc3eb&amp;amp;scene=21#wechat_redirect"&gt;胡郁的一次专访中&lt;/a&gt;，他就对科大讯飞语音识别技术的发展路线做过清晰的介绍：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞很好地跟随了语音识别的发展历史，深度神经网络由 Geoffrey Hinton 与微软的邓力研究员最先开始做，科大讯飞迅速跟进，成为国内第一个在商用系统里使用深度神经网络的公司。谷歌是最早在全球范围内大规模使用深度神经网络的公司，谷歌的 Voice Search 也在最早开创了用互联网思维做语音识别。在这方面，科大讯飞受到了谷歌的启发，在国内最早把涟漪效应用在了语音识别上面，因此超越了其他平台。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;科大讯飞最初使用隐马尔可夫模型，后面开始在互联网上做，2009 年准备发布一个网页 demo，同年 9 月份安卓发布之后开始转型移动互联网，并于 2010 年 5 月发布了一个可以使用的手机上的 demo；2010 年 10 月份发布了语音输入法和语音云。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;整个过程中最难的地方在于，当你不知道这件事情是否可行时，你能够证明它可行。美国那些公司就是在做这样的事情。而科大讯飞最先领悟到，并最先在国内做的。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到今年 10 月份刚好过去一年，科大讯飞的语音识别技术在此期间依然推陈出新，不断进步。去年 12 月 21 日，在北京国家会议中心召开的以「AI 复始，万物更新」为主题的年度发布会上，科大讯飞提出了以前馈型序列记忆网络（FSMN, Feed-forward Sequential Memory Network）为代表的新一代语音识别系统。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cXGicdTiciazC0I6f8l77vqeEoGMRrULEcqV6dyaDwescVk19W9wj4PGDQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;论文摘要：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;在此论文中，我们提出了一种新的神经网络架构，也就是前馈型序列记忆网络（FSMN），在不使用循环前馈的情况下建模时间序列中的 long-term dependency。此次提出的 FSMN 是一个标准的全连接前馈神经网络，在其隐层中配备了一些可学习的记忆块。该记忆块使用一个抽头延时线结构将长语境信息编码进固定大小的表征作为短期记忆机制。我们在数个标准的基准任务上评估了 FSMN，包括语音识别和语言建模。实验结果表明，FSMN 在建模语音或语言这样的序列信号上，极大的超越了卷积循环神经网络，包括 LSTM。此外，由于内在无循环模型架构，FSMN 能更可靠、更快速地学习。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来通过进一步的研究，在 FSMN 的基础之上，科大讯飞再次推出全新的语音识别框架，将语音识别问题重新定义为「看语谱图」的问题，并通过引入图像识别中主流的深度卷积神经网络（CNN, Convolutional Neural Network）实现了对语谱图的全新解析，同时打破了传统深度语音识别系统对 DNN 和 RNN 等网络结构的依赖，最终将识别准确度提高到了新的高度。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后来，科大讯飞又推出了全新的深度全序列卷积神经网络（Deep Fully Convolutional Neural Network, DFCNN）语音识别框架，使用大量的卷积层直接对整句语音信号进行建模，更好的表达了语音的长时相关性，比学术界和工业界最好的双向 RNN 语音识别系统识别率提升了 15% 以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBVRabcLJkjdEGhbwAo4ITz6PAOAEYhlPpV44E9gY5mKna244FKmicFA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;DFCNN 的结构图&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DFCNN 的结构如图所 示，DFCNN 直接将一句语音转化成一张图像作为输入，即先对每帧语音进行傅里叶变换，再将时间和频率作为图像的两个维度，然后通过非常多的卷积层和池化（pooling）层的组合，对整句语音进行建模，输出单元直接与最终的识别结果（比如音节或者汉字）相对应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;搜狗语音识别&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;纵观整个互联网行业，可以说搜狗作为一家技术型公司，在人工智能领域一直依靠实践来获取更多的经验，从而提升产品使用体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在前几天的锤子手机新品发布会上罗永浩现场演示了科大讯飞的语音输入之后，一些媒体也对科大讯飞和搜狗的输入法的语音输入功能进行了对比，发现两者在语音识别上都有很不错的表现。比如《齐鲁晚报》的对比结果：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;值得一提的是，得益于创新技术，搜狗还拥有强大的离线语音识别引擎，在没有网络支持的情况下依旧可以做到中文语音识别，以日常语速说话，语音识别仍然能够保持较高的准确率。这一点科大讯飞表现也较为优秀，两者可谓旗鼓相当。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;整体体验下来，搜狗在普通话和英文的语音输入方面表现，与讯飞相比可以说毫不逊色，精准地识别能力基本可以保证使用者无需进行太多修改。此前在搜狗的知音引擎发布会上，搜狗语音交互技术项目负责人王砚峰称「搜狗知音引擎具备包括端到端的语音识别、强大的智能纠错能力、知识整合使用能力以及多轮对话和复杂语义理解能力」，这些都有效保证了搜狗语音输入在识别速度、精准度、自动纠错、结合上下文语意理解纠错方面收获不错的表现。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;八月份，搜狗发布了语音交互引擎——知音，其不仅带来了语音识别准确率和速度的大幅提升，还可以与用户更加自然的交互，支持多轮对话，处理更复杂的用户交互逻辑，等等。知音平台体现出搜狗在人工智能技术领域的长期积累，同时也能从中看出他们的技术基因和产品思维的良好结合。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cfJmmVV0ZbP4CBWibicb3SlSQjNya8q0BtjlBiauIQNo7tvoWpl10YcFkA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;搜狗知音引擎&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;搜狗把语音识别、语义理解、和知识图谱等技术梳理成「知音交互引擎」，这主要是强调两件事情，一是从语音的角度上让机器听的更加准确，这主要是识别率的提升；另一方面是让机器更自然的听懂，这包括在语义和知识图谱方面的发展，其中包括自然语言理解、多轮对话等技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cV8PxINicWJXJ9FEawYWavia1CNzde2ibyot94tV8ibSibIw0tUoFNA2gdvw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;语音识别系统流程：语音信号经过前端信号处理、端点检测等处理后，逐帧提取语音特征，传统的特征类型包括 MFCC、PLP、FBANK 等特征，提取好的特征送至解码器，在声学模型、语言模型以及发音词典的共同指导下，找到最为匹配的词序列作为识别结果输出。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKZW6DiaAocs5146zTGq6G0RBwNr4SuZgnYUOPJTIibRuiauDAaZUYLOSA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;CNN 语音识别系统建模流程&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据搜狗上个月的一篇微信公众号文章写道：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;在语音及图像识别、自然语言理解等方面，基于多年在深度学习方面的研究，以及搜狗输入法积累的海量数据优势，搜狗语音识别准确率已超 97%，位居第一。&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不过遗憾的是，搜狗还尚未公布实现这一结果的相关参数的技术信息，所以我们还不清楚这样的结果是否是在一定的限定条件下实现的。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像TechCrunch 统计的美国有 26 家公司开发语音识别技术一样，中国同样有一批专注自然语言处理技术的公司，其中云知声、思必驰等创业公司都在业内受到了极大的关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLnHUIeJZIyticlic6SpxIt7b3dIdAldSU3w3tCgj84DXeHOSzbC6tbEQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上图展示了云知声端到端的语音识别技术。材料显示，云知声语音识别纯中文的 WER 相对下降了 20%，中英混合的 WER 相对下降了 30%。&lt;br/&gt;&lt;br/&gt;在今年 6 月机器之心对云知声 CEO 黄伟（参见：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715927&amp;amp;idx=1&amp;amp;sn=818a7ef31a186c81f7a6dabfe00326c2&amp;amp;scene=21#wechat_redirect"&gt;专访云知声CEO黄伟：如何打造人工智能「云端芯」生态闭环&lt;/a&gt;）的专访中，黄伟就说过 2012 年年底，他们的深度学习系统将当时的识别准确率从 85% 提升到了 91% 。后来随着云知声不断增加训练数据，如今识别准确率已经能达到 97% ，属于业内一流水平，在噪音和口音等情况下性能也比以前更好。&lt;br/&gt;&lt;br/&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716127&amp;amp;idx=1&amp;amp;sn=723be60f4cb849ae453eeab2183f6017&amp;amp;scene=21#wechat_redirect"&gt;思必驰的联合创始人兼首席科学家俞凯&lt;/a&gt;是剑桥大学语音博士，上海交大教授。他在剑桥大学待了 10 年，做了 5 年的语音识别方面的研究，后来做对话系统的研究。整体上，思必驰做的是语音对话交互技术的整体解决方案，而不是单纯的语音识别解决方案。因此在场景应用中，思必驰的系统和科大讯飞的系统多有比较，可相互媲美。&lt;br/&gt;&lt;br/&gt;当然，此领域内还有其他公司的存在。这些公司都在努力加速语音识别技术的提升。语音识别领域依然有一系列的难题需要攻克，就像微软首席语音科学家黄学东接受机器之心专访时所说的那样，「理解语义是人工智能下一个需要攻克的难题，要做好语音识别需要更好的语义理解，这是相辅相成的。」&lt;/span&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
    <item>
      <title>技术| 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</title>
      <link>http://www.iwgc.cn/link/3204726</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自SebastianRuder Blog&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Sebastian Ruder&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：冯滢静&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是词嵌入系列博客的 Part2，全面介绍了词嵌入模型， Part1请点击&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719971&amp;amp;idx=2&amp;amp;sn=c7e0d1f6dd4e9ddce291e9bc2c85c65f&amp;amp;chksm=871b029db06c8b8b7557095989dd3fdb57b86a1d7923c388ca1e74255d07f08992bb0461d958&amp;amp;scene=21#wechat_redirect" style="font-size: 12px; text-decoration: underline;"&gt;&lt;em&gt;&lt;span&gt;技术 | 词嵌入系列博客Part1：基于语言建模的词嵌入模型&lt;/span&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csHIKEblezwKfQKC9eNPNmd6x1ONG6bMsRjOsJHz6XDqTKEtFx4ev8A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于Softmax的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层次Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微分Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNN-Softmax&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基于采样（Sampling）的方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;具有适应的重要性采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目标采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;噪音对比估计&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;负采样&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;低频的标准化&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;其他方法&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;选择哪一种方法？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇博文是我们词嵌入及其表示系列的第二篇。在上一篇博文里，我们提供了词嵌入模型的概述，并介绍了 Bengio 等人在2003年提出的经典神经语言学习模型、Collobert 和 CWeston 在2008年提出的 C&amp;amp;W 模型，以及Mikolov 在2013年提出的 word2vec 模型。我们发现，设计更好的词嵌入模型的最大挑战，就是如何降低softmax 层的计算复杂度。而且，这也是机器翻译（MT）（Jean等人[10 ]）和语言建模（Jozefowicz等人[6 ]）的共通之处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这篇博文里，我们将要重点介绍过去几年的研究中 softmax 层的不同近似方法，它们其中的一些被运用在语言建模和机器学习。在下一篇博文里，我们才会介绍别的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了统一以及便于比较，让我们简单重新介绍一下上一篇博文的重点：我们假设训练集是一串包括 T 个训练词的字符序列 w1,w2,w3,⋯,wT ，每一个词来源于大小为 |V| 的词汇库 V。模型大体上考虑n个词的上下文 c。我们将每一个词和一个 d 维度的输入向量（也就是表示层的词嵌入）vw 以及输出向量 v′w（在softmax层的权重矩阵的对于词的表示）联系在一起。最终，我们相对于我们的模型参数 θ 来优化目标函数 Jθ。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们一起来回顾一下，softmax 层对于一个词 w 在上下文 c 出现的概率的计算公式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cSM7Vh72kwdkmn1icCfS58XRTmCwEZLEogeNaUybOAPU1Riam40mKjfBQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，h 是倒数第二层的输出向量。注意到和之前提到的一样，我们用c来表示上下文，并且为了简洁，舍弃目标词 wt 的索引 t。计算softmax的复杂度很高，因为计算 h 和 V 里每个词 wi 的输出向量的内积需要通过一部分的分母的求和来获得，从而得到标准化后目标词 w 在上下文 c 的概率。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来我们将会讨论近似 softmax 所采用的不同方法。这些方法可以分成两类：一类是基于 softmax 的方法，另一类则是基于采样的方法。基于 softmax 的方法保证 softmax 层的完好无损，但是修改了它的结构来提升它的效率。基于采样的方法则是完全去掉 softmax 层，优化其它目标函数来近似 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于 softmax 的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层次的 softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层次的softmax（H－Softmax）是 Morin 和 Bengio[3]受到二叉树启发的方法。根本上来说，H-softmax 用词语作为叶子的多层结构来替代原 softmax 层的一层，如图1所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这让我们把对一个词出现概率的计算分解成一连串的概率计算，我们将无需对所有词作昂贵的标准化。用 H-Softmax 来替代单一的 softmax 层可以把预测词的任务带来至少50倍的速度提升，因此适用于要求低延迟的任务，比如 Google 的新通讯软件 Allo 的实时沟通功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cTFc71iaicfxRXvC23pN5ewrIYkZRSgAmFvUbaAicGKibLyOt7tpHFDnl1w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：多层词的 softmax&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把常规的 softmax 想成是只有一层深度的树，每个 V 中的词都是一个叶子节点。计算一个词的 softmax的概率则需要标准化所有 |V| 个叶子的概率。反之，如果我们把 softmax 当成一个每个词都是叶子的二叉树，我们只需要从叶子节点开始沿着树的路径走到那个词，而无序考虑其它的词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为一个平衡的二叉树的深度为 log2(|V|)，我们只需要通过计算最多 log2(|V|) 个节点来取得一个词最终的概率。注意到这个概率都已经经过了标准化，因为二叉树中所有叶子节点的概率之和为1，所以形成了一个概率分布。想要粗略地验证它，我们可以推理一个树的根节点（图1中的节点0），它的所有分支必须相加为1。对于每个接下来的节点，概率质量分解给它的分支，直到最终到达叶子节点，也就是词。因为这其中没有损失概率，而且所有词都是叶子，所有词的概率的总和必须为1，所以分层次的 softmax 定义了在 V 上所有词的一个标准化的概率分布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体而言，当我们遍历树的时候，我们需要计算每个分支点的左右两个分支来计算这个节点的概率。正因如此，我们要为每个节点指定一个表示。对比于规律的 softmax，对于每个词，我们因此不需要 v'w 的输出词嵌入——反之，我们用给每个节点 n 都指定词嵌入 v′n。因为我们有 |V|−1 个节点，而每一个都拥有一个唯一的表示，H-Softmax 参数都和普通的 softmax 几乎一样。我们现在可以计算给定上下文 c 一个节点 n 的右分支（或左分支）的概率了，方式如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAicibJK1LvFZ28em66ZtbEteGvlvOnWicJGceVWRRHWiaKRaiaiaCpTiapNpA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个方法和普通的 softmax 的计算方式几乎一致。现在，我们不计算h和词嵌入 v′w 的点乘积，而是计算每个树节点的h和词嵌入 v′w。另外，我们不计算整个词汇库里所有词的概率分布，我们仅仅输出一个概率，在这个例子中这个概率是 sigmoid 函数的节点 n 的右分支的概率。相反地，左分支的概率是 1−p(right|n,c)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9coiaLqKEFJs9fo63g3FTiaQQ4v5cdAojvAwHKmXSEhbGubm5w2rqlkvBA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 2：多层次的 softmax计算&amp;nbsp;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由此易得一个词w在上下文c中的为左右分支的概率之积。举个例子，在上下文「the」、&lt;span&gt;「dog」&lt;/span&gt;、&lt;span&gt;「and」&lt;/span&gt;、&lt;span&gt;「the」&lt;/span&gt;之中，在图 2 中词「cat」的概率可以通过计算从节点1向左，经过节点2转右，再在节点后转右所得的概率来计算。Hugo Lachorelle 在他的课程视频 (https://www.youtube.com/watch?v=B95LTf2rVWM)中提供了一个更详细的介绍。Rong[7] 也很好地解释了这些概念，并且推导了 H-Softmax 的导数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;显然，树的结构十分重要。直观上来说，如果我们让模型在每个节点都来学习二元分类，比如我们可以让相似的路径获得相似的概率，我们的模型应该可以获得更好的表现。基于这一点，Morin 和 Bengio 给树提供 WordNet中的 synsets 的聚类。然而，他们的模型表现却不如常规的softmax。Mnih 和 Hinton [8] 用一个聚类算法来训练树结构来低轨地把词分成两堆，并且他们的模型在一部分的计算中表现和常规的 softmax 相当。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是，只有在我们提前知道想要预测的那个词（以及它的路径）时，我们才能够加速训练。在测试阶段，当我们需要预测最可能出现的词时，尽管缩小了范围，我们仍然需要计算所有词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，我们不需要用&lt;span&gt;「左」和&lt;span&gt;「右」&lt;/span&gt;&lt;/span&gt;来指定子节点，我们可以用一个对应路径的位向量来索引节点。在图 2，如果我们假设用比特 0 来表示向左和比特 1 来表示向右，我们可以用0-1-1来表示一条左－右－右的路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们回顾一下，一个平衡二叉树的路径长度是 log2|V|。如果我们设置 |V|=10000，这就相当于一条大约长度为13.3 的路径长度。相似的，我们可以用平均长度为13.3的路径的位向量来表示每一个词。在信息论中，这指代一串信息长度为 13.3 比特的字。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;字的信息内容小结&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先回忆，一个词 w 的信息量（信息熵）I(w)是它的概率的负对数 I(w)=−log2p(w)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在数据集中所有词的熵H就是在一个单词库的所有词的信息熵的期望：H=∑i∈Vp(wi)I(wi)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也可以把一个信息源的信息熵想成是用来编码这部分信息所用的比特数。对于抛掷一枚公平的硬币，每次需要1比特；而对于一个总是输出相同符号的信息源，我们只需要0比特。对于一个平衡二叉树，我们平等对待每一个词，每个词 w 的熵 H 将拥有同样的信息量 I(w)，因为每个词都有同样的概率。在一个 |V|=10000 大小的平衡二叉树中平均的词信息熵 H 就恰好是它的平均路径长度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKR0xvouGYoYt3RoRFNPt3CQyfWOqibfR0ibIDoRSpPeuwZDDibg4NUkmA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们之前讲过，树的结构十分重要。值得注意的是，利用树的结构不但可以获得更好的表现，更可以加速运算：如果我们将更多信息加载进树中，那么更少信息的词将不意味着更短的路径，因为有些词出现频率更高，就可以用更少的信息去编码它。一个 |V|=10,000 长度的信息库的词信息熵大约为 9.16。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，考虑到出现频率，我们可以减少一个词汇库中每个词中的平均比特数。在这个例子中，我们从 13.3 减少到 9.16，相当于加速了 31%。Mikolov 等人 [1]把哈夫曼树运用在多层次 softmax，把更少的比特赋给更常出现的词。比如，「the」这个最常见的英语单词，在树中只需要最少比特数的编码，而第二常见的单词将会被指定第二少比特数的编码，以此类推。虽然我们仍然需要用相同数量的词去编码一个数据集，然而有更高频率出现短的编码，所以平均而言，去给每个词编码只需用更少的比特数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个类似于哈夫曼编码的编码又被称为信息熵编码，因为每个编码词的长度大约是和我们观察到的每个符号的熵成正比。Shannon [5] 在他的实验中建立了英语的信息率的下界，每个字母大约是 0.6 到 1.3 比特；根据每个词的长度为 4.5，这相当于每个单词为2.7到5.85比特。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这个和语言建模（我们在上一篇博文中讲到的那样）联系起来：语言模型的评价标准的困惑度应该是 2H，其中H就是信息熵。一个一元的熵是 9.16，因此它有一个非常高的困惑度 2^9.16=572.0。我们可以将这个值更加具体化，观察一个困惑度为572的模型就像从一个信息源中选择单词，而每个单词有572种选择，每种选择概率相等，且互相独立。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这么说吧：Jozefowicz 等人在 2016 年开发的这个最新的语言模型，在 1B Word Benchmark 中，每个词拥有 24.2 的困惑度。这个模型因此需要大约4.60比特来编码一个词，因为 2^4.60=24.2，非常接近于 Shannon 描述的实验下界。我们能否，以及如何使用这个模型去建立一个更好的多层次 softmax 层？这些问题仍留待我们去探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微分 Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Chen等人[9]介绍了一个经典 softmax 层的变形—— 微分Softmax（Differentiated Softmax，英文缩写为D-Softmax）。D-Softmax 基于不是所有的单词都需要同样多的参数：许多高频词可以拟合许多参数，而非常低频词则只能拟合很少的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了做到这一点，他们不用常规 softmax 层的 d×|V| 大小的包含输出词嵌入 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cCbHjibecAfibMSdDsCe46w79x89D4aibEYKGowQnSBJHExd1Gnsk7ialwA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的稠密矩阵，而用一个稀疏矩阵。然后他们把 v′w 排列成块，根据他们的频率排列，而每块的词嵌入的维度都是 dk。块的数量和他们的向量大小均为可以优化的超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdExer2zIaU8T1CH179unBARXsWPnKoV9bxWPMrRgZhResAia55v2CaQ/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图3：微分 softmax（Chen等人(2015)）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图3中，分区A的词嵌入的维度是 dA（这些是高频词的词嵌入，因为它们被赋予更多的参数），而分区 B 和 C 的词嵌入分别有 dB 和 dC 维度。注意到所有不属于任何分区的区域，也就是图1中的这些没有阴影的区域，都设为 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之前的隐藏层 h 都被当成是把每个对应维度的分区的特征串在一起。在图3中的h由大小分别为 dA、dB 和 dB 的分区组成。D-Softmax 不计算矩阵－向量的乘积，而是计算每个分区的乘积和它们在h的分区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为许多词只需要相对来说较少的参数，计算 softmax 的复杂度降低了。对比 H-Softmax，这个加速在测试阶段仍然存在。Chen 等人（2015）发现 D-Softmax 是在测试阶段最快的方法，而且是最准确的模型之一。然而，因为它给低频词赋予了更少的参数，D-Softmax 对于低频词的建模效果并不好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CNN-Softmax 方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个对经典 softmax 层的改进受到了 Kim 等人最近的对于通过一个字母层次（character-level）的 CNN 的输入词嵌入 vw 的研究启发。Jozefowicz 等人（2016）建议对输出词嵌入做相同的事情，即通过一个字母层次的CNN。注意到如果我们像在图4中的在输入和输出有一个 CNN，生成输出词嵌入 v′w 的 CNN 必须和生成输入词嵌入 vw 的 CNN 不一样，就像是输入词嵌入和输出词嵌入必须不一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csXJPjEiaclKa88v8e40TNrbw6sibibcMnkPwpmAQmWE3c9GNk054uzm9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图4: CNN-Softmax（Jozefowicz 等人 ，2016年）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然这个仍然需要计算常规 softmax 的标准化，这个方法很大程度上减少了模型参数的数量：我们现在不存储d×|V|的词嵌入矩阵，而仅仅是追踪 CNN 的参数。在测试阶段，输出向量 v′w 可以提前计算，所以模型的表现不会受损。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，因为字母都在练习空间中表现，且因为得到的矩阵都倾向于学习一个把字母映射到词的平滑函数，基于字母的模型常常会难以区分拼写相似而意思迥异的词。为了避免这个问题，研究者们加上一个通过每个词学习的连接系数，就能很大程度地减少常规的 softmax 和 CNN-softmax 的表现差异。调整修正项（correction term）的维度，研究者就可以取得模型大小和表现好坏的平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究者也强调不需要用 CNN-softmax，而是把前一层h的输出传入一个字母层次的 LSTM，它输出词语的方式可以是一次输出一个字母。因此，每一个时间步中，softmax 输出的并不是词，而是字母的概率分布。然而，他们不能得到和这一层相当的表现效果。Ling 等人[14] 为机器翻译采用一个相似的层，得到了具有竞争力的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;基于采样的方法&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上文中讨论的这些方法仍然保持着 softmax 大体的结构，而基于采样的方法则完全和 softmax 层无关。它们的方式是近似其它的损失函数的 softmax 的分母的正则化来。然而，基于采样的方法只是在训练时有效——在推断（inference）中，完全的 softmax 仍然需要计算来获得一个正则化后的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了直观上看 softmax 分母在损失函数的影响，我们将会推导我们相对于我们模型 θ 的损失函数 Jθ 的梯度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在训练过程中，我们希望给训练集中的每个词 w 减少模型的交叉熵损失函数（cross-entropy loss）。这就是我们的 softmax 的输出的负对数。如果您不确定 softmax 和 cross-entropy 的关系，请看看Karpathy的解释 (http://cs231n.github.io/linear-classify/#softmax-classifier)。我们的损失函数如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c2s5OvdiblDD98icF2pVmSM6wdkrAlq1RBdFnlPGLyIjuNp9z2IRJ54icA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意在实际操作中，Jθ 就是在整个数据集的所有负对数概率的平均值。为了获得这个推导，我们可以把 Jθ 分解成 ：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cc28a98FibJJaqM9Z5ZcK3HEMNtRtn6bNXpnicKy1OcJibMI3iaLuQrHJyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cX5fBZSjYZiaosEaZjj18eIs7LQjxbY21SZ8B0ibQ1ianrmAiclPM5OibXCQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了简洁，并且和 Bengio 和 Senécal [4 , 15]的标号对应起来（注意到第一篇文章中，他们计算了正对数的梯度），我们用 −E(w) 来代替 h⊤v′w 的点积。我们的损失函数于是看起来像这个形式：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9czrTnJnxb1YtXMcPgKx9kNWYJs85PvcEdXXicsF9UnsNvGpexEjM6bwg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了反向传播，我们现在可以计算相对于我们的模型参数 θ 的 Jθ 的梯度 ∇：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibb57T6eIt9RBpXOfYfcyqX50QzZsFT0env3mKaibo3MddDlyKKw9QPA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 logx 的梯度是1x，运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cBicpdmib8xb0PPWxoarS5NiaBnx7W8ribT4mx1UTgsSibxu1qtWmKWaZhFw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以在和中移动梯度：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cIl0KSVjhsIkjq4YibsnQcqic1HHAcIf79mS8VCr3l0c8mtkc62twy6kw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为 exp(x) 的梯度就是 exp(x)，再一次运用链式法则可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cuJLJr4kkCOs1xO1SlQNp8vcDKGribAMyicmDPOTnibt6YXGtF4NPFYM7w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以把它写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cmRm7iayjzaYcu6PyvMEaDW09yaoYW1QkIicKMAJZPCo97XFwWeMFEYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意到：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDXVdfWYA3OCXLOz1AY6znenNyWcYL3cP4Kv61UibFkdz9wL0Wgkya4w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只是 wi 的 softmax 概率 P(wi)（为了简洁，我们忽略它对上下文 c 的依赖）。替换它，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyWdiaqibr7K72GKqdLuoGFUKSphg8cpCVmPzguicqVickjHw9s98FicjTYA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，重新把负系数放到和前面，可得：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cic7kKz3l2sNOZDk87dXXEYQyo0leyTPgSGDbQiaakLVqzgyznveLtczQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio和Senécal（2003）发现了梯度最终由两部分组成：一个是给目标词 w 的正反馈（上一个式子中的第一项），一个是给其它词 wi 的负反馈，由它们的概率 w（第二项）反映出来。我们可以发现，这个负反馈只是对于所有V中的词 wi 的梯度 E 的&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cDicOh0m3Iia8DqYUqxDW1bGaU0oQbmjZvstS0BrmAooJCKTT0jtV0SPQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cJQZ6YAALBzZAYfiaic1rTT8VPuK2TE08A2JicWbz1aFnL3Xx2dhxkia03g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在大多数的基于采样方法的难点是去近似负反馈，让它更容易地去计算，因为我们不想将所有在 V 中的词的概率加起来。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以用蒙特卡洛方法去近似任何概率分布的期待值 E，也就是所有概率分布的随机样本的平均值。如果我们知道网络的分布，即 P(w)，我们就可以从中采样 m 个词 w1,⋯,wm ，然后近似得到如上的期望：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1iaKjjtjgwu7GvJIibul4C6o5NI9pyTTLKS0V8EBRW9YrbibKUYEFcbsw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，为了从概率分布 P 中采样，我们需要计算 P，但这其实是我们最开始想要避免的。于是，我们找到另一个概率分布 Q，我们称之为建议分布（proposal distribution）。Q 应该便于从中采样，而且可以用来作为蒙特卡洛抽样法的基础。我们偏向于选取和 P 相近的概率作为 Q，因为我们希望得到的近似期望能够尽量准确。在语言建模中一个非常直接的选择就是用一元文法分布（unigram distribution）作为 Q 的训练集。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这也就是经典的重要性抽样方法（Importance Sampling，英文缩写 IS)：它使用蒙特卡洛抽样，通过一个提议分布Q来近似一个目标概率分布 P。然而，这仍然需要给每个采样的词 w 计算 P(w)。为了避免这个，Bengio 和 Senécal (2003年) 用Liu [16]中提出的有偏估计量（biased estimator）。这个预测函数可以在 P(w) 当成乘积计算时使用，这正是我们要处理的情况，因为每个除法都可以转化成乘法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从根本上来说，我们不必花大代价计算 Pwi 并得到梯度 ∇θE(wi) 的权重，我们只需要利用提议分布Q来得到权重的一个因子。对于一个有偏见的 IS，这个因子就是&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKKOburZqykXNAxdQf9icVJVN3aLgwfm9fBTUibRdibbDIF4DWbmvicJYicQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cOEicmHnbo25icNKia8pWXTU616hOJ54IFXccicv4fqO6CyPqvicLiar8lQIA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;且&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ccFUe9xS5QmuicPSeRDuwFk04jyCRbViaUmq8kD2Alokmo43yYSQ1qTibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用 r 和 Q 而非 Bengio 和 Senécal (2003年, 2008年) 论文中的 w 和 W 来避免重名。因为我们可以发现，我们仍然计算 softmax 的分母，但是用提议分布 Q 来替换分母的标准化。因此，我们的用以近似期望的有偏差的预测函数如下：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意，我们用越少的样本，我们的预测就会越差。另外，我们需要在训练阶段调整我们的样本数量，因为如果样本数量太小，在训练阶段，网络的分布 P 可能和一元分布 Q 分岔，导致整个模型无法收敛。因此，Bengio 和 Senécal 为了防止可能的分岔采用了一个计算有效样本大小的方法。最后，研究者们声称这个方法相对于传统的softmax能够加速19倍。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自适应性重要性采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bengio 和 Senécal (2008年) 注意到对于重要性采样方法，替换掉越多复杂的概率分布，比如二元（bigram）和三元（trigram）分布，那么接下来在训练阶段，将对于从模型的真实分布 P 来对一元分布 Q 的分岔的对抗将无效，因为n元（n-gram）分布看起来和训练好的神经语言模型的分布挺不一样。作为替代模型，他们提出了一个用在训练阶段调整适应过的 n 元分布来更好地跟随目标分布P。最后，他们根据一些混合方程来插值（interpolate）一个二元分布和一个一元分布，而这些混合方程的参数都是他们用不同频率组的 SGD 训练的，训练目标为最小化目标分布 P 和提议分布 Q 的 Kullback-Leibler 差异。在实验中，他们声称训练效果提高了大约100倍。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目标采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jean 等人（2015年）提出了在机器翻译中运用具有适应性的重要性采样方法。为了让模型更好地适应 GPU 上的并行计算及有限内存，他们将目标单词的数量限制到采样最少必须达到的数量。他们将训练集分区，在每个分区只保留一定数量的单词，形成了总词汇表的一个分集 V′。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这根本上表现了一个独立的提议分布 Qi 可以在训练集的每个分区 i 中运用，这给所有词汇表分集 V′i 内的词赋予同等的概率，而其它的词则概率为0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;噪音对比估计方法&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;噪音对比估计 （NCE）（Gutmann 和 Hyvärinen） [17] 由 Mnih 和 Teh [18] 作为一个比重要性采样方法（IS）更稳定的采样方法提出，因为我们发现 IS 具有一定风险出现建议分布 Q 和分布 P 分岔的情况，而这就是需要优化的地方。不同于之前的方法，NCE 并没有试图直接预测一个词的概率，而是用一个辅助性的损失函数来同时最大化正确词的概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆 Collobert 和 Weston（2008年）的成对排序标准，它将正数窗口排列在「受损的」窗口之前，这一点我们在上一篇博文已经讲到。NCE 做类似的事：我们训练一个用来从噪音区分目标词的模型。因此，我们可以将预测正确词的任务简化到一个二元分类任务，其中模型试图从噪音样本中区分正确、真实的数据，如图4所示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1adCl7nogIcJmZlT6iaictdVDibsHibvcz0WvSUIWCN8uTc71Ctictxo0Ag/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 4:噪音对比估计&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于每个词，给定了 n 个在训练集之前出现的词 wt−1,⋯,wt−n+1，我们可以从噪音分布 Q 来生成 k 个噪音样本 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为在 IS 中，我们可以从训练集的一元分布中采样。因为我们需要数据标签来完成我们的二元分类任务，我们指定在上下文 ci 中所有的正确词 wi 为真（y=1），而所有噪音采样&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cThQeKMnfGFicWjE8AOvoXOgtwVwdCMKJwl0K4l0f3dG798KbcdSZ7Pg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为假（y = 0）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不去计算我们的噪音样本的期望&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9crbV8CnzHCuqRCvbQZYciaSZb81K8vdOmkCCADiaHUib264CuwAHrjJAiaw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为获得这个期望仍然需要把所有 V 中的词加起来从而预测负标签的标准化的概率，而是再次用蒙特卡洛法近似求平均值：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cYUABqwbDqciaP5mUnGmAbibOib5UeOQ0cIHrAREkby6UgSgtW95l5KPxQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以简化为：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c1LaD38BuaEYl5c2wdusjTH2TBDArzibdVMIxAfEtPiaUN65ecBGpaugA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为每个在上下文 c 中正确的单词 wi 生成 k 个噪音样本，我们有效地从两个不同的分布中生成词：正确的词从训练集 Ptrain 的实际分布中采样且依赖于它们的上下文 c，而噪音样本则来自噪音分布 Q。我们因此可以用两个分布的混合模型来表示采样到正样本或负样本的概率，它们基于分别的样本数量来取得权重：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cdOuP3fNY2ZVe92AE4XicusmNHaZbeLicw0XJd6eAF5PSuicDxhWicGc66w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据这个混合模型，我们现在可以计算一个样本来自于训练分布 Ptrain 的概率，它就是一个 y 对于 w 和 c 的条件概率：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cvMiaS2C3ZspQOrhOAAZqy6oQqsVicUsPq8JxBLJukcrtT6u0kicicgicUqw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它可以简化成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c6zxKO7iboQ6eLdo4G20xOltzNibicAAQw1sZpicBSEyichL0L2WAyN3nPgg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们不知道 Ptrain 的值（正是我们希望计算的），我们用我们的模型 P 的概率来替换 Ptrain：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cLibByWY4fXbOmYMZp7kAo6hT3vYXUlzs8XnVeZ6Kghic4L7ibkNiaTcNYg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;预测噪音样本（y=0）的概率因此就是简单的 P(y=0|w,c)=1−P(y=1|w,c)。请注意计算 P(w|c)，也就是给定它的上下文 c，一个词 w 的概率本质上就是我们对于 softmax 的定义：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cwkssghydynxK27TJQzmTbnZ6mnxtIO6iaSR6ge7e1gbn4qs6vXKbDaA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了记号简便和不被混淆，让我们把 softmax 的分母命名为 Z(c)，因为这个分母仅依赖于 h，它从 c 中生成（假定一个固定的 V）。Softmax 于是看起来像这样：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9csqnNicNhT0hpxAu6ZicOxxpn0oLs8UD3R2XF3AGWRK23Lzdib7Pyga7mw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mnih 和 Teh（2012 年）和 Vaswani 等人 [20] 实际上将 Z(c) 固定在 1，他们声称这样不会影响模型的表现。这个假设有一个良好的附带效果，那就是可以减少模型的函数数量，同时保证模型可以自己标准化，而不需要依赖特意标准化 Z(c)。确实，Zoph 等人 [19] 发现即使模型学习这个参数，Z(c) 和 1 非常接近，而且具有小的方差。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们在上面的 softmax 算式中可以设 Z(c) 为 1，对于在上下文 c 中的词 w，我们就得到了如下的概率：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在可以插入上式的这一项来计算 P(y=1|w,c)：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cAsQ2v4wTem2aVYqItLdrVXKaic3v4Oex5KkiaVEQu8YFKIwBbLvDj3xw/0?wx_fmt=png"/&gt;&lt;br/&gt;插入这一项到我们的 logistic 回归的目标中，就能得到完整的 NCE 损失函数：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cl0ribiczQ3gEwcxPTWAjjJ9zZTibyb1vnAb2ZZP7IchESrj0L2CyZT5Sw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请注意到 NCE 有一个很好的理论上的保证：可以证明当我们增加噪音样本的数量 k 时，NCE 的梯度趋向于 softmax 函数的梯度。Mnih 和 Teh（2012 年）提出 25 个样本就足够使模型表现能够和常规的 softmax 相当，且能够提升 45 倍的运算速度。对于 NCE 的更多信息，Chris Dyer 发表了一些非常好的笔记 [21]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个 NCE 的缺点就是，因为不同的噪音样本来源于对每个训练词 w 的采样，噪音样本和它们的梯度都不能在稠密矩阵中存储，这减少了在 GPU 上使用 NCE 的好处，因为它不能受益于快速稠密矩阵乘法运算。Jozefowicz 等人（2016 年）和 Zoph 等人（2016 年）独立地提出在所有训练词中共享噪音样本，从而 NCE 的梯度可以用稠密矩阵运算来计算，在 GPU 上更加高效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 IS 的相似性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Jozefowicz 等人（2016 年）的研究认为 NCE 和 IS 不仅都是基于采样的方法，同时也高度关联。他们发现 NCE 用一个二元分类的任务，IS 可以类似地用一个代理损失函数（也称上界损失函数，surrogate loss function）表示：IS 并不像 NCE 一样用一个 logistic 损失函数去做二元分类任务，而是优化用一个 softmax 和一个交叉熵损失函数做一个多元分类的任务。他们发现当 IS 做多元分类的任务时，它为语言建模提供了一个更好的选择，因为损失函数带来了数据和噪音样本的共同更新，而不是像 NCE 一样的独立更新。事实上，Jozefowicz 等人（2016 年）用 IS 做语言建模，并在 1B Word Benchmark 数据上却取得了优异表现（就像上文所述）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;负采样方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mikolov 等人（2013）发现并推广的目标——负采样（NEG），可以被看成是 NCE 的一个近似。就像我们之前所说的，NCE 可以被证明是在样本数量 k 增大后对 softmax 损失函数的近似。NCE 的简化算法 NEG 避开了这个保证，因为 NEG 的目标就是学习高质量的词表征，而不同于在测试集上获得低困惑度这一个语言建模的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NEG 也用一个 logistic 损失函数来最小化训练集内的词的负对数可能性。让我们回忆一下，NCE 计算给定上下文 c 一个来自实际的训练概率分布 Ptrain 的词 w 的概率，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibXFmOoB9PA90Cr3ZiaavZeG2ua1ejWBU57oGECs4oePpk7LmejmPXxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCE 和 NEG 的主要差别在于 NEG 只是用简化 NEG 的计算来近似这一个概率。因此，它将最昂贵的项 kQ(w) 设为 1，我们就得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9ctmfyXQQxokVicmhgzPWTFB12I5CPJssCGVq7ibEibsKvqe0RYSg05sHHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当 k=|V| 而且 Q 是一个均等分布时，kQ(w)=1 刚好为真。在这个情况下，NEG 等价于 NCE。我们设 kQ(w)=1 而不是其它常数的原因可以从重写这个算式来得到，因为 P(y=1|w,c) 可以变型成 sigmoid 函数&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;如果我们将它插入回之前的 logistic 回归函数中，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cUFzzWicd7WsEIdEAwrfYgXvwofgx7WFuztAt5jtpibdCCUFWoGjtgibew/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;稍微简化一下，我们得到：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cKbv5qqXtiaibiaRPN8CNXtxhZTuSfMicckRwJC3uuT9nR1NibmFfUXlJx0A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;设:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cQbUic1UBuFDUNXblhmdsEu1pE2LqGQEpnicNdbdicKJEQpTRyoaAaXUtg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终得到 NEG 损失函数：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cgO2fzKgP75TWM0zBVWvfVicxlAeW1q8Kpcwv0PCvl3Kt9et1xrzicibibg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了和 Mikolov 等人（2013 年）的标号统一起来，h 必须被 vwI 替换，v′wi 需被 v′wO 替换，以及 vw~ij 需被 v′wi 替换。另外，和 Mikolov 的 NEG 的目标相反，我们 a) 在整个词汇集中优化目标，b) 最小化负对数可能性而非最大正对数可能性（如上文所说），和 c) 已经把&amp;nbsp;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibbusiaTad1ibiaFIbahCjHvypCC40a2jjEDcfZic2jHVbjcUqUnE85zz5w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;的期望替换成它的蒙特卡洛近似。对于更多推导 NEG 的信息，敬请参见 Goldberg 和 Levy 的笔记 [22]。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经看到 NEG 只是在 k=|V| 而且 Q 是一个均等分布时和 NCE 相等价。在其它情况下，NEG 只是和 NCE 近似，也就是说它并没有直接优化正确词的可能性，这是语言建模的关键。虽然 NEG 可能因此对于学习词嵌入非常有用，但是它不能保证渐近一致性（asymptotic consistency），这使得它不适合语言建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然 Devlin 等人 [23] 提出的自标准化技术（self-normalisation）不是一个基于样本的方法，它为语言模型的自身标准化技术更直观，我们待会儿会介绍。我们之前提到的把 NCE 的损失函数的分母 Z(c) 设为 1，这个函数实质上自标准化了。这是一个有用的性质，因为它允许我们跳过标准化项 Z(c) 的昂贵计算。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;请回忆，我们的损失函数 Jθ 最小化我们的训练数据的所有词 wi 的负对数可能性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c49zZn4bAesL95KxKJib0TmtbFtjdfxyBic6e3cQkF7GFZQZm1oLJKDwQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们像之前一样可以把 softmax 分解成和：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cibeynoia42hvCmjniauWiabU6mj3dw9wxqDiawynZFpEMYiau15Dx3y2CbNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果我们可以限制我们的模型，使得它可以让 Z(c)=1 或 logZ(c)=0，那么我们可以避免计算标准化项 Z(c)。Devlin 等人（2014 年）因此提出给损失函数加上一个平方错误惩罚项，来鼓励模型把 Z(c) 保持着尽量接近 0：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c13ibXA4RLeuYbjl9mwYmkSTGbay84ENvOknj5zdVF5AyOocHB5HqIUQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这可以写成：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cyicpvmo3DZaYAVmzrL98g7dHc6ianaAJqSOxrcNLDN1c0iaMl7zpdcbAA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中 α 允许我们去做模型准确度和平均数自标准化的取舍。我们大体就可以保证 Z(c) 将和我们希望的一样非常接近于 1。在它们的 MT 系统的解码时间内，Devlin 等人（2014 年）接着设置 softmax 的分母为 1，而且仅仅使用分母和它们的惩罚项来计算 P(w|c) ：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cqC78Hnn2gbu3KLkflib9OdR1N14CPdZur9BhQ2NLS1QrKTQuNdTuAGw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们称自标准化加速了大概 15 倍，而相比于常规的非自标准化的神经语言模型，在 BLEU 分数上只有少量的退步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;低频的标准化方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Andreas 和 Klein [11] 提出仅仅标准化一部分训练样本已经足够，且仍能获得和自标准化一样的行为。他们由此提出低频的标准化（Infrequent Normalisation，或 IN），仅仅在惩罚项中取小部分样本，来形成一个基于样本的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们先来把之前的损失函数 Jθ 分解成两个独立的和：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cZHwZp3t1jwky9WrF9lUx8R6ia7ibt7zGshS1xQwXaDCFx802EjliaaCCA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在在训练数据中，通过仅仅计算一个词的子集 C（它包含词 wj）来在第二项中取小部分样本，从而从上下文 cj 采样（因为 Z(c) 仅依赖于上下文 c）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9cObEotOxXgr92yKTQuEEKU024BmiaYQNQF7GjxMYZrFve10oITCCib3Hw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中，γ 控制着子集 C 的大小。Andreas 和 Klein（2015 年）提出 IF 综合了 NCE 和自标准化的优点，因为它不用给所有的训练样本计算标准化项（NCE 完全不用计算），但是又像自标准化一样允许去做模型准确度和平均数自标准化的取舍。他们观察到当它只用从十分之一的样本中采样时，它能够加速 10 倍，而没有明显的模型表现损失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;其它方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;至此，我们已经集中讲解了近似或者完全避免 softmax 分母 Z(c) 的计算的方法，因为它就是计算中最昂贵的一项。我们因此不去特别留意&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;，&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即倒数第二层 h 和输出词嵌入层 v′w 的点积。Vijayanarasimhan 等人 [12 ] 提出用快速的位置敏感哈希（locality-sensitive hashing）来近似&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UtLKuicticyx54XibbjAFO9c58Fy9ClyrKYWV7ToibQDm1F66KrdtTrn5vCwlEKQibNLGUrKXroM83CA/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，尽管这个方法在测试阶段可以加速模型，但是在训练阶段，这个加速实际上会消失，因为词嵌入必须重新标号，且批（batch）的数量会增加。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;选择哪个方法呢？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看完了最流行的基于 softmax 的和机遇采样的方法，我们展示了对于经典的 softmax 有很多替代品，而且几乎它们所有都在速度上有重要提升，它们大多数也略有模型表现上的不足。所以，这自然引到了一个问题，就是对于某一个特定的任务，哪个方法是最好的方法。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Approach方法&lt;/th&gt;&lt;th&gt;Speed-up factor加速倍数&lt;/th&gt;&lt;th&gt;During training?在训练阶段吗？&lt;/th&gt;&lt;th&gt;During testing?在测试阶段吗？&lt;/th&gt;&lt;th&gt;Performance (small vocab)表现（小词汇集上）&lt;/th&gt;&lt;th&gt;Performance(large vocab)表现（大词汇集上）&lt;/th&gt;&lt;th&gt;Proportion of parameters 参数的采用百分比&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Softmax&lt;br/&gt;Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;1x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Hierarchical Softmax 多层次的softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;25x (50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very poor&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Differentiated Softmax 微分 Softmax&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&amp;lt; 100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;CNN-Softmax&lt;br/&gt;CNN-Softmax方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad - good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;30%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Importance Sampling 重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(19x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Adaptive&lt;br/&gt;Importance Sampling 具有适应性的重要性采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Target Sampling 目标采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;2x&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Noise Contrastive&lt;br/&gt;Estimation 噪音对比估计方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;8x (45x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very bad&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); word-break: break-all;"&gt;Negative Sampling 负采样方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(50-100x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;Self-Normalisation 自标准化方法&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;(15x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;&lt;br/&gt;&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;6x (10x)&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;X&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;-&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;very good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;good&lt;/td&gt;&lt;td style="color: rgb(51, 51, 51); background-color: rgb(255, 255, 255);"&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;表格 1：比较语言建模中近似 softmax 的几种方法&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在表格 1 中比较了我们之前介绍的几种方法的表现。加速倍数和表现均来自于 Chen 等人（2015 年）的实验，同时我们在括号中记录了原作者们提出的加速倍数。第三和第四列分别显示了在训练阶段和测试阶段是否达到这个加速倍数。请注意，加速倍数的不同也许是因为没有优化，或者原作者没有用到 GPU，显卡运算常规的 softmax 比一些其它方法更加高效。部分方式没有可用的比较的方法，其表现则大部分参考相似的方法，比如自标准化方法应该和低频标准化相当，重要性采样方法和具有适应性的重要性采样方法与目标采样法相当。Jozefowicz 等人（2016 年）提出的 CNN-Softmax 方法的表现则根据纠正（correction）的大小时好时坏。在所有的方法里，除了 CNN-Softmax 方法中用了明显更少的参数，其它方法仍然需要存储输出词嵌入。微分 Softmax 方法因为能够存储一个稀疏权重矩阵从而减少了参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像一直以来，没有哪个方法对于所有的数据集或者任务来说都是最好的。对于语言建模，常规的 softmax 在小词汇库数据集上仍然有一个非常好的表现，比如说 Penn Treebank，甚至能够在中数据集中也很好，比如 Gigaword，但是在大数据集上则很差，比如 1B Word Benchmark。目标采样方法、多层次 Softmax，和低频标准化方法则在大词汇库上表现更好。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微分 Softmax 方法则大体上对于小词汇库和大词汇库上都表现不错。有趣的是，多层次 Softmax（HS）在小词汇库上表现不好。然而，在所有的方法之中，HS 都是最快的，而且在给定的时间内能够处理最多的训练样本。负采样法则在语言建模任务上表现不佳，却在学习词表示上有非常好的表现，从 word2vec 的成功可以看得出来。请注意，所有的结果都不可尽信：Chen 等人（2015 年）在报告里说到在实际中运用噪音对比估计方法存在困难；Kim 等人（2016 年）用多层次 Softmax 在小词汇库上获得了很好的表现，而重要性采样方法则被 Jozefowicz 等人（2016 年）提出的最先进的语言模型所用在一个大词汇库中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，如果你真的准备去用上述的方法，TensorFlow 实现 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling) 了一些基于采样的方法，也解释&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; (https://www.tensorflow.org/extras/candidate_sampling.pdf) 了其中一部分方法的不同点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;小结&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这篇对于一些不同近似 softmax 的方法的介绍，不仅可以用于提升和加速词的表示的训练，也对语言建模和机器翻译有所帮助。正如我们看到的，大部分方法都非常相关，且源于同一点：必须找到方法来近似昂贵的 softmax 分母的标准化计算。记住这些方法，我希望你现在能更好地训练且理解你的模型，你甚至可以准备去自学更好的词表示模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如我们看到的，学习词的表示是一个非常广的领域，对于成功有很多相关因素。在之前的博文中，我们学习了流行的模型的建筑，而在这篇博文中，我们着重学习关键部分，softmax 层。在下一篇博文中，我们将介绍 GloVe，一个依赖于模型因素分解，而不是语言建模。我们将把我们的注意力转移到其它的对于成功学习词嵌入起关键作用的一些超参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Morin, F., &amp;amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. http://doi.org/10.1002/j.1538-7305.1951.tb01366.x &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from http://arxiv.org/abs/1411.2738 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Chen, W., Grangier, D., &amp;amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from http://arxiv.org/abs/1512.04906 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Jean, S., Cho, K., Memisevic, R., &amp;amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from http://www.aclweb.org/anthology/P15-1001 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Andreas, J., &amp;amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vijayanarasimhan, S., Shlens, J., Monga, R., &amp;amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from http://arxiv.org/abs/1412.7479 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Ling, W., Trancoso, I., Dyer, C., &amp;amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from http://arxiv.org/abs/1511.04586 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bengio, Y., &amp;amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. http://doi.org/10.1109/TNN.2007.912312 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. http://doi.org/10.1017/CBO9781107415324.004 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Gutmann, M., &amp;amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Mnih, A., &amp;amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML』12), 1751–1758. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Zoph, B., Vaswani, A., May, J., &amp;amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vaswani, A., Zhao, Y., Fossum, V., &amp;amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from http://arxiv.org/abs/1410.8251 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Goldberg, Y., &amp;amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.』s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp;amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL』2014, 1370–1380.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 24 Oct 2016 17:43:56 +0800</pubDate>
    </item>
  </channel>
</rss>
