<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | Facebook 开源人工智能环境CommAI-env，目标是实现人机之间的语言交流（附论文）</title>
      <link>http://www.iwgc.cn/link/2854955</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Facebook Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Facebook 今天开源重磅项目 CommAI-env，这是一个开发基于通信的人工智能系统的平台，本文整合了该项目的 README.md 文件和相关论文的摘要。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2oJ45SVbGgYCqUTQpUoDc0UUe4qpQYkmBjNKZKWC2oWHhyf51kM3Qhw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;开源地址：https://github.com/facebookresearch/CommAI-env&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env（基于通信的人工智能环境（Environment for Communication-based AI））是一个用于训练和评估人工智能的平台，该平台已经在论文《A Roadmap towards Machine Intelligence》中进行过了描述。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2UveKaNObbFpNSI1IxW7HVCydHwZn1YQNhHoCyib9ZgWmnuyHdxm68pQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;介绍&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env 是一个用于训练和测试人工智能系统（Learner（学习器，可以系统开发者所选择的任意语言编程））的平台，其使用了一个基于通信（communication）的设置，其中它可以通过一个 bit 层面的接口与 Environment（环境）进行交互。该 Environment 会要求 Learner 去解决一些基于通信的 Task（任务），并为其已经成功完成的每一个任务实例分配一个 Reward（奖励）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 Learner 会以随机的顺序处理所有任务中的多个实例，而且其必须尽可能多地解决它们以获得最大的奖励。目前可以实现的任务包括计数问题、Learner 必须学习记忆项的列表并回答有关问题的任务、或通过基于文本的引导方案按引导指令执行任务（任务的详情描述请参阅该文档：https://github.com/facebookresearch/CommAI-env/blob/master/TASKS.md）。其任务范围现已开放：我们正在扩展任务范围，我们也邀请其他人能够参与进来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env 的最终目标是提供一个可用于从头开始训练 Learner 的环境，从而使其能够实现通过语言的与人类的真正交互。尽管这样的任务可能看起来微不足道（但你可以试试在我们所支持的混杂的模式中解决它们——你的英语知识根本没用！），但我们认为其中大部分任务都超出了现有的基于学习的算法的能力；而要让一个 Learner 能够解决所有这些任务，就必须要在与人类交互所需的通信智能（communicative intelligence）上取得重大的进展，并且还要能从人类老师那里获得进一步的学习。注：我们并不是说 CommAI-env 中的任务覆盖了一个智能通信代理所应该处理的所有技能。我们是说：为了解决 CommAI-env，智能代理必须要有非常宽泛的学习能力，这样它应该才可以通过与人类交互和其它方式获得它快速所需的所有进一步的任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下是 CommAI-env 有别于其它目前用于训练和测试人工智能系统的环境和数据集（比如，OpenAI Gym、Allen AI Science Challenge、MazeBase 或 bAbI）的一些基本特征，这些特征被设计出来的目的是为了鼓励开发者开发快速的、通用的、基于通信的 Learner。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CommAI-env 的关注重点完全是基于通信的任务，其中所有的通信都是通过 Learner 和 Environment 之间的一个共同的 bit 层面的接口实现的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在单个 CommAI-env 会话（session）中，Learner 会被暴露在许多种任务中，所以它必须学会识别不同的任务，并将不同的技能合适地应用到这些任务上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;许多任务都是渐进的，在这个意义上，解决其中一个或多个任务应该能让其它任务的解决更简单，只要 Learner 有对数据和算法的长期记忆（例如，一旦一个 Learner 解决了基本的计数任务以及如何将物体和属性关联起来，那么计数一个物体的属性就会更容易）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;训练和测试阶段并没有明显的区分：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;一方面，Learner 应该不只是记忆一个固定任务集合的解决方案，而且还要学习如何将其泛化到其所遇到的新任务上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;另一方面，就和人类一样，Learner 应该只需要少数几次遭遇后就能解决基本的问题：因此，学习的速度应该被考虑在评估之中。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们计划当该平台足够成熟时开展一个基于 CommAI-env 的比赛。为此，CommAI-env 的评估配置的任务集合将不同于开发版本中所包含的任务。这些任务不同的方面可能包括：它们可能基于不同的（自然和人工）语言、它们可以需要学习 Learner 周围的新物体和新位置、它们可能需要重新组合在开发过程中所学到的技能、等等。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;运行该环境有以下两步:&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;# Creating a configuration file (for instance, by copying the full training set)&lt;br/&gt;cp task_config.sample.json tasks_config.json&lt;br/&gt;# Running the environment, in the simplest case, just providing the configuration file as an argument&lt;br/&gt;python run.py tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;默认情况下，该环境会在人类模式下运行（见下方）。如果你想用一个给定的算法来运行该环境，请见下方相应的部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;配置&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，你应&lt;/span&gt;&lt;span&gt;该创建一个配置文件标明要投给 Learner 的任务和命令。你可以从复制对应全部训练集的文件开始，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;cp tasks_config.sample.json tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人类模式&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了在控制台接口上运行这个系统，在这个系统中你可以冒充该&lt;span&gt; Learner &lt;/span&gt;，运行如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;python run.py tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这会为你提供一个基于控制台的用户界面，与一下运作的环境互动。每次当该环境看上去静了下来并期待&lt;span&gt; Learner &lt;/span&gt;回答时，控制就转换到能输入字符串返回环境的用户手中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两者之间的沟通，当下的时间和积累的奖励都会显示在屏幕上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更好地理解该学习算法面对的问题的种类，你可以使用该 scrambled flag 运行该环境，它会用随机的伪词（pseudo-word）替换所观察到的词汇中的每一个词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;警示：注意人类模式关于输入有两个来自教授者的假设。第一个假设是关于字符编码。虽然输入事实上达到了位（bit）级别，但是对于人类用户来说，阅读一长串 bit，还是会不舒服，所以我们在将它们投放到屏幕上之前将它转换成了字符串。第二个假设是话轮转换机制（turn-taking convention），在这个假设下，当环境已经产生了两个连续的空间后，我们将控制交给人类。学习算法不能安全地假设这些机制（convention），因为它们可以在后续的任务迭代中被修改。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;指定学习算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用一个给定的学习算法来运行该环境，你可以使用学习器 class 的完全限定名称 -l 或 --learner flag。例如，你可以使用任何一个样本学习器：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;&lt;span&gt;learners.sample_learners.SampleRepeatingLearner&lt;/span&gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;&lt;span&gt;learners.sample_learners.SampleMemorizingLearner&lt;/span&gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;限定一个学习算法需要定义两个功能：next 和 reward.next 从该环境中接收一个 bit，并且应该回到&lt;span&gt; Learner &lt;/span&gt;说出的下一个 bit。reward 告知&lt;span&gt; Learner &lt;/span&gt;一个给定的接收奖励。在 Python 中，你可以从下面的代码片断开始创建一个&lt;span&gt; Learner &lt;/span&gt;：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;from learners.base import BaseLearner&lt;br/&gt;class MySmartLearner(BaseLearner):&lt;br/&gt; &amp;nbsp; &amp;nbsp;def reward(self, reward):&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# record receiving a rewarddef next(self, input_bit):&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# figure out what should be# the next bit to be spikenreturn next_bit&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用编程语言 X 定义一个&lt;span&gt; Learner &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也可以用其他任何一种编程语言俩定义学习算法。为此，我们要包含一个learner.base.Remotelearner，它在一个任意的&lt;span&gt; Learner &lt;/span&gt;二进制与该环境之间建立一个 zeromq socket 接口。该环境作为一个服务器。为了方便，该用户能指定命令来发布这个学习者，所以它是在同一个过程中与环境一起发布的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当该 Session 创建后，该环境和学习器就被初始化了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该学习器通过向环境发送一个招呼「hello」来开始。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;循环：接受奖励，接受环境 bit，发送回复 bit&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;#include &amp;lt;string.h&amp;gt;&lt;br/&gt;#include "zmq.h"int main()&lt;br/&gt;{&lt;br/&gt; &amp;nbsp; // This is an example of a silly learner that always replies with '.'char reply[1];&lt;br/&gt; &amp;nbsp; int n = 0;&lt;br/&gt; &amp;nbsp; const char* response = "00101110"; &amp;nbsp;// '.' utf-8 code// connectvoid *context = zmq_ctx_new();&lt;br/&gt; &amp;nbsp; void *to_env = zmq_socket(context, ZMQ_PAIR);&lt;br/&gt; &amp;nbsp; int rc = zmq_connect(to_env, "tcp://localhost:5556");&lt;br/&gt;&lt;br/&gt; &amp;nbsp; // handshakezmq_send(to_env, "hello", 5, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; // talkwhile (true)&lt;br/&gt; &amp;nbsp; {&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// receive rewardzmq_recv(to_env, reply, 1, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// receive teacher/env bitzmq_recv(to_env, reply, 1, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// reply&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;reply[0] = response[n % strlen(response)];&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;zmq_send(to_env, reply, 1, 0);&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;n += 1;&lt;br/&gt; &amp;nbsp; }&lt;br/&gt;&lt;br/&gt; &amp;nbsp; zmq_close(to_env);&lt;br/&gt; &amp;nbsp; zmq_ctx_destroy(context);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; return 0;&lt;br/&gt;}&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用 Learner Binary Run Session：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;python run.py tasks_config.json -l learners.base.RemoteLearner \&lt;br/&gt;--learner-cmd "/my/learner/binary"&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;控制台视角&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;鉴于人类模式，默认视角展示了一个控制台界面，你能观察正在进行的两部分之间的通信，当运行一个自动学习算法时，该视角默认为一个更简单的存在，从而让算法更快的运行。然而，仍然有可能通过参数 -v ConsoleView，或等效的 --view ConsoleView 来返回到控制台视角。例如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;python run.py -l learners.sample_learners.SampleRepeatingLearner -v ConsoleView tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要求：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Python 2.6+&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;zeromq (for remote learners)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文标题：通向机器智能的路线图&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2gFia8vkeaOMlN5Nk0Gn1OtibjwPicOfJMEPqKiaDgKvsD5zDzvZEwAVYQQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;智能机器的发展是计算机科学领域中面临的最大挑战之一。在这篇论文中，我们提出了这些机器应该有的一些基本性质，尤其是沟通和学习上。我们探讨了一个用于逐步教一台机器学习基于自然语言的交流的环境，将它作为与人类用户实现更加复杂的互动前提。我们也提出一些猜想，是关于该机器应该支持的算法，以便于能便捷地从该环境中学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Sep 2016 11:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | PaddlePaddle之后，百度开源深度学习硬件基准DeepBench</title>
      <link>http://www.iwgc.cn/link/2854957</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Baidu Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、老红、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌、微软、Facebook 等传统的人工智能技术巨头之外，百度近来也加入到了技术开源的浪潮之中，继 PaddlePaddle 之后，百度又宣布开源了一项深度学习基准 DeepBench。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;开源地址：&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;https://github.com/baidu-research/DeepBench&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;1.DeepBench&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 是一个开源的基准工具，用来测量深度神经网络训练中的基础操作的表现。使用神经网络库，这些操作在不同的硬件平台被执行。如今测试基准工具 DeepBench 在 github 上已经开源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 的主要目标是 benchmark 对深度学习在不同硬件平台上而言很重要的运算。尽管深度学习背后的基础计算已经被很好的理解了，但在实践中它们被使用的方式惊人的不同。例如，矩阵相乘运算基于被相乘矩阵的大小和 Kernel 实现，可能是 compute-bound，也可能是 bandwidth-bound, 或者 occupancy-bound。因为每个深度学习模型带着不同的参数使用这些运算，面向深度学习硬件和软件的优化空间还是很大的，也是不足的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 试图解决这个问题，「在被用于训练深度神经网络时，在基础运算上哪种硬件提供最好的表现？」我们在低层次上详细说明了这些运算，建立深度学习处理器的团队很适合在硬件模拟中使用 DeepBench。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.1 DeepBench 适合用在哪里？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习生态系统包含不同的模块。我们想要强调 DeepBench 适合用于该生态系统的哪部分。在下面的图表中，描述了关于深度学习的软件和硬件组件。在最顶端是百度的 PaddlePaddle、Theano、TensorFlow、Torch 等这样的深度学习框架，这些框架使得我们能够建立深度学习模型。它们包含层（layer）这样的基础建筑模块，可通过不同的方式连接从而创造模型。为了训练这些模型，框架使用英伟达的 cuDNN 和英特尔的 MKL 这样的基础神经网络库。这些库执行矩阵相乘这样的用来训练深度学习模型的运算。最后，在英伟达 GPU 或英特尔 Xeon Phi 处理器这样的硬件上训练这些模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2e9M6RfMFXubcjvko6xfibLiccfnxQO07Ncfc327Gc0ibXiby4UQlJwn8Ww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 使用神经网络库 benchmark 基础运算在不同硬件上的表现。它对建立应用的深度学习框架或深度学习模型没用。我们不能测量使用 DeepBench 训练整个模型所需要的时间。为不同应用建立的模型的表现特性彼此间差别很大。因此，我们要 benchmark 涉及到深度学习模型训练中的潜在运算。benchmark 这些运算有助于提高硬件供应商的 意识，也有助于软件开发者了解深度学习训练的瓶颈。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.2 方法论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 包括一系列的基础操作（稠密矩阵相乘、卷积和通信）以及一些循环层类型。在开源的代码中有一个 Excel 表格描述了所有的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前向和后向的运算都会被测试。该基准的第一代版本将注重在 32 位浮点算法中的训练表现。未来的版本可能扩展到注重推理工作负载（inference workloads）和更低精度的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使存在更快的独立库或公开了更快的结果，我们也将使用供应商提供的库。大部分用户将默认使用供应商提供的库，而且这种库更代表用户的体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.3. Entry&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在释放在 4 个硬件平台上的结果：英伟达的 TitanX、M40、TitanX Pacal 和英特尔的 Knights Landing。硬件供应商或独立用户可运行大致的基准，并将结果输入到表格中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;2. 运算的类型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.1 稠密矩阵相乘&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;稠密矩阵相乘存在于大部分深度神经网络中。它们被用于执行全连接层和 vanilla RNN，以及为其他类型的循环层建立基石。有时它们也被用作快速执行新类层（在这里面自定义的代码不存在）的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当执行 GEMM 运算 A * B = C 时，A 和 B 中的一个或两个都能被随意的换位。描述一个矩阵问题的常用术语是 triple（M,N,K）, 该术语描述了矩阵的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2uGysicfIFOj1QHCpoMFePBv7fDkO4wMDufEMGsbs5j5XQlYITrr05EQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2 卷积&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积构成了网络中在图像和视频操作上的绝大多数的浮点计算，也构成了语音和自然语言模型网络中的主要部分，从模型表现角度来看，它可能也是唯一最重要的层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积有 4 或 5 维的输入和输出，为这些维提供了大量可能的排序。在改基准的第一代版本，我们只考虑了在 NCHW format 中的表现，即数据是在图像、特征映射、行和列中展示的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有很多计算卷积的技术对不同大小的过滤器和图像来说都是很好的选择，包括：direct approaches、基于矩阵相乘的方法、基于 FFT 的方法以及基于 Winograd 的方法。在该基准的第一代版本，我们没考虑不同方法的准确率，因为普遍共识是 32 位浮点计算对它们每个方法而言都是足够准确的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3 循环层（recurrent layers）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环层总是由之前的运算与一元（unary）或二元（binary）运算这样的简单计算结合而成的——这些简单运算不是计算密集型的，通常只需占据总体运算时间的一小部分。然而，在循环层中，GEMM 和卷积运算相对较小，所以这些更小运算的成本变得有极大影响。如果开始计算就有一个很高的固定成本，那上述内容就尤其准确。也可以为循环矩阵使用额外的存储格式，因为转换成一个新的存储格式的成本可被分摊到循环计算的许多步骤上。如果能做到这一点，那么从一个自定格式转换或转换成一个自定义格式的时间应该被包含在整体时间之内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在一个时间步骤内和跨序列的时间步骤上，这些因素会导致很多优化的可能性，因此测定运算的原始性能并不一定能够代表整个循环层的的性能。在这样的基准上，我们仅关注一个循环层，即使还存在其它更多的优化机会（如果考虑它们的层叠（stack））。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输入的计算不应该被包含在用于循环层计算的时间内，因为其可以作为一个大的乘法计算，然后被实际的循环运算所消化。所以在 h_t = g(Wx_t + Uh_t-1) 中，Wx_t 对于所有 t 的计算时间不应被包含在循环层的时间内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向计算应该在考虑权重而非输入的基础上计算更新（update）。所有的循环工作完成以计算权重更新，所以同时考虑输入来计算更新会掩盖我们想要测定的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;vanilla RNN 的非线性应该是一个 ReLU。LSTM 的内在非线性应该是标准运算——门（gate）是 S 型函数，激活（activation）是双曲正切函数。LSTM 不应该有窥视孔连接（peephole connections）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.4. All-Reduce&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在的神经网络通常在多 GPU 或多系统与 GPU 并行的情况下训练，这主要有两个技术分类：同步和异步。同步技术依赖于保持参数和所有模型实例的同步，它通常要保证在优化步骤执行前，所有模型实例有一些梯度的备份。最简单运行这些计算结果的 Message Passing Interface (MPI) 被称为 All-Reduce。有很多可以执行 All-Reduce 的方法，我们可以依靠数字的排列、数据的大小和网络的拓扑结构来执行。这种基准测试的方式在执行时是没有限制的，所以它的结果是不确定的。异步的方法则非常的不同，在这个版本的基准测试中我们不会测试这些方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了评估 All-Reduce，我们使用了下面的库和基准：NVIDIA's NCCL Ohio State University (OSU) Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCCL 库包括一组标准的通信程序。这个库支持在单个节点任意数量的 GPU 运行，并且它能在单个或多个进程中运行，但 NCC 程序不支持多节点下的 All-Reduce。为了能够在多节点下评估 All-Reduce，我们使用了 OSU 下的 benchmark。我们在三个执行过程中 (NCCL single process, NCCL MPI, OpenMPI) 报告了最短的延迟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Nvidia 8 GPU 系统的拓扑结构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个节点都有两个 GPU 插槽，而每个插槽都有一个 PCIe root complex。&lt;/span&gt;&lt;span&gt;每个节点都有两个 GPU 插槽，而每个插槽都有一个 PCIe root complex。每一套插槽都有两个 PLX 开关，它们通过 16 个 PCIe v3 的 lanes 各自连接到 CPU 插槽中。每个 PLX 插槽有两个 GPU，所有的 GPU 通过 16 个 PCIe v3 的 lanes 进行同时通信。这两个 CPU 插槽通过 Intel 的 QPI 连接，而跨界点的互联则是通过 InfiniBand FDR。下图显示了一个原理图的节点，在图中，所有的设备均由同一个虚线框内的 PCL root 连接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t22b2ic5ZGyos6e4fGLUWBR3jtbX331VTibN2TNGVnXsvuibRdK35b6ibuyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;英特尔 Xeon Phi 和 Omni-Path 系统的拓扑结构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MPI_AllReduce 时间是在英特尔 Xeon Phi 7250 处理器上测定的——在英特尔内部的带有 fat-tree 拓扑的 Intel® Omni-Path Architecture (Intel® OPA) series 100 fabric 结构的 Endeavor 集群上，使用了 Intel MPI 5.1.3.181。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 结果&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这部分，我们记录了一些运算的表现。下面这些结果是随机挑选的，它们只是为了演示几个应用的运算表现。下面的结果仅包括了特定操作和参数下最快的处理器的时间和浮点运算速度。完整的结果可以在库里查看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些软件库（例如 cuDNN 和 OpenMPI）和一些硬件系统的细节同样在 github 的库里适用。如有问题，请随时和我们联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦更多硬件平台的结果被发现可以适用，它们都会被加到库里面来。我们也欢迎所有硬件厂商为此贡献结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.1. GEMM Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t26OqvhvvTQEVm395lDtHE2VCYX0NH2gbKHxGic592ibxnQHjSlW7Cu3Lg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.2. Convolution Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t25oxl1fFBhibRCwSNRSETMqSwicHaY78Igxqx4GP3cRKMuqSUZCFFUibwQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.3. Recurrent Ops Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;周期性的操作内核仅能在 NVIDIA 的硬件上运行，而周期性的标准检查程序也将很快可以在 Intel 的硬件上运行。在今年十月份我们将会得到这些结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2FXbgGkpzRLSdNEVqfauH5psbhGLsHQ6Aplm11gQNMv8B5A0l3AWCDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.4. All-Reduce Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们仅仅只有一个 Pascal GPU，所有我们不能在 NVIDIA's TitanX Pascal GPU 运行 All-Reduce benchmark。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2jUqWWxibH4n3dffVFS4Y9gu7uAGMxLjvxIJ4SqLL8ibaqZzicjR1jOaTQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们欢迎来自各界的贡献，具体体现在如下两个方面：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 深度学习研究者/工程师：如果你是一个正在从事新的深度学习应用的研究者或者工程师，你可能会在训练你的模型的时候有不同的运算结果和工作量。而我们对于那些能够逆向影响你模型表现（速度）的底层运算结果非常感兴趣。请将这些运算结果和工作量反馈给我们。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 硬件供应商：我们也非常愿意接受来自其他硬件硬件供应商的贡献。我们对 benchmark results 的接受态度十分开放，无论你是大公司还是基于深度学习训练的小型创业公司，都请将你们的 benchmark results 反馈给我们！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Sep 2016 11:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Deep Learning School第二天：Yoshua Bengio压轴解读深度学习的基础和挑战</title>
      <link>http://www.iwgc.cn/link/2841694</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心整理&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今天早上，Deep Learning School 第二天的讲座结束（点击《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719393&amp;amp;idx=2&amp;amp;sn=22daaa88f7737c21587ba1e8bb217af7&amp;amp;chksm=871b00dfb06c89c92a1cac99880d347d9169368eea2a239df5ed26f58ddb9d5a4fe07af02aee&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719393&amp;amp;idx=2&amp;amp;sn=22daaa88f7737c21587ba1e8bb217af7&amp;amp;chksm=871b00dfb06c89c92a1cac99880d347d9169368eea2a239df5ed26f58ddb9d5a4fe07af02aee&amp;amp;scene=21#wechat_redirect"&gt;视频 | Deep Learning School 第一天：4 小时的深度学习盛宴&lt;/a&gt;》观看第一天的精彩内容）。John Schulman、Yoshua Bengio 等人分别作了关于深度学习的主题演讲，直播时长达到 10 个小时（如今公开的视频只有 4 个小时）。机器之心对第二天的讲座进行了整理，尤其重点介绍了 Yoshua Bengio 的讲演（PPT 截图，最后一位演讲者），读者可通过此篇文章介绍选择性的观看视频内容。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;第二天讲座观看地址：https://www.youtube.com/watch?v=9dXiAecyJrY&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;第一位演讲者：John Schulman（OpenAI 研究科学家）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：策略梯度和 Q-学习：上升到力量、对抗和统一（Policy Gradients and Q-Learning: Rise to Power, Rivalry, and Reunification）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先概述一下深度强化学习中目前最好的研究成果，其中包括最近的在视频游戏（如 Atari）、棋盘游戏（如 AlphaGo）和模拟机器人上的应用。然后我会给出这些成果背后的两种核心方法的教学介绍：策略梯度 和 Q-学习。最后，我将给出一个新的分析以说明这两种方法有多么类似。本演讲的主题将不仅是问「什么有效？」，而且还有「它在什么情况下有效？」以及「它为什么有效？」；另外还要找到这些问题的可用于调节具体的实现和设计更好的算法的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第二位演讲者：Patrice Lamblin&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：Theano 教学（Theano Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Theano 是一个 Python 库，允许在 CPU 或 GPU 上高效地定义、优化和评估涉及多维数组的数学表达式。自 Theano 诞生以来，它就一直在深度学习社区最受欢迎的框架之一，而且还有许多用于深度学习的框架也是基于它而构建的，其中包括 Lasagne、Keras、Blocks 等等。这个教程将首先关注 Theano 背后的概念以及如何构建和评估简单的表达，然后我会介绍如何定义和训练更为复杂的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;第三位演讲者：Adam Coates（百度硅谷人工智能实验室（Silicon Valley AI Lab）主任）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：用于语音的深度学习（Deep Learning for Speech）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：传统的语音识别系统具有大量模块，其中每一个都需要单独的艰难的工程开发。现在深度学习已能创造能执行大多数传统引擎的「端到端」的任务的神经网络，这能极大地简化新型语音系统的开发，并开启了实现人类水平的表现的大门。在本教程中，我们将概览一遍类似百度的「Deep Speech」模型的端到端系统的开发步骤。我们将会将这些片段组合起来成为一个最先进的语音系统的「比例模型」——现在已经在驱动生产型的语音引擎的神经网络的小型版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第四位演讲者：Alex Wiltschko（&lt;strong&gt;Twitter 的 Advanced Technology 部门的研究工程师，Whetlab 联合创始人）&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TxxoCK1Vtxmck1ANLiaooTqzkMjEAhj07WiaZd5Fj5XxOafhwqVAiad8hg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：Torch 教学（Torch Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Torch 是 Lua 语言环境中一个用于科学计算的开放平台，其关注的重点是机器学习，尤其是深度学习。Torch 为 GPU 计算提供了一流的支持，而且是以一种清晰的、交互式的和必需的风格，这是 Torch 和其它阵列库之间的显著不同点。尽管 Torch 从广泛的行业支持中获益匪浅，但却是社区所有的和社区开发的生态系统。包括 Torch NN、TensorFlow 和 Theano 在内的所有神经网络库都依靠自动微分（AD：automatic differentiation）来管理复杂函数组件的梯度计算。我将介绍一些自动微分的广义背景，这是基于梯度的优化的基础概念，还将展示 Twitter 通过 torch-autograd 对自动微分的灵活实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TC3YoRPw4yN5RLSAVt8KfLFGujgU69Uw9llc5nLia22joF8KGmFadAGA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TNOTusXyrib5xyCoOKG8zudqgmKctnaxhSiaTRfYvkZacuicfUlK8Q7yJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Torch 得到了很多企业组织的支持，发展非常快&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T2KxWB4ox6v5MF3G8PQ9fq9ia468QLEuxTGmOuQXuXibSF8kW8GcGMmxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;各种框架在产业界-研究界坐标上的位置&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第五位演讲者：Quoc Le（&lt;strong&gt;谷歌研究科学家，博士导师是吴恩达教授&lt;/strong&gt;）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TEwNARerhiakRfySaNNII66zI8BS6LwHYVUQcEdEnkBEeDWJ5OJp99gg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：用于自然语言处理和语音的序列到序列学习（Sequence to Sequence Learning for NLP and Speech）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先提出序列到序列（seq2seq）学习和注意模型（attention model）的基础，以及它们在机器翻译和语音识别上的应用。然后我将讨论带有指针（pointer）和函数（function）的注意。最后我会描述强化学习可以在 seq2seq 和注意模型中发挥怎样的作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TGwB8LEfxBSick8icnCMvkDxuYoTVAjiaUPaeial2kibzxevFvOGJDPAoLNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;带有注意的序列到序列（seq2seq）&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前在许多翻译任务重表现最好的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 使用词分割或词/字符混合（而不只是词）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 梯度裁剪以阻止梯度爆炸&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 使用 LSTM&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T38aj7jicmTxSV2ulKBlwGGM94vCQHxyByUXTUkxZ21pljb9toRuvodg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于语音的 seq2seq&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第六位演讲者：Yoshua Bengio&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TMPVRapf75Xzo5dibYlXUPGcGxbcHyWeaCC8LgKNz2EHlPp7BwhfSedg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：深度学习的基础和挑战（Foundations and Challenges of Deep Learning）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：为什么深度学习的效果这么好？未来还面临着什么挑战？这个演讲将首次探讨深度学习成功的关键因素。首先，根据 no-free lunch 定理，我们将讨论深度网络获取抽象的分布式表征的表达力。其次，我们将讨论我们的实际优化神经网络的参数的惊人能力——尽管存在非凸性（non-convexity）。然后我们将思考一些未来的难题，包括核心的表征问题——理解变化的基本解释因素，尤其是对于无监督学习，为什么这对于将强化学习带向下一阶段来说是非常重要的，以及仍然存在挑战性的优化问题，比如长期依赖的学习，理解深度网络的优化全局，以及为什么生物大脑的学习方式的秘密仍然值得从深度学习的角度来破解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TWEiaFLSNg9bAEugbhhTnsSuT2otrY988fWgjHibfA1uMDufNMNS5dHfA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TtXNs88IuaV7jicsBjpwiaSCLAUKnlht1I0Got83p5xCTVRbiczdKydTQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P2: 深度学习、人工智能和天下没有免费的午餐&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要用机器学习实现人工智能，你需要这五大关键成分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 许多许多数据&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 非常灵活的模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 足够的计算机算力&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 可以克服「维度的诅咒」的强大的先验知识&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 在计算上高效的推理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TnGHXARBBWqpv1rDfLibqrb6ZHzVtTk0YxqBdUtseeG7GAZTy7icFmjoQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P3:绕过维度的诅咒&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要为我们的机器学习模型构建组合性，正如人类语言通过组合来表达复杂想法的含义一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;组合性的使用能够给表现力带来巨大的提升&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分布式表征/嵌入：特征学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度架构：多层特征学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;先验的假设：组合性可用于有效地描述我们周围的世界&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TfyTdPLqEIfmANzRUcCYrOicdrgtHEOnXmyAibnrDXEMYFjxIRmRbyDibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P4: 非分布式表征&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聚类、n-grams、最近邻、RBF SVM、局部非参数密度估计 &amp;amp; 预测、决策树等&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个可以区分开的区域的参数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可区分的区域的序号和参数的序号成线性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T4jiaYs0BiamChTj09HpbfibLgqzlOkVDHRBp3MfYBLv033ooic8NQVp1Yw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P5:对分布式表征的需求&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因子模型、PCA、RBM、神经网络、稀疏编码、深度学习等&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个参数都可以影响很多区域，而不只是局部区域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可区分的区域的序号几乎与参数序号成指数增长&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;非局部地归纳成从未被见过的区域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T9kjk5ORiahEWJY0YxfCyxH8y4rrCGe8Erls8QWI49ibFyXZA7zwich4sg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P7：无需了解其它特征的指数级大量的配置就能发现每个特征&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TpCLnHRKHHPwSvxa3icl2hPBjDcGKE6GxibJtFPd4NFnGmdwjSIHyTicQw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P6：隐藏单元发现语义上有意义的概念&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Zhou et al &amp;amp; Torralba, arXiv1412.6856, ICLR 2015&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练神经网络识别地方，而非物体&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Tj9mRLGgIUSYvQFvHsDTxwbTiawgicur3om1f9xdY3eV0AtacuVnxJ3aA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P8：分布式表征的巨大优点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Twtgx4we7P9bEQBvC0StJ6UzvMme4Fh6iblPgG39ib1iacgAachvibSe1yA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P9：深度的先验知识是非常有用的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TsAia3aXsqLqv8ySibIdjub9QYtjKiaUvnoqQXlZx8VXCgJX46mGLyPiaVQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P10：「浅的」计算机程序&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TKb40WZCONpPaaZZwYQJgUt2CTqPYeEEsPM2vkNSAiabGQWJR20QbrdA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P11：「深的」计算机程序&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TiaFFHkamjpLU6GWT9ffJoLwZINWEfsMKMJ35Ld58C8c2Knd1VssJhfg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P12：深度的巨大优越性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TyUwltmPbVpj61gvVQ0MV59x4yc5dnwiaWaIAzQI0TNibicVUpBoa1giblg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P13：揭开神秘：神经网络的局部极小值→不需要凸性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TSEUFzGpxVdDquA7GPmmibaWH3kIpaopquW6IibOibPAIm2qMp2LlTLwKA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P14：鞍点（saddle points）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;低维情况局部最小值占主导，高维情况鞍点占主导&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大多数局部最小值接近底部（全局最小误差）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TgPDdZFm0HMoIKu8kdVtOBVE8QVqacicIibpdic9I8x5tautbsJXLQaGYw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P15：低指数临界点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Choromanska et al &amp;amp; LeCun AISTATS 2015, The Loss Surface of Multilayer Nets 表明 deep rectifier nets 类似于球形自旋玻璃模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大型模型的低指数临界点汇聚在全局极小值之上的一个带中&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Txv3qR2uCX2qzCHQViboZeWrKOLrsb5mHuVHqgk0PsNqbFTL3bYprvZg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P16：深度学习：超越模式识别，迈向人工智能&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Th4QLe6usPyfdK0gJIXDibYldATicvrOY4pJia7AketTe8NHZsql8gXlKw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;P17:用于深度学习的注意机制&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TJostTtrBicoLhoyv29Uxl7OUPYfVoHqu7b8Ybjle3nIcD3IkyKgKWVQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P18：使用循环网络和注意机制的端到端机器翻译&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TKvsyfiaW2wXGcB2us9BJiayC2vy3hwUIvB5Smj3A8o8m2CoPe9lzEjBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P19：IWSLT 2016——Luong &amp;amp; Manning（2016）TED 演讲机器翻译（英语-德语）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T7ibjCibGIePIwIP6tJheZMkmLWLVLZlhXnvSNIKvfsiaTPY1tbUQmOxDg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P20：下一个前沿：推理和问答&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TQYloH9rpTtBDxdwO8FIUWCn5LfOArfhNeBrHZmviacfibJnrRXVBiblhw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P21：使用梯度下降学习长期依赖是很困难的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TIYhWfPMibDJPSkanwHO5ZeBFnSyiaah1RDNxTTLAG70ib1gXgAkibU3FKQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P22：延迟和分层以更进一步&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TNktj2AibCiaj98wYhGoSibBNvMvAQNEyibx7Z3dfwhtTuvicfDsGgxgXUibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P23：用于记忆获取的记忆机制能实现推理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经图灵机（Graves et al 2014）和记忆网络（Weston et al 2014）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用一种形式的注意机制来控制一个记忆的读写权限&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该注意机制在注意焦点的位置上输出一个 softmax&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TJ6nuU0Qibf3vTVAj4iaupNO1LZQENUMCAPonsWUQmiatbwFruibSadCwQQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P24：大型记忆网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;记忆 = 状态性质&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于记忆的网络是特殊的 RNN&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个存储在外部存储中的心智状态可以维持任意长时间，直到其被重写（部分或不）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;遗忘 = 消失的梯度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;记忆 = 高维状态，避免或减少对遗忘/消失的需求&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T59UtbhYFykMAeibz1o3ia5Doib0LRYP8boRT3GsXqbgNTVsQFAGmg1Ncw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P25：下一个大挑战：无监督学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近来的进步基本上集中于监督深度学习领域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在无监督深度学习领域存在真正的技术难题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可能存在的好处：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用巨量无标注的数据&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回答关于被观察到的变量的新问题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正则化矩阵-迁移学习-领域适应&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;优化更简单（局部训练信号）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结构化的输出&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对没有给出模型或领域模拟器的强化学习是必需的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TTcjsbcTFFmd6JfCMN3deDDyVPpZ7nryDn3zKPkaGlTewAJ9F9kRm3w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P28：不变性和 disentangling&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T82qQSx9iaXfibgFJLiadkd0wNwkxBje2jwvOmmBfhDjT7CvymDF0SLNSw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P29：学习多层次的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习的最大好处是允许实现更高程度的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更高程度的抽象能够解开不变性的因子，这能带来远远更为简单的泛化和迁移&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TABpzlVrjVTfBL0TfsMGViba7Q7nb7uSIwO5KWicpn7DT1K98L19H7lcQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P30：追寻机器大脑和生物大脑的学习的关键原理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在整个大脑（或整个皮层）的网络层面上的联合学习仍然是一个秘&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向传播确实很赞，但我们目前还不清楚其所对应的生物机制是什么，也不知道怎么将其泛化到无监督学习上&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在弥合反向传播、玻尔兹曼机和 STDP 之间的鸿沟上实现了一些进展：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 《连线》长文揭秘微软Project Catapult：人工智能时代押注FPGA</title>
      <link>http://www.iwgc.cn/link/2841695</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Wired &lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、李亚洲、虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;可编程的芯片将主导未来的互联网世界，几大科技巨头都认识到这一点。微软也不例外，这家做了四十年软件的公司，也在向硬件进军，而 FPGA 成为了微软未来竞争的押注。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2012 年 12 月的某一天，Doug Burger 站在 Steve Ballmer 面前，尝试着预测未来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ballmer，微软的那个大嗓门 CEO，坐在&lt;span&gt;微软在西雅图郊外的蓝天研发实验室（blue-sky R&amp;amp;D lab）基地&lt;/span&gt; 99 号楼一层的演讲室。桌子排成 U 型，Ballmer 被他的高级助围住，开着笔记本电脑。Burger，一位四年前加入微软的计算机芯片研究员，正在为高管们描绘一个新想法，Project Catapult。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TrCpaWK6jeaOhkBibBRfdoTJgUqXK2VYZYpmv2DzvyuTBRzNr8NP9FHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Doug Burger&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Burger 解释道，技术世界正在迈向一个新轨道。未来将是少数几家互联网巨头运作着几个巨型互联网服务，这些服务非常复杂并与之前的服务非常不同，所以这几家公司不得不打造全新的架构来运行它们。不仅仅是驱动这些服务的软件，巨头们还得造出硬件，包括相应的服务器和网络设备。Project Catapult 将会为微软所有的服务器——几百万台——提供专用芯片，这些芯片可以用来为特定的任务重新编程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是还没等待 Burger 介绍到这个芯片时，Ballmer 突然抬起了头。Ballmer 刚到微软研究时，就说他希望看到研发中心有新进展，而不是一个战略简报。「他开始拷问我，」Burger 说。微软花了 40 年建立起像 Windows、Word 和 Excel 这样的 PC 软件，然而它才发现自己只是刚刚涉足互联网。微软还没有编程计算机芯片所必须的工具和工程师，这是一项困难、耗时、专业且有些奇怪的任务。微软编程计算机芯片听起来就像是可口可乐要做鱼翅汤了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TyE2pibbyQwQuCAOPwX0QiaX4zUe0sn4uNBpbTFtuqW9iaWiaGVlrYsibx5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Project Catapult 目前的样子&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Burger，着装整齐，轻微秃头，能冷静的分析问题，就像很多优秀的工程师一样。他转过身，告诉 Ballmer 像谷歌和亚马逊这样的公司一直正在超这个方向发展。他说世界上的硬件制造商不会提供微软需要用来运行线上服务的硬件。他说，如果微软不打造自己的硬件，就会落后。Ballmer 听完后并不买账。但是过了一会儿，另一个声音参与到这场讨论中来。这个声音来自陆奇，他管理者 Bing，微软的搜索引擎。两年来，陆奇的团队一直在和 Burger 讨论可再编程芯片的事情。Project Catapult 不仅仅是一种可能，陆说：他的团队已经开始着手做了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，微软已经有了现场可编程门阵列（field programmable gate arrays，FPGA），Burger 和陆都相信这个可编程芯片可以改变世界。FPGA 目前已支持 Bing，未来几周，它们将会驱动基于深度神经网络——以人类大脑结构为基础建模的人工智能——的新搜索算法，在执行这个人工智能的几个命令时，速度比普通芯片快上几个数量级。有了它，你的计算机屏幕只会空屏 23 毫秒而不是 4 秒。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TkjQ3SLx21muPPCJ3dr17ibt4GuZUnRV7X5ueysdj7FLSiciatkKKymSoA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Catapult 团队成员 Adrian Caulfield, Eric Chung, Doug Burger, 和 Andrew Putnam&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 不只是要追赶 Google。Project Catapult 暗示着改变未来全球系统运作的方式。从美国的亚马逊到中国的百度，所有的互联网巨头都在用硅替代他们的标准服务器芯片——中央处理单元，也叫 CPU，这些硅制成的芯片可以让它们跟上人工智能的快速变化。微软现在每年花在硬件上的钱在 50 亿到 60 亿美元，以维持其线上帝国的运转。所以这样的工作「再也不仅仅是研究了，」Satya Nadella 说道，他在 2014 年接任了微软 CEO 一职。「它有极为重要的优先性。」也就是 Burger 当年在 99 号大楼中要解释的，并让他和他的团队耗费多年，克服种种挫折，不断重新设计，与体制对抗，最终实现的一种新的全球超级计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;一种全新的古老计算机芯片&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年 12 月，微软研究院 Andrew Putnam 离开西雅图度假，回到了位于科罗拉多斯普林斯的家中。当时正是圣诞节前两天，他还没开始大采购。正在他开车去商场的路上，电话突然响了，另一端正是他的老板 Burger。Burger 当时打算节后面见 Bing 高管，但他需要一份能在 FPGA 上运行 Bing 机器学习算法的硬件设计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Putnam 找到最近的星巴克开始规划设计，这大约花了他 5 个小时，所以他仍有时间去购物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当时 Burger 45 岁，Putnam 41 岁，两人过去都是学者。Burger 曾在特克萨斯大学奥斯汀分校担任计算机科学教授，他在那里工作了 9 年，专攻微处理器，还设计了一款名为 EDGE 的新型芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Putnam 曾在华盛顿大学工作 5 年，担任研究员并主要从事 FPGA 研究。当时可编程芯片已经存在了好几十年，但它们大多被当作处理器的一部分。2009 年 Burger 将 Putnam 挖到微软，两人开始探索用可编程芯片提升线上服务速度的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TH5cctXxQiboTQiaZRUC1zQvFb4PickWYnJpjs3n0E0HzWgCDfDEseEv4g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Project Catapult V1，即 Doug Burger 团队曾在微软西雅图数据中心测试过的版本。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的搜索引擎是一个依靠成千上万台机器运行的在线服务。每台机器都需要靠 CPU 驱动，尽管英特尔等公司不断改进它们，这些芯片还是跟不上软件更新的脚步。很大程度上，是因为人工智能浪潮的来临。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 搜索等服务已经超出了摩尔定律预言的处理器能力，即每 18 个月处理器上晶体管的数量翻一倍。事实还证明增加 CPU 并不能解决问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但同时，为新出现的需求制造专用芯片，成本是非常昂贵的。恰好 FPGA 能弥补这个不足，Bing 决定让工程师制造运行更快、比流水线生产的通用 CPU 能耗更少、同时可定制的芯片，从而解决不断更新的技术和商业模式变化所产生的种种难题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;圣诞节后的会面中，Burger 为必应高管们拿出了一套用 FPGA 提升搜索速度，同时功耗较低的方法。高管们不置可否。在接下来的几个月中，Burger 团队根据 Putnam 圣诞节时画出的草图构建了原型，证明其运行必应的机器学习算法时速度可以提升 100 倍。「那时他们才表现出浓厚兴趣」，当时的团队成员、现瑞士洛桑联邦理工学院院长 Jim Larus 告诉我们，「但同样也是艰难时光的开始。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原型是一个使用六个 FPGA 的专用盒，由一整个机架的服务器共享。如果盒子吱吱作响，表明它们需要更多 FPGA——考虑到机器学习模型的复杂性需求会越来越大——这些机器就会停止工作。必应的工程师非常厌恶这件事。「但他们没错，」Larus 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正是这个原型吸引了陆奇。他给了 Burger 足够的资金，可以在 1600 台服务器上装配 FPGA 并进行测试。在中国和台湾硬件制造商的帮助下，团队花费半年时间制造出了硬件产品，并在微软数据中心的一组机架上进行测试。但一天晚上灭火系统出现了问题。他们花了三天时间修复机架——它仍能工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2013 年到 2014 年的几个月中，测试显示必应「决策树」机器学习算法在新芯片的帮助下，可以提升 40 倍运行速度。2014 年夏天，微软公开表示要很快要将这些硬件应用到必应实时数据中心。但是在那之后，微软暂停了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;不只是 Bing 搜索&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 在前几年一直是微软线上发展的核心，但到 2015 年，公司有了其他两个主要的在线服务：商务应用套件 Office 365 和云计算服务 Microsoft Azure。和其他竞争者一样，微软高层意识到运营一个不断成长的在线帝国的唯一有效方法是在同样的基础上运营所有的服务。如果 Project Catapult 将转变微软的话，那 Bing 也不能被排除在外。它也要在 Azure 和 Office 365 内部工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;问题是，Azure 高官们不在乎加速机器学习，他们需要联络的帮助。Azure 数据中心的流量跳动增长的太快，服务的 CPU 不能跟上脚步。最终，Azure 首席架构师 Mark Russinovich 这样一批人看到了 Catapult 能帮助解决这些问题，但不是为 Bing 设计的那种解决问题的方式。他的团队需要可编程的芯片，将每个服务器连接到主要网络，如此他们在数据流量到达服务器之前就能开始处理了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T7BMwMRPcEESVcEoH5Scxk7oQSI5XIWkE7TSicd9vLt1oqntAgGKvYTg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;FPGA 架构的第一代原型是一个被一架服务器共享的单个盒子（Version 0）。然后该团队转向为每个服务器设计自己的 FPGA（Version 1）。然后他们将芯片放到服务器和整体网络之间。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，研究 FPGA 的这伙人需要重新开发硬件。在第三代原型中，芯片位于每个服务器的边缘，直接插入到网络，但仍旧创造任何机器都可接入的 FPGA 池。这开始看起来是 Office 365 可用的东西了。最终，Project Catapult 准备好上线了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Larus 将许多重新设计描述为噩梦，这不是因为他们需要建立新的硬件，而是他们每次都需要重新编程 FPGA。他说，「这非常的糟糕，要比编程软件都糟糕，更难写、难纠正。」这是一项非常繁琐的工作，像是改变芯片上的小逻辑门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然最终的硬件已经有了，微软还要面对每一次重新编程这些芯片时都会遇到的同样挑战。「这是一个看世界思考世界的全新视角，」Larus 说。但是 Catapult 硬件的成本只占了服务器中所有其他的配件总成本的 30%，需要的运转能量也只有不到 10%，但其却带来了 2 倍原先的处理速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个布局非常大。微软 Azure 用这些可编程的芯片来路由、加密和压缩数据。Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。而且据微软的一名员工说，Office365 正在尝试在加密和压缩上使用 FPGA 以及机器学习——这一举措将惠及其 2310 万用户。最终，Burger 说道，这些会驱动所有的微软服务。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;这真的起作用吗？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Peter Lee 说，「这仍然使我迷惑。我们让公司做这些事。」Lee 监管着微软内部一个被称为 NExT 的组织，NExT 是 New Experience and Technologies 的缩写。在 Nadella 接任 CEO 之后，他个人推动了 NExT 的创建，代表了从 Ballmer 十年统治的重大转变。该组织的目标是培养能在近期实现的研究，而不是远期研究，这能改变微软如今的进程，而非多少年后的进程，就像增强现实设备 HoloLens 一样。当然也包括 Project Catapult。Burger 说，「起跳点就在前面，来自于非 CPU 技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TYPa0vfLTXOrXy4ZNmsXicboiasUic4COd58S2NJiaow8ylrzeer5nCFTQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Peter Lee&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所有的互联网巨头，包括微软，如今都在用图像处理单元增补 CPU，GPU 可为游戏和其他高度视觉化的应用渲染图像。例如，当这些公司训练神经网络识别图像中的人脸时（输入百万张图片），GPU 可处理很多的计算。像微软这样的巨头也使用可替代的硅片在训练后执行神经网络。而且，即使定制芯片异常昂贵，谷歌在设计执行神经网络的处理器上也走得相当远了，也就是他们设计的 TPU。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TPU 上，谷歌为追求速度牺牲了长期灵活性。也就是说，在识别进入智能手机的指令时，TPU 想要消除所有的延迟。但问题是如果神经网络模型改变的话，谷歌必须要建立新的芯片。但在 FPGA 上，微软在打一场长久战。尽管在速度上比不上谷歌的定制芯片，微软可在需要的时候重新编程芯片。微软不只能为新的人工智能模型编程，也能为任何任务重新编程。而且这些设计在接下来几年如果有用，微软能一直采用这种 FPGA 的程序，并建立专用芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TVv0iaIQiceDuZZlqKyQc54MYlWMR9KXWDD4UQEQDz2CicwViaGoTCWxPXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;该硬件的新版本 V2，是一张能插入微软任一服务器终端的芯片，并能直接连接到网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的服务很广，也使用如此多的 FPGA，如今他们正在改变全球芯片市场。FPGA 出自一家名为 Altera 的公司，英特尔副总裁 Diane Bryant 告诉我为什么英特尔会在去年夏天收购 Altera，这是一笔价值 167 亿美元的收购，也是芯片制造商史上最大的一笔收购。她说，到 2020 年，所有主要的云计算公司的 1/3 的服务器将使用 FPGA。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是科技名词缩写之间的纠缠：GPU、CPU、TPU、FPGA。但它们也是将成为关键的代名词。在云计算上，微软、谷歌、亚马逊这些公司驱动着世界上很大一部分技术，以至于这些可选择的芯片将驱动大范围的 app 和在线服务。Lee 说，直到 2030 年，Project Catapult 将继续扩展微软全球超级计算机的能力。在这之后，他说，微软就能转向到量子计算了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之后当我们谈到手机时，Nadella 也告诉了我同样的事。他们读取自同样的微软蓝图，正在触摸量子技术驱动的超快计算机的未来。想象建立量子机器多么的难，就像白日梦一样。但在几年前，Project Catapult 也如同白日梦一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：https://www.wired.com/2016/09/microsoft-bets-future-chip-reprogram-fly/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 伯克利大学和Adobe开源最新的深度学习图像编辑工具 iGAN（附论文）</title>
      <link>http://www.iwgc.cn/link/2841696</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Github&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;近日，伯克利和 Adobe 在 Github 上开源了新的深度学习图像编辑工具 iGAN。这是在 ECCV 2016 接收的的论文 Generative Visual Manipulation on the Natural Image Manifold 中作者们介绍的工具。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=m0331056q04&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Github 开源介绍&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;给定一些用户笔触（stroke），我们的系统能够实时产生最佳满足用户编辑的逼真图像样本。我们的系统基于 GAN、DCGAN 这样的深度生成模型。该系统有以下两个目标：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TIfuibMiaIRNwTBq9cwOe5rGpe7dibdZwa2JpBia1rbxkUHKK8rPOhA2Zgw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;受笔触效果颜色和形状的启发，系统中的智能绘画界面能自动生成图像&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;系统中的交互视觉 debugging 工具能理解并可视化深度生成模型。通过与该生成模型交互，开发者能理解该模型可生成什么视觉内容，也有助于理解模型的缺陷。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在研究支持更多的生成式模型（例如，变自编码器）和更多的深度学习框架（比如，TensorFlow）。欢迎提议更多的新变化或贡献更多的新特征（比如，TensorFlow 分支）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开源代码地址：&lt;span&gt;https://github.com/junyanz/iGAN#igan-interactive-image-generation-via-generative-adversarial-networks&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：在自然图像流形上的生成式视觉操作（Generative Visual Manipulation on the Natural Image Manifold）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Tz2EWoOSx5lOEabG5uwpQ07j9jkkAGOWvqPicCVaZicmw5YLmMeqpHYHg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：真实图像流形上的操作一直具有挑战性，因为它需要以一种用户可控的方式调整图像外貌，还要保留结果的真实性。除非用户有相当好的艺术技能，不然在编辑时候很容易减少自然图像的流形。在此论文中，我们提出使用生成式对抗网神经网络直接从数据中学习自然图像的流形。然后，我们定义了一类图像编辑操作，并依赖一直学习到的流形束缚它们的输出。该模型能自动调整输出，保持所有的编辑都是尽可能真实的。我们所有的处理方法都依据约束最优化来表达，几乎是实时的情况下被应用。我们在真实图像形状和颜色操作任务上评估该算法。该方法可进一步用于将一张图像改变为类似的一张，也可基于用户的涂鸦乱画生成新的图像。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>干货 | 深度学习名词表：57个专业术语加相关资料解析（附论文）</title>
      <link>http://www.iwgc.cn/link/2831065</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自wildml&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文整理了一些深度学习领域的专业名词及其简单释义，同时还附加了一些相关的论文或文章链接。本文编译自 wildml，作者仍在继续更新该表，编译如有错漏之处请指正。文章中的论文与 PPT 读者可点击阅读原文下载。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;激活函数（Activation Function）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了让神经网络能够学习复杂的决策边界（decision boundary），我们在其一些层应用一个非线性激活函数。最常用的函数包括 &amp;nbsp;sigmoid、tanh、ReLU（Rectified Linear Unit 线性修正单元） 以及这些函数的变体。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adadelta&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adadelta 是一个基于梯度下降的学习算法，可以随时间调整适应每个参数的学习率。它是作为 Adagrad 的改进版提出的，它比超参数（hyperparameter）更敏感而且可能会太过严重地降低学习率。Adadelta 类似于 rmsprop，而且可被用来替代 vanilla SGD。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Adadelta：一种自适应学习率方法（ADADELTA: An Adaptive Learning Rate Method）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福 CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adagrad&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adagrad 是一种自适应学习率算法，能够随时间跟踪平方梯度并自动适应每个参数的学习率。它可被用来替代vanilla SGD (http://www.wildml.com/deep-learning-glossary/#sgd)；而且在稀疏数据上更是特别有用，在其中它可以将更高的学习率分配给更新不频繁的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福 CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adam&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adam 是一种类似于 rmsprop 的自适应学习率算法，但它的更新是通过使用梯度的第一和第二时刻的运行平均值（running average）直接估计的，而且还包括一个偏差校正项。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Adam：一种随机优化方法（Adam: A Method for Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;仿射层（Affine Layer）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络中的一个全连接层。仿射（Affine）的意思是前面一层中的每一个神经元都连接到当前层中的每一个神经元。在许多方面，这是神经网络的「标准」层。仿射层通常被加在卷积神经网络或循环神经网络做出最终预测前的输出的顶层。仿射层的一般形式为 y = f(Wx + b)，其中 x 是层输入，w 是参数，b 是一个偏差矢量，f 是一个非线性激活函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;注意机制（Attention Mechanism）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意机制是由人类视觉注意所启发的，是一种关注图像中特定部分的能力。注意机制可被整合到语言处理和图像识别的架构中以帮助网络学习在做出预测时应该「关注」什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：深度学习和自然语言处理中的注意和记忆（http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Alexnet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alexnet 是一种卷积神经网络架构的名字，这种架构曾在 2012 年 ILSVRC 挑战赛中以巨大优势获胜，而且它还导致了人们对用于图像识别的卷积神经网络（CNN）的兴趣的复苏。它由 5 个卷积层组成。其中一些后面跟随着最大池化（max-pooling）层和带有最终 1000 条路径的 softmax (1000-way softmax)的 3个全连接层。Alexnet 被引入到了使用深度卷积神经网络的 ImageNet 分类中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自编码器（Autoencoder）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自编码器是一种神经网络模型，它的目标是预测输入自身，这通常通过网络中某个地方的「瓶颈（bottleneck）」实现。通过引入瓶颈，我们迫使网络学习输入更低维度的表征，从而有效地将输入压缩成一个好的表征。自编码器和 PCA 等降维技术相关，但因为它们的非线性本质，它们可以学习更为复杂的映射。目前已有一些范围涵盖较广的自编码器存在，包括 降噪自编码器（Denoising Autoencoders）、变自编码器（Variational Autoencoders）和序列自编码器（Sequence Autoencoders）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;降噪自编码器论文：Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;变自编码器论文：Auto-Encoding Variational Bayes&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;序列自编码器论文：Semi-supervised Sequence Learning&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;平均池化（Average-Pooling）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;平均池化是一种在卷积神经网络中用于图像识别的池化（Pooling）技术。它的工作原理是在特征的局部区域上滑动窗口，比如像素，然后再取窗口中所有值的平均。它将输入表征压缩成一种更低维度的表征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;反向传播（Backpropagation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向传播是一种在神经网络中用来有效地计算梯度的算法，或更一般而言，是一种前馈计算图（feedforward computational graph）。其可以归结成从网络输出开始应用分化的链式法则，然后向后传播梯度。反向传播的第一个应用可以追溯到 1960 年代的 Vapnik 等人，但论文 Learning representations by back-propagating errors常常被作为引用源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：计算图上的微积分学：反向传播（http://colah.github.io/posts/2015-08-Backprop/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通过时间的反向传播（BPTT：Backpropagation Through Time）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过时间的反向传播是应用于循环神经网络（RNN）的反向传播算法。BPTT 可被看作是应用于 RNN 的标准反向传播算法，其中的每一个时间步骤（time step）都代表一个计算层，而且它的参数是跨计算层共享的。因为 RNN 在所有的时间步骤中都共享了同样的参数，一个时间步骤的错误必然能「通过时间」反向到之前所有的时间步骤，该算法也因而得名。当处理长序列（数百个输入）时，为降低计算成本常常使用一种删节版的 BPTT。删节的 BPTT 会在固定数量的步骤之后停止反向传播错误。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Backpropagation Through Time: What It Does and How to Do It&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分批标准化（BN：Batch Normalization）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分批标准化是一种按小批量的方式标准化层输入的技术。它能加速训练过程，允许使用更高的学习率，还可用作规范器（regularizer）。人们发现，分批标准化在卷积和前馈神经网络中应用时非常高效，但尚未被成功应用到循环神经网络上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：分批标准化：通过减少内部协变量位移（Covariate Shift）加速深度网络训练（Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用分批标准化的循环神经网络（Batch Normalized Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;双向循环神经网络（Bidirectional RNN）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;双向循环神经网络是一类包含两个方向不同的 RNN 的神经网络。其中的前向 RNN 从起点向终点读取输入序列，而反向 RNN 则从终点向起点读取。这两个 RNN 互相彼此堆叠，它们的状态通常通过附加两个矢量的方式进行组合。双向 RNN 常被用在自然语言问题中，因为在自然语言中我们需要同时考虑话语的前后上下文以做出预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：双向循环神经网络（Bidirectional Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Caffe&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Caffe 是由伯克利大学视觉和学习中心开发的一种深度学习框架。在视觉任务和卷积神经网络模型中，Caffe 格外受欢迎且性能优异&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分类交叉熵损失（Categorical Cross-Entropy Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分类交叉熵损失也被称为负对数似然（negative log likelihood）。这是一种用于解决分类问题的流行的损失函数，可用于测量两种概率分布（通常是真实标签和预测标签）之间的相似性。它可用 L = -sum(y * log(y_prediction)) 表示，其中 y 是真实标签的概率分布（通常是一个one-hot vector），y_prediction 是预测标签的概率分布，通常来自于一个 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;信道（Channel）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习模型的输入数据可以有多个信道。图像就是个典型的例子，它有红、绿和蓝三个颜色信道。一个图像可以被表示成一个三维的张量（Tensor），其中的维度对应于信道、高度和宽度。自然语言数据也可以有多个信道，比如在不同类型的嵌入（embedding）形式中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;卷积神经网络（CNN/ConvNet：Convolutional Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNN 使用卷积连接从输入的局部区域中提取的特征。大部分 CNN 都包含了卷积层、池化层和仿射层的组合。CNN 尤其凭借其在视觉识别任务的卓越性能表现而获得了普及，它已经在该领域保持了好几年的领先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n类——用于视觉识别的卷积神经网络（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：理解用于自然语言处理的卷积神经网络（http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度信念网络（DBN：Deep Belief Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DBN 是一类以无监督的方式学习数据的分层表征的概率图形模型。DBN 由多个隐藏层组成，这些隐藏层的每一对连续层之间的神经元是相互连接的。DBN 通过彼此堆叠多个 RBN（限制波尔兹曼机）并一个接一个地训练而创建。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：深度信念网络的一种快速学习算法（A fast learning algorithm for deep belief nets）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Deep Dream&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是谷歌发明的一种试图用来提炼深度卷积神经网络获取的知识的技术。这种技术可以生成新的图像或转换已有的图片从而给它们一种幻梦般的感觉，尤其是递归地应用时。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：Github 上的 Deep Dream（https://github.com/google/deepdream）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：Inceptionism：向神经网络掘进更深（https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Dropout&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Dropout 是一种用于神经网络防止过拟合的正则化技术。它通过在每次训练迭代中随机地设置神经元中的一小部分为 0 来阻止神经元共适应（co-adapting），Dropout 可以通过多种方式进行解读，比如从不同网络的指数数字中随机取样。Dropout 层首先通过它们在卷积神经网络中的应用而得到普及，但自那以后也被应用到了其它层上，包括输入嵌入或循环网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Dropout: 一种防止神经网络过拟合的简单方法（Dropout: A Simple Way to Prevent Neural Networks from Overfitting）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：循环神经网络正则化（Recurrent Neural Network Regularization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;嵌入（Embedding）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个嵌入映射到一个输入表征，比如一个词或一句话映射到一个矢量。一种流行的嵌入是词语嵌入（word embedding，国内常用的说法是：词向量），如 word2vec 或 GloVe。我们也可以嵌入句子、段落或图像。比如说，通过将图像和他们的文本描述映射到一个共同的嵌入空间中并最小化它们之间的距离，我们可以将标签和图像进行匹配。嵌入可以被明确地学习到，比如在 word2vec 中；嵌入也可作为监督任务的一部分例如情感分析（Sentiment Analysis）。通常一个网络的输入层是通过预先训练的嵌入进行初始化，然后再根据当前任务进行微调（fine-tuned）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度爆炸问题（Exploding Gradient Problem）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度爆炸问题是梯度消失问题（Vanishing Gradient Problem）的对立面。在深度神经网络中，梯度可能会在反向传播过程中爆炸，导致数字溢出。解决梯度爆炸的一个常见技术是执行梯度裁剪（Gradient Clipping）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微调（Fine-Tuning）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Fine-Tuning 这种技术是指使用来自另一个任务（例如一个无监督训练网络）的参数初始化网络，然后再基于当前任务更新这些参数。比如，自然语言处理架构通常使用 word2vec 这样的预训练的词向量（word embeddings），然后这些词向量会在训练过程中基于特定的任务（如情感分析）进行更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度裁剪（Gradient Clipping）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度裁剪是一种在非常深度的网络（通常是循环神经网络）中用于防止梯度爆炸（exploding gradient）的技术。执行梯度裁剪的方法有很多，但常见的一种是当参数矢量的 L2 范数（L2 norm）超过一个特定阈值时对参数矢量的梯度进行标准化，这个特定阈值根据函数：新梯度=梯度*阈值/L2范数（梯度）{new_gradients = gradients * threshold / l2_norm(gradients)}确定。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GloVe&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Glove 是一种为话语获取矢量表征（嵌入）的无监督学习算法。GloVe 的使用目的和 word2vec 一样，但 GloVe 具有不同的矢量表征，因为它是在共现（co-occurrence）统计数据上训练的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：GloVe：用于词汇表征（Word Representation）的全局矢量（Global Vector）（GloVe: Global Vectors for Word Representation ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GoogleLeNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GoogleLeNet 是曾赢得了 2014 年 ILSVRC 挑战赛的一种卷积神经网络架构。这种网络使用 Inception 模块（Inception Module）以减少参数和提高网络中计算资源的利用率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用卷积获得更深（Going Deeper with Convolutions）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GRU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GRU（Gated Recurrent Unit：门控循环单元）是一种 LSTM 单元的简化版本，拥有更少的参数。和 LSTM 细胞（LSTM cell）一样，它使用门控机制，通过防止梯度消失问题（vanishing gradient problem）让循环神经网络可以有效学习长程依赖（long-range dependency）。GRU 包含一个复位和更新门，它们可以根据当前时间步骤的新值决定旧记忆中哪些部分需要保留或更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Highway Layer&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Highway Layer　是使用门控机制控制通过层的信息流的一种神经网络层。堆叠多个 Highway Layer 层可让训练非常深的网络成为可能。Highway Layer 的工作原理是通过学习一个选择输入的哪部分通过和哪部分通过一个变换函数（如标准的仿射层）的门控函数来进行学习。Highway Layer 的基本公式是 T * h(x) + (1 - T) * x；其中 T 是学习过的门控函数，取值在 0 到 1 之间；h(x) 是一个任意的输入变换，x 是输入。注意所有这些都必须具有相同的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Highway Networks&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ICML&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即国际机器学习大会（International Conference for Machine Learning），一个顶级的机器学习会议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ILSVRC&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即 ImageNet 大型视觉识别挑战赛（ImageNet Large Scale Visual Recognition Challenge），该比赛用于评估大规模对象检测和图像分类的算法。它是计算机视觉领域最受欢迎的学术挑战赛。过去几年中，深度学习让错误率出现了显著下降，从 30% 降到了不到 5%，在许多分类任务中击败了人类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Inception模块（Inception Module）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception模块被用在卷积神经网络中，通过堆叠 1×1 卷积的降维（dimensionality reduction）带来更高效的计算和更深度的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用卷积获得更深（Going Deeper with Convolutions）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Keras&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kears 是一个基于 Python 的深度学习库，其中包括许多用于深度神经网络的高层次构建模块。它可以运行在 TensorFlow 或 Theano 上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LSTM&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长短期记忆（Long Short-Term Memory）网络通过使用内存门控机制防止循环神经网络（RNN）中的梯度消失问题（vanishing gradient problem）。使用 LSTM 单元计算 RNN 中的隐藏状态可以帮助该网络有效地传播梯度和学习长程依赖（long-range dependency）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：长短期记忆（LONG SHORT-TERM MEMORY）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：理解 LSTM 网络（http://colah.github.io/posts/2015-08-Understanding-LSTMs/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最大池化（Max-Pooling）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;池化（Pooling）操作通常被用在卷积神经网络中。一个最大池化层从一块特征中选取最大值。和卷积层一样，池化层也是通过窗口（块）大小和步幅尺寸进行参数化。比如，我们可能在一个 10×10 特征矩阵上以 2 的步幅滑动一个 2×2 的窗口，然后选取每个窗口的 4 个值中的最大值，得到一个 5×5 特征矩阵。池化层通过只保留最突出的信息来减少表征的维度；在这个图像输入的例子中，它们为转译提供了基本的不变性（即使图像偏移了几个像素，仍可选出同样的最大值）。池化层通常被安插在连续卷积层之间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;MNIST&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MNIST数据集可能是最常用的一个图像识别数据集。它包含 60,000 个手写数字的训练样本和 10,000 个测试样本。每一张图像的尺寸为 28×28像素。目前最先进的模型通常能在该测试集中达到 99.5% 或更高的准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;动量（Momentum）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动量是梯度下降算法（Gradient Descent Algorithm）的扩展，可以加速和阻抑参数更新。在实际应用中，在梯度下降更新中包含一个动量项可在深度网络中得到更好的收敛速度（convergence rate）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：通过反向传播（back-propagating error）错误学习表征&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层感知器（MLP：Multilayer Perceptron）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层感知器是一种带有多个全连接层的前馈神经网络，这些全连接层使用非线性激活函数（activation function）处理非线性可分的数据。MLP 是多层神经网络或有两层以上的深度神经网络的最基本形式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;负对数似然（NLL：Negative Log Likelihood）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参见分类交叉熵损失（Categorical Cross-Entropy Loss）&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经网络机器翻译（NMT：Neural Machine Translation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NMT 系统使用神经网络实现语言（如英语和法语）之间的翻译。NMT 系统可以使用双语语料库进行端到端的训练，这有别于需要手工打造特征和开发的传统机器翻译系统。NMT 系统通常使用编码器和解码器循环神经网络实现，它可以分别编码源句和生成目标句。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经图灵机（NTM：Neural Turing Machine）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NTM 是可以从案例中推导简单算法的神经网络架构。比如，NTM 可以通过案例的输入和输出学习排序算法。NTM 通常学习记忆和注意机制的某些形式以处理程序执行过程中的状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：神经图灵机（Neural Turing Machines）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;非线性（Nonlinearity）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;参见激活函数（Activation Function）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;噪音对比估计（NCE：noise-contrastive estimation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;噪音对比估计是一种通常被用于训练带有大输出词汇的分类器的采样损失（sampling loss）。在大量的可能的类上计算 softmax 是异常昂贵的。使用 NCE，我们可以将问题降低成二元分类问题，这可以通过训练分类器区别对待取样和「真实」分布以及人工生成的噪声分布来实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：噪音对比估计：一种用于非标准化统计模型的新估计原理（Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用噪音对比估计有效地学习词向量（Learning word embeddings efficiently with noise-contrastive estimation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;池化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参见最大池化（Max-Pooling）或平均池化（Average-Pooling）&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;受限玻尔兹曼机（RBN：Restricted Boltzmann Machine）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RBN 是一种可被解释为一个随机人工神经网络的概率图形模型。RBN 以无监督的形式学习数据的表征。RBN 由可见层和隐藏层以及每一个这些层中的二元神经元的连接所构成。RBN 可以使用对比散度（contrastive divergence）进行有效的训练，这是梯度下降的一种近似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;第六章：动态系统中的信息处理：和谐理论基础&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：受限玻尔兹曼机简介（An Introduction to Restricted Boltzmann Machines）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;循环神经网络（RNN：Recurrent Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RNN 模型通过隐藏状态（或称记忆）连续进行相互作用。它可以使用最多 N 个输入，并产生最多 N 个输出。比如，一个输入序列可能是一个句子，其输出为每个单词的词性标注（part-of-speech tag）（N 到 N）；一个输入可能是一个句子，其输出为该句子的情感分类（N 到 1）；一个输入可能是单个图像，其输出为描述该图像所对应一系列词语（1 到 N）。在每一个时间步骤中，RNN 会基于当前输入和之前的隐藏状态计算新的隐藏状态「记忆」。其中「循环（recurrent）」这个术语来自这个事实：在每一步中都是用了同样的参数，该网络根据不同的输入执行同样的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：了解 LSTM 网络（http://colah.github.io/posts/2015-08-Understanding-LSTMs/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程第1部分——介绍 RNN （http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;递归神经网络（Recursive Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;递归神经网络是循环神经网络的树状结构的一种泛化（generalization）。每一次递归都使用相同的权重。就像 RNN 一样，递归神经网络可以使用向后传播（backpropagation）进行端到端的训练。尽管可以学习树结构以将其用作优化问题的一部分，但递归神经网络通常被用在已有预定义结构的问题中，如自然语言处理的解析树中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用递归神经网络解析自然场景和自然语言（Parsing Natural Scenes and Natural Language with Recursive Neural Networks ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ReLU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即线性修正单元（Rectified Linear Unit）。ReLU 常在深度神经网络中被用作激活函数。它们的定义是 f(x) = max(0, x) 。ReLU 相对于 tanh 等函数的优势包括它们往往很稀疏（它们的活化可以很容易设置为 0），而且它们受到梯度消失问题的影响也更小。ReLU 主要被用在卷积神经网络中用作激活函数。ReLU 存在几种变体，如Leaky ReLUs、Parametric ReLU (PReLU) 或更为流畅的 softplus近似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：深入研究修正器（Rectifiers）：在 ImageNet 分类上超越人类水平的性能（Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：修正非线性改进神经网络声学模型（Rectifier Nonlinearities Improve Neural Network Acoustic Models ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：线性修正单元改进受限玻尔兹曼机（Rectified Linear Units Improve Restricted Boltzmann Machines &amp;nbsp;）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;残差网络（ResNet）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度残差网络（Deep Residual Network）赢得了 2015 年的 ILSVRC 挑战赛。这些网络的工作方式是引入跨层堆栈的快捷连接，让优化器可以学习更「容易」的残差映射（residual mapping）而非更为复杂的原映射（original mapping）。这些快捷连接和 Highway Layer 类似，但它们与数据无关且不会引入额外的参数或训练复杂度。ResNet 在 ImageNet 测试集中实现了 3.57% 的错误率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于图像识别的深度残差网络（Deep Residual Learning for Image Recognition）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RMSProp&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RMSProp 是一种基于梯度的优化算法。它与 Adagrad 类似，但引入了一个额外的衰减项抵消 Adagrad 在学习率上的快速下降。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;PPT：用于机器学习的神经网络 讲座6a&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;序列到序列（Seq2Seq）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;序列到序列（Sequence-to-Sequence）模型读取一个序列（如一个句子）作为输入，然后产生另一个序列作为输出。它和标准的 RNN 不同；在标准的 RNN 中，输入序列会在网络开始产生任何输出之前被完整地读取。通常而言，Seq2Seq 通过两个分别作为编码器和解码器的 RNN 实现。神经网络机器翻译是一类典型的 Seq2Seq 模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;随机梯度下降（SGD：Stochastic Gradient Descent）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随机梯度下降是一种被用在训练阶段学习网络参数的基于梯度的优化算法。梯度通常使用反向传播算法计算。在实际应用中，人们使用微小批量版本的 SGD，其中的参数更新基于批案例而非单个案例进行执行，这能增加计算效率。vanilla SGD 存在许多扩展，包括动量（Momentum）、Adagrad、rmsprop、Adadelta 或 Adam。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Softmax&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Softmax 函数通常被用于将原始分数（raw score）的矢量转换成用于分类的神经网络的输出层上的类概率（class probability）。它通过对归一化常数（normalization constant）进行指数化和相除运算而对分数进行规范化。如果我们正在处理大量的类，例如机器翻译中的大量词汇，计算归一化常数是很昂贵的。有许多种可以让计算更高效的替代选择，包括分层 Softmax（Hierarchical Softmax）或使用基于取样的损失函数，如 NCE。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow是一个开源 C ++ / Python 软件库，用于使用数据流图的数值计算，尤其是深度神经网络。它是由谷歌创建的。在设计方面，它最类似于 Theano，但比 &amp;nbsp;Caffe 或 Keras 更低级。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Theano&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Theano 是一个让你可以定义、优化和评估数学表达式的 Python 库。它包含许多用于深度神经网络的构造模块。Theano 是类似于 TensorFlow 的低级别库。更高级别的库包括Keras 和 Caffe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度消失问题（Vanishing Gradient Problem）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度消失问题出现在使用梯度很小（在 0 到 1 的范围内）的激活函数的非常深的神经网络中，通常是循环神经网络。因为这些小梯度会在反向传播中相乘，它们往往在这些层中传播时「消失」，从而让网络无法学习长程依赖。解决这一问题的常用方法是使用 ReLU 这样的不受小梯度影响的激活函数，或使用明确针对消失梯度问题的架构，如LSTM。这个问题的反面被称为梯度爆炸问题（exploding gradient problem）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;VGG&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 是在 2014 年 ImageNet 定位和分类比赛中分别斩获第一和第二位置的卷积神经网络模型。这个 VGG 模型包含 16-19 个权重层，并使用了大小为 3×3 和 1×1 的小型卷积过滤器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于大规模图像识别的非常深度的卷积网络（Very Deep Convolutional Networks for Large-Scale Image Recognition）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;word2vec&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;word2vec 是一种试图通过预测文档中话语的上下文来学习词向量（word embedding）的算法和工具 (https://code.google.com/p/word2vec/)。最终得到的词矢量（word vector）有一些有趣的性质，例如vector('queen') ~= vector('king') - vector('man') + vector('woman') （女王~=国王-男人+女人）。两个不同的目标函数可以用来学习这些嵌入：Skip-Gram 目标函数尝试预测一个词的上下文，CBOW &amp;nbsp;目标函数则尝试从词上下文预测这个词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：向量空间中词汇表征的有效评估（Efficient Estimation of Word Representations in Vector Space）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：分布式词汇和短语表征以及他们的组合性（Distributed Representations of Words and Phrases and their Compositionality）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：解释 word2vec 参数学习（word2vec Parameter Learning Explained）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 25 Sep 2016 14:11:02 +0800</pubDate>
    </item>
    <item>
      <title>视频 | Deep Learning School第一天：4小时的深度学习盛宴</title>
      <link>http://www.iwgc.cn/link/2831066</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心整理&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个周六和周日（当地时间），斯坦福大学将在商学研究生院 CEMEX auditorium 举办为期两天的 Deep Learning School 讲座，探讨近期深度学习领域内取得的新进展。Yoshua Bengio、吴恩达、Open AI 的 Andrej Karpathy 等 12 人会进行专题讲演，斯坦福将在 YouTube 上对此次讲座进行直播。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;昨天晚上，YouTube 上的第一天的讲座已经完成了直播，Hugo Larochelle、Andrej Karpathy、Richard Socher、Sherry Moore、Ruslan Salakhutdinov 和吴恩达这六位深度学习资深研究者为我们带来了长达 4 小时的深度学习盛宴（直播后公开的视频还并不完整，但吴恩达的讲演可观看）。在观看之后，机器之心对第一天的已经公开的演讲内容进行了简单的梳理，视频观看地址如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲观看地址：https://www.youtube.com/watch?v=eyovmAtoUx0&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第一位演讲者：Hugo Larochelle（Twitter 研究科学家，舍布鲁克大学助理教授）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他的演讲主题是《前馈神经网络简介（Introduction to Feedforward Neural Networks）》&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJic41iaqTABHgMnlKksIVOibwvt0UuTY3M4kMP0eqmsfnkdEibicjdH2mfvqw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解前馈神经网络的一些基础概念。开始时，我将简单回顾下前馈网络的基础多层架构，以及自动微分和随机梯度下降（SGD）的反向传播。然后，我将讨论下最近普遍被用于深度神经网络训练的一些思路，比如 SGD 的变体、batch normalization 和 无监督预训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJic4wGDhy1BbAxoxDCRgaInxw4Svj0eNL8yFF5J997VtNd8C3xseC2NbA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第二位演讲者：Andrej Karpathy（谷歌 DeepMind 研究科学家，不久之前刚在斯坦福大学完成了自己的博士学位。参阅《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=1&amp;amp;sn=aaa7cc47f27129bbced25e6d090e2c1d&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=1&amp;amp;sn=aaa7cc47f27129bbced25e6d090e2c1d&amp;amp;scene=21#wechat_redirect"&gt;李飞飞高徒 Andrej Karpathy：计算机科学博士的生存指南》&lt;/a&gt;）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他的演讲主题是《用于计算机视觉的深度学习（Deep Learning for Computer Vision）》&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解进行图像理解的卷积神经网络（ConvNet）的设计，ImageNet 大规模视觉识别挑战赛上顶级模型的历史，以及该领域最近的一些开发模式。我也将会谈及关于视觉识别任务环境中的卷积神经网络架构，比如物体检测、分割、视频处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第三位演讲者：Richard Socher（MetaMind 创始人兼 CEO/CTO，2016 年 Salesforce 收购了 MetaMind 后，他成为了 Salesforce 的首席科学家）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他的演讲主题是《用于自然语言处理的深度学习（Deep Learning for NLP）》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解深度学习用于自然语言处理时的基础知识：词向量、循环神经网络、受语言学影响的任务和模型。最后，我将讲解一下将这些模型像乐高一样放到一起，产生被称为动态记忆网络的强大深度架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第四位演讲者：Sherry Moore（谷歌工程师，根据其 LinkedIn 介绍，她擅长「现代处理器架构、企业服务器架构、固件开发和操作系统开发；具有非常好的硬件和软件调试技能」。）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJic0x9jWr5e8ia9paIFGftuIhRw2E76qRth0B00q4q5ACHvicxKe1mrGPhQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;她的演讲主题是《TensorFlow Tutorial》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解深度学习用于自然语言处理时的基础知识：词向量、循环神经网络、受语言学影响的任务和模型。最后，我将讲解一下将这些模型像乐高一样放到一起，产生被称为动态记忆网络的强大深度架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJicGicZKvMVQyqqMuO22zDE6bFjVwVTYCzSaDxibFAibO0Il66fGXjWr0xDw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第五位演讲者：Ruslan Salakhutdinov（卡内基梅隆大学计算机科学学院机器学习系副教授，主要研究领域是统计机器学习。）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJicq0Zh1rbzWialPh46XIzoXVmAfkx9yMIAzpc1rq3gPLJdElgvSIDWKwQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他的演讲主题是深度无监督学习的基础（Foundations of Deep Unsupervised Learning）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：建立能够从高维数据提取有用信息的智能系统一直是很多人工智能任务的核心，包括视觉物体识别、信息检索、语音感知和语言理解。在此教程中，我将讨论许多流行的无监督学习的数学基础，包括稀疏编码、自动编码器、受限玻尔兹曼机（RBM）、深度玻尔兹曼机和变分自编码器。我将进一步证明在视觉物体识别、信息检索和自然语言处理应用中，这些模型能够从高维数据中提取有用的层级表征。最后，如果时间允许，我将简要讨论下能对图像生成自然语言描述的模型，以及使用注意力机制从描述中生成图像的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJicEyMCSIapSwgZAITX7nZb5rC62KswuXyGrOc61cU1WrM7ibXBbxICSUA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJic8n9ZF7GlM2rmAiaYFUsibUlpRTwVK6zzXp4U6GHKLUTMTGIZPTGmZXJQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;多模态深度玻尔兹曼机（DBM: Deep Boltzmann Machine）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJicfdEymMwNk4ate3Ov9LusdQm35eSxoNEJUqrhic0ib9IyL2aJrbmVhhiag/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;总结：对深度无监督模型有效的算法&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;文本/图像检索/目标识别&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;图像描述&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;学习分层结构&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;HMM 解码器&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多模态数据&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目标检测&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第六位演讲者：吴恩达（百度首席科学家；Coursera 联合主席兼联合创始人；斯坦福大学客座教授）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJiciaUHpI41cMa2Uj4CRR4f34WbL1icONdKXo90CqqibnickO7yqKBPsVvLEg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他的演讲主题是《Visionary Lecture》，吴恩达的演讲没有 PPT，直接板书。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJic5quNibibCd2sNkP0HUYuO7k2glNIFzYvfnQjduBoAfkicEiamDaNqvYzlg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;更大训练数据量的神经网络可以实现显著更好的表现&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9ZJhO9FibJqiaU0OAmUvImJicsItjEW3lMoCs0sib3LUvPY6GqlRwyRRia4PWbZbwUhcDtXLJ81GiacH6w/0?wx_fmt=jpeg"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;目标是打造人类水平的语音系统&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 25 Sep 2016 14:11:02 +0800</pubDate>
    </item>
    <item>
      <title>机器之心招聘：驶向未来的飞船还有九张船票</title>
      <link>http://www.iwgc.cn/link/2831067</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;qqmusic class="res_iframe qqmusic_iframe js_editor_qqmusic" scrolling="no" frameborder="0" musicid="5063032" mid="0006h6tp08iItd" albumurl="/Y/X/003BEImT2ABnYX.jpg" audiourl="http://ws.stream.qqmusic.qq.com/C1000006h6tp08iItd.m4a?fromtag=46" music_name="Hang&amp;nbsp;Me,&amp;nbsp;Oh&amp;nbsp;Hang&amp;nbsp;Me" commentid="2461305076" singer="Oscar&amp;nbsp;Isaac&amp;nbsp;-&amp;nbsp;醉乡民谣&amp;nbsp;电影原声带" play_length="200000" src="/cgi-bin/readtemplate?t=tmpl/qqmusic_tmpl&amp;amp;singer=Oscar%20Isaac%20-%20%E9%86%89%E4%B9%A1%E6%B0%91%E8%B0%A3%20%E7%94%B5%E5%BD%B1%E5%8E%9F%E5%A3%B0%E5%B8%A6&amp;amp;music_name=Hang%20Me%2C%20Oh%20Hang%20Me"&gt;&lt;/qqmusic&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器之心是国内领先的前沿科技媒体和产业服务平台，关注人工智能、机器人和神经认知科学，坚持为从业者提供高质量内容和多项产业服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体业务方面，机器之心最早在微信公众号平台开始运营，目前已经覆盖了微信、今日头条、百度百家、腾讯内容开放平台等多个大型内容平台，并运营着自己的官方网站。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在媒体之外，机器之心还将依托联想之星Comet Labs的全球资源平台为人工智能领域的参与者提供各项产业服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为生产更多的高质量内容，提供更好的产业服务，我们需要更多的小伙伴加入进来！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;记者（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;工作职责：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责人工智能和前沿技术领域的常规内容生产，撰写人物故事和产业报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.有独立的选题策划和操作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.挖掘国内外人工智能领域的优秀创业公司并进行报道；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.对国内外人工智能领域的创业者、公司高管、行业专家、科研专家进行深度专访；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.协助其他部门完成相关工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;岗位要求：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.优秀的写作能力，能够驾驭特写、人物故事、常规报道和资讯等各类内容形式；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.对前沿科技充满兴趣和热情，拥有迅速掌握某个特定行业或领域的学习能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对内容有品味，文字功底深厚，执行力强；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.1-2 年科技媒体从业经历， 能够适应新媒体和创业公司的工作节奏，有良好的职业精神和团队意识。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&lt;strong&gt;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;全职编译（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.编译、校对英文文章；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.撰写技术、产品、公司和行业相关文章，撰写分析报告。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.卓越的英语翻译能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.英语或计算机相关专业毕业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对前沿技术有一定了解和热情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线上运营（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司内容产品线上微信、微博、今日头条等渠道的运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.负责上述渠道推广效果分析和经验总结，建立有效运营手段；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.配合线下活动运营进行策划执行工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.配合内容团队进行选题策划和对外推广。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.1-2 年新媒体运营相关经验，具备一定的文字功底，有线下活动组织和策划经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.熟悉微信后台操作，有多渠道沟通经验者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.善于沟通交流，有一定抗压和创新能力，强责任心和高执行力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线下活动运营（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司线下活动的策划与运营；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.与内容团队和线上运营共同策划相关选题；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.执行公司商务需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.较强的沟通能力和资源整合能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.执行力强，具备一定的创新能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.1-2 年线下活动组织和策划经验，有活动相关供应商资源者优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;商务总监（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责公司特定行业客户的商务合作推动与对接，维护与建设积极的客户关系；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.参与制定公司的商务目标、原则、计划和战略，建立完善公司的商务体系及相关制度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.负责重大业务商务谈判的策略制定和执行以及合同的签订；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.负责拓展新的业务渠道，拓展特定行业的典型大客户，并跟踪协调执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，3 年以上工作经验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.良好的沟通、协调和商务谈判能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.具有较强的业务开拓能力、市场洞察力和行业分析能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.有较强责任感和抗压能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;高级分析师（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.人工智能行业数据监测、分析及行业研究报告的撰写；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2. 负责与人工智能领域内各企业的沟通及定期跟踪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.本科及以上学历，计算机科学、商科、社会学或相关专业者优先；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.具备一定的数据分析和洞察能力，对ha数据拥有一定敏锐度；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟练使用相关数据分析工具并进行信息搜集整理、图表制作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.良好的文字功底和写作能力和英文阅读能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Web 前端开发（1 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.负责机器之心网站部分页面及交互优化修改；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.跟远程后台人员合作，优化整个系统，参与站点维护工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.对现有产品的可用性测试和评估提出改进方案，持续优化产品的用户体验；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.积极参与工作相关技术研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.拥有良好的口头表达能力、善于学习新的技术；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.年以上 web 前端开发经验，懂 nginx 或者 Apache 操作，有一定的 Linux 系统操作经验，熟悉常用命令；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.熟悉常用的 js 库，例如 jquery，需掌握 amazeui 敏捷开发框架（bootstrap 亦可）；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.熟悉常用的前端 MVC 框架，必须熟悉 gulp、sass；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5.具有良好的沟通能力和团队协作能力, 能够与产品经理, 项目经理, 形成良好, 有效的沟通。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;实习生（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.网站、新媒体内容更新；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.翻译、校对英文文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.协助线下活动运营。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.出色的英语阅读和中文写作能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.每周能保证 2-3 天坐班；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.积极的学习态度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4.计算机科学等理工科专业、英语专业专业优先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：北京&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;兼职编译（2 名）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作职责：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.翻译、校对英文文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;岗位要求：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.卓越的英语翻译能力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2.每周能保证一定的翻译时间；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3.强责任心和高执行力；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;工作地点：不限&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有意者请将简历发送至：hr@jiqizhixin.com，或添加微信 JuveAlex，zhoayunfeng1984&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 25 Sep 2016 14:11:02 +0800</pubDate>
    </item>
    <item>
      <title>一周论文| Question Answering Models</title>
      <link>http://www.iwgc.cn/link/2831068</link>
      <description>&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;引&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;本期paperweekly的主题是Question Answering Models，解决这一类问题可以很好地展现AI理解人类自然语言的能力，通过解决此类dataset可以给AI理解人类语言很好的insights。问题的定义大致是，给定较长一段话的context和一个较短的问题，以及一些candidate answers，训练一些可以准确预测正确答案的模型。&lt;/span&gt;&lt;span&gt;此问题也存在一些变种，例如context可以是非常大块的knowledge base，可以不提供candidate answers而是在所有的vocabulary中搜索答案，或者是在context中提取答案。基于Recurrent Neural Network的一些模型在这一类问题上给出了state of the art models，本期paperweekly就带领大家欣赏这一领域有趣的工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、Attention-over-Attention Neural Networks for Reading Comprehension&lt;/span&gt;&lt;span&gt;, 2016&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER, 2016&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering, 2016&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Attention-over-Attention Neural Networks for Reading Comprehension&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;1&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; line-height: 25.6px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/h2&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu and Guoping Hu&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;单位&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;iFLYTEK Research, China&lt;br/&gt;Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;关键词&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Question Answering, Attentive Readers&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;来源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201608&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;问题&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文优化了attention机制，同时apply question-to-document and document-to-question attention，提升了已有模型在Cloze-Style Question Answering Task上的准确率。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;模型&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文解决的是Cloze-style question answering的问题，给定一个Document和一个Query，以及一个list的candidate answers，模型需要给出一个正确答案。已有的模型大都通过比较每一个Query + candidate answer和context document的相似性来找出正确答案，这种相似性measure大都通过把query 投射到context document每个单词及所在context的相似性来获得。本文的不同之处在于模型还计算了context投射到每个query单词的相似度，进一步丰富了context和query相似度的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAksicmOYMcuZabNTDiaIotg7YoxJeUMz1oLkicEX2IPGOHt12d4SQUib32A/640?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，document和query都会被model成biGRU。&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAaiamQVv3zPiamPadkIhAJ3lavzMghcCribRkpB5EYxicYI2EcySBg5J42Q/0?"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后使用document biGRU和query biGRU的每一个position做inner product计算，可以得到一个similarity matrix。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAs6R8Lgw8YjaVMM48N9OXE9747ryQCkN1ICSrricFWcribB46csoWKECQ/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对这个matrix做一个column-wise softmax，可以得到每个query单词在每个document单词上的similarity。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAfiafPBcZ9ko8hv1myG2niaTibnzeiawWRunRlHqDNA1QRIldBufKLZwkWg/0?"/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;similarly，对这个matrix做一个row-wise softmax，可以得到每个document单词在每个query单词上的similarity&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAwfdZA4rwBCYl93vvjsmKE3icqonFWhEcjXS9kH3kWibQD1Su5S6dxDeQ/0?"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;取个平均就得到了每个query单词在整个context document上的similarity。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAtFlSxmJyaH1UHjT2bcsO8GLXYojXqHWdG2Oh35kZvefRWPl6YGw2kQ/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后把alpha和beta做个inner product就得到了每个context document word的probability。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAgicjop6O1ZLErlywkE0ic3V8c0HibF4I4p6w32IibibRkO4UjqQd1bMyTIw/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个candidate answer的probability就是它出现在上述s中的probability之和。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAJ6cibcSpqpRCiazzK32RcA6g1g5FgADpf7H5hEM8myK6D9z0Df8HVDOQ/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Loss Function可以定义为正确答案的log probability之和。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAHsvibsjZcxdrwp8RFpsCm7Xrnc8vCQZ8Cv9tpWJHY1GDjBVlo1fJEQw/0?"/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;资源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;cnn和daily mail datasets&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;Children’s book test&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;利用attentive readers解决question answering问题最早出自deep mind: teaching machines to read and comprehend。后来又有Bhuwan Dhingra: Gated-Attention Readers for Text Comprehension和Danqi Chen: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task，以及其他相关工作，在此不一一赘述。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;简评&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文很好地完善了attentive reader的工作，同时考虑了query to document and document to query attentions，在几个data set上都取得了state of the art效果，思路非常清晰，在question answering问题上很有参考价值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;2&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Shuohang Wang, Jing Jiang&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;单位&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Singapore Management University&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;关键词&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Machine comprehension, Match-LSTM, Pointer Net&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;文章来源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv，201608&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;问题&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;提出一种结合match-LSTM和Pointer Net的端到端神经网络结构，来解决SQuAD数据集这类没有候选项且答案可能是多个词的machine comprehension问题。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;模型&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文提出的模型结合了match-LSTM(mLSTM)和Pointer Net(Ptr-Net)两种网络结构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、match-LSTM&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;mLSTM是由Wang和Jiang提出的一种解决文本蕴含识别（RTE）问题的一种神经网络结构。模型结构见下图，该模型首先将premise和hypothesis两句话分别输入到两个LSTM中，用对应LSTM的隐层输出作为premise和hypothesis中每个位置对应上下文信息的一种表示（分别对应图中的Hs和Ht）。对于hypothesis中的某个词的表示ht_i，与premise中的每个词的表示Hs计算得到一个权重向量，然后再对premise中的词表示进行加权求和，得到hti对应的上下文向量a_i（attention过程）。最后把hypothesis中该词的表示ht_i和其对应的context向量a_i拼接在一起，输入到一个新的LSTM中。该模型将两个句子的文本蕴含任务拆分成词和短语级别的蕴含识别，因此可以更好地识别词之间的匹配关系。&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAUWPzCPMlrz2TQfVibww2XypnBozzVRlaJdRMVwDxBPA5eDdBLgKLvlQ/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、 Pointer networks&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该模型与基于attention的生成模型类似。区别之处在于，pointer networks生成的结果都在输入序列中，因此pointer networks可以直接将attention得到的align向量中的每个权重直接作为预测下一个词对应的概率值。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、 Sequence Model &amp;amp; Boundary Model&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文提出的模型结构见下图，具体到本文的神经网络结构，可以简单分为下面两部分：&lt;br/&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZAbz6PjyldphXXyUpuwf7NLe1KAj8AD6A7UwZibLpicd8Zg7h8SSfRhjKw/0?"/&gt;&lt;br/&gt;&lt;span&gt;（1）Match-LSTM层：该部分将machine comprehension任务中的question作为premise，而passage作为hypothesis。直接套用上述的mLSTM模型得到关于passage每个位置的一种表示。为了将前后方向的上下文信息全部编码进来，还用相同的方法得到一个反向mLSTM表示，将两个正反方向的表示拼接在一起作为最终passage的表示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）生成答案序列部分，论文中提出了两种生成方法：&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Sequence方法与Pointer Net相同，即根据每一个时刻attention的align向量生成一个词位置，直到生成终止符为止。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Boundary方法则是利用SQuAD数据集的答案均是出现在passage中连续的序列这一特点，该方法仅生成首尾两个位置，依据起始位置和终止位置来截取passage的一部分作为最终的答案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;本文在SQuAD数据集上进行实验，两种方法实验结果较之传统LR方法均有大幅度提升。其中Boundary方法比Sequence方法效果更好。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;资源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;[SQuAD]&lt;br/&gt;(&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;https://rajpurkar.github.io/SQuAD-explorer/&lt;/span&gt;&lt;/a&gt;&lt;span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;ul class=" list-paddingleft-2" style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;数据集相关论文&lt;br/&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型相关论文&lt;br/&gt;Learning Natural Language Inference with LSTM&lt;br/&gt;Pointer networks&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;简评&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本篇论文提出的模型是第一个在SQuAD语料上应用端到端神经网络的模型，该模型将Match-LSTM和Pointer Networks结合在一起，利用了文本之间的蕴含关系更好地预测答案。&lt;br/&gt;本文提出了两种方法来生成答案，其中Boundary方法巧妙地利用SQuAD数据集的答案均是文本中出现过的连续序列这一特点，只生成答案的起始和终止位置，有效地提升了模型的效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;3&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;单位&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Baidu IDL&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;关键词&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Question Answering, Sequence Labeling, CRF&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;来源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201609&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;问题&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;作者给出了一个新的中文的QA数据集, 并且提出了一个非常有意思的baseline model.&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;模型&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;1、WebQA Dataset&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者来自百度IDL, 他们利用百度知道和一些其他的资源, 构建了这个中文的QA数据集. 这个数据集里所有的问题都是factoid类型的问题, 并且问题的答案都只包含一个entity (但是一个entity可能会包含多个单词). 对于每个问题, 数据集提供了若干个’evidence’, 这些evidence是利用搜索引擎在网络中检索的.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、Recurrent Sequence Labeling Model&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者把QA类型的问题看做sequence labeling问题, 给出的模型大概分三部分:&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnuC9h3OkCFg5uaNI1hL7ZALznDzpUhib6iaIe56zkWiahicb6j5kIpzUwVPeia1H3fRCOl9jqDjQ7NBPg/0?"/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（1）Question LSTM&lt;br/&gt;这部分很简单, 就是普通的单向LSTM, 对整个Question sequence进行encoding, 之后计算self-attention, 并用attention对question encoding求加权平均作为问题的representation.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（2）Evidence LSTMs&lt;br/&gt;这部分比较有意思, 首先, 作者从数据中提取出两种feature: 每个词是否在question和evidence中共同出现, 以及每个词是否同时在多个evidence中出现. 之后, 模型用一个三层的单向LSTM对evidence/quesiton/feature进行编码.&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;第一层: 将evidence/question representation/feature进行连接, 放进一个正向LSTM.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;第二层: 将第一层的结果放入一个反向LSTM.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;第三层: 将第一层和第二层的结果进行连接, 放进一个正向LSTM.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;（3）CRF&lt;br/&gt;经过evidence LSTMs, question和evidence的representation已经揉在一起, 所以并不需要其他QA模型(主要是Attention Sum Reader)广泛用的, 用question representation和story representation进行dot product, 求cosine similarity. 这时候只需要对evidence representation的每一个time step进行分类就可以了, 这也是为什么作者将数据标注成IOB tagging的格式, 我们可以直接用一个CRF层对数据进行预测. 在一些实验中, 作者将答案之前的词用O1, 答案之后的词用O2进行标注, 这又给了模型关于非答案词的位置信息(正确答案是在这个词的前面还是后面).&lt;/span&gt;&lt;/p&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;资源&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;WebQA dataset&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; max-width: 100%; font-size: 14px; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;Baidu Paddle&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;相关工作&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;ul class=" list-paddingleft-2" style="margin-bottom: 10px; max-width: 100%; box-sizing: border-box; color: rgb(64, 64, 64); word-wrap: break-word !important;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关于CRF进行序列标注的问题, 可以参考这篇文章.&lt;br/&gt;Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. arXiv:1508.01991v1.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关于multi-word答案选择在SQuAD dataset上的模型, 可以参考这篇.&lt;br/&gt;Shuohang Wang, Jing Jiang. 2016. Machine Comprehension Using Match_LSTM and Answer Pointer. arXiv: 1608.07905v1.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;strong&gt;&lt;span&gt;简评&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;首先对所有release数据集的人表示感谢.&lt;br/&gt;关于dataset部分, 百度利用了自己庞大的资源收集数据. 第一, 百度知道里的问题都是人类问的问题, 这一点相比于今年前半年比较流行的CNN/CBT等等cloze style的问题, 要强很多. 第二, 数据集中包含了很多由多个词组成的答案, 这也使数据集的难度大于CNN/CBT这种单个词作为答案的数据. 第三, 对于每个问题, 并没有给出备选答案, 这使得对于答案的搜索空间变大(可以把整个evidence看做是备选答案). 第四, 对于每一个问题, dataset中可能有多个supporting evidence, 这也迎合了最近multi-supporting story的趋势, 因为对于有些问题, 答案并不只在某一个单一的文章中(对于百度来说, 如果搜索一个问题, 那么答案并不一定在单一的搜索结果网页中), 那么一个好的model需要在有限的时间内对尽可能多的搜索结果进行检索.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于model部分, 本文尝试将QA问题看做是序列标注问题, 某种意义上解决了multiword answer的难点. 熟悉前半年QA paper的人都会对Attention Sum Reader以及延伸出来的诸多模型比较熟悉, 由于用了类似Pointer Network的机制, 一般的模型只能从文中选择story和question的cosine similarity最高的词作为答案, 这使得multiple word answer很难处理, 尤其是当multiple answer word不连续的时候, 更难处理. 而CRF是大家都熟知的简单高效的序列标注工具, 把它做成可训练的, 并且放在end to end模型中, 看起来是非常实用的. 在Evidence LSTM的部分, 加入的两个feature据作者说非常有帮助, 看起来在deep learning 模型中加入一些精心设计的feature, 或者IR的要素, 有可能能够对模型的performance给予一定的提升. 在entropy的角度, 虽然不一定是entropy reduction, 因为这些信息其实本来已经包含在question/evidence中了, 但是有可能因为你提供给模型这些信息, 它就可以把更多精力用在一些其他的特征上?&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外值得一提的是, 最近Singapore Management University的Wang and Jiang也有所突破, 在SQuAD dataset(也是multiple word answer)上一度取得了state of the art的结果, 他们用的mLSTM模型也十分有趣.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;总结&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;这一类model都大量使用了Recurrent Neural Network(LSTM或者GRU)对text进行encoding，得到一个sequence的hidden state vector。然后通过inner product或者bilinear term比较不同位置hidden state vector之间的similarity来计算它们是正确答案的可能性。可见Recurrent Neural Network以及对于Similarity的定义依旧是解决此类问题的关键所在，更好地改良这一类模型也是提升准确率的主流方法。笔者认为，similarity的计算给了模型从原文中搜索答案的能力，然而模型非常缺乏的是推理和思考的能力（其实也有相关工作&lt;/span&gt;&lt;a target="_blank" rel="external" style="color: rgb(64, 64, 64); text-decoration: underline; max-width: 100%; box-sizing: border-box; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;Towards Neural Network-based Reasoning&lt;/span&gt;&lt;/a&gt;&lt;span&gt;），如果模型能够配备逻辑思考能力，那么解决问题的能力会大大增强。非常期待有新的思路能够出现在这一领域中，令AI能够更好地理解人类语言。以上为本期PaperWeekly的主要内容，感谢&lt;strong&gt;eric yuan&lt;/strong&gt;、&lt;strong&gt;destinwang&lt;/strong&gt;、&lt;strong&gt;zewei chu&lt;/strong&gt;、&lt;strong&gt;韩晓伟&lt;/strong&gt;四位同学的整理。 &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;广告时间&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;微信公众号：&lt;strong&gt;PaperWeekly&lt;/strong&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkSMlEJFo90NY8rCXr3mJMBibduVHxMKSHzQtVkHz8kNwpjCKBiccGuqLE0WpPuAbdtEs6cTF5iabpAQ/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：&lt;strong&gt;PaperWeekly&lt;/strong&gt;（&lt;/span&gt;&lt;a target="_blank" rel="external" style="color: rgb(64, 64, 64); text-decoration: underline; max-width: 100%; box-sizing: border-box; font-size: 14px; word-wrap: break-word !important;"&gt;http://weibo.com/u/2678093863&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br/&gt;知乎专栏：&lt;strong&gt;PaperWeekly&lt;/strong&gt;（&lt;/span&gt;&lt;a target="_blank" rel="external" style="color: rgb(64, 64, 64); text-decoration: underline; max-width: 100%; box-sizing: border-box; font-size: 14px; word-wrap: break-word !important;"&gt;https://zhuanlan.zhihu.com/paperweekly&lt;/a&gt;&lt;span&gt;&amp;nbsp;）&lt;br/&gt;微信交流群：微信+ &lt;strong&gt;zhangjun168305&lt;/strong&gt;（请备注：加群 or 加入paperweekly）&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;</description>
      <pubDate>Sun, 25 Sep 2016 14:11:02 +0800</pubDate>
    </item>
    <item>
      <title>聚焦 | 这个周末，享受Yoshua Bengio、吴恩达、Andrej Karpathy等人的深度学习直播讲座</title>
      <link>http://www.iwgc.cn/link/2820644</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Standford&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em;"&gt;&lt;span&gt;这个周六、周日（当地时间），斯坦福（Standford CEMEX auditorium）将举办为期两天的讲座，探讨近期深度学习领域内取得的新进展。Yoshua Bengio、吴恩达、Open AI 的 &lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=1&amp;amp;sn=aaa7cc47f27129bbced25e6d090e2c1d&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719010&amp;amp;idx=1&amp;amp;sn=aaa7cc47f27129bbced25e6d090e2c1d&amp;amp;scene=21#wechat_redirect"&gt;Andrej Karpathy &lt;/a&gt;等 12 人将进行专题讲演，届时斯坦福将在 YouTube 上对此次讲座进行直播。此次讲座无疑与 Bengio 8 月份组织深度学习暑期班（&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=1&amp;amp;sn=ff7d748b149e7952c9fa3b53cefd5afc&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718578&amp;amp;idx=1&amp;amp;sn=ff7d748b149e7952c9fa3b53cefd5afc&amp;amp;scene=21#wechat_redirect"&gt;重磅 | Yoshua Bengio 深度学习暑期班学习总结，35 个授课视频全部开放（附观看地址）&lt;/a&gt;）一样是一个很好的学习机会。在此篇文章中，机器之心对 12 位大牛即将讲演的主题进行了介绍，读者可以针对自己感兴趣的深度学习研究领域在这个周末享受这些讲座。YouTube 直播地址如下。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;周六 YouTube 直播地址：https://www.youtube.com/watch?v=eyovmAtoUx0&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;周日 YouTube 直播地址：https://www.youtube.com/watch?v=9dXiAecyJrY&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9WfdJmsQegTV58ZvX8druMVYjfHoS0vEdhswHA41AicibbhUwgPTPjWicic1KiadEGPpOkCKM9LMniahlg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;周六&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Hugo Larochelle：介绍前馈神经网络（Introduction to Feedforward Neural Networks）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9:00-10:00&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解前馈神经网络的一些基础概念。开始时，我将简单回顾下前馈网络的基础多层架构，以及自动微分和随机梯度下降（SGD）的反向传播。然后，我将讨论下最近普遍被用于深度神经网络训练的一些思路，比如 SGD 的变体、batch normalization 和 无监督预训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Andrej Karpathy：计算机视觉中的深度学习（Deep Learning for Computer Vision）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;10:15-11:45&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解进行图像理解的卷积神经网络（ConvNet）的设计，ImageNet 大规模视觉识别挑战赛上顶级模型的历史，以及该领域最近的一些开发模式。我也将会谈及关于视觉识别任务环境中的卷积神经网络架构，比如物体检测、分割、视频处理。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Richard Socher：自然语言处理中的深度学习（Deep Learning for NLP）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;12:45-2:15&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将讲解深度学习用于自然语言处理时的基础知识：词向量、循环神经网络、受语言学影响的任务和模型。最后，我将讲解一下将这些模型像乐高一样放到一起，产生被称为动态记忆网络的强大深度架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Sherry Moore：TensorFlow Tutorial&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2:45-3:45&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Ruslan Salakhutdinov：深度无监督学习的基础（Foundations of Deep Unsupervised Learning）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4:00-5:30&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：建立能够从高维数据提取有用信息的智能系统一直是很多人工智能任务的核心，包括视觉物体识别、信息检索、语音感知和语言理解。在此教程中，我将讨论许多流行的无监督学习的数学基础，包括稀疏编码、自动编码器、受限玻尔兹曼机（RBM）、深度玻尔兹曼机和变分自编码器。我将进一步证明在视觉物体识别、信息检索和自然语言处理应用中，这些模型能够从高维数据中提取有用的层级表征。最后，如果时间允许，我将简要讨论下能对图像生成自然语言描述的模型，以及使用注意力机制从描述中生成图像的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;吴恩达：Visionary Lecture&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6:00-7:00&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;周日&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;John Schulman：策略梯度和 Q-学习：上升到力量、对抗和统一（Policy Gradients and Q-Learning: Rise to Power, Rivalry, and Reunification）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9:00-10:30&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先概述一下深度强化学习中目前最好的研究成果，其中包括最近的在视频游戏（如 Atari）、棋盘游戏（如 AlphaGo）和模拟机器人上的应用。然后我会给出这些成果背后的两种核心方法的教学介绍：策略梯度 和 Q-学习。最后，我将给出一个新的分析以说明这两种方法有多么类似。本演讲的主题将不仅是问「什么有效？」，而且还有「它在什么情况下有效？」以及「它为什么有效？」；另外还要找到这些问题的可用于调节具体的实现和设计更好的算法的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Patrice Lamblin：Theano 教学（Theano Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;10:45-11:45&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Theano 是一个 Python 库，允许在 CPU 或 GPU 上高效地定义、优化和评估涉及多维数组的数学表达式。自 Theano 诞生以来，它就一直在深度学习社区最受欢迎的框架之一，而且还有许多用于深度学习的框架也是基于它而构建的，其中包括 Lasagne、Keras、Blocks 等等。这个教程将首先关注 Theano 背后的概念以及如何构建和评估简单的表达，然后我会介绍如何定义和训练更为复杂的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adam Coates：用于语音的深度学习（Deep Learning for Speech）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;12:45-2:15&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：传统的语音识别系统具有大量模块，其中每一个都需要单独的艰难的工程开发。现在深度学习已能创造能执行大多数传统引擎的「端到端」的任务的神经网络，这能极大地简化新型语音系统的开发，并开启了实现人类水平的表现的大门。在本教程中，我们将概览一遍类似百度的「Deep Speech」模型的端到端系统的开发步骤。我们将会将这些片段组合起来成为一个最先进的语音系统的「比例模型」——现在已经在驱动生产型的语音引擎的神经网络的小型版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Alex Wiltschko：Torch 教学（Torch Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2:45-3:45&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Torch 是 Lua 语言环境中一个用于科学计算的开放平台，其关注的重点是机器学习，尤其是深度学习。Torch 为 GPU 计算提供了一流的支持，而且是以一种清晰的、交互式的和必需的风格，这是 Torch 和其它阵列库之间的显著不同点。尽管 Torch 从广泛的行业支持中获益匪浅，但却是社区所有的和社区开发的生态系统。包括 Torch NN、TensorFlow 和 Theano 在内的所有神经网络库都依靠自动微分（AD：automatic differentiation）来管理复杂函数组件的梯度计算。我将介绍一些自动微分的广义背景，这是基于梯度的优化的基础概念，还将展示 Twitter 通过 torch-autograd 对自动微分的灵活实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Quoc Le：用于自然语言处理和语音的序列到序列学习（Sequence to Sequence Learning for NLP and Speech）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4:00-5:30&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先提出序列到序列（seq2seq）学习和注意模型（attention model）的基础，以及它们在机器翻译和语音识别上的应用。然后我将讨论带有指针（pointer）和函数（function）的注意。最后我会描述强化学习可以在 seq2seq 和注意模型中发挥怎样的作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Yoshua Bengio：深度学习的基础和挑战（Foundations and Challenges of Deep Learning）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;6:00-7:00&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：为什么深度学习的效果这么好？未来还面临着什么挑战？这个演讲将首次探讨深度学习成功的关键因素。首先，根据 no-free lunch 定理，我们将讨论深度网络获取抽象的分布式表征的表达力。其次，我们将讨论我们的实际优化神经网络的参数的惊人能力——尽管存在非凸性（non-convexity）。然后我们将思考一些未来的难题，包括核心的表征问题——理解变化的基本解释因素，尤其是对于无监督学习，为什么这对于将强化学习带向下一阶段来说是非常重要的，以及仍然存在挑战性的优化问题，比如长期依赖的学习，理解深度网络的优化全局，以及为什么生物大脑的学习方式的秘密仍然值得从深度学习的角度来破解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 24 Sep 2016 15:49:45 +0800</pubDate>
    </item>
  </channel>
</rss>
