<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 语音识别新里程碑：微软新系统词错率低至6.3%（附论文）</title>
      <link>http://www.iwgc.cn/link/2688676</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Microsoft&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Richard Eckel&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;微软研究者在追求计算机像人类一样理解语音的道路上取得了新的里程碑。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJjNjPliaNXrYLCWIzgh3GPWZfykoj3aCibWSTHtHDOqcLop2xgYguv17Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软首席语音科学家黄学东在产业标准 Switchboard 语音识别任务的最新基准评估中报告出了这样一成果，微软研究者取得了产业中最低的 6.3% 的词错率（WER）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这周二发表的一篇研究论文中，黄学东说：「我们最好的单个系统在 NIST 2000 Switchboard 集上取得了 6.9% 的词错率。我们相信这是取得最好表现的不基于系统结合的单个系统。在 Switchboard 测试数据上，数个声学模型的结合将前沿成果推进到了 6.3% 的词错率。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上周，在旧金山举办的国际语音交流和技术大会 Interspeech 上，IBM 宣称他们取得了 6.6% 的词错率。20 年前，最好的研究系统的词错率是 43%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄说：「这一新的里程碑得益于过去 20 年中由来自不同组织的人工智能社区开发出的各种新技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJOlG27zVBQPTViaMDbuRW8XRLvRicR5q4wnG1829AsTWrwJCS73WnQ4XQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 和微软都提到了深度神经网络时代的到来，其受到了大脑生物处理方法的启发，也成为了语音识别技术进步的关键推动力。计算机科学家已经尝试了数十年去训练计算机系统做图像识别、语音理解这样的任务，但到目前为止，这些系统的准确度仍然不尽如人意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络构建于一系列的计算层之中。今年早些时候，微软研究员利用一个深度残差神经网络（deep residual neural network）系统赢得了 ImageNet 计算机视觉挑战赛，该系统使用了一种新型的交叉层网络连接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJFApmqAsjias8FPVwEibWAEG0IYsdXfvIgcib4RaiajSMnvic8iaQH7KnKeaw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Geoffrey Zweig&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的 Speech &amp;amp; Dialog 研究组的首席研究员兼管理者 Geoffrey Zweig 领头此次的 Switchboard 语音识别工作。他在微软带领新型训练算法、高度优化的卷积和循环神经网络模型、CNTK 这样的工具等内容的开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文：微软 2016 对话语音识别系统（The Microsoft 2016 Conversational Speech Recognition System）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJboMsrKleE606EJcBibpyFLwmIuDLOuANenxkP725frLicA0OXwFWK0og/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们描述了微软的对话语音识别系统，在该系统中我们结合了近期在基于神经网络的声学和语言模型上的进展，推进了在 Switchboard 识别任务上的顶尖成果。受到机器学习集成技术（machine learning ensemble techniques）的启发，该系统使用了一系列卷积和循环神经网络。I-vector 建模和 lattice-free MMI 训练为所有声学模型架构带来了显著的提升。使用了多个前向和反向运行 RNNLM 的语言模型重新计分（Language model rescoring）与基于后验的词系统结合为系统性能带来了 20% 的增益。最好的单个系统使用 ResNet 架构声学模型和 RNNLM rescoring，在 NIST 2000 Switchboard 任务上实现了 6.9% 的词错率。结合系统取得了 6.3% 的词错率，代表了在这一基准任务上对先前成果的改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年，由于对卷积和循环神经网络的精心设计和优化，在降低语音识别错误率上我们已经看到了快速发展。尽管我们对基础架构已经很好地认识一段时间了，但它近期才成为了进行语音识别的最好模型。惊人的是，对声学模型和语言模型而言都是如此。相比于标准的前馈 MLP 或 DNN，这些声学模型有能力对大量带有时间不变性的声学环境建模，而且卷积模型还能应对频率不变性的情况。在语言模型中，循环模型通过对连续词表征（continuous word representations）的归纳能力，在传统的 N-gram 模型上实现了进步。同时，集成学习（ensemble learning）已经在多种神经模型得到了普遍的应用，从而通过减少偏差和方差改进稳健性。在此论文中，我们广泛地使用模型的集成，同时也改进单个组件模型，从而推进在对话电话语音识别（CTS）中的进展，CTS 从上世纪 90 年代就已经成为了检验语音识别任务的一项基准。这一系统的主要特征包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对卷积神经网络和长短期记忆（LSTM）网络这两种基础声学模型架构的集成，每个架构也有多种变体；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 LACE 卷积神经网络中的一个注意机制，其可以有区别地为不同距离的语境赋予权重；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Lattice-free MMI 训练；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在所有模型中使用基于 i-vector 的改编版本；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在前向和反向过程中都运行带有多个循环神经网络语言模型的 language model rescoring；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;融合网络系统组合与最好系统子集搜索的耦合，这正是在有许多候选系统的情况下所需的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该论文在其他部分对该系统进行了详细描述。Section 2 描述了 CNN 和 LSTM 模型。Section 3 描述了我们对 i-vector 改编版的部署。Section 4 展现了 lattice-free MMI 训练过程。语言模型 rescoring 是该系统的一个重大部分，在 Section 5 中有描述。实验结果呈现在 Section 6 中，随后是对相关工作和结论的讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ1I6omFmQMwEgcia2b57iasnc6Cy1NzIibLDPwFKglVpJCzYHE6dvmbB9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 4. 在 eval 2000 set 上的来自 i-vector 和 LFMMI 的性能改进&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ8OcjXl8BK8b2IZ00icLh92BTQbRDApNKibSy6np2RGCl1nsclMETPKGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;表 5. 在 eval 2000 set 上不同声学模型的词错率。除非特别标注，所有的模型都在 2000 小时的数据上进行训练，有 9000 个 senones（聚类的结果）。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>业界 | MIT推出并行计算编程语言Milk：为大数据应用提速</title>
      <link>http://www.iwgc.cn/link/2688677</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Larry Hardesty&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、杜雪&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在今天的计算机芯片中，内存管理（memory management）是基于计算机科学家所称的局部性原理（principle of locality）：如果一个程序需要存储在某个内存位置的一个数据块，它可能也会需要这个数据块附近的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这种假设在大数据的时代已被打破，现在的程序常常需要在许多巨大的数据集上任意离散的少量数据上进行操作。因为从它们的主内存区块中读取数据是今天芯片的主要性能瓶颈，所以不得不进行的更为频繁的读取会极大地减慢程序的执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本周，在并行架构和编译技术国际会议（International Conference on Parallel Architectures and Compilation Techniques）上，来自 MIT 计算机科学和人工智能实验室（CSAIL）的研究者展示了一种名为 Milk 的新型编程语言，该语言可让开发者在需要处理大型数据集中离散数据点的程序中更高效地管理内存。研究人员在一些常用算法上对该语言进行了测试，发现使用这种新语言编写的程序的速度可达到用已有语言所编写的程序的 4 倍。但研究者相信进一步的工作可以让其速度得到更大的提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电气工程和计算机科学教授 Saman Amarasinghe 说，今天的大数据集给已有的内存管理技术带来问题的原因不仅仅是因为它们很大，更多的则是因为它们是稀疏的（sparse）。也就是说，解决方案的规模并不总是与问题的规模成正比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「在社会环境中，我们习惯了应对更小的问题。」Amarasinghe 说，「如果你看看这栋楼（CSAIL）里面的人，你能看到我们全部是互连的。但如果你从整个行星的规模上来看，我的朋友的数量也不会扩大规模。这颗行星上有几十亿人，但我的朋友仍然只有几百人。突然你就有了一个非常稀疏的问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Amarasinghe 说，类似地，比如说有一个有 1000 位客户的在线书店，可能会为其访客提供 20 本最受欢迎的书的清单。然而，这并不意味着，如果这家在线书店的客户达到 100 万了，它就会为其访客提供 20,000 本最受欢迎的书的清单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;本地思考（Thinking locally）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天的计算机芯片并没有为稀疏数据进行优化——其实事实情况正好相反。因为从芯片的主内存区块读取数据的速度很慢，所以现代芯片的内核或处理器中都有自己的「缓存（cache）」——一种相对小的、本地的且高速的内存区块。内核不会一次只从主内存中读取单个数据项，而是会一次读取一整个数据块。而这个数据块是通过局部性原理进行选择的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;局部性原理（principle of locality）的工作方式很容易理解。以图像处理任务为例：如果一个程序的目的是在图像上应用一种视觉滤镜（visual filter）而且一次只能处理图像上的一块，那么当内核请求一个块时，它应该就还会收到其缓存所能容纳的所有相邻的块，这样它就可以一块接一块地处理，而不需要再取更多数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但如果该算法的目标是从在线零售商数据库的 200 万种书中取出仅仅 20 种，那这种方法就不管用了。如果它请求与某一种书相邻的数据，很有可能其相邻的 100 种书的数据都是没有关联的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从主内存中一次只读取一个数据项是非常低效的。「这就像是，每次你想要一勺麦片时，你都需要打开冰箱、打开牛奶盒、倒出一勺牛奶、盖上牛奶盒、将它放回冰箱。」Vladimir Kiriansky 说，他是电气工程和计算机科学的博士生，同时也是这篇新论文的第一作者。Amarasinghe 和 Yunming Zhang 是他的合作者，Zhang 也是一位电气工程和计算机科学的博士生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;批处理（Batch processing）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Milk 只是简单地为 OpenMP 增加了一些命令；OpenMP 是一种 C 或 Fortran 等语言的扩展，可以帮助用来更轻松地为多核处理器编写代码。使用 Milk，程序员可以在任何指令附近插入几行代码，其可以在整个大数据集中迭代，寻找相对较少数量的项。Milk 的编译器（将高级代码转换成低级指令的程序）然后可以据此找到管理内存的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 Milk 程序时，如果一个内核发现它需要一个数据项，它不会发出请求从主内存中读取它（以及其相邻的数据）。它会将该数据项的地址加入到一个本地存储的地址列表中。但这个列表足够长时，该芯片的所有内核都会池化（pool）它们的列表，然后将这些地址按临近排布的形式组合到一起，并将它们重新分配给内核。如此一来，每一个内核都只请求了它知道自己需要的数据项，而且可以高效地检索得到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种描述的层面较高，但实际上的细节会复杂得多。事实上，大部分现代计算机芯片都有多级缓存，一级比一级大，但效率也因此更低。Milk 编译器不仅必须跟踪内存地址列表，还要跟踪存储在这些地址中的数据，而且它常常将这两者在各级缓存之间进行切换。它也必须决定哪些地址应当被保留（因为可能需要被再次访问），哪些应当被丢弃。研究者希望能够进一步提升这种能够编排这种复杂的数据芭蕾舞的算法，从而进一步提升性能表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「今天许多重要的应用都是数据密集型的，但不幸的是，内存和 CPU 之间不断增大的性能鸿沟意味着当前的硬件还没有发挥出它们的全部潜力。」斯坦福大学计算机科学助理教授 Matei Zaharia 说，「Milk 通过优化常见编程架构中的内存访问来帮助解决这一鸿沟。这项成果将关于内存控制器设计的详细知识和关于编译器的知识结合了起来，能为当前的硬件实现良好的优化。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Michael Jordan最新论文：少于单次通过，随机受控的随机梯度方法</title>
      <link>http://www.iwgc.cn/link/2688678</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lihua Lei、Michael I. Jordan&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜雪、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJXYZeFoiaGicaLlOKsFbfBny7eINMgQqgE5zvricr0TziaO5OpSKC3cJLpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们为基于梯度的优化开发并分析了一套程序，我们称之为随机受控的随机梯度（SCSG: stochastically controlled stochastic gradient）。作为 SVRG 算法家族的一员，SCSG 利用梯度在两个级别上估算。与这家族已有的其他算法不同，SCSG 的计算成本和通信成本不需要线性扩展样本量 n；事实上，当目标精确度较低时，这些成本是独立于 n 的。一个在 MNIST 数据集上的 SCSG 的试验评估显示：只需要带有 2.6 MB 内存的日常所用的机器和 8 次磁盘访问，它就能在这个数据集上生成精确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>SyncDaily | 英伟达发布两款基于Pascal的深度学习芯片、以色列发明通过声音和触觉「看见」环境的感官替代设备</title>
      <link>http://www.iwgc.cn/link/2688679</link>
      <description>&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;英伟达发布 Tesla P4&amp;amp;P40 两款基于 Pascal 架构的深度学习芯片、以色列发明让盲人通过声音和触觉「看见」环境的感官替代设备......机器之心日报，精选一天前沿科技优质内容。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;谷歌新专利：自动驾驶汽车探测警车&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;9 月 13 日，NVIDIA（英伟达）在北京国际饭店会议中心召开 GTC China 2016 大会。GTC 是全球最大最权威的 GPU 开发者和行业大会，展示各行业中运用 GPU 技术最重要的创新成果。在会上，NVIDIA 发布了 Tesla P4 和 Tesla P40 两款 Pascal 架构 GPU。本次集成了 72 亿个晶体管的 Tesla P4（2560 个 CUDA 核心）和 120 亿个晶体管的 Tesla P40（3840 个 CUDA 核心）是用来让用户识别和查询语音、图像或文本的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJbfJbQCpiaiaXe5ERHgJ2EXLjluB3PXMuR1Jjj8SSrJen01JXdysOprvA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJtIttohMtibJMNIabd72xzvUr5g6dF2tsG1zjnMZG7dDCPIgCxgviaKKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tesla P4&amp;amp;P40 的性能相当于 40 个 CPU，响应速度是 CPU 解决方案的 45 倍。同时，Pascal 架构能助推深度学习加速 65 倍，最新一代的架构 Pascal 是首个专为深度学习而设计的 GPU。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;人工智能学习侠盗猎车5场景，升级自动驾驶汽车安全系数&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;据 MIT 科技评论报道，英特尔实验室与德国达姆施塔特大学合作在 GTA V（Grand Theft Auto V）这款游戏的基础上开发了一款机器学习程序，这款程序能够带来更安全更好的自动驾驶汽车。给这款游戏添上适当的补丁后，进入游戏你就能看见城市驾驶，郊区开车，土路，自行车，利尔喷气式飞机降落在四车道的高速公路这些场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJAPa8cNnpUKVZeYsia573e8J5CK33FYQ1YYlDyTIS228moaqllcsOia8w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而英特尔实验室和达姆施塔特大学的研究人员要想出如何将游戏图像分解成人工智能程序可以识别的类别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果人工智能了解道路、建筑和人们的方位，它就可以学习如何在这些道路上安全驾驶。而 GTA V 的图形非常先进，人工智能通过学习 GTA V 也许可以把这些经验变成实际知识用在真实世界的驾驶中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;Udacity联合英伟达与奔驰，推出「无人驾驶车」课程&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最新的 TechCrunch Disrupt 2016 大会上，优达学城（Udacity）正式对外发布了「无人驾驶工程师」的纳米学位，这也是全球首门可以在线学习的无人驾驶车工程师培训项目，由优达学城（Udacity）联合梅赛德斯-奔驰、NVIDIA 以及刚被 Uber 收购的 Otto 联合推出，同时滴滴出行将会作为人才雇佣上的合作方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;课程将于 9.14- 27 日接受线上报名，学员将会经过一轮选拔，10 月中旬开始 3 个学期的学习，内容将涉及深度学习、计算机视觉、传感器融合、定位、控制器、汽车动力学、汽车硬件等技术，并且学员将在优达学城自有的真实无人驾驶车上创建并运行自己的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;以色列发明让盲人通过声音和触觉「看见」环境的感官替代设备&lt;/h2&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;耶路撒冷希伯来大学知名的大脑与多感官研究实验室最近又出了两个新装备。这些感官替代设备（SSDs）可以通过提供来自声音和触觉的视觉信息来帮助盲人「看见」周围的环境。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，这些装备已经准备好大量投入市场。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个是 EyeCane，用于定位方向的设备，外观很像手电筒，能发出红外线，将距离转化为听觉和触觉的线索，使用户能够感觉到周围五米（16 英尺）内的物体。经过短暂培训后，使用者可以借助 EyeCane 来在简单的环境中估计距离，避开障碍物，导航。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=g0328p0qp55&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个设备是 EyeMusic 是一个应用程序和微型摄像系统，将图像转换成供「大脑进行视觉处理的音景（soundscope）」来传递对象的颜色、形状、和图像位置。经过训练后盲人可以用这个设备来识别字母表中的字母，「看」见动物图片，甚至在一个复杂的视觉景观中找到一个对象或人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=s0328is5bo2&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="margin-top: 8px; font-weight: bold; max-width: 100%; white-space: normal; border-color: rgb(216, 40, 33); text-align: justify; color: rgb(216, 40, 33); line-height: 28px; font-family: 微软雅黑; border-bottom-width: 2px; border-bottom-style: solid; min-height: 32px; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;span&gt;&lt;/span&gt;Evernote 将要把所有数据搬上谷歌云端&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;全球拥有 2 亿用户的 Evernote 如今正在转移包括用户的 50 亿条笔记在内的所有数据，而目的地是谷歌的云端平台。这家公司也将开始使用谷歌的机器学习的应用程序接口（API）来帮助接入并以多种不同的方式使用这些数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Evernote 将关闭以前的那套基于私人云架构的存储架构。Evernote 的 CTOanirban Kundu 说前两个由谷歌的机器学习 API 替代的是语音-文本的翻译中的语音识别；和自然语言处理用于帮助搜索上下文内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据转移将在今年 10 月开始，大约在年底完工。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编辑整理，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 斯坦福开发机器学习脑机接口，帮助猴子打出了莎士比亚名句</title>
      <link>http://www.iwgc.cn/link/2675473</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自IEEE Spectrum&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Eliza Strickland&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;span&gt;给一只猴子一台打字机，早晚有一天它能打出莎士比亚全集，这是概率定理中的著名例子。最近猴子们不负所望，终于打出了哈姆雷特的那句家喻户晓的名言「To be or not to be. That is the question」。这是斯坦福神经义肢移动实验室的一项最新研究技术，他们在猴子大脑中控制运动的区域里植入了微型电极阵列，并使用了机器学习算法，开发出一个脑机通信接口。猴子们使用这个接口能在一分钟内打出 12 个单词，这是目前最高记录，而且研究员 Paul Nuyukian 说，这项技术还有很大的提升空间。点击&lt;span&gt;&lt;span&gt;「&lt;/span&gt;阅读原文&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;，查看论文链接。&lt;/span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEz8ics76Iyk3Hs21VtvIchXyFYibXoNKrkffqA6r8gpHhkCnTNExdDBgA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图像来源：斯坦福大学&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「To be or not to be. That is the question.（存在或灭亡，这是一个问题。）」这句话也是猴子 J 使用大脑植入物来控制计算机光标打出来的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先说清楚，这只猴子不知道它打出来的是莎士比亚的名句，它对哈姆雷特这句著名的独白也没有什么深刻的理解。猴子 J 和它的伙伴猴子 L 都是经过训练后才能使用它们的神经植入体来移动电脑屏幕上的光标，当屏幕上的圈圈变成绿色时，它们就点击这些圈圈。斯坦福大学的研究员们将这些文字放入那些目标中来模拟打字任务。所以要打出哈姆雷特的这句名言，第一个字母「T」圈圈要被点亮，然后是「O」，以此类推。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么要做这些练习？难道这只是让记者抛出「无限猴子定理」的一个借口？&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们一直有这样一个看法：根据概率定理，如果你给一只猴子一台打字机和无限的时间，它最终会随机打出莎士比亚的所有作品。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而这里的情况并不是这样，生物工程师们有一个更加接地气的动机。通过模拟这个打字任务，他们证明了他们的大脑计算机接口可以惠及那些无法正常沟通的人，包括肌萎缩侧索硬化症（ALS）晚期患者，也被称为也被称为 Lou Gehrig 病，这些人最终会全身瘫痪，包括口腔和其他面部肌肉，但思维还是清晰的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项实验创下了一个用意念（mind）打字的新纪录：一只猴子一分钟打出了 12 个单词。「据我们所知，这是目前能达到的最高水平了，」Paul Nuyukian 说，他是斯坦福神经义肢移动实验室（Neural Prosthetics Translational Lab）研究员，也是这项研究相关论文的合作者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=q0328kyui8n&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;它的原理是这样的：这些猴子的大脑中位于运动皮质的部分植入了微小的电极阵列，这部分控制着四肢运动。当猴子在训练光标控制任务时，那些电极会测量猴子神经元的电活动，首先会移动它们的胳膊，同时相机会小心地跟踪着这些动作。机器学习算法在数据流中发现模式，并将这些模式转换成猴子的一个意图来移动光标向左，向右，向上，向下。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年大脑打字的记录是一名 ALS 患者创下的，一分钟打出 6 个单词。那项试验的研究者团队更大，其中也有 Nuyujukian，他是 BrainGate 联盟的成员。今年能创下新纪录还是要得益于之前研究中的软件；猴子们使用的系统中装了两个聪明的串联的算法，一个用来解码光标的移动，另一个用来解码猴子要点击哪个字母的意图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Nuyujukian 在采访中告诉 IEEE Spectrum，这项技术还有很大的提升空间。目前这些猴子使用接口一次只能选一个字母，他说，但是未来的接口可以借用一些智能手机上的小技术。「我能想象出一个更加聪明的接口，它能自动完成这些单词。」「谷歌和苹果已经做了很多研究工作，试图从我们不精确的手指移动中获取最大化的信息输入，我们可以大大利用这个技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该技术的人类试验已在进行中。在去年的某个大会上，研究者们展示出了一项试验，试验中一名患有 ALS 的女性使用了一台外接安卓平板电脑的光标控制系统来浏览网页、写邮件。这类人类试验说明这些研究者并不是在开什么莎士比亚猴子的玩笑。他们正在尝试帮助那些瘫痪的人能够实现一定程度上自理，赋予他们表达自己思想的能力。但是这些研究者们自己的乐趣并不太多。Nuyujukian 泄露了猴子们早期用这个测试系统打出来的文字，「A banana, a banana, my kingdom for a banana!」以及「a banana by any other name would smell as sweet.」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：一种非人灵长类动物的大脑–计算机打字接口（A Nonhuman Primate Brain–Computer Typing Interface）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;脑机接口（BCI）可以记录大脑活动并将这些活动信息翻译成有用的控制信号。它们能用来帮助瘫痪的人通过控制计算机光标或者机器义肢这类终端执行器恢复部分身体功能。通信神经假体（Communication neural prostheses）是在计算机或移动设备上控制用户界面的脑机接口。我们给两只猕猴植入电极阵列，模拟打字任务，以此展示了一副通信假体。猴子们用了两个目前性能最好的 BCI 解码器，通过一次只提示一个符号/字母的方式，打出了单词和句子。平均来看，使用只与速度相关的二维 BCI 解码器（该解码器根据停留时间（dwell）从符号中进行选择）从一篇报纸文章上复制文本，猴子 J 和猴子 L 的打字速度分别达到了 10.0 个和 7.8 个每分钟（wpm）。有了一个能离散点击关键选项的 BCI 解码器后，打字速度分别上升到 12.0wpm 和 7.8wpm。这是目前使用 BCI 进行沟通能达到的最快速度。然后我们量化比特率和打字速度之间的关系，发这个关系现近似线性：打字速度（以 wpm 计算）是比特率（以字节/秒计算）的三倍。我们还比较了能达到的比特率指标和信息转换速度，并讨论了它们适用于现实世界的打字情境。虽然这项研究还不能模拟单词认知负荷和句子规划（sentence planning）的影响，但它证明了 BIC 作为通信接口的可能性，并代表了对于一个给定 BCI 吞吐量期望能实现的打字速度上限。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心经授权编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 主流深度学习框架对比：看你最适合哪一款？</title>
      <link>http://www.iwgc.cn/link/2675474</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自deeplearning4j.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Aäron van den Oord、Heiga Zen、Sander Dieleman&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;近日，Deeplearning4j 在自己的官方网站发表了一篇对比 Deeplearning4j 与 Torch、Theano、Caffe、TensorFlow 的博客文章，同时 Deeplearning4j 在文章中也对自己的框架进行了较为详细的介绍（多有溢美之词）。机器之心对全文进行了编译，文中观点仅代表原作者立场。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Theano &amp;amp; Ecosystem&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Torch&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Tensorflow&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Caffe&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNTK&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DSSTNE&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Speed&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DL4J: Why the JVM?&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DL4S: Deep Learning in Scala&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Machine-Learning Frameworks&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Further Reading&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Theano 与生态系统&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习领域内的很多学术研究人员依赖于 Theano，这个用 Python 编写的框架可谓是深度学习框架的老祖宗。Theano 像 Numpy 一样，是一个处理多维数组的库。与其他库一起使用，Theano 很适合于数据探索和进行研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 Theano 之上，已经有很多的开源的深度库建立起来，包括 Keras、Lasagne 和 Blocks。这些库的建立是为了在 Theano 偶尔的非直觉界面上更简单地使用 API。（截止到 2016 年 3 月，另一个与 Theano 相关的库 Pylearn2 可能即将死亡。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相反，Deeplearning4j 能在 JVM 语言（比如，Java 和 Scala）下将深度学习带入生产环境中，创造出解决方案。Deeplearning4j 意在以一种可拓展的方式在并行 GPU 或 CPU 上将尽可能多的环节自动化，并能在需要的时候与 Hadoop 和 Spark 进行整合。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python+Numpy&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）计算图是很好的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）RNN 完美适配计算图&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）原始 Theano 在某种程度上有些低水平&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）高层次 wrappers（Keras，Lasange）减轻了这种痛苦&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）错误信息没有帮助&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）大型模型有较长的编译时间&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比 Torch 更「臃肿」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）对预训练模型支持不佳&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）在 AWS 上有很多 bug&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Torch&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Torch 是一个用 Lua 编写的支持机器学习算法的计算框架。其中的一些版本被 Facebook、Twitter 这样的大型科技公司使用，为内部团队专门化其深度学习平台。Lua 是一种在上世纪 90 年代早期在巴西开发出来的多范式的脚本语言。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Torch 7 虽然强大，却并未被基于 Python 的学术社区和通用语言为 Java 的企业软件工程师普遍使用。Deeplearning4j 使用 Java 编写，这反映了我们对产业和易用性的关注。我们相信可用性的限制给深度学习的广泛使用带来了阻碍。我们认为 Hadoop 和 Spark 这样的开源分布式应该自动具备可扩展性。我们相信一个商业化支撑下的开源框架是保证工具有效并建立一个社区的合适解决方案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）很多容易结合的模块碎片&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）易于编写自己的层类型和在 GPU 上运行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Lua（大部分库代码是 Lua 语言，易于读取）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）大量的预训练模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）Lua（小众）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）你总是需要编写自己的训练代码（更不能即插即用）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）对循环神经网络不太好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）没有商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）糟糕的文档支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌创造 TensorFlow 取代 Theano，其实这两个库相当类似。Theano 的一些创造者，比如 Ian Goodfellow 在去 OpenAI 之前就是在谷歌打造 TensorFlow。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;目前，TensorFlow 还不支持所谓的「inline」矩阵运算，但会强迫你按序 copy 一个矩阵，并在其上进行运算。copy 大型矩阵非常耗费成本，相比于其他先进的深度学习工具 TensorFlow 要花费 4 倍的时间。谷歌说他们正在解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;像大部分深度学习框架一样，TensorFlow 在 C/C++ 引擎之上使用 Python API 编写，从而加速其运行。对 Java 和 Scala 社区而言，它并非一个合适的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow 不只是面向深度学习，也有支持强化学习和其它算法的工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌开放 TensorFlow 的目标看起来是想吸引更多的人，共享他们研究人员的代码，标准化软件工程师进行深度学习的方式，并吸引他人对谷歌云服务的兴趣——TensorFlow 针对谷歌云服务进行了优化。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;TensorFlow 并非商业支持下的，而且看起来谷歌也不可能成为支持开源企业软件的企业。它只为研究者提供新工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如同 Theano，TensorFlow 生成一个计算图（比如一系列矩阵运算，像 z=Simoid（x）, 其中 x 和 z 都是矩阵）并进行自动微分。自动微分很重要，因为每次实验一个新的神经网络的时候，你肯定不想手动编写一个反向传播新变体的代码。在谷歌的生态系统中，计算图后来被 Google Brain 使用进行一些繁重工作，但谷歌还未开源其中一些工具。TensorFlow 只是谷歌内部的深度学习解决方案的一半。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;从企业的角度来看，一些公司需要考虑的是他们是否想依赖谷歌的这些工具。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Caveat：在 TensorFlow 中的所有运算并不都像在 Numpy 中一样。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python+Numpy&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）计算图抽象，如同 Theano&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）比 Theano 更快的编译速度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）进行可视化的 TensorBoard&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）数据和模型并行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比其它框架慢&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）比 Torch 更「臃肿」；更神奇；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）预训练模型不多&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）计算图是纯 Python 的，因此更慢&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）无商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）需要退出到 Python 才能加载每个新的训练 batch&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不能进行太大的调整&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）在大型软件项目上，动态键入易出错&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Caffe&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Caffe 是一个知名的、被普遍使用的机器视觉库，其将 Matlab 的快速卷积网接口迁移到了 C 和 C++ 中。Caffe 不面向其他深度学习应用，比如文本、声音或时序数据。如同其他框架一样，Caffe 选择 Python 作为 API。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deeplearning4j 和 Caffe 都能用卷积网络进行图像分类，都展现出了顶尖水平。相比于 Caffe，Deeplearning4j 还提供了任意数量芯片的并行 GPU 支持，以及许多可使得深度学习在多个并行 GPU 集群上运行得更平滑的看起来琐碎的特征。Caffe 主要被用于作为一个托管在其 Model Zoo 网站上的预训练模型的源。Deeplearning4j 正在开发一个能将 Caffe 模型导入到 Spark 的解析器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优缺点：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）在前馈网络和图像处理上较好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）在微调已有网络上较好&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）不写任何代码就可训练模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（+）Python 接口相当有用&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）需要为新的 GPU 层编写 C++/CUDA&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不擅长循环网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）面对大型网络有点吃力（GoogLeNet，ResNet）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）不可扩展&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;（-）无商业化支持&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CNTK&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNTK 是微软的开源深度学习框架，是「Computational Network Toolkit（计算网络工具包）」的缩写。这个库包括前馈 DNN、卷积网络和循环网络。CNTK 提供一个 C++ 代码上的 Python API。虽然 CNTK 有一个许可证，但它还未有更多的传统许可，比如 ASF2.0，BSD，或 MIT。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;DSSTNE&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;亚马逊的 Deep Scalable Sparse Tensor Network Engine（DSSTNE）是一个为机器学习、深度学习构建模型的库。它是最近才开源的一个深度学习库，在 TensorFlow 和 CNTK 之后才开源。大部分使用 C++ 编写，DSSTNE 似乎很快，尽管它如今没有其它库那样吸引大量关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;(+) 处理稀疏的编码&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;(-) 亚马逊可能不会共享要得到其样本的最好结果所必需的所有信息&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Deeplearning4j 使用 ND4J 执行的线性代数计算展现出了在大型矩阵相乘上的至少比 Numpy 快两倍的速度。这也是为什么我们的 Deeplearning4j 被 NASA 喷气推进实验室里的团队采用的原因之一。此外，Deeplearning4j 在多种芯片上的运行已经被优化，包括 x86、CUDA C 的 GPU。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Torch7 和 DL4J 都可并行，但 DL4J 的并行是自动化的。也就是说，我们对工作节点和连接进行了自动化，在 Spark、Hadoop 或者与 Akka 和 AWS 上建立大规模并行的时候，能让用户对库进行分流。Deeplearning4j 最适合于解决特定问题，而且速度很快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;为什么要用 JVM（Java 虚拟机）？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们常被问到我们为什么要为 JVM 实现一个开源深度学习项目，毕竟现在深度学习社区的相当大一部分的重点都是 Python。Python 有很好的句法元素（syntactic elements）让你可以不用创建明确的类就能进行矩阵相加，而 Java 却需要这么做。另外，Python 还有丰富的科学计算环境，带有 Theano 和 Numpy 这样的原生扩展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但 JVM 及其主要语言（Java 和 Scala）也有一些优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，大部分主要的企业和大型政府机构严重依赖于 Java 或基于 JVM 的系统。他们已经进行了巨大的投资，他们可以通过基于 JVM 的人工智能利用这些投资。Java 仍然是企业中使用最广泛的语言。Java 是 Hadoop、ElasticSearch、Hive、Lucene 和 Pig 的语言，而这些工具正好对机器学习问题很有用。另一种 JVM 语言 Scala 也是 Spark 和 Kafka 的编程语言。也就是说，许多正在解决真实世界问题的程序员可以从深度学习中受益，但它们却被一些语言上的障碍隔开了。我们想要使深度学习对许多新的受众更有用，让他们可以直接将其拿来使用。在全世界 1000 万开发者中，他们使用得最多的语言就是 Java。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二，Java 和 Scala 实际上比 Python 更快。任何使用 Python 本身写成的东西——不管其是否依赖 Cython——都会更慢。当然啦，大多数计算成本较高的运算都是使用 C 或 C++ 编写的。（当我们谈论运算时，我们也考虑字符串等涉及到更高层面的机器学习过程的任务。）大多数最初使用 Python 编写的深度学习项目如果要被投入生产，就不得不被重写。Deeplearning4j 可以依靠 JavaCPP 来调用预编译的原生 C++，这能极大地加快训练速度。许多 Python 程序员选择使用 Scala 做深度学习，因为当在一个共享代码基础上与其他人合作时，他们更喜欢静态类型和函数式编程。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第三，虽然 Java 缺乏稳健的科学计算库，但把它们编写出来不就好了，这个我们可以通过 ND4J 完成。ND4J 运行在分布式 GPU 或 CPU 上，可以通过 Java 或 Scala API 接入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，Java 是一种安全的、网络的语言，本身就能在 Linux 服务器、Windows 和 OS X 桌面、安卓手机和使用嵌入式 Java 的物联网的低内存传感器上跨平台工作。尽管 Torch 和 Pylearn2 可以通过 C++ 优化，但其优化和维护却比较困难；而 Java 是一种「一次编程，到处使用」的语言，适合那些需要在许多平台上使用深度学习的公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;生态系统&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Java 的普及只能通过其生态系统加强。Hadoop 是用 Java 实现的；Spark 在 Hadoop 的 Yarn 运行时间内运行；Akka 这样的库使得为 Deeplearning4j 构建分布式系统是可行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总的来说，Java 拥有经过了高度检验的基础设施，拥有大量各种各样的应用，而且使用 Java 编写的深度学习网络也能与数据保持紧密，这可以让程序员的工作更简单。Deeplearning4j 可作为一种 YARN 应用而运行和供应。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Java 也可以从 Scala、Clojure、Python 和 Ruby 等流行的语言中被原生地使用。通过选择 Java，我们能尽可能最少地排除主要的编程社区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Java 并不如 C/C++ 快，但其比其它语言快得多。而且我们已经构建一种可以通过加入更多节点来进行加速的分布式系统——不管是用 CPU 或是 GPU 都可以。也就是说，如果你想要更快，就加入更多卡吧！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们正在使用 Java 为 DL4J 开发 Numpy 的基本应用（包括 ND-Array）。我们相信 Java 的许多缺点可以被很快地克服，而它的许多优点也还将继续持续一段时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;SCALA&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在开发 Deeplearning4j 和 ND4J 的过程中，我们对 Scala 给予了特别的关注，因为我们相信 Scala 有望变成数据科学的主导语言。使用 Scala API 为 JVM 编写数值计算、向量化和深度学习库能推动该社区向这一目标迈进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要真正理解 DL4J 和其它框架之间的差异，你必须真正试一试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器学习框架&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上面列出的机器学习框架更多是专用框架，而非通用机器学习框架，当然，通用机器学习框架也有很多，下面列出几个主要的：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;sci-kit learn：Python 的默认开源机器学习框架&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Apache Mahout：Apache 上的旗舰机器学习框架。Mahout 可用来进行分类、聚类和推荐。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SystemML：IBM 的机器学习框架，可用来执行描述性统计、分类、聚类、回归、矩阵分解和生存分析（Survival Analysis），而且也包含支持向量机。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Microsoft DMTK：微软的分布式机器学习工具包。分布式词嵌入和 LDA。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Yann LeCun 最新论文：基于能量的生成对抗网络（附论文）</title>
      <link>http://www.iwgc.cn/link/2675475</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arxiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Junbo Zhao, Michael Mathieu, Yann LeCun&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEzoWvOf8XwZ0EibTvFNzaic789Mr6t1otsLgKUtYanUNNOv89EnaPYMfw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在这里介绍「基于能量的生成对抗网络（Energy-based Generative Adversarial Network，简称 EBGAN）」模型，该网络将 GAN 框架中的鉴别器（discriminator）视为与数据流形（data manifold）和其它所有更高能量的区域的低能量区域相关联的能量函数（energy function）。和概率 GAN 类似，需要训练一个生成器（generator）来产生具有最小能量的对比样本，同时还要训练该能量函数将高能量分配给那些生成的样本。将鉴别器视为能量函数让我们可以在通常的二元判别网络之外还能使用范围广泛的架构和损失函数。在 EBGAN 的所有实例中，其中之一是沿着作为重构误差（reconstruction error）的能量使用一个自动编码器（auto-encoder）。我们研究表明这种形式的 EBGAN 能在训练过程中得到比通常的 GAN 更稳定的表现。我们也表明只需训练一个单尺度（single-scale）的架构就能生成高分辨率的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1 导语&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生成对抗网络（GAN，Goodfellow et al., 2014）已经为图像生成（Denton et al., 2015; Radford et al., 2015; Im et al., 2016; Salimans et al., 2016）、视频预测（Mathieu et al., 2015）以及一些其它领域带来了显著的进步。GAN 的基本思想是同时训练一个鉴别器和一个生成器。训练鉴别器的目的是为了将来自真实数据集的样本和生成器产生的「假」样本区分开；而训练生成器的目的是产生鉴别器无法将其与真实数据样本区分开的样本。要做到这一点，该生成器使用了来自一个易于取样的随机源的输入向量，然后生成被送入鉴别器的「假」样本。在训练过程中，生成器通过接收鉴别器对应输入的输出的梯度来进行「欺骗」。在 Goodfellow et al. (2014) 中的原始配方的 GAN 中，鉴别器输出一个概率，而在特定的情况下，当生成器产生的分布和数据分布匹配时会发生收敛（convergence）（Goodfellow et al., 2014）。从优化的角度看，GAN 的收敛可被看作是达到一个目标函数的一个鞍点（saddle point），该目标函数在对应于此时的鉴别器参数时值最小，在对应于生成器参数时值最大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这项成果中，我们通过将鉴别器作为能量函数（或对比函数）而非概率的形式从基于能量（energy-based）的角度明确地构建了该 EBGAN 框架。也就是说，该能量函数被看作是一个可训练的惩罚函数（penalty function），它会将低能量值赋予高数据密度的区域，并将更高的能量值赋予其它区域。我们断定这种角度的 GAN 训练为架构和训练流程的选择提供了更大的灵活性。尽管通常可以通过吉布斯分布（Gibbs distribution）将能量转换成概率（更多详情在 2.3 节），但在这种基于能量的角度中对规范化的规避为鉴别器在学习合适的对比函数上提供了更大的自由。在这种基于能量的解读（energy-based interpretation）的范围中，Goodfellow et al. (2014) 提出的一般的二元鉴别器可被看作是许多定义对比函数和损失函数的方法中的一种，正如 LeCun et al. (2006) 为监督式和弱监督式设置及 Ranzato et al. (2007) 为无监督设置所描述的那样。为了进行概念验证，我们采用了一种自动编码器架构，其中用重构损失（reconstruction loss）作为鉴别器。更多关于基于能量的学习和 GAN 的解读可见附录 5.&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;我们的主要贡献总结如下：&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(i) 我们为 GAN 提供了一种基于能量的解释，并据此提出了一套新架构；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(ii) 通过一个详尽的网格搜索实验，我们验证了一个关于一个全连接的 workhouse 中 GAN 和 EBGAN 的超参数和架构设置的完整集合。在不同的超参数的元参数（架构）设置下，EBGAN 表现出了更好的训练稳定性和增强了的稳健性（robustness），从而可以减少用于调节 GAN 的人类工作；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(iii) 基于编码器表征（encoder representation），我们引入了一个 pull-away 项，其可以帮助防止生成器仅关注一种或少数几种模式；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;(iv) 我们的研究表明我们的模型能够在单尺度（single-scale）的配置下从 256×256 的 ImageNet 数据集中生成合理的高分辨率图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2 模型&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种 EBGAN 实例如图 1 所示。具体而言，生成器 G 以随机向量 z 作为输入，并将其转换成像素空间 G(z)。鉴别器 D 被看作是能量函数，它既接收真实图像，也接收生成的图像，并据此评估能量值 E，其中 E ∈ R。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEzp4eqeacicg0xwoaIuQGLGZ94GO6ShZLoTU2Bw65cRen5RIMISxX6Xg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 1：EBGAN 架构&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4 实验&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.2 MNIST&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEoU0xTKOVKrvK4nEDoF806CJia7Eksbib2tTicpERT3sGvsDoKzQ3wJUiag/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); text-align: justify;"&gt;&lt;span&gt;图 5：根据 MNIST 上网格搜索的模型样本生成。左图 (a)：最好的 GAN 生成；中图 (b)：最好的 EBGAN 生成；右图 (c)：最好的 EBGAN-PT 生成。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.3 LSUN &amp;amp; CELEBA&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEsCibhicvsk3qcGMsoHhDXeYClEA5STEek87lfm7muSGXqe2EvbB0XbuQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 6：根据 LSUN 卧室全图像的生成。左图 (a)：DCGAN 生成；右图 (b)：EBGAN-PT 生成&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEGOq3NVJKmH4ibsJSVXRsUhA87jaXrcTrCyRGUc4JRbOyb01Cd4vG4EA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;图 7：根据 LSUN 卧室增强图像块的生成。左图 (a)：DCGAN 生成；右图 (b)：EBGAN-PT 生成&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEIOoTgg5U15Q75q3ibbhBCPdNmbZCyFvCQZhibsLKl1ReFNsjts96770Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 8：根据 CelebA 脸部数据集的生成。左图 (a)：DCGAN 生成；右图 (b)：EBGAN-PT 生成&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4.4 IMAGENET&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnERJQTEDAmicWQrckdJvYOmpn230lnOJuwnCa35lbTJWDkibwJK3zBmXfA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136); text-align: center;"&gt;&lt;span&gt;图 9：使用一个 EBGAN-PT 的 ImageNet 128×128 图像生成&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEdL1F5E6T6kgI7CYIhhsjw6llwhrN9HRKerkpynUfia81GCY0qNtWXJg/0?wx_fmt=png"/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图 10：使用一个 EBGAN-PT 的 ImageNet 256×256 图像生成&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;点击「阅读原文」，下载论文↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 马赛克已被人工智能征服，再也遮不住脸了</title>
      <link>http://www.iwgc.cn/link/2675476</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Wired&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：LILY HAY NEWMAN&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Cindy、杜夏德、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;马赛克已经成为人们用来遮盖那些视觉媒体中私密部分的常用工具。新闻、编辑文档、网页上常常会出现模模糊糊的文字、人脸、以及证件牌照。这个技术并不复杂，但已经足够使用了，因为人们看不出或读不懂经过扭曲的内容。然而，问题是人们不再是仅有的图象识别大师。随着计算机视觉越来越强大，它开始能看到那些我们看不到的东西。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;来自德克萨斯大学奥斯汀分校 (University of Texas at Austin) 和康奈尔理工学院（Cornell Tech）的研究人员说，他们已经训练了一个软件，这个软件可以通过学习阅读和观看图象中隐藏的内容来暗中破坏如模糊或马赛克这类标准内容遮盖技术下的隐私利益，如从模糊的门牌号码到一张有背景中的打了马赛克的人脸。他们甚至不需要煞费苦心地开发延伸新的图片显像（image uncloaking）方法。团队人员发现主流的机器学习方法，就是通过一组样本数据来「训练」计算机，而不是编程，可以轻而易举地实现这个解码模糊图像的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们在这篇论文中用到的技术是非常标准的图像识别技术，」论文的作者之一，来自 Cornell Tech 的 Vitaly Shmstikov 说道。由于这项研究中采用的机器学习方法已经广为人知，网上的教程和培训手册中都有，Shmatikov 说一个了解最基础的技术知识的人就可以实行这些类型的攻击。而且，目前已经有了更强大的物体和人脸识别技术，这些技术在破解视觉编辑方法上更有潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9Fk3z6POMzibAz4qC8XiaRnEWCI8u2W9qnkwCVVYiciaXaYtTtYHkZhAMcloC8qHRsX8lz0pjZ05Npbw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;来自四个数据库的图象。最左边的是原图，后面的四列图像中马赛克越来越多，最后三列用 P3 展示了三个等级的遮蔽效果。图片越模糊，机器学习软件识别出潜在图象的成功率越低。但是在研究人员的大部分试验中，识别模糊文字或图像的效率依旧可以达到 50%。&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些研究人员可以击破三种隐私保护技术，最简单的是 YouTube 的专有模糊工具。YouTube 允许上传者在视频中模糊他们想要掩盖对象或数据，但是这个团队可以用他们的破解技术来确认视频中的人脸的。另一个例子是，研究人员破解了像素化技术，（又称图像拼接，马赛克）。为了产生不同像素等级的图像，他们采用了自己部署的标准马赛克技术。研究人员说这些技术可以在 Photoshop 和其它常见的软件中找到。最后，他们破解了一个名为隐私保护照片分享（Privacy Preserving Photo Sharing 或 P3）的工具，这个工具可以在 JPEG 照片中加密标识数据，使人眼看不到整体的图象，但同时保证其它数据要素（data components）清晰，让计算机依旧能处理文档，比如压缩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了要执行破解，团队通过给神经网络输入用于分析的四大知名图像数的据集来让它们进行图像识别。输入的单词、人脸、或者物体越多，神经网络就可更好的锁定目标。一旦神经网络确认训练集内的相关对象的精确度达到接近 90% 或者更高，研究人员就会通过三种隐私工具混淆图象，然后进一步训练他们的神经网络在原始图像的知识的基础之上识别马赛克或者模糊的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，他们使用那些还未以任何一种形式暴露给神经网络模糊的测试图像来看看图片识别是否可以识别出人脸、物体、和手写数字。对一些数据集和遮蔽技术来说，神经网络识别的成功率越过 80%，甚至 90%。在马赛克的案例中，越是模糊的图片，成功率就越低。但是他们去除模糊的机器学习软件的准确率总是位于 50% 到 75% 之间。最低的成功率是 17%，使用的数据集是用 P3 编校系统模糊处理过的明星脸孔数据集。然而如果用计算机随机地识别人脸、图形、和数字，研究人员计算出每一组测式的成功率最多是 10%，至少是 5%，这意味着尽管识别的成功率相对地比较低，但依旧比单纯的猜测要准确。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加利福尼亚大学 圣地亚哥 分校（University of California, San Diego) 的 Lawrence Saul 说，尽管这个团队的机器学习方法不能总是洞悉图片上编辑的效果，但它依旧代表着像素化和模糊作为一种隐私工具的兴起。「为了窥伺隐私，你不需要在 99.9 的情况下都能进行重建」一个图片或一段文字。「如果 40% 或 50% 的情况下你可以猜出人脸或指出内容是什么，那足够表示这种隐私保护方法该被废弃。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;值得注意的是该研究并不是从头开始做图象重建，不能逆转模糊化得过程，真的重建识别出的人脸或物体的图片。这种技术只能寻找它要找的，不一定是精确的图象，但可能是它以前见过的图片，如特定的对象或以前识别出的人脸。比方说，火车站的 CCTV 模糊地记录了每个过路的人脸，但它却不能识别每一个人。但如果你怀疑在特定时间曾走过的某个人，它就可以在模糊视频的茫茫人海中指出那个人。Saul 提到一个额外的挑战是在从现实场景更广泛的阵列中收集到的模糊图片上测试神经网络，而不是只在已有的更标准化的图像数据集上进行测试。但是基于他们目前的发现，他人为更多的实际应用也是很有可能的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;研究人员更大的目标是提醒隐私和安全社区，发展机器学习作为识别和数据收集的一种工具是不能被忽视的。有多种方式可用来抵制这种攻击，如 Saul 指出的，使用提供全面覆盖的黑盒子，而不是在图像内容后面留下痕迹的图像扭曲方法。最好可以随机切除脸的部分，在覆盖该脸部之前模糊它，这样尽管混淆被攻破了，该人的身份依旧不会暴露。「我希望这篇论文引发的结果是，没有人在没有经过这种分析验证的情况下就宣称他们的隐私技术是安全的。」Shmatikov 说道。目前，用一个丑陋的黑色斑点盖住视频中某人的脸可以没有打马赛克更常见。但防止计算机视觉从这些像素中读取出我们不希望其读出的信息将可能很快变得有必要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 13 Sep 2016 17:01:13 +0800</pubDate>
    </item>
    <item>
      <title>专访 | 第四范式首席科学家杨强教授：未来人工智能会让二流科学家失业</title>
      <link>http://www.iwgc.cn/link/2660000</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：赵云峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="line-height: 1.6; color: rgb(136, 136, 136);"&gt;&lt;span&gt;杨强，第四范式联合创始人、首席科学家。杨强教授在人工智能研究领域深耕三十年，是国际公认的人工智能全球顶级学者，ACM 杰出科学家，两届「KDD Cup」冠军。现任香港科技大学计算机与工程系主任，是首位美国人工智能协会（AAAI）华人院士，AAAI 执行委员会唯一的华人委员，国际顶级学术会议 KDD、IJCAI 等大会主席，IEEE 大数据期刊等国际顶级学术期刊主编。杨强教授在数据挖掘、人工智能、终身机器学习和智能规划等研究领域都有卓越的贡献，是迁移学习领域的奠基人和开拓者，他发表论文 400 余篇，论文被引用超过两万次。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em style="line-height: 1.6; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;em&gt;近日，机器之心对杨强教授进行了专访，他对迁移学习、人工智能行业与技术进行了深入讲解，并对人工智能从业者提供了众多有价值的建议。&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8W8EqqWUWQmBIEh117bd2VrLkY43u3hrFic6dhSw9xFZQiaxLDQvx0UHMiavkhK7ibdZkcY6QO19b03g/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;杨强教授&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关于迁移学习&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您目前主要从事哪方面的研究工作？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我现在做的一项研究是把深度学习、强化学习和迁移学习结合起来，让深度学习有目标。基于有目标的延迟反馈的机器学习在现有的深度学习中做的不是太好，比如 RNN 能够解决 序列到序列的学习（Sequence to Sequence Learning)，但它比较短视，没有最终目标和最终反馈。这就会出现很多问题，比如说推荐系统就没有办法在对话中有效地，自然地使用。同时，在自然语言对话中，只会出现毫无目的的闲聊，使得用户体验会不太好。要改变这些问题，就一定要引入强化学习，这样才可以进行推理并具有长期的目标。同时在这些算法之上再加一个迁移学习的算法层。这样，可以把一个通用的学习模型「个性化」到每个人不同的需求和兴趣上。这是我们目前所聚焦的研究领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：提到不同学习方法的结合，ACM 8 月份 CACM（communication of the ACM）刊文《强化学习的复兴（Reinforcement Renaissance）》，深度学习和强化学习结合的深度强化学习带来了更好的表现，您之前也在演讲中提过两者的结合，能具体解释一下吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;强化学习原来特别地难以有效的扩展，这是因为它的状态空间太大；另外，这些状态，都是凭某个专家的经验来人为地定义的，而并不是学习出来的。但是，现在通过和深度学习的结合，我们可以把强化学习的目标和反馈拿出来，把规划的目标转化成一个学习的目标，即 lost function ，而用来训练一个「端到端的」深度学习系统。DeepMind 在这方面就做的比较成功。这样，深度学习就有目的性了，因为可以得到有效的反馈机制来帮助学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，我们也要看到，这样做了以后，缺点也随之出现——比如说 DeepMind 那个模型，它是不可解释的，因此很难把人的经验放进去，也很难在这个强化学习模型上面做任何的「知识短路」，即个性化。我们看到，迁移学习是在状态空间上的一种知识短路，这是我们的一个新发现：即迁移学习更容易在知识结构上从小数据中学习。这样，在应用中，可以先训练一个深度学习和强化学习的合并模型，然后用近似的方法把状态显现化，最后，再在这个近似空间的转移中做一个迁移学习模型。这就是我们现在所做的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：能否介绍一下迁移学习这几年发展的亮点以及现阶段的研究难点，比如您之前提到过的「两个领域衡量标准」问题。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;迁移学习在这几年有挺大突破，比如它和深度学习的结合。迁移学习的一个发现就是用特征做迁移效果非常好，关键是怎么找到这些好的特征将两个领域给结合起来，换句话说，就是要找到合适的迁移机制。我们可以通过什么把知识从一个领域迁移到另外一个领域呢？我们发现最好就是找到一些比较通用的特征，比如说在大陆这边，汽车的驾驶员坐在左边，而在香港驾驶员坐在右边。那么，你怎么让一个大陆的驾驶员一来到香港就马上就适应，而回到大陆又可以迅速调整回来，做到左右逢源呢？那就需要找到一个知识的表达方式——即驾驶员和马路的关系的表达——司机的位置如果是靠路中间，那不管在大陆还是在香港都肯定没错。而这种通用的表达方式就是深度学习可以帮你找到的。当你把两个不同领域都作为输入给深度学习系统，它会帮你找到一个共同的不变的表达，然后就可以通过这个不变量来做迁移。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，迁移学习在最近的一个进展就是，通过深度学习发现不同隐含层有不同的迁移能力，比如说，在音频上偏高层就比较容易迁移；视频上偏低层比较容易迁移，但每一层能迁移的知识和量不一样。这样我们对迁移能力就能有了定量认识，又往前走了一步。这只是一个很好的研究方向。但是迁移学习目前比较难的一点，是衡量两个领域之间的距离。过去的研究，学者们只是靠纯统计的方法。而现在，有了深度学习以后，就可以把在不同层次的特征拿过来，发现不同层次的距离是不一样的，而利用这些不同的「知识点」来理解迁移学习的能力。这一点是一个新的突破口。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：最近有一项关于用迁移学习研究非洲贫困的案例，斯坦福研究者回避了其他收集成本过高的指标，而是使用卫星图像获取的灯光信息来判断贫困程度，您如何看待里面的技术，以及此项研究本身涉及的意义呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;是的，这是个很有趣的案例。在这个案例里，灯光就变成了一个不变量。它能反映贫富，又能反映路段，只要预训练灯光，就可以把这段知识迁移到那段去，这是一个很好的例子。但有特别需要说明的一点是，这是其中一种迁移学习的手段，叫传递式迁移，是说从 a 到 b 到 c 三个领域的传递，这个链条可以任意的长，从 1 到 N 。其实我们日常中都在用这种方式，比如学生第一学期上的课和最后一个学期上的课就可以看成一个迁移链条，上完这门课再上下一门，很多知识就可以被迁移和应用，新东西学起来就觉得容易，课程一个个过来就可以毕业了。我们人类已经在使用这种方式——把一个难的问题分解成一系列问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：目前迁移学习的研究成果在哪些领域应用的比较好？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;迁移学习在好几个领域都能发挥作用，比如说电商上面的推荐，你做了一个领域的推荐模型，当出现一个新的领域时，就可以迁移过去，这两个领域有区别，但有些是共通的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一个应用的例子是推荐系统里的一个非常棘手的问题，就是「冷启动」。就是说，在没有任何用户数据的情况下，我们如何能够让系统推荐的结果还不错？一个做法是，可以从一个类似的领域迁移过来。第二个是「个性化」：每个人都希望在手机上了解我们的智能助手，「懂我」的意思就是已经基于你的需求进行个性化了。使用迁移学习就可以利用你的数据从一个通用模型迁移到你的个人模型。第三应用领域是小数据，大家都知道大数据可以用深度学习做，但大数据的获取只有少数大公司才能做到，而大部分公司是没有这个资源的，他们只有小数据，有个办法就是从大数据获得的模型往小数据迁移。第四个是可以用在隐私方面，我们如果能把一个大数据的模型实现本地化。这样，就没有必要去把本地隐私的数据上传到云端，而个人隐私就可以获得保护，我觉得这是解决隐私问题最有效的一个方式，如果迁移学习能解决的话，那加密、利用随机扰乱等技术来保护隐私的办法都不用做了，因为这些方法对模型的效果影响很大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：关于隐私，您在 2015 年发了一篇关于差分隐私（Differential Privacy）的文章。随着您刚才提到的「个性化人工智能」的推进，敏感数据的隐私问题日渐受到更多关注。您能基于当时那篇文章讲解一下吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这是当时在华为诺亚方舟实验室和上海联通合作的一个项目。我们在电信数据的挖掘上，发现数据挖掘中一个很受追捧的概念就是「差分隐私」技术。这个概念在学术界很流行，但我们发现在工业上，它的应用却很少。这个算法在实际问题中用不了的原因，是它没有考虑一个重要因素：一方面要保护隐私，另一方面要保证模型的表现不下降。如果保护的多，那模型的效果就会下降的就特别多，而这种效果的下降是可以用钱来衡量的。而隐私如果也能换成钱，那么，就可以在这两者间做一个权衡。但是，从来没有人这么做过。所以我们当时写了一篇论文来指出了这一点。也就是，过去大家一味的去关心隐私其实是一种偏颇，而隐私的问题应该把效果和价值来综合来考虑。差分隐私在学术界是一种很优秀的做法，但可能不适合工业界，因为在工业界需要从效果总体上进行权衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：迁移学习可以实现「举一反三」（和人类智能类似），这好像与我们要实现的人工智能终极目标最为相似，那如果接下来人工智能要取得突破，迁移学习会成为其中最关键的路径吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;迁移学习只是路径之一，应该说，更重要的是表达学习，即学习知识的表达。迁移是知识表达的一个试金石：如果表达找的好，那就迁移的好，深度学习是表达的一个路径，但不是唯一路径。如果要把知识表达进行分解的话，其中的迁移能力是特别重要的，比如能做比喻学习（learning by analogy）等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于人工智能行业&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您在大企业研究院（华为诺亚方舟、微信联合实验室）、高校（HKUST）和创业公司（第四范式）进行研究，这些机构在研发方法和目标上有何异同？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;在工业界应该是应用研究，更多着眼把一个技术实用化以产生价值，而这个技术最好今天就解决问题；大学是长远的，更理论化的研究，目标比较高远，大学的研究所应该做明天要做的事情。现在我看到一个现象是，公司有实验室在做大学的事情，大学有实验室做公司的事情，我预计这些很难成功。因为他们都在做别人应该做的事情。如果公司做纯高校的事情一定长久不了，他们要产生价值，公司无法去养这么多学者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您提到过人工智能的五个条件：清晰的商业目标、高品质的大数据资源、清晰的问题定义和领域边界、了解人工智能跨界人才、强大的硬件计算能力。「第四范式」这个公司目前在做的金融行业对此非常符合，能否透露一下，公司下一个重点拓展的行业是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：金融可能是我们的第一个点，但第四范式的重要目标是做一个平台，这个平台能够让大众变成人工智能应用者，大家只要自己有数据和应用问题，想利用这个平台，都可以来用。为什么先选择金融领域，传统金融领域需求比较大，门槛比较高，如果在这个领域成为一个领先者就很容易保持优势，有了这个优势之后可以铺开做。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：科技巨头对人工智能创业公司的收购越来越频繁，这是否会加剧您提到的「人工智能是富人的游戏」这种现象？对于第四范式来说，考虑过被巨头收购的可能吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这肯定会加剧，相信一些创业公司聚集了一批优秀人才，但是也存在有些创业公司的目的就是被巨头收购，最后逐渐就变成巨头一统天下了，小的诸侯国被吞并，秦朝统一六国，这并不有利于百花齐放。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那对于第四范式来说，我们汇聚了机器学习领域的优秀科学家、工程师和咨询专家，从实战中不断优化研发与服务水平。目前第四范式在工业界发展非常好，我们也希望它在科学上不负众望，能够规避人工智能被个别的集团所垄断的风险、从而引导人工智能走向大众，为社会所用，这也是我作为一个科学家的使命。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：现在看到有种趋势，比如说从论文发表来看，公司在人工智能的某些前沿研究上已经超过了高校。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;因为公司现在有资本有数据，所以吸引了很多人，这些人是冲着资源去的，比如说数据和机器，这是现在的一个阶段。但是比如说理论研究问题，目前我们对深度学习本身的理解还不够深，如果要解决这个问题，就不应该在公司做。但是你要去实现大规模的深度学习系统，利用大数据去做一件前人没有做的事情，那一定要在公司做，在大学做不了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但特别要指出的是，我经历了很多公司，发现它们并不是像外界所想的那样就一定有数据，很多数据其实是大学来找更加方便，因为大学是中立的、非盈利机构，大家更乐意把数据给出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关于人工智能技术&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：近期机器学习领域有哪些让您觉得很有趣的研究吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;机器学习领域里一个很强的特点是：可以把感知的东西学到，但很难推理。所以我觉得一个挺好的方向是让机器学习去做推理。一个例子是 Facebook 做的机器阅读（Machine Reading），它可以在读的文章里做推理。虽然它很简单，但指出了一个方向——加入注意力模型之后就可以做符号推理。但如果能够 scale 到一阶逻辑去做大规模推理和定理证明的话（也是我们目前在做的研究），还有很长的路。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，开始有一些工作把规则，逻辑和深度学习相结合，这可以起到解释模型和获得知识的作用，把人的知识赋予到统计学习的模型里，这是很好的方向，但目前那些方法还不够，我们希望在这方面多做一些研究。因为规则是在任何一个垂直领域都必不可少的，并不是任何东西都需要从零开始学。规则的好处是准确和通用，坏处是缺乏覆盖的广度比较有限，而统计学习可以应付各种例外的发生，如何把这两者更好的结合起来是一个很有趣的方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：人工智能如果取得继续突破的话，是否需要把规则和统计结合起来？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;统计和逻辑的结合在人工智能的发展中必不可少，AlphaGo 就是这样一个例子，它非常深入的将搜索和学习这两者结合了起来。像传统符号主义的蒙特卡洛树搜索，基于统计的深度学习（比如估值网络和策略网络），然后在这两者的结合之上再加上强化学习。现在看来，虽然这事是三者比较生硬的结合，但已经取得非常大的成绩。再下面，就是看能不能把人工智能做的像人脑一样有效，不是各自独立的三块，而是在一起的。如何用一个机器模型就能同时做符号搜索，深度学习和强化学习这三件事，这是一项很有挑战但非常有趣的研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：那人工智能需要从神经科学领域获得更多灵感和线索吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;是的，确实可以获得很多的灵感和线索。蒲慕明院士在 2016 中国人工智能大会的演讲中介绍了很多神经科学的发现。首先，他们发现在生物领域也存在 BP 算法的现象。如果这个神经学的发现启发了人工智能的研究，那将就是一个完整的故事，但是，神经学的这个发现是在计算机领域提出 BP 算法之后发现的。今天，这个发现也会对人工智能有启发。其次，人工智能里的最小计算单元往往是同类型的神经元，但蒲慕明院士认为，人脑的神经元并不是都是同类的，而是每一类有各自专门功能的。如果我们在人造神经网络中设计这样一些神经元种类，也将是很有趣的研究问题。第三就是如何学习和计算一个「忘记机制」神经学发现，人脑是在进行有选择的忘记，而这种机制是智能必不可少的体现。但是，在我们人工智能的学习系统里，并没有特别设计这种忘记机制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，联接主义在人脑研究中大行其道的，但在计算领域并不是如此，深度学习可能是一个例外。但其他的——比如说符号主义的搜索——大部分都是孤立的，是单 CPU 大规模算法在进行，而不是并行，这些都是需要探索和发现的。但我们回来说，人工智能可以借鉴人类大脑，但不应该被人类大脑所局限。我们最后可能会发现，新的人造的智能结构，可能人脑也没有，（但可能外星人有）。所以，可能还有一些新的智能算法在等待我们来发现。如果真是那样，那也不错。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：对话系统是现在比较热的研究领域，科技巨头也都提出 bots ，目前在这方面还存在哪些研究难点吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;对话系统的一个难点是把目标引入，如果你只会聊天但不会实现目标，那就没有商业前景；另外一个目标是如何把规则和统计学习结合好，因为有些特殊领域是需要有规则来规范的。第三个目标是怎么样把个性化引入，这就是迁移学习所应该发挥的价值。如果把三者统一在一个系统里完整实现，可能还需要有很长的研究，如果能做出来，那就是解决对话问题的一个非常优美的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于迁移学习，去年 Science 那篇文章 Human-level concept learning through probabilistic program induction 里提到的单个例学习，即 one example learning。这实际上是一种迁移学习的做法，他们把一个问题分解成参数学习和结构学习两种，他们发现如果参数学习如果能够从别的地方迁移过来，那只做结构学习就可以了，而结构学习恰恰又特别好用，只需要一个例子就可以解决了。所以前面用了迁移学习，后面用了结构学习，就把 one example learning 实现了，是这样一个 trick 。这给我们带来一个很好的概念，就是说在对话系统中，你就可以把自然语言的结构学习和参数学习分开，采取分而治之的办法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：比如说在自然语言处理方面。那迁移学习应用自然语言方面会有独特优势吗？能实现不同语言间的迁移吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;可以实现不同语言之间的迁移。很多迁移学习的任务会比机器翻译的任务要简单，机器翻译需要很高密度的数据来对应每一句话，你要收集很多的平行语料，但是有很多学习任务并不需要做语言之间的关系，比如说分类、聚类，像这样不需要机器翻译的，就可以用迁移学习来建立两种语言（可以看成是两个领域）之间的共同表示，就是一个中性语言，通过这个中性语言进行迁移。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：Chris Manning SIGIR 2016 主题报告 Natural Language Inference, Reading Comprehension and Deep Learning 中有一页有一个形象的「压路机」比喻，列了深度学习在哪一年会对特定领域的传统算法进行「碾压」，比如说语音是在 2011 年、视觉是 2013 年、自然语言处理是 2015 年, IR 是 2017 年。您对此怎么看？KDD 应该在哪年？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KDD（数据挖掘）和 IR 是有区别的，IR 是赋予机器搜索的能力，自动化为主要代表，主体并不需要引入人，所以用机器学习比较合适。但是 KDD 的最终目的是为人服务，所以是离不开人的。因为 KDD 和数据挖掘中没有人，是全自动的话，那就是机器学习了。所以，如果是为人而发掘知识、为人做解释，就需要比深度学习更多的东西：虽然里面很多东西可以用深度学习来解决，但深度学习里有很多东西是不可解释的，所以从这一点上来，仅仅用深度学习来做数据挖掘说是不合适的。在和人打交道这一方面，深度学习不可能碾压 KDD 。数据挖掘是为人做数据分析的辅助工具，而机器学习则是力图模拟人的行为。对于两者的区别，我做过一个比喻：你训练一只狗，若干年后，如果它忽然有一天能帮你擦鞋洗衣服，那么这就是数据挖掘。如果有一天，它化妆成狼外婆跑了，那这就是机器学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，深度学习确实可以取代某些算法。另外，我觉得 KDD 和 IR 的基因还是不一样的，这得看原领域和深度学习的重合度：做 KDD 研究的很多人是从数据库过来的，他们是的目的是管理信息，这就不能仅仅引入机器学习；同时，KDD 的有些人是机器学习过来的，他们可以引入深度学习。但，也有心的问题：那就是模型的可解释性怎么办？如何向人类解释模型的功能和结构？因此，KDD 为深度学习引入了这样一个契机——不是深度学习碾压 KDD，而是 KDD 和深度学习一起来发挥作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：机器学习对基础科学研究有什么重要的推动和价值？许多科学研究现在面对着海量的实验，观测数据，比如天体物理，粒子物理，生命科学，材料科学等，机器学习会在基础科学研究中发挥重要作用吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;机器学习对基础科学的发展应该有很大促进，在这些传统科学领域，很多人可能现在没有意识到人工智能可能带来的影响，但我们看到，深度学习的出现，只是计算机出现以来的数字革命中的一环。下一步到底要到哪儿去？是不是有可能要把科学家变成「数据民工」？比如说，把天文学家就变成操纵望远镜的天体数据的民工，把生物学家变成摆弄小白鼠的生物数据的民工？虽然这是一个未来可能出现的极端现象，但从计算机革命的角度来说，这个未来并不是不可能！当然，科学家可以去创造一些理论并去验证它，但这样的科学家的助手们可能会变成一些机器人。所以，整个科学研究会出现一个本质上的变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：戴文渊之前在介绍「第四范式 · 先知」平台时，提到这个平台的目的是让数据科学家「失业」，那人工智能会不会让科学家失业？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;人工智能会让很多二流科学家失业，一流科学家还是很安全的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;对人工智能从业者的建议&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您对目前行业内深度学习热有什么看法？年轻从业人员应该如何对待这种现象？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;深度学习过热不是一个坏事，我们也不应该拒绝。对于年轻人来说，大家要用平常心来看：这是一个学习算法，学习能力比较强，能够容纳更多的训练数据，我们发现它能做过去想象不到的事情，现在还是有很多红利去获取。所以年轻人如果要做的话，要尽量多动手，多编程，多了解内核的东西，而不仅仅把深度学习当成黑箱来用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您认为国内人工智能领域在科研和产业上还有哪些缺陷和不足吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我觉得国内的人工智能研究不能太跟风，对自己的研究理念和创新要有信心。在科研和产业都是如此，要创新，尤其是在大学的研究者，每个大学的教授应该是独树一帜的，自己领先一个子领域，而不是跟着别人去做。对公司的要求？公司要首先考虑生存，但在产业上也不要以为人工智能可以包罗万象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您有着天文学和计算机科学的复合背景，研究天文学的这段经历对您后续研究机器学习有什么帮助或者启发呢？您的物理学专业背景为您后来人工智能领域的研究工作重提供什么样的思维方式，思维习惯等方面的借鉴和帮助？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;最大的启发是知道一个问题要换几个不同的角度去想，物理学家特别容易这样，他观察一个东西，可能就联想到十万八千里，看到行星就想到原子。这种联想能力是物理学里特别流行，但在计算机领域不是这样，培养一个学生出来很好的编程，拿竞赛金牌，他都不一定有联想能力。所以我特别受益于这种训练，这可能不仅仅是从物理学来的，而是从跨领域来的，所以我建议年轻人可以接触最起码两个领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器之心：您有本关于介绍如何做学术研究的著作《学术研究——你的成功之路》，对于人工智能领域的学术研究，您能否给研究者提供一些建议？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;任何学科都有一个系统性，对于研究我提过五点，这个对人工智能也适用。第一个就是研究的问题有用，重要。第二是这个问题可以给专业外的人都能说清楚，能自己很简洁地表达出来，能讲明白。第三个是要说清楚这个问题为什么难，就是问题到现在还没有人做过，不知道怎么做。第四点是，虽然问题还不知道怎么做，但你知道怎么把问题进行分解，分成一段一段来做，每个阶段都有一点进步，就是现在网络上说的「小目标」。第五是得有数据来验证你的想法，否则就是空想。这五个条件对学术和商业都适用。如果你觉得一个研究特别好，但你没有办法拿到数据，那你一开始就不要花时间做。另外，补充一点特别重要的，大家要明白别人做过些什么，要看很多论文，并能对过去的工作有所批判。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：您在学术界和产业界都取得了非凡的成绩，这和日常的时间规划、研究技巧和学习方法密不可分，能分享一下这方面的经验和心得吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;杨强：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;时间规划，研究技巧和学习方法，这些都会因人而异，但是，学术和工业的成功有一个共同点，就是——我特别受益于锻炼身体，再忙也要抽出时间来锻炼身体。中国的学者到国外去，要给人一种很健美的形象、要有精神。而且我们会发现如果我们经常锻炼身体的话，很多时间规划的问题就迎刃而解了，因为锻炼之后你会发现头脑特别清楚，会注意到很多细节，分清楚事情的轻重缓解，之后就特别容易去做了。总之，在锻炼身体之后，以前你觉得特别难的问题都不会觉得是问题了，原来觉得特别烦恼的事情也没有了。这是给大家的一个建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.c&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 12 Sep 2016 16:38:47 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 谷歌 NIPS 2016 提交的8篇论文：从无监督学习到生成模型（附论文下载）</title>
      <link>http://www.iwgc.cn/link/2660001</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;2016 年度神经信息处理系统大会（NIPS 2016）将于今年 12 月份在西班牙巴塞罗那举办。谷歌近期公布了他们被 NIPS 接收的一批论文，共有 12 篇，其中 4 篇尚未公开，此次机器之心将先介绍谷歌这 8 篇论文。&lt;span&gt;点击文末「阅读原文」下载全部论文&lt;/span&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;用于通过视频预测的物理交互的无监督学习（Unsupervised Learning for Physical Interaction through Video Prediction）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Chelsea Finn, Ian Goodfellow, Sergey Levine&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：代理（agent）学习与世界交互的一个核心挑战是预测行动如何影响环境中的物体。现有的学习物理交互中的动力学的很多方法需要有标注的对象信息。然而，要将真实世界的交互学习的扩展到多种场景和对象中，获取有标注的数据变得越来越不切实际。为了在无标注的情况下学习物理对象的运动，我们通过从前面的帧来预测像素运动分布的方法，开发出一种以行动为条件的视频预测模型（action-conditioned video prediction model），其能明确地对像素运动建模。因为我们的模型可以明确地预测运动，所以它相对于对象外观是部分不变的，这使它可以归纳之前没看到过的对象。为了探索用于真实世界交互代理的视频预测，我们还引进了一个包含 50,000 次机器人交互的涉及推这个动作的数据集，包括一个带有全新对象的测试集。在这个数据集中，对以机器人未来动作为参照条件的精确视频预测达到了学习在不同行动过程基础上对未来进行「视觉想象（visual imagination）」的程度。试验发现，与之前的方法相比，我们提出的方法不仅产生了更精确的视频预测，还更精确地预测到了对象运动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;DeepMath——用于前提选择的深度序列模型（DeepMath - Deep Sequence Models for Premise Selection）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Alex A. Alemi, Francois Chollet, Geoffrey Irving,Christian Szegedy, Josef Urban&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们研究了自动定理证明（automated theorem proving）中用于前提选择的神经序列模型（neural sequence model）的有效性——自动定理证明是数学形式化（ formalization of mathematics）的主要瓶颈之一。我们为这一任务提出了一种两段式方法，该方法能在 Mizar 语料库上的前提选择任务上得到很好的结果，同时还能避开当前最佳的模型的人工设计的特征。据我们所知，这是深度学习第一次被用于定理证明。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;域分割网络（Domain Separation Networks）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Konstantinos Bousmalis、George Trigeorgis、Nathan Silberman、Dilip Krishnan、Dumitru Erhan&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：大规模数据收集与标注的成本使得机器学习算法应用于新的任务与数据集时成本高的惊人。一种避免这种高成本的方法是在合成数据上训练模型，其中在模型成功被应用之前，标注域适应算法（ annotations domain adaptation algorithms ）能巧妙的处理这些模型。现有的方法要么集中在从一个域到另个域的表征映射，要么集中在学习提取对该域而言恒定的特征。受到私密共享组件分析（private-shared component analysis）上成果的启发，我们可以很明确地学习提取可划分为两个子空间的特征：一个组件对每个域是私密的，一个组件在所有域间共享。我们的模型被训练我们在源域（source domain）中关心的任务，但也使用分割的表征重建来自两种域的图像。我们的新架构得到的模型的表现超出了许多当前最佳的无监督域适应情境（unsupervised domain adaptation scenarios ），另外还能产生私密和共享的表征的可视化，从而使得我们能够解释该域适应过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通过瞬态混沌在深度神经网络中的指数级表达力（Exponential expressivity in deep neural networks through transient chaos）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们将黎曼几何与高纬度混沌的平均场理论（ mean field theory ）结合起来，学习带有随机权重的泛型的、深度神经网络的信号传播的特性。我们的结果显示出一个 &amp;nbsp;order-to-chaos expressivity phase transition，其中混沌相中的网络计算非线性函数，这些函数的全局曲率（global curvature）随深度呈指数级增长，而不是宽度。我们证明深度随机函数的这一泛型类别不能被任何浅网络有效计算，这超出了先前研究单函数分析的限制。此外，我们正式且量化的证明了这一长久的推测：深度网络能将输入空间中的高度曲率流形（ highly curved manifold）解（disentangle）到隐藏空间的平面流行中。我们对深度网络表达力的理论分析能广泛地应用到任意非线性上，并提供之前关于深度函数几何学抽象概念的量性基础（ quantitative underpinning）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;对神经网络更深度的理解：初始化的力量和表现性上的双重视角（Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Amit Daniely, Roy Frostig, Yoram Singer&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们开发出了神经网络和 ompositional kernels 之间的一种通用二元性，这对更好地理解深度学习大有裨益。我们发现由通用随机初始化生成的初始表征能够足以表达对偶核空间中的所有函数。因此，虽然在最糟糕的情况下训练目标函数很难优化，但初始化权重还是为优化形成了一个良好的开端。我们的二元视角（dual view）同样显示了神经网络的务实性（pragmatic）和审美性（aesthetic），也突出了神经网络的表达能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;出现、推断、重复：使用生成模型的快速场景理解（Attend, Infer, Repeat: Fast Scene Understanding with Generative Models）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey E. Hinton&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们提出了一个用于结构化图像模型（可以对目标进行明确的推理）中的有效推理的框架。这种方法是通过使用一个循环神经网络来执行概率推理——该循环神经网络可以处理场景元素且一次处理一个。关键的是，该模型自身可以学习选择合适数量的推理步骤。我们使用这种学习方案来执行部分特定的 2D 模型（可变大小的变自编码器）和完全特定的 3D 模型（概率渲染器（probabilistic renderers））中的推理。我们的研究证明这样的模型可以在没有任何监督的情况下学习识别多个目标——计数、定位和分类一个场景中的元素，比如：在神经网络的单次前向通过中分解带有多个目标的 3D 图像。我们的研究进一步表明，相比于监督式的方法，我们的网络可以产出更精确的推理，而且它们的结构也可以使归纳得到进一步的提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;标题：A Neural Transducer&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Navdeep Jaitly, David Sussillo, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, Samy Bengio&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：Sequence-to-sequence 模型在很多任务上已经达到了令人瞩目的效果。但是，有两类任务是不适合的，第一种是当更多数据进来时需要进行增量预测的任务；第二种是有很长的输入序列和输出序列的任务。这是因为它们以整个输入序列为条件来生成输出序列。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在本篇论文中，我们提出了 Neural Transducer ，当有更多输入进来时，可以进行增量预测，而不需要重新做整个计算。不像 sequence-to-sequence 模型，Neural Transducer 计算是基于部分观察到的输入序列和部分生成的序列来计算下一步分布。在每一步，transducer 可以决定对许多输出符号（output symbols）输出零。数据通过一个编码器进行处理，然后成为 transducer 的输入。在每一步都发出一个信号的离散决策使得用传统的 BP 算法学习就变得非常困难。但是，通过动态编程算法来训练 transducer 生成目标离散决策是可行的。我们的实验展示了，Neural Transducer 在处理数据输入时的输出预测时表现得很好。我们同时发现，Neural Transducer 在长序列上也表现得很好，即使不使用注意力机制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;标题：深度学习游戏（Deep Learning Games）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作者：Dale Schuurmans, Martin Zinkevich&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们研究了在玩游戏上的监督学习的减少，这揭示了新的连接和学习方法。对于 convex one-layer 问题，我们的研究证明了训练问题中的全局极小化器（global minimizer）和简单游戏中的纳什均衡的对应关系。然后我们证明该游戏可以如何被扩展成带有微分凸门的一般非循环神经网络（general acyclic neural networks with differentiable convex gates），从而在纳什均衡和深度学习问题的关键（或 KKT）点之间建立了双映射（bijection）。我们基于这些连接研究了替代性的学习方法，然后发现后悔匹配（regret matching）可以实现有竞争力的训练表现，同时还能得到比当前深度学习方法更稀疏的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载全部论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 12 Sep 2016 16:38:47 +0800</pubDate>
    </item>
  </channel>
</rss>
