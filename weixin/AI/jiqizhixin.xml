<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>深度 | MIT量子专家Seth Lloyd：量子计算更擅长机器学习，发现传统计算无法发现的数据模式</title>
      <link>http://www.iwgc.cn/link/2715048</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Edge.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、Rui Sun、李亚洲、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;让我们来思考下量子计算的未来，我不知道将来的每一台智能手机里是否都有量子计算机，或者我们是否会拥有量子 app 或者 quapps，从而借助量子计算机使我们的通讯更加安全，并且帮我们找到一些有趣的东西。这是一个很难完成的任务。很可能在计算机和智能手机中将拥有量子微处理器，完成特定的任务。&lt;/span&gt;&lt;br/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;而这也是我们这些设备内部的相关技术发展的方向。如果有来自量子力学的优势，那我们就要采用这些优势，就像光合作用中的能量流动带有量子特性一样，如果量子计算的「怪招」能帮助我们，那就尽管使用一下这种「怪招」好了。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=j03285o919x&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen="" style=" width: 556px;  z-index: 1; "&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;em style="color: rgb(136, 136, 136);"&gt;&lt;span&gt;SETH LLOYD，教授，量子力学工程师，MIT&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的重要性——一切自然界行为的通用语言&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前，研究的兴趣又重新回到将量子力学和量子信息应用于量子引力学理论（译者注：量子引力，是对引力场进行量子化描述的理论，属于万有理论之一。研究方向主要尝试结合广义相对论与量子力学，为当前的物理学尚未解决的问题。当前主流尝试理论有：超弦理论、循环量子引力理论。引力波的发现，为量子引力理论提供了新的佐证。）上，以及探索宇宙本质的基础理论。事实证明，量子信息会给在苦苦寻求这些问题答案的人们带来很多帮助，例如，你掉进黑洞时会发生什么？如果你掉进一个黑洞，会有任何关于你的信息逃离此黑洞吗？这些就是斯蒂芬霍金等专家研究了几十年的问题。事实证明，量子信息学对我们找到这些问题的答案大有益处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;25 年前，我开始研究量子计算的相关问题，也就是原子、分子、光子和基本粒子如何处理信息。那时全世界研究这个问题的人也不过五六个，而现在有成千上万人。在任何一个快速扩张的领域都会出现各种分支。对于如何理解世界的基础问题的研究依然有很多分支，在其如何处理信息方面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，还有很多了解自然运行方式的实际问题。例如，过去十几年关于光合作用的研究已经非常清晰——光粒子来自于太阳，然后被叶绿素分子吸收，能量在一片树叶中形成并转移到更多树叶中——这是以一种非常量子力学的方式来进行的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们研究量子计算所使用的一些模型也恰好可以用来解释光合作用的原理。事实证明，光合植物、细菌和藻类所使用的量子力学都极其复杂。它们会利用到量子相干性和量子纠缠这样的影响，实现高效的能量传输。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我对此的观点是，如果一点量子「诡计」就可以帮你更快的繁殖，那你一定要使用量子「诡计」。结果显示，植物、细菌和藻类已经使用量子诡计超过了 10 亿年来让它们生存的更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，在量子信息和量子计算领域正在发生的是，量子信息是一切自然界行为的通用语言这一点越来越清晰。几年前，Physics Today（这是美国物理学会物理学家的杂志）里有张插页，这不是一张非常性感的插页，但它拥有物理学的所有部分，包括高能物理学、固定物理学、弦理论、力学物理和纳米物理等。而放在正中央的恰恰是量子信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原因在于，这个插页展示了物理的那些部分会和其他部分产生联系，这个领域的谁和其他领域什么人能对上话。他们把量子信息放在中间是因为每个人都在和量子信息进行对话。因此，研究光合作用的物理化学家突然开始和我这样的研究者开始对话，并且开始一起做植物和细菌的实验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在进行人工实验。在我们的案例中，它们是模仿光合作用中高效能量传输机制的人造和细菌造的系统。事实上，借助于量子信息理论，我们已经构建了比以往更加高效、甚至是有史以来最为高效的自然发生系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的进展和迅猛发展——更擅长机器学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与此同时，除了所有这些理论发展，我们在开发处理信息的设备方面也取得了巨大进步，比如说在量子计算机方面。D-Wave 开发特定目的的量子计算机，而不是能够破解 NSA 代码，让其从心底感到恐惧的通用量子计算机。当然，如果 NSA 有心脏的话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些系统正在迅猛发展，用户购买它们并尝试在解决困难问题方面是否比传统计算机更快。这个问题尚无定论，我们不知道它们是否比传统计算机快。同时，还有些人在开发超导系统、由原子或离子组成的系统，以及光学系统，他们在建构量子计算机方面做的更好，期待着在未来 5-10 年出现能够解决传统计算机永远无法解决的问题的量子计算机。想到关于量子计算机的新想法是令人激动的时刻。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子模拟计算机是一个旧思路，源自于 Richard Feynman，你可以使用量子计算机模拟其他量子系统。20 年前，我写了第一个关于如何编程量子计算机的算法，从而探索量子系统如何运行。在接下来几年，我们打算弄一些设备，能让我们建立量子力学模拟，也就是在内部黑洞发生了什么。我们可以关注下这些模型能做到什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几年前，我的一些朋友和我使用小型量子计算机模拟时间旅行中发生的事情，因为时间旅行的理论内在是量子力学的。当你把一个光子传送回十亿分之一秒之前，并杀死其前身。恩，我们的实验就测试了这样做会发生什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很幸运没有防止虐待光子协会的存在，因为我们的实验杀死了大量的光子。结果证明，一个光子回到过去杀死自己的前身总是会失败，因为时间旅行的量子理论表明你不能回到过去并做一些自相矛盾的事，比如杀死自己。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算如今最有趣的应用是映射传统计算。如今传统计算中最大的进展是编程机器学习，采用计算机处理大量数据，搞清楚里面的模式。NSA 使用这些监控我们，谷歌也使用这种计算监控我们，亚马逊也是如此。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在机器学习中，生活在大数据时代我们毫无秘密。人类每天生成阿佛加德罗量级的数据。谷歌、亚马逊、微软这样的公司正在处理这样的数据，发现我们生活中的方方面面，从而向我们出售产品。计算机越来越擅长处理数据，发现其中的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子力学系统有着这样的特征，能够生成传统系统难以生成的模式。结果表明量子计算机能检测并识别传统计算机难以检测的模式。比如，如果你有过去 50 年中道琼斯的逐笔交易数据，这就是一个大数据集。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你说，「如果我能经受一定量的损失，或者我想要有一定的回报，我处理这些数据发现对我而言最好的投资组合。」好，用一个相当小的量子计算机（在接下来 5 年或者更长时间将会有这样的计算机），你就可以发现比在传统计算机上得到的更准确的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机通过在更微观的层次上存储和处理信息而运行。例如，如果你有一个电子，你可以让它像 0 一样旋转，你也可以让它像 1 一样自旋，你也可以让它同时 0 和 1 的存在，这是量子计算的主要特征。一个量子比特，qubit，能同时是 0 或 1；这也是为什么量子计算比传统计算强力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;过去的 20 年或更久的时间内，我的同事和我一直在使用电子、光的例子建立量子计算机。所以，一个电子在电场中像 0 一样的摆动，也可以像 1 一样摆动，也可以同时 0 和 1 一样的摆动。我们已经建立了这样的量子计算机和量子通信系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是一个理论家，所以实验主义者不像我在实验室中那样使用螺丝刀，我会破坏东西。但我与实验主义者紧密合作了超过 20 年，建立这样的设备，他们用小型的，有几个量子位的设备开始，但结果证明要有一把量子位才足够演示量子计算的功效。量子计算机如今变得越来越大，我们有了十几位的，不就将会有 50 个量子位的，然后是 500 个量子位。因为如今我们有建立大规模量子计算机的明显路径。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统计算机遵循著名的摩尔定律，但它不是自然界遵循的定律。它只是对科技进展的一种观察，每两年计算机组件小一倍，组件的数量就翻倍。量子计算机不遵循摩尔定律。原因是建立量子位，并把它们组合在一起是一个复杂的过程。你是在微观的级别上操作的，很难做到这一点。你不需要要精准的进行控制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一个与常见摩尔定律平行的摩尔定律。事实上，它才符合量子理论。随着时间前行，我们在微观层次控制事物的能力越来越好。控制事物的同样能力让我们能做越来越强力的量子计算机。我们的量子计算机与传统计算机相比如今仍是无用的。我记得我以前有一个 16k 存储的计算机，几年后就是 64 K 了，如今是 100 G 的存储或1个TB的存储。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机仍处于只有少量量子位的阶段——可使用的 10 量子位，很快有 50 量子位，然后 100 量子位。尽管与传统计算机相比不具优势，但因为对特定问题量子计算机比传统计算机更强大，这意味着在接下来 5 到 10 年，一旦我们做到 几百量子位（很快就会发生），我们将能够解决传统计算机无法解决的难题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;解决什么样的难题？比如说一个 500 量子位的计算机将能够分解大型数字的因子、破解密码、打破对 NSA 监控的恐惧。但它也能做一些类似监控的事，比如量子机器学习，发现大量数据中的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;去年 12 月，我在 NIPS 大会上就量子机器学习组织了一个量子会议。我们预期数十人参加这个会议，但最后却有 150 人参与，以至于我无法进入会场。传统机器学习领域的人总是在观望新的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;他们惊人的发现机器学习难题像是一些事的拓扑学一样，想搞清楚一堆数据中空洞的数量。你知道，拓扑学研究事物是否有孔洞或者缺口或者空洞或者链接组件，这是分析数据的人想要发现的世界的特征。但做这些事的传统算法虽然有效，但只限于小数量的孔洞，因为它们不能处理这样的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;相比之下，如果有一台小型的量子计算机，甚至是仅有几百个量子比特，那你就能发现复杂的模式和拓扑系统，就像你用传统计算机所永远不会发现的洞、缺口和缝隙。我们已经进一步一个全新的阶段。量子计算的第一个二十年是一些从理论中衍生出的非常有趣的想法，和物理学的其他分支建立联系，接下来将出现各种你很喜欢用的算法，只要你愿意拥有一台已经足够强大到去执行这些算法的量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们处在量子计算机正在变强大的边缘，它们能够进行这些分析，对其他量子系统进行模拟（传统系统无法做到），发现传统计算机无法发现的数据中的模式。我们即将进入一个令人激动的时刻，来迎接量子计算的到来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谁将马上拥有量子计算机？答案是每个人。MIT 有 5-6 个实验室都配备了量子计算机，研究者正尝试着对它们进行扩展使其变得更大。全球有几百只团队正在开发量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些实验室中的量子计算机非常有趣，它们看起来有些不同，取决于各自的用途。超导量子计算机的比特是超导电流的，以顺时针的方向在回路中转一圈——这是零；超导电流以逆时针在回路中转一圈——这是一；一个超电流一次往两个方向转，这很难想想，但确实会发生——这同时是零和一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机内部的设备其实是芯片。通过相对传统的方法将超导电路蚀刻在芯片上。然后，芯片会连接到外部世界进来的电线上，因为超导体必须被放置在氦稀释的冰箱里，绝对零度以上的千分之十五度。它放置在那里就像是一个啤酒桶，在冷却时会滴滴答答作响。然后你使用普通计算机与它进行对话。你用自己的键盘输入，这将信号发送到芯片上，然后芯片处理这些信号，并且通过自己不可思议的量子力学机制得到答案。这些事情目前还有些庞大，仅仅是因为需要被安置在一台稀释冰箱里。你不能把它放到膝盖上，因为它会压扁你。但它们现在已经如果紧凑到你可以把它放在你的办公室里，只要你想。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有一台特定功能的量子计算机，量子退火炉（quantum annealer），由 D-Wave 制造。这是一台商用设备，已经有不少人购买了。洛克希德马丁已经购买了 D‑Wave 的计算机，谷歌和 NASA，美国军方也是。他们购买的原因是这些设备很有趣。没人能够准确理解内部原理。它们非常神秘的以量子力学的方式进行计算，因为神秘就是量子力学的特点。用户正在购买这些设备应用于自己的领域，来尝试下是否能够解决哪些传统计算机无法解决的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 D-Wave 设备基于我和我的研究生 Bill Kaminsky 在 2002 年写的几篇论文。这是一种商业化的设备，如今已经有一些人买了。Lockheed Martin 买了一台 D-Wave 计算机，谷歌和 NASA 买了一些，军队也正在购买这种计算机。他们之所以购买时因为这些计算机很有趣。没人真正理解计算机内发生了什么，它们以自己的量子力学方式相当神奇的处理事情，因为保持神秘是量子力学的一个本质。人们买这些设备是想看下能否在上面解决传统设备无法解决的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们没能申请专利，D-Wave 继续前行并建立了这种设备。他们免费的使用了这些东西，为什么不能呢？又没有专利。当他们建立的时候，计算机确实不像预期的那样工作，在整个计算中保持低能态。达到更高的能量水平令人振奋，但它仍要解决该难题。为什么这样？没人知道。我与 D-Wave 的人一起工作过，想搞清楚为什么在不该成功的时候成功了。也就是从此，我为所有东西申请了专利，即使我不知道它们时候有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算的另一个有趣点依赖于量子光学和光。很多年以来，这些设备都很大，因为它们包含一堆的大型激光固定在光学台上，被百万张镜子覆盖。研究生在校准这些设备时要非常谨慎，以便于所有的光束以正确的方式存在。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个领域中有一个惊人的进展，因为基于电话通讯技术，如今人们能够将所有的东西安置在一个芯片上。你将一个足球场大小的光学台缩小到一个芯片上，所有的东西都在上面。然后用硅树脂细线蚀刻该芯片，光子沿着这些线跃升，彼此相融，彼此交互，然后从另一端出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些都是很伟大的设备，玩起来也很有趣。关于这些设备的一件事越来越凸显，过去的五六年，即使某种程度上这些设备很简单（也就是光通过一个芯片），但光子在镜面反弹并彼此互融，这些行为非常的神奇。如果你发送 20 个光子从小端口进入芯片，你问光子从其他 20 个端口出来的概率是多少，传统上这很难计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;没有人知道如何做到。然而，芯片能自动的做到。它能生成没人知道如何在传统计算机上生成的模式。它们有一些我们即使使用最大的传统超级计算机也无法生成的怪异的量子特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种设备的一种可能用途是学习。机器学习的一个共同特点是，如果你有一个可以生成一组特定模式的设备，它也能识别一组相同的模式。现在我们正在进行一项实验，来尝试一下能否让这一切发生。当我们拥有了用其中一个芯片生成的模式，那能否训练另一个芯片去识别那些模式？如果我们能做到这一点，那么我们就已经能训练一个量子设备去识别那些不可能被传统计算机生成或识别的模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些模式太诡异了。因为它们不能由任何传统设备生成，顾名思义，它们就像是你以前从未见过的东西。除了制造这些我们不知其形的 funky 模式外，量子计算机也可以做那些普通的机器学习任务，比如识别数据中的大规模模式——我们现在一直使用的日常功能，比如人脸识别、语音识别和字符识别等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你正在投资股票市场，一个非常重要的问题是，是否存在一些隐藏的按照某种模式的「动力」来驱动所有股票？如果你知道那个「动力」的话就可以赚很多钱。一台量子计算机可以比传统计算机更加有效地找到这种模式。一台量子计算机可以处理大量普通任务，即使是只有几百量子比特的小型量子计算机，也能做一些传统计算机做不了的事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然后，就像量子计算机能够发现这些传统计算机无法发现的模式一样，还有很多更加疯狂的事情。就识别功能而言，我不知道这些模式对什么有利，但是它们在涉及加密应用的问题上非常有用。比如用无人可以破解的方式去编码信息。如果你把个人信息与这些没有人可破解的模式放在一起，然后——上帝作证——没有人能够解密你的信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算和数字计算机的历史进程对比&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将量子计算的当前状态和过去二十年的进展与数字计算机进行对比是非常有价值的。建造一台数字计算机的想法是 20 世纪 30 年代中期由克劳德·艾尔伍德·香农（他那篇颇有影响的哈佛硕士论文的一部分）和和德国的克兰德·楚泽提出。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，当时第一台设备是在第二次世界大战期间开始建造的。直到上世纪 50 年代中期，人们才有了这些巨大且非常昂贵的设备。他们中很少有人会花很多钱去开发它。极少量的比特就要花费大量工作，而且它们常常崩溃。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;建立一台量子计算机的想法是我在 1993 年提出的。在那之后不久，人们开始建造简单的量子计算机。这是很难的，就跟我们开始建造传统计算机的头 20 年一样难。现在我们正处于这个阶段——拥有一个房间大小的量子计算机，以及照顾它们的穿白大衣的实验室技术人员。它们难以操作，会发生故障，而且只有几十个量子比特；但是毫无疑问，我们正在取得进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有趣的是，有关计算机有这样一句「如果你建造了它们，它们就会来。」我的许多 MIT 资深同事都曾参与早期的计算机研究，比如马文·明斯基（Marvin Minsky）、鲍勃·盖勒格（Bob Gallager）。当他们告诉我过往的美好岁月时（因为他们喜欢自己的工作），他们遇到的其中一件事情是，计算机科学在 20 世纪 50 年代的起源是非常令人兴奋的。但当拥有了一台可以在上面运行算法的设备，即使它体格庞大而且在今天看来是令人难以置信的脆弱不中用，就立刻出现了巨大变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在之后的短短几年内，人们一旦开发出第一台可以运行程序的计算机，这些计算机科学家先驱们——在当时来说甚至都没有被叫做计算机科学——就开发出了许多今天我们所知道的最强大的方法，比如说蒙特卡洛和单纯形法，这些算法如今被我们广泛应用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的未来——这是一个激动人心的时刻&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是一个非常激动人心的时刻，它令人兴奋，因为这些从事理论工作的聪明人突然有了一个可以玩的玩具。他们很快想出了与这个相当昂贵的玩具相关的大量有趣游戏。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算领域现在正处于这样一个阶段。我们有这些玩具——这些复杂、不是那么强大的量子计算机，但我们可以用它来玩游戏。我们可以尝试人们遇到的问题，可以看看会发生什么。人们正在提出一些非常有趣的游戏。因此，对于像我这样的人来说，量子计算非常令人兴奋，因为这个领域里充满了拥有奇思妙想的年轻人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我所认识的一些世上最杰出的青年科学家都被吸引到了这个领域，因为它很有趣：你可以玩有趣的游戏；问题很大；你可以发问有关宇宙本质的问题。比如你可以问：我能否识别出一个潦草的 5 或 7？然后你可以与人合作并说：「嘿，我有主意了，我们可以试试吗？」你走在麻省理工学院的走廊上，而有人说：「是的，我们可以尝试一下。让我们看看会发生什么。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;仅仅从智力游戏的角度来看，现在的量子计算领域是一个非常令人愉快的地方。对我来说它很伟大，因为我可以与这些比我聪明得多的怪咖们共事，而这也是一个很大的乐趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们来思考下量子计算的未来，我不知道将来的每一台智能手机里是否都有量子计算机，或者我们是否会拥有量子 app 或者 quapps，从而借助量子计算机使我们的通讯更加安全，并帮我们找到一些有趣的东西。这是一个很难完成的任务。很可能我们的计算机和智能手机中将拥有量子微处理器，完成特定的任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其中的原因很简单：无论如何这都是我们设备内部的实际技术的前进方向。如果量子力学能够带来什么好处的话，我们就会利用它们，正好同能量在光合作用中流动所采用的一种量子力学的方式相同。如果量子「把戏」中有什么好处的话，那么就是它本身。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;技术不是中立的。事实上技术的主要用途之一，只是让富裕而强大的公司利用技术来利用普通人，这是一种自然的经济运作方式。其他一群人失去了他们的工作。有关技术的一件非常自然的事情是，它令一些工作做起来更高效、更容易的同时，也意味着做这些工作的人最终会有更多的工作，因为你的雇主会让你做更多的事情。然后就会产生一群失业者。技术可以使事情变得更有效，但它不一定会使我们的生活变得更容易或更美好。事实上它往往使我们工作更加困难，这是我所反对的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我发现最简单的是告诉人们进展的真相，而这对他们也有好处。有一些财富五百强公司正在大力投资量子计算。IBM 、微软、谷歌、英特尔等，日本的 NEC 已经向量子计算投了一大笔钱，相当多的公司都决定去投资这一领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当他们问我，「我们是否很快就能有一台真的能生产出来并出售的量子计算机？」我说，「好吧，也许并不。」尽管我们现在离这个目标更近了。事实上随着一些新的技术进展，特别是超导量子计算和光学量子计算，我们很有可能将拥有人们能生产出来并出售的量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌、IBM、微软或英特尔这样的公司有一个很好的理由去投资量子计算。这是一项拥有极大前景的技术，即使是在当下，它也不是那个包含在日常的智能手机中的东西。这个理由与计算的一般特性有关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当人们第一次建立起这些体育馆大小的巨型计算机并将其置于体育馆中时，他们对于这些计算机的用处没有丝毫线索。他们在想，「哦，我们会用它来分析类似炮弹轨迹、材料属性这样的东西。」但我们在过去的几十年里都经历过的一件事是，计算机已经做到了那些你从来不会认为它们能做的事情。此外信息处理技术以一种无人预料的方式下爆发了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在仅仅谈论计算机是没有意义的，因为一切都是计算。你的智能手机是一个非常强大的计算机。你的汽车发动机包含 20 至 50 个微处理器，它们一直在进行计算，而这是实现更高燃料效率和污染控制的秘籍，诸如此类。它也被证明是污染控制欺诈的手段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大量设备中有着计算，也存在着信息处理。你碰触的所有东西几乎都可以以一种复杂的方式处理信息，这如今变得很常见。如果你的公司是做信息处理的，那知道接下来的进展就非常重要了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;量子计算的军备竞赛&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 长期投资于量子计算。刚开始时，也就是 20 多年前，他们就开始强力投资量子计算。这是因为他们有两个该领域的建立者 Rolf Landauer 和 Charlie Bennett 帮助他们开发量子计算。那时发生惊人的事情是件很明显的事情。即使他们没有投资 10 亿美元，也每年投资千百万美元做量子计算研究。我不知道具体投资是多少。结果是，他们有世界上最好的、最聪明的人为他们工作，研究这个话题，这些人知道发生了什么，用自己建立的量子计算机搞事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对其他公司而言也是如此。这些公司是年轻工作者极棒的工作地方，也是产出新思路的主要场所。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 也与量子信息处理有及其紧密的关系。但由于 DARPA 的本质，它们的项目经理总是想要来个全垒打。而且，从一开始就很明显，量子计算是一个有全垒打潜力的技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是首个政府拨款的量子计算项目的首席投资者，这是 DARPA 1994 年的一个项目。Jeff Kimble 带领这个团队。DARPA 当时意识到量子计算是他们需要关注的事情。事实上，在过去 20 年中，DARPA 已经投资了量子计算的不同方面的多个项目，其中的很多项目都是成功的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算中的很多基础进展都以各种形式受到过 DARPA 的资助。虽然不知道 DARPA 的老大怎么想，但我认为这是一件好事。而且一个特定项目最终开发出的东西经常与他们一开始想要做的不同。但结果证明，这些项目出来的一些衍生成果非常强大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 是第一个认识到量子力学在光合作用（photosynthesis）中扮演重要角色的基金资助机构。他们创造了第一个资助研究光合作用和能量转换中量子相干性和量子纠缠这样的特殊影响的项目。这是一个很成功的项目，得到了美好的成果。我正在研究项目的一些衍生品是在能量转换上要比自然中更加高效的人造和细菌造的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 染指了很多研究，在量子研究上也上下其手。它开发了量子计算的很多基础思路。IARPA 衍生于 DARPA，它也是量子信息处理前沿的主要投资者。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为它成长的太快，因为它在众多领域都有所影响，如今在量子计算和量子信息处理方面有很多的子领域。有许多技术员正在建立量子计算机，其中有很多卓越的人才。在超导量子计算机领域的人才有：被谷歌聘请的 John Martinis；我在 MIT 的同事 Will Oliver；在 Delft 的团队。然后，也有很多人在极力关注新型量子算法这样的疯狂想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我在 MIT 的同事 Soctt Aaronson（Shtetl-Optimized 上的知名博主）有着非凡的想法。他和他的同事正在绘制你可以在量子计算机上解决的一系列问题，这是一项很棒的工作。要建一个量子计算机，其中最成功、最强力的设备是离子阱。你可以采用一堆离子、原子，剥去其中的电子，使用陷阱诱捕它们，然后用激光摧毁它们。我在马里兰大学的同事 Chris Monroe 是该领域的先驱。因斯布鲁克大学的 Rainer Blatt 也在这上面做出了惊人的成就。MIT 的 Ike Chuang 在建立这种设备上也做出了大量的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于量子信息我最喜欢的部分是我们所说的狂野的东西，也就是「Hey，让我们了解下宇宙如何产生的，从量子信息的角度想下它是如何组合在一起的。」考虑下量子引力，依据量子信息没人能理解量子引力这种东西，这也是我已经做了 15 或 20 年的东西，而且如今也有不少人在研究它。这相当有趣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大神级人物包括 Caltech 的 John Preskill 和 MacArthur 奖获得者 Alexei Kitaev。他们两个都是很棒的人，在这个领域都取得了巨大成果。如同我所说的，量子信息领域的一大特色就是这些高素质的年轻研究员。刚被斯坦福聘请的 Patrick Hayden 在研究量子力学和量子引力的问题。Brian Swingle 在还是 MIT 的研究生时就想出了量子引力和量子信息之间其中的一个主要连接，仅靠自己他就做到了这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;中国是量子信息竞赛中的后来者，大约是 4 年前开始的，他们在清华建立了一个研究量子计算的机构。这个机构很优秀，他们也在做很伟大的事。中国做量子信息的也有一些很优秀的实验者，比如潘建伟。新加坡国立大学在量子信息处理上也有很多惊人的项目，他们也是该领域的领军队伍之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;日本量子信息领域也有很多优秀的研究员。比如我的同事 Yasunobu Nakamura 和东京科技研究所的团队。NEC 在这里也有一个很厉害的团队，有很多优秀的人。研究量子计算的最大团队集中在加拿大滑铁卢的量子计算研究所。在这里，黑莓的创始人 Mike Lazaridis 捐赠了百万美元作为用来创造优秀的研究团队的种子基金。加拿大的这个研究所的领头者是 Raymond Laflamme。这是现在世界上最大的量子计算研究的集中地。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在欧洲，有很多非常棒的量子团队。维也纳大学就有一群非常出色的人，Anton Zeilinger 和 Philip Walther 在那里待了很长一段时间；在牛津的 Vlatko Vedral 和剑桥的 Richard Jozsa 的领导下，这两个学校也有很多很棒的项目；还有牛津的 David Deutsch，他正是这一领域的创建者。你很难见到他，因为他总是在夜晚出没。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这几年，我与 David Deutsch 交谈过多次，其中有一段特别有趣的对话。当我在 MIT 准备一个会议议程的时候，David 在视频链接里出现了：我坐在大会议室的第一排，正对着 40 英尺的大屏幕，而 David 那 40 英尺高的脑袋就在屏幕上对着我讲话。会议室里没有任何其他人，我们也仅仅是讨论一些物理问题，但那感觉简直就像在与《绿野仙踪》里的巫师对话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;David 是一个非常聪明的人，也是一个思想家。他第一个意识到量子计算机可以做到一些传统计算机无法做到的事情，而这是在 80 年代中期发生的事情。他花了很多时间寻找量子计算机能够在某件事上表现得更好的例子。他有这方面的直觉，并且随后提出了量子计算机的正式概念，但是在五年多的时间里，他没有找到能够证明量子计算机能表现得更好的例子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当他最终有所斩获的时候，他向众人展示了，在一个传统计算机需要两到三步才能解决的问题上，量子计算机只需要一步。这确实是一个进步，但那个问题本身不是大家关心的问题。尽管如此，他并没有放弃，而是一直在寻找更好的证据。最终，凭借着自己的才华与意志，他终于让整个学术界意识到了量子计算机的重要性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随后，其他人开始研究这一领域，并且产生了许多更加有用的想法。量子理论完全改变了寻找函数周期的问题，随后 Peter Shor 借此提出了著名的因子分解与密码破解算法。接着，这一领域就开始了科研竞赛。在过去的二十年，自量子计算的复兴以来，自 Shor 在 1994 年提出算法以来，量子计算已经从几个人的规模，壮大成了拥有几千人、多方向的领域。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;而在这整个时间段里，David 不改初心，仍旧在他认为最重要的事情上不断钻研。在刚过去的十年间，他在他自己称为「量子构造理论」领域进行研究。根据我的不完全理解，他试图从基于量子计算理论中，衍生出现实世界最本质的东西。祝愿他最后能够成功。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 专访MIT CSAIL实验室首位女性主管：从7大领域谱写计算的未来</title>
      <link>http://www.iwgc.cn/link/2715049</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Forbes&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者：Peter High&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick R&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); font-size: 12px;"&gt;Daniela Rus 是麻省理工学院计算机科学与人工智能实验室（Computer Science and Artificial Intelligence Laboratory/CSAIL）首位女性主任，同时也是 MIT （Andrew and Erna Viterbi）工程学院的电气工程与计算机科学专业教授，并获得了 2002 年的美国麦克阿瑟学者奖 。&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以研究范围和成员资格作为衡量标准，MIT 的计算机科学与人工智能实验室（CSAIL）是最大的校内实验室。超过 250 家公司经由 CSAIL 孵化，包括 Akami、iRobot、3Com 和 Meraki。CSAIL 的研究分为 7 大重点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;计算生物学&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;图像与视觉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言与学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;计算理论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;机器人技术&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;系统&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Daniela Rus 是麻省理工学院计算机科学与人工智能实验室（Computer Science and Artificial Intelligence Laboratory/CSAIL）主任，同时是 MIT （Andrew and Erna Viterbi）工程学院的电气工程与计算机科学专业教授，并获得了 2002 年的美国麦克阿瑟学者奖 （MacArthur Fellow）。她是 CSAIL 的第一位女性领导，这一特质被她用来帮助鼓励其他女性追随她的步伐，进入到实验室所重视的领域。从她的岗位来看，她已经能够见证并影响一些正推动着当前数字革命的技术的上升趋势，所有这些内容在我们的这次访谈中都有涉及。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Peter High：你可以介绍下你的实验室的背景吗？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Daniela Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;50 多年来，从第一个分时（time-sharing）系统和第一个计算机密码，到公共密钥加密（public key encryption）和自由软件（free-software）运动，CSAIL 的研究已经推动了计算的边界，并在数字革命中扮演了一个重要的角色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibXQicp9a6UAKubCcTY5EfXbXOLqyeA1hMWicRVObWKMA7BSWgFnvVZiaXILPK9t2SKXUcV4BuFDSGuw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;MIT CSAIL 主任 Daniela Rus&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 拥有超过一千名成员，其中包括五百个博士生和博士后，这使它成为 MIT 最大的跨学科研究实验室。成员们涵盖了从像我一样的机器人专家，到数据安全、计算生物学、软件设计和预测分析学专业的专家。这种兴趣的不同组合让实验室可进行重要的跨学科研究，我们相信这种研究将对全球产生重大影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 启动的第一个项目是「Project MAC」，该项目认为两个人可以同时使用同一台计算机，而该机器大概有一个房间那么大。这令我感到吃惊，我们在短短的 50 年间，从多人共用一台机器的幻想走向了一个计算不可或缺的世界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:CSAIL 的目标是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的目标是谱写计算的未来。我们想用计算机科学来处理医疗和教育等领域的重大挑战，从创造更好的医疗诊断工具，到开发零事故汽车，以及鼓励儿童学习编程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CSAIL 的创建者们着手实现了多用户同时计算，并将它看做是人类能够使用机器来增强自身智力的第一步。这就是 CSAIL 一直以来所追求的目标。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，计算方面最紧迫的问题总是在发生变化。计算机已经缩小到了口袋大小，如今它们存在于我们的手机、汽车、电视机——甚至洗衣机里！新的挑战轮番上演，思考着如何使计算机变得更好、更强、更能干。我们还希望利用计算去解决世界所面临的重要问题，例如在医疗保健、教育和隐私方面。我们努力把科幻小说的世界推向科学，继而推向现实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;High:你如何安排实验室的轻重缓急？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们会问这样一个问题：「我们如何开发出能够进行智能推理且交互起来更直观的计算机，让它们可以做繁重的家务，每天开车送我们去上班？我们的计算系统应该采用什么样的原则和模型，使它们能够运行得更快、更好、更安全、更容易、更有效率？我们是否能够增强自己对数学计算的认识，从而可以应用它来解决现实世界的问题，比如改善我们的数据安全性？」这些问题似乎令人生畏，但其影响深远，为解决世界所面临的问题和挑战开辟了新的途径。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有 50 多个研究小组致力于数百个不同项目，但他们都有一个共同的主题，即发现新的方法以使计算机变得更聪明、更易使用、更安全和更有效。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们思考计算领域的最重要的长期挑战，并组织各项资源去架构它，必要时会伸向其他学科。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年来，我们已经着重在自己的特色专业课题方面发起了行动计划。我们的网络安全计划汇集了密码学家、加密研究员和加密政策专家，共同致力于网络攻击的预防和恢复。我们的大数据计划寻求于利用那些对于现有工具来说处理起来过于庞大、快速或困难的海量信息。我们正致力于系统、人工智能、计算、医疗保健和自主性方面的一些新举措。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们不断地重新评估事情的优先级，确保将精力集中在那些我们认为计算机科学可以发挥作用的最重要的挑战上。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:麻省理工学院有这么多的实验室。你是如何与他们合作的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我积极地与 CSAIL 和 MIT 的许多研究人员共事，这些大学教授的研究课题范围包括3D打印、海洋机器人、计算折叠和机械工程等。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管许多这些合作伙伴关系是我事先便知晓的，然而最令人兴奋的合作是来自于午餐时与坐我旁边的教授进行谈话。每天与研究人员一起工作是一种乐趣，他们的想法非凡且时而有些出神，他们天生拥有好奇心并总是寻求看问题的新方式。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也与其他实验室有着非常多的接触，围绕着研究所的最高优先级课题进行合作。比如我们正与医学工程与科学研究所（Institute for Medical Engineering &amp;amp; Science/IMES）建立一个合作关系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;High:你是如何进入计算机科学领域的？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我出生在罗马尼亚，父母是计算机科学家和物理学家，这无疑提早给了我得天独厚的科学环境。我是 Jules Verne 的头号粉丝，而且尤其喜欢《海底两万里》。我还看了《迷失太空》的重播，喜爱电脑天才 Will Robinson 和他的 B9 机器人。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我父亲在我高中时鼓励我学习编程，并教我布尔代数。从那时起，我知道自己想要在大学学习计算机科学，然后去康奈尔研究生院。当我的博导 John Hopcroft 谈到计算机科学的宏伟愿景——它们如何能够利用这些方程和算法来让物理机器做所有这些人类不能或不想做的事——时，我就决定将机器人作为我的研究方向。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：你是 CSAIL 的前辈以及第一位女性主管。鉴于技术领域的性别差距，你是否将你的晋升和奖励看作是一个其他女性可追随的模型？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;作为两个女儿的母亲，我很清楚要确保女孩获得接触科学和技术的每一个机会是多么重要。我试图为我的女儿树立一个好榜样，鼓励她们去追随激情，并让她们知道，如果你投入到工作中就能够追求生活中你想得到的任何东西。我的工作部分是由一个信念驱动，我认为每个人都应该知道如何利用计算去解决问题。多年来人们一直在谈论增加计算机的可访问性，以最终使每个孩子都有一台笔记本电脑。但我发现，机器人对儿童的吸引力甚至比计算机「更」大 ，所以我相信机器人比其他任何东西都更能帮助儿童学习几何、物理、编程和许多其他东西。技术素养同阅读写作和数学一样重要，因为它是我们周围的一切。我们想让儿童从小就对机器人感兴趣和感到兴奋，使他们成为未来的创新者。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High:你认为什么方法最能将更多的女孩和女性吸引到 STEM 项目中去？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;简言之，我们需要从幼儿园开始把计算思维作为所有成绩中的一个强制性主题。我们还需要找到教授原则的方法，还需要为学生提供动手的项目，从而向他们展示计算如何能够「控制」他们。我通常喜欢把计算视为一个——毫不夸张地说——超级强权，它让你以全新的方式去探索世界和与之互动。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于越来越多的工作需要以一个更复杂的方式来使用计算机，我们迫切需要填补编程基础教学方面的知识差距。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开发这些能力不只是「学习编程」，而是可以这样说：「利用编程来学习」。它不是仅仅把这些技能作为目的本身，而是在于学习这些东西如何令你以不同的方式去看待这个世界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们已经尽力在尝试的一个方法是通过活动来检查我们的一些项目，比如我们一年一度的「Hour of Code」，这个活动向大约 200 名当地公立学校的学生开放。能够向这些学生展示编程是多么的使人兴奋且有趣，这是一件令人高兴的事情。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：机器人是 CSAIL 所重点关注的关键领域。你和你的团队如何为各种高级机器人应用程序划分优先处理机会？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的总体目标是推动自主性科学。我们致力于开发的技术，能够使得单个机器人或机器人团队在无人监督的情况下合作完成复杂任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以看到这些领域出现的一些最令人兴奋的进展，像是软体机器人（soft robotics）、人机互动、自动驾驶、3D打印，以及致力于开发出能够更好地解决问题的机器人的领域。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如软体机器人，一般比「硬」机器人更安全且更有弹性。它们在某些任务中甚至会更有效。其灵巧的结构使它们能够更容易地改变方向或挤进狭窄的空间。如果它们击中了某个东西则不太可能将其打破，而它们的内部程序甚至会利用这些碰撞来获得周围环境的信息。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同时，3D 打印可以让机器人变得更便宜，并对那些没有昂贵制造技术的人而言更易于生产。我们已经展示了使用简单的家用材料，比如纸张和塑料，就能够生产出几乎能从打印机里走出来的功能型机器人。我想只需几年时间，世界就会变成这样——机器人像今天的智能手机一样普通——–那时你将能够走进一个当地的「robo-Kinko’s」去订购一个你自己的机器人，用以处理家庭或办公室的特殊任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;High：你和你的团队认为你们的工作在商业上的应用可以达到何种程度？你们与私企之间有合作吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Rus：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;我们的实验室自 1963 年创建以来，从 CSAIL 剥离出来的公司已超过 250 家，包括 Akamai、iRobot、3Com 和Meraki。CSAIL 拥有一些贡献美国商业图景的特殊东西——即需要深入研究并长期探索的可在现今产生影响的技术。我们非常喜欢培养这种思考创新的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;企业家也像我们的设备一样需要（人际）网络。他们需要导师来帮助他们弥合研究与创业之间的距离。他们需要具备操作经验的人。他们需要资本。我们在 2012 年 推出了一个创业计划倡议，来确保使我们的学生企业家成功的先决条件得到落实。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们非常清楚我们的技术能够如何积极地影响世界。通过 CSAIL 联盟计划（CSAIL Alliance Program），我们与许多公司合作，共同探讨我们的研究能够如何直接地改善人们的生活。我们在大数据、网络安全和无线方面也有跨领域行动计划，我们会与行业赞助商合作，将我们的研究导向我们认为将对消费者最为有利的特定方向上。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在更大规模的项目中我们还有其他的特定行业合作者。例如今年秋天，我们启动了丰田- CSAIL 联合研究中心（ Toyota-CSAIL Joint Research Center），它将专注于开发自动驾驶技术。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;具体说来，我们正致力于开发高度自动化的汽车，总有一天它会既充当全权掌控的「受雇司机」，又作为可以让人类保持控制的同时仍然能够介入事故预防的「守护天使」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们很高兴能够继续与业界合作，找出将我们的学术成果用于实践的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>一周论文| Word2Vec 作者Tomas Mikolov 的三篇代表作</title>
      <link>http://www.iwgc.cn/link/2715050</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Paper Weekly&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;微信公众号：paperweekly&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;引&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Word2Vec从提出至今，已经成为了深度学习在自然语言处理中的基础部件，大大小小、形形色色的DL模型在表示词、短语、句子、段落等文本要素时都需要用word2vec来做word-level的embedding。Word2Vec的作者Tomas Mikolov是一位产出多篇高质量paper的学者，从RNNLM、Word2Vec再到最近流行的FastText都与他息息相关。一个人对同一个问题的研究可能会持续很多年，而每一年的研究成果都可能会给同行带来新的启发，本期的PaperWeekly将会分享其中三篇代表作，分别是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、Efficient Estimation of Word Representation in Vector Space, 2013&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、Distributed Representations of Sentences and Documents, 2014&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、Enriching Word Vectors with Subword Information, 2016&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Efficient Estimation of Word Representation in Vector Space&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;1&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;作者&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Google Inc., Mountain View, CA&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Word Representation, Word Embedding, Neural Network, Syntactic Similarity, and Semantic Similarity&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201309&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何在一个大型数据集上快速、准确地学习出词表示？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;传统的NNLM模型包含四层，即输入层、映射层、隐含层和输出层，计算复杂度很大程度上依赖于映射层到隐含层之间的计算，而且需要指定上下文的长度。RNNLM模型被提出用来改进NNLM模型，去掉了映射层，只有输入层、隐含层和输出层，计算复杂度来源于上一层的隐含层到下一层隐含层之间的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文提出的两个模型CBOW (Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)结合了上面两个模型的特点，都是只有三层，即输入层、映射层和输出层。CBOW模型与NNLM模型类似，用上下文的词向量作为输入，映射层在所有的词间共享，输出层为一个分类器，目标是使当前词的概率最大。Skip-gram模型与CBOW的输入跟输出恰好相反，输入层为当前词向量，输出层是使得上下文的预测概率最大，如下图所示。训练采用SGD。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxclmeuYV7dHrN0EeyVPmsvLJoTsdHQ0vj9HBI1gddia91XicU3qzDuvmg/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Code:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;C++代码&lt;/a&gt;&lt;br/&gt;&lt;span&gt;Dataset:&amp;nbsp;&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;SemEval-2012&lt;/a&gt;&lt;span&gt;,用来评估语义相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Bengio[1]在2003年就提出了language model的思路，同样是三层（输入层，隐含层和输出层）用上下文的词向量来预测中间词，但是计算复杂度较高，对于较大的数据集运行效率低；实验中也发现将上下文的n-gram出现的频率结合进去会提高性能，这个优点体现在CBOW和Skip-gram模型的输出层中，用hierarchical softmax（with huffman trees）来计算词概率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;本文的实验结果显示CBOW比NNLM在syntactic和semantic上的预测都要好，而Skip-gram在semantic上的性能要优于CBOW，但是其计算速度要低于CBOW。结果显示用较大的数据集和较少的epoch，可以取得较好的效果，并且在速度上有所提升。与LSI和LDA相比，word2vec利用了词的上下文，语义信息更加丰富。基于word2vec，出现了phrase2vec, sentence2vec和doc2vec，仿佛一下子进入了embedding的世界。NLP的这些思想也在用于recommendation等方面，并且与image结合，将image跟text之间进行转换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;Distributed Representations of Sentences and Documents&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;2&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Quoc V. Le, Tomas Mikolov&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Google Inc, Mountain View, CA&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;sentence representation&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;ICML 2014&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;基于word2vec的思路，如何表示sentence和document？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxBFVXDWWha857By9iaCFuL19VpdBCriaO2OhAvYRZSA514af29nT2IyEg/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用one-hot的表示方法作为网络的输入，乘以词矩阵W，然后将得到的每个向量通过平均或者拼接的方法得到整个句子的表示，最后根据任务要求做一分类，而这过程中得到的W就是词向量矩阵，基本上还是word2vec的思路。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来是段落的向量表示方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxTbbjibNTtzzz7CibV1MHML99DNWOs8VUwLiaE6KbNwvibdkia8PDcJhtD3w/0?"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;依旧是相同的方法，只是在这里加上了一个段落矩阵，用以表示每个段落，当这些词输入第i个段落时，通过段落id就可以从这个矩阵中得到相对应的段落表示方法。需要说明的是，在相同的段落中，段落的表示是相同的。文中这样表示的动机就是段落矩阵D可以作为一个memory记住在词的context中遗失的东西，相当于增加了一个额外的信息。这样经过训练之后，我们的就得到了段落表示D，当然这个段落就可以是一段或者一篇文章。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后一种就是没有词序的段落向量表示方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm7swUOVrHxiar8MD7Rq4PpxMwc2yC1w3VZe5ynQgzreTjKlYKk6xxAD46iccWZuwkaDYkpePicduXEw/640?wx_fmt=jpeg"/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从图中就可以感觉到这个方法明显和skip-gram非常相似，这里只是把重点放在了段落的表示中，通过段落的表示，来预测相应的context 词的表示。最后我们依然可以得到段落矩阵D，这样就可以对段落进行向量化表示了。但是输入起码是句子级别的表示，而输出则是词的向量表示，因此个人比较怀疑这种方法的合理性。&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;这篇文章是word2vec的方法提出一年后提出的方法，因此本文并没有使用目前非常流行的word2vec的训练方法来训练词向量，而是利用word2vec的思路，提出了一种更加简单的网络结构来训练任意长度的文本表示方法。这样一方面好训练，另一方面减少了参数，避免模型过拟合。优点就是在训练paragraph vector的时候加入了一个paragraph matrix，这样在训练过程中保留了一部分段落或者文档信息。这点在目前看来也是有一定优势的。但是目前深度学习发展迅速，可以处理非常大的计算量，同时word2vec以及其变种被应用得非常普遍，因此该文章提出的方法思路大于模型，思路我们可以借鉴，模型就不具有优势了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;Enriching Word Vectors with Subword Information&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;3&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;作者&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;单位&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Facebook AI Research&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;Word embedding, morphological, character n-gram&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;来源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;arXiv, 201607&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;如何解决word2vec方法中罕见词效果不佳的问题，以及如何提升词形态丰富语言的性能？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实验分为三个部分，分别是（1）计算两个词之间的语义相似度，与人类标注的相似度进行相关性比较；（2）与word2vec一样的词类比实验；（3）与其他考虑morphology的方法比较。结果是本文方法在语言形态丰富的语言（土耳其语，法语等）及小数据集上表现优异，与预期一致。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;资源&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;源码公布在Facebook的fastText项目中：&lt;/span&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;https://github.com/facebookresearch/fastText&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a target="_blank" rel="external" style="text-decoration: underline; font-size: 14px;"&gt;&lt;br/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;相关工作&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;利用语言形态学来改进nlp的研究源远流长，本文提及的许多关于character-level和morphology的有趣工作值得参考。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 style="white-space: normal; line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;&lt;strong&gt;简评&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;文章中提出的思路对于morphologically rich languages（例如土耳其语，词缀的使用极为普遍而有趣）来说十分有意义。词缀作为字母与单词之间的中层单位，本身具有一定的语义信息。通过充分利用这种中层语义来表征罕见词汇，直观上讲思路十分合理，也是应用了compositionality的思想。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用形态学改进word embedding的工作十分丰富，但中文NLP似乎很难利用这一思路。其实个人感觉中文中也有类似于词缀的单位，比如偏旁部首等等，只不过不像使用字母系统的语言那样容易处理。期待今后也有闪光的工作出现在中文环境中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;从Word2Vec到FastText，从word representation到sentence classification，Tomas Mikolov的工作影响了很多人。虽然有个别模型和实验结果曾遭受质疑，但终究瑕不掩瑜。word2vec对NLP的研究起到了极大地推动作用，其实不仅仅是在NLP领域中，在其他很多领域中都可以看到word2vec的思想和作用，也正是从word2vec开始，这个世界变得都被vector化了，person2vec，sentence2vec，paragraph2vec，anything2vec，world2vec。以上为本期Paperweekly的主要内容，感谢&lt;strong&gt;memray&lt;/strong&gt;、&lt;strong&gt;zhkun&lt;/strong&gt;、&lt;strong&gt;gcyydxf&lt;/strong&gt;、&lt;strong&gt;jell&lt;/strong&gt;四位同学的整理。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWib6Sw6RA7ddKj6ODvvgQgvOqQRBn3u9vwibdJz3FwdX0kKHNZBPf2sKgUhpMBByltT0t9NRH3zianGg/640?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微博账号：PaperWeekly（http://weibo.com/u/2678093863 ）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;知乎专栏：PaperWeekly（https://zhuanlan.zhihu.com/paperweekly ）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Fri, 16 Sep 2016 19:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 图文并茂的神经网络架构大盘点：从基本原理到衍生关系</title>
      <link>http://www.iwgc.cn/link/2707037</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自THE ASIMOV INSTITUTE&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：FJODOR VAN VEEN&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着新型神经网络架构如雨后春笋般地时不时出现，我们已经很难再跟踪全部网络了。要是一下子看到各种各样的缩写（DCIGN、BiLSTM、DCGAN……），真的会让人有点招架不住。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为此，Fjodor Van Veen 写出了一篇包含了大量架构（主要是神经网络）的盘点性文章，并绘制了直观的示意图进行说明。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIozc8EXjnQRNnwsphcY2wqXu4YvVibcg1JNE4rRe0OKibYra7aQujPCFGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;将这些架构绘制成节点图的一个问题：它并没有真正展示这些架构的工作方式。比如说，变自编码器（VAE）可能看起来和自编码器（AE）一样，但其训练过程却相当不同。训练好的网络的使用案例之间的差别甚至更大，因为 VAE 是生成器（generator），你可以在其中插入噪声来得到新样本；而 AE 只是简单地将它们的输入映射到其所「记得」的最接近的训练样本。所以必须强调：这篇概览中的不同节点结构并不能反映出这些架构的内在工作方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;列出一份完整的列表实际上是不可能的，因为新架构一直在不断出现。即使已经发表了，我们可能很难找到它们，而且有时候还会不自觉地忽略一些。所以尽管这份清单能为你提供人工智能世界的一些见解，但无论如何请不要认为这份清单是全面的；尤其是当你在这篇文章写出后很久才读到时（注：本文原文发表于 2016 年 9 月 14 日）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于本文中图片所描绘的架构，作者都写了一点非常非常简短的说明。如果你很熟悉其中一些架构，但不熟悉另一些，你可能会觉得这些说明会有用处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;前馈神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（FF 或 FFNN：Feed Forward neural networks）是非常简单的：它们从前向后馈送信息（从输入到输出）。神经网络常被描述为层级形式，其中的层（layer）可能是输入层、隐藏层或输出层。一个单独的层不存在什么连接（connection），而通常相邻的两个层是完全连接的（一个层的每一个神经元都连接到另一个层的每一个神经元）。其中可以说是最简单的实际网络具有两个输入单元和一个输出单元，其可用于对逻辑门进行建模。人们常常通过反向传播（back-propagation）来训练 FFNN，从而让该网络获得配对的数据集——「输入的内容」和「我们想要得到的输出」。这被称为监督学习（supervised learning），其相反的方法被称为无监督学习（unsupervised learning），其中我们只需要给出输入然后让网络自己填补空白。被反向传播的误差（error）常常是输入和输出之间差分（difference）的某种变体（如 MSE 或只是线性差分）。如果该网络有足够的隐藏神经元，那么理论上它总是能够建模出输入和输出之间的关系。实际上它们的使用存在很大的限制，但它们常被用来与其它网络结合以构建新的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoYJOMkKhnPxOQmT1tXHqCGIHJbFicMibvqib59E9slpghjA7GCtVlk5T5A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;径向基函数&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBF: Radial basis function）网络是使用径向基函数作为激活函数（activation function）的 FFNN。没什么其它的了。但这不意味着它没有用处，但大部分带有其它激活函数的 FFNN 都没有自己的专用名称。这主要是因为人们在正确的时间发明了它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoZK46l9hfANzd0kft6Gpib6ibROQNjdwgJ1Tjz9huxc8Iiba3moVAzP8xw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;霍普菲尔德网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（HN: Hopfield Network）是一种每一个神经元和其它每一个神经元都有连接的网络；它是完全纠缠在一起的意大利面条，其中所有的节点都是全功能的。在训练之前，每一个节点都是输入；在训练过程中，每一个节点都是隐藏；之后它们都是输出。这种网络的训练是：将神经元的值设置成我们想要的模式，从而计算出权重（weight）。之后权重便不再变化。一旦为一种或多种模式进行了训练之后，这种网络总是会收敛成其学习过的一种模式，因为这种网络只能稳定在这些状态。请注意它并不是符合预期的状态（悲伤的是它并不是魔法黑箱）。因为该网络的总「能量（energy）」或「温度（temperature）」在训练过程中会逐渐减小，所以它总会一部分接一部分地稳定下来。每一个神经元都一个可以扩展到这个温度的激活阈值，而如果该神经元的输入总和超过了该阈值，那么输入就会使神经元从两个状态（通常是 -1 或 1，有时候是 0 或 1）之中选择一个。网络的更新可以同步完成，但更常见的是一个接一个更新神经元。如果是一个接一个地更新，就会创建一个公平随机（fair random）的序列来组织哪些单元以哪种顺序更新（公平随机是指所有（n）的选择在每 n 个项中只恰好发生一次）。这样你就能分辨网络何时达到了稳定（收敛完成）：一旦每一单元都被更新后而其中没有任何改变，那么该网络就是稳定的（即退火了的（annealed））。这些网络常被称为联想记忆（associative memory），因为其会收敛到与输入最相似的状态；人类看到半张桌子就能想象出另一半，类似地，如果给这种网络提供半张桌子和一半噪声，那么该网络就能收敛出一张桌子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoQUibSkBabC7ggyvFTvqiaTlD9xjKEiaZ3uBRu4HdGKwvTyIZzKvibJzk5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;马尔可夫链&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（MC：Markov Chain）或离散时间马尔可夫链（DTMC: discrete time Markov Chain）是 BM 和 HN 的某种前辈。可以这样理解：从我目前所处的节点开始，到达我周围任何节点的概率是多少？它们是无记忆的（即马尔可夫特性（Markov Property）），这意味着你所得到的每一个状态都完全依赖于其之前的一个状态。尽管算不上是神经网络，但它们确实类似于神经网络，并提供了 BM 和 HN 的理论基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo9l0nEQvRtRtjx5d0KYZNwt6kjLmXb9U9OaxybNbjo94km4yCSMftJA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BM：Boltzmann machines）和 HN 非常相似，除了：一些神经元被标记为输入神经元，而其它的仍然是「隐藏的」。这些输入神经网络会在整个网络更新结束时变成输出神经元。其开始时是随机权重，然后通过反向传播学习，最近也有人使用对比发散（contrastive divergence）的方法（使用一个马尔可夫链来确定两个信息增益之间的梯度）。和 HN 相比，BM 的神经元有时也有二元激活模式（binary activation patterns），但其它时间它们是随机的：一个单元处在一个特定状态的可能性。BM 的训练和运行过程非常类似于 HN：首先为输入神经元设置特定的钳位值（clamped values），然后该网络就自由了（不需要外力了）。自由了之后这些单元能得到任何值，然后我们在输入和隐藏神经元之间反复来回。它最后会在合适的温度下达到平衡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoMIHO0icG0JicFyg70RicjFNnZS9N1k1iaeQNgcsFMOs19TYVOxHThNLicCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;受限玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RBM：Restricted Boltzmann machines）类似于 BM（这毫不奇怪），所以也类似于 HN。BM 和 RBM 之间的最大不同之处是 RBM 是更受限的，所以也可被更好地使用。它们并不将每一个神经元和其它每一个神经元连接起来，而是只将每组不同的神经元和其它每一组连接起来，所以输入神经元不会直接连接到其它输入神经元，隐藏神经元之间也没有连接。RBM 可以以类似 FFNN 的方式训练，但也有一点不同：不是前向通过数据然后反向传播误差，而是前向通过数据之后再将这些数据反向传回（回到第一层）。在那之后再使用前向和反向传播进行训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo5nHUmWbXBrJfaEvBtEBI42ISBBXZxfXfIdvNribfqcjzsN6vyEc4sRA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（AE：Autoencoders）有一点类似于 FFNN，因为 AE 更像是 FFNN 的一种不用的用例，而非一种根本上不同的架构。自编码器背后的基本思想是自动编码信息，也因此得名。其整个网络有一种沙漏般的形状——其隐藏层比输入层和输出层都小。AE 也是围绕中间层对称的（根据层的数量是奇数或偶数，中间层有 1 层或 2 层）。最小层总是位于中间，这里的信息得到了最大的压缩（该网络的阻塞点（ chokepoint））。中间以上的所有部分被称为编码（encoding）部分，中间以下的所有部分则被称解码（decoding）部分，中间部分则被称为代码（code）。人们可以通过馈送输入以及将误差设置成输入和输出之间的差异的方式，使用反向传播来训练它们。当涉及到权重时， AE 还可以对称式的构建，所以编码权重和解码权重一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo03R1IdiblEmC8DKJ1Npa1AZY5oqniaqlEAzrOZheGia6BHueF8S04kUsA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稀疏自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SAE: Sparse autoencoders）在某种程度上是 AE 的反面。它不是在更少的「空间（space）」或节点中教一个网络表征一些数据，而我们试图在更多空间中编码信息。所以不再是网络在中间收敛然后扩展回输入大小，我们直接消除了中间内容。这些类型的网络可被用于从数据集中提取许多小特征。如果我们以类似于 AE 的方式训练一个 SAE，在几乎所有情况下你都只会得到一个相当无用的恒等网络（输入即是输出，没有任何变换或分解）。为了防止这种情况，我们不反馈输入，而是反馈输入加稀疏驱动器（sparsity driver）。这个稀疏驱动器可以以阈过滤器（threshold filter）的形式，其中只有一个特定的误差会被传播回去和训练，在这次通过过程中其它的误差都将是「无关的」，会被设置为 0。在某种程度上这类似于脉冲神经网络（spiking neural networks），其中并不是所有的神经元在所有时间都在放电（以及为生物合理性给出分数）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoVLich5P7QqGIn8pmWXaQXZwia1SH6JwgUzosoTrl909ARThb8wqZiboKg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;变自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（VAE：Variational autoencoders&amp;nbsp;）的架构和 AE 一样，但被「教授」了不同的东西：输入样本的一个近似概率分布。这有点回到本源的感觉，因为它们和 BM 及 RBM 的联系更紧密一点。但它们确实依赖于贝叶斯数学来处理概率推理和独立（probabilistic inference and independence），以及依靠重新参数化（re-parametrisation）来实现这种不同的表征。这种推理和独立部件理解起来很直观，但它们或多或少依赖于复杂的数学。其基础可以归结为：将影响考虑在内。如果某种事物在一个位置发生，而其它地方则发生其它事物，那么它们不一定是相关的。如果它们不相关，那么误差传播应该考虑一下这一点。这是一种有用的方法，因为神经网络是大型的图（graph，从某种角度来看），所以在深入到更深的层时如果排除掉一些节点对其它节点的影响，就会带来帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoLhK2lTuynsA7rPTwRbetepQLqf42v3vl5JoGJ5qGzAUtjGywuOxS9A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;去噪自编码器&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DAE: denoising autoencoders）是一种输入中不仅包含数据，也包含噪声（比如使图像更有颗粒感）的自动编码器。但我们以同样的方式计算误差，所以该网络的输出是与不带噪声的原始输入进行比较。这能让网络不会学习细节，而是学习更广泛的特征，因为学习更小的特征往往会被证明是「错误的」，因为更小的特征会不断随噪声变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIom8aNXHMas0LlU5llYQG6KNia03jw5ldRicUvOECuHEsU9oIBXSvciaVyA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度信念网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DBN: deep belief networks ）基本上是 RBM 或 VAE 堆叠起来的架构。事实已经证明这些网络可以堆叠起来高效地训练，其中的每一个 AE 或 REM 只必须编码编码之前的网络即可。这种技术也被称为贪婪训练（greedy training），其中贪婪是指得到局部最优的解决方案，从而得到一个合理的但可能并非最优的答案。DBN 可通过对比发散（contrastive divergence）或反向传播进行训练，以及学习将数据表征为概率模型，就像普通的 RBM 或 VAE 一样。一旦通过无监督学习训练或收敛成了一个（更）稳定的状态，该模型就可被用于生成新数据。如果采用对比发散进行训练，它甚至可以对已有的数据进行分类，因为其神经元已经学会了寻找不同的特征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIos3J9KTzcMg67bBVZ2estY6AEibksUHvXxwIXrUEnm1KDI9w8rx6brxA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（CNN：convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）和其它大多数网络非常不同。它们主要被用于图像处理，但也可应用于音频等其它类型的输入。CNN 的一种典型的用例是让网络对输入的图像进行分类，比如，当输入的图像上有猫时输出「cat」、有狗时输出「dog」。CNN 往往开始带有一个输入「扫描器（scanner）」，其目的是不一次性解析所有的训练数据。比如要输入一张 200×200 像素的图像，你并不需要一个带有 40000 个节点的层。事实上，你只需要创建一个比如说 20×20 的扫描输入层，这样你就可以从该图像的一个 20×20 像素的部分开始输入（通常是从左上角开始）；一旦这个输入完成后（可能是用于训练），你再输入下一个 20×20 像素：将该扫描器向右移 1 个像素。注意人们不会一次性移动 20 个像素（扫描器的宽度），也不是将图像分解成 20×20 的块；相反，而是让扫描器在图像上「爬行」。然后这些输入数据被送入卷积层（convolutional layers），这和普通的层不一样，其中所有的节点并非连接到所有的节点。每一个节点仅将它自己与其近邻的单元连接起来（到底多近取决于具体的实现，但通常不止一点点）。这些卷积层往往会随着网络越来越深而缩小，大部分是按照输入可以轻松整除的因子（所以 20 后面的层可能是 10 ，然后是 5）。这方面常使用 2 的幂，因为它们可以通过 32, 16, 8, 4, 2, 1 这样的定义完全整除。除了这些卷积层，它们常常还有池化层（pooling layer）。池化是一种滤除细节的方法：一种常见的池化技术是最大池化（max pooling）——其中我们取比如 2×2 的像素，然后根据最大量的红色传递这些像素。为了将 CNN 应用到音频上，基本上是输入音频波然后缓慢移动音频片段，一段接一段。CNN 的真实世界实现往往会在末端连接一个 FFNN 以便进一步处理数据，这可以实现高度非线性的抽象。这样的网络被称为 DCNN，但这两者的名字和缩写往往可以混用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoW79bJlXfY5cQPbQLhxBTGBFb64ibLlP9lRQiaAzRtJWJ7N3o2hEv9mYw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;解卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DNN：Deconvolutional neural networks）也被称为逆图形网络（IGN： inverse graphics networks），是反向的卷积神经网络。比如给网络输入一个词「cat」，然后训练它生成一张类似猫的图像（通过将其与真实的猫图片进行比较）。和普通的 CNN 一样，DNN 也能和 FFNN 结合使用，但我们就不给这种网络缩写了。我们也许可以将其称之为深度解卷积神经网络，但你也可以认为当你在 DNN 的前端和后端都接上 FFNN 时，你得到的架构应该有一个新名字。请注意在大多数应用中，人们实际上并不会为该网络送入类似文本的输入，而更多的是一个二元的分类输入向量。比如设 &amp;lt;0, 1&amp;gt; 是猫，&amp;lt;1, 0&amp;gt; 是狗，&amp;lt;1, 1&amp;gt; 是猫和狗。CNN 中常见的池化层往往会被相似的逆向运算替代，主要使用偏差假设（biased assumptions）做插值和外推（interpolation and extrapolation ）（如果一个池化层使用的是最大池化，你可以通过其逆向过程产生特定度更低的新数据）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo2TF31peYW3w5W7IS2dn7gqJm6hXzJzsSIoAyaxWZNxbfeyDc4Og4bA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度卷积逆向图网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DCIGN：Deep convolutional inverse graphics networks）的名字比较有误导性，因为它们实际是 VAE，但有 CNN 和 DNN 分别作为编码器和解码器。这些网络试图在编码中将特征建模为概率，以便于它能在曾经分别看到猫和狗的情况下，学习产生同时带有猫和狗的图片。类似的，你能给它输入一张带有猫和狗的图片，要求网络去掉图片中的狗，即使之前你未曾做过这样的操作。已有演示表明这些网络也能学习模型图片上的复杂变化，比如改变光源或者 3D 目标的旋转。这些网络往往通过反向传播训练。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIokRw3rtLTUmiaCI54j51ztiawEOZic5lryoHqwJPwWLRUia9OYMiassnWfHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;生成式对抗网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GAN：Generative adversarial networks）源于不同的网络类型，它们是双胞胎：两个网络一起工作。GAN 包含任意两种网络（尽管通常是 FF 和 CNN），一个网络的任务是生成内容，另一个是用于评判内容。判别网络要么获取训练数据，要么获取来自生成网络的内容。判别网络能够多好地准确预测数据源的程度然后被用来作为生成网络的误差。这创造了一种竞争方式，判别器区别真实数据与生成数据上做得越来越好，而生成器也变得对判别器而言越来越难以预测。这效果很好的部分原因是即使相当复杂的类噪音模式最终也是可预测的，但生成的类似于输入数据的内容更难以学习进行区别。GAN 训练起来相当难，因为不仅要训练两个网络（每个解决各自的问题），两个网络的动态也要平衡好。如果预测或生成相比于对方更好，GAN 收敛不好，因为存在有内在的分歧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoNWSgWHRibWrvmAeYziaxoicLfxG2dOxHOmTNuKo5eB4RmQBD9vtvslIhg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（RNN：Recurrent neural networks）是带有时间联结的 FFNN：它们不是无状态的，它们随时间变化在通路与连接之间有联系。神经元不只从前面层中被输入信息，也从来自它们自己的之前的通过中获得信息。这意味着你输入信息和训练网络的顺序很重要：输入「牛奶」然后是「甜饼」与输入「甜饼」然后是「牛奶」相比可能会产生不同的结果。RNN 的一个重大问题是梯度消失（或爆炸）问题，取决于使用的激活函数，信息随时间渐渐损失，就像很深的 FFNN 随深度变化消失信息一样。直观上这看起来不是大问题，因为这些只是权重，不是神经元状态，但随时间变化的权重正是来自过去信息的存储。如果权重达到 0 或 1,000,000 的值，先前的状态就不在具有信息性。RNN 理论上可被用于多个领域，因为大部分的数据形式没有时间线上的变化（也就是不像声音和视频），所以时间决定的权重被用于序列之前的东西，不是多少秒之前发生的内容。大体上，循环网络是发展或完善信息的较好选择，比如 autocompletion（自动完成）任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoibDcrxP15h4Y5ib1cDW8ofB3tJegjF0fmojcLtjxRKN5DJBzsxia6mrww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSTM：Long / short term memory）试图通过引入门（gate）和明显定义的记忆单元对抗梯度消失（爆炸）问题。这个思路受到电路图的启发，而不是生物学上的概念，每个神经元有一个记忆单元和 3 个门：输入、输出、遗忘（ input, output, forget）。这些门的功能是通过禁止或允许其流通确保信息。输入门决定来自上层的信息有多少被该单元存储。输出层在另一端做同样的事，并决定下一层多么了解该细胞的状态。遗忘门看起来像是一个奇怪的东西，但有时被遗忘反而更好。已有实验表明 LSTM 能够学习复杂的序列，比如像莎士比亚一样写作，或者创造交响乐。注意每个门在之前神经元中都有一个权重，所以运行起来需要更多的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIo6ush5bNXBSm2XVw0l9dHrJuqjGUPibQQmYZ9VB9NG7FaewoxKk9ma2Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;门循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（GRU：Gated Recurrent Units）是 LSTM 的一种轻量级变体。它们有一个门，连线方式也稍微不同：没有输入、输出、遗忘门，它们有一个更新门（update gate）。该更新门既决定来自上个状态的信息保留多少，也决定允许进入多少来自上个层的信息。重置的门函数很像 LSTM 中遗忘门函数，但位置稍有不同。GRU 的门函数总是发出全部状态，它们没有一个输出门。在大多案例中，它们的职能与 LSTM 很相似。最大的不同就是 GRU 更快、更容易运行（但表达力也更弱）。在实践中，可能彼此之间要做出平衡，当你需要具有更大表达力的大型网络时，你可能要考虑性能收益。在一些案例中，额外的表达力可能就不再需要，GRU 就要比 LSTM 好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoKTZV5cN1E3SIbD7fshEFOyvGDQBy2ZwSHmCs2mbZMicyc4ahYvy7nmg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经图灵机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（NTM：Neural Turing machines）可被理解为 LSTM 的抽象化，并试图将神经网络去黑箱化（ un-black-box，让我们洞见里面到底发生了什么。）NTM 中并非直接编码记忆单元到神经元中，里面的记忆是分离的。这种网络试图想将常规数字存储的功效与永久性和神经网络的效率与表达力结合起来。这种网络的思路是有一个可内容寻址的记忆库，神经网络可以直接从中读取并编写。NTM 中的「Turing」来自于图灵完备（Turing complete）：基于它所读取的内容读取、编写和改变状态的能力，意味着它能表达一个通用图灵机可表达的一切事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;双向循环神经网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiRNN：Bidirectional recurrent neural networks）&lt;/span&gt;&lt;strong&gt;&lt;span&gt;、双向长短期记忆网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiLSTM：bidirectional long / short term memory networks ）&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;strong&gt;&lt;span&gt;双向门控循环单元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（BiGRU：bidirectional gated recurrent units）在词表中并未展现，因为它们看起来和各自单向的结构一样。不同的是这些网络不仅连接过去，也连接未来。举个例子，通过一个接一个的输入 fish 这个词训练单向 LSTM 预测 fish，在这里面循环连接随时间记住最后的值。而一个 BiLSTM 在后向通路（backward pass）的序列中就被输入下一个词，给它通向未来的信息。这训练该网络填补空白而非预报信息，也就是在图像中它并非扩展图像的边界，而是可以填补一张图片中的缺失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoq4L9tcXDddYXBicia4pRxUApwVmqskGyAB3UGbWmA0ERFyl1nxIXDO2A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度残差网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（DRN：Deep residual networks）是非常深度的 FFNN 网络，有着额外的连接将输入从一层传到后面几层（通常是 2 到 5 层）。DRN 并非是要发现将一些输入（比如一个 5 层网络）映射到输出的解决方案，而是学习将一些输入映射到一些输出 + 输入上。大体上，它在解决方案中增加了一个恒等函数，携带旧的输入作为后面层的新输入。有结果显示，在超过 150 层后，这些网络非常擅长学习模式，这要比常规的 2 到 5 层多得多。然而，有结果证明这些网络本质上只是没有基于具体时间建造的 RNN ，它们总是与没有 gate 的 LSTM 相对比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIodzeRf3sxfhibMgeD08QDWnqPPb6vtjS7QTqJfibACPqR9BLEibDQm8AQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;回声状态网络&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（ESN：Echo state networks）是另一种不同类型的网络。它不同于其他网络的原因在于它在不同神经元之间有随机连接（即，不是在层之间整齐连接。），而且它们训练方式也不同。在这种网络中，我们先给予输入，向前推送并对神经元更新一段时间，然后随时间观察输出，而不是像其他网络那样输入信息然后反向传播误差。ESN 的输入和输出层有一些轻微的卷积，因为输入层被用于准备网络，输出层作为随时间展开的激活模式的观测器。在训练过程中，只有观测器和隐藏单元之间连接会被改变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoTqE2dneEzvlWHyuJuhuenQicibmeYgOroneF6sBzbZ3jmzpRjeOTACbw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;液态机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（LSM：Liquid state machines）看起来与 ESN 非常类似。不同的是，LSM 是脉冲神经网络（spiking neural networks）这一类型的：用阈值函数取代 sigmoid 激活函数，每个神经元也是一个累加记忆细胞。所以当更新神经元的时候，里面的值并不是被设为临近值的总和，也不是增加到它自身上。一旦达到阈值，它将能量释放到其他神经元。这就创造出了一种类似 spiking 的模式——在突然达到阈值的之前什么也不会发生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoJCe2uWVmDXbmxW82tc7XOFgibvcwicugcdvibXiavCL0vVkcfAzTPTd8eg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;支持向量机&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;（SVM：Support Vctor Machines）能发现分类问题的最佳解决方案。传统上只能够分类线性可分的数据，比如说发现哪个图像是加菲猫，哪张图片是史努比，不可能有其他输出。在训练过程中，SVM 可被视为在一张图上（2D）标绘所有数据（加菲猫和史努比），并搞清楚如何在这些数据点间画条线。这条线将分割数据，以使得加菲猫在一边，史努比在一边。调整这条线到最佳的方式是边缘位于数据点之间，这条线最大化到两端。分类新数据可通过在这张图上标绘一个点来完成，然后就简单看到这个点位于线的哪边。使用核（kernel）方法，它们可被教授进行 n 维数据的分类。这要在 3D 图上标绘数据点，从而让其可分类史努比、加菲猫、Simon’s cat，甚至分类更多的卡通形象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoxKoWSebXKI5vA2XoDr3IfjagXWxBvHx0xwrCUwia96QEQKicZic2VeibHA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，我们介绍&lt;strong&gt; Kohonen 网络&lt;/strong&gt;（KN，也称自组织（特征）映射（SOM/SOFM：self organising (feature) map））。KN 利用竞争学习在无监督情况下分类数据。向网络输入信息，然后网络评估那个神经元最匹配该输入信息。然后调整这些神经元以更好地匹配输入，在这个过程中拖带（drag along）着临近神经元。临近神经元能移动多少取决于它们与最好的匹配单元之间的距离。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>业界 | Claudia Perlich Quora 问答集：机器学习能力将成为数据科学家的基本要求</title>
      <link>http://www.iwgc.cn/link/2707038</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Quora&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、孙瑞、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Claudia Perlich 是纽约大学的客座教授，也是 Dstillery 公司的首席科学家，主要的工作是为潜在的品牌客户设计、开发、分析和优化驱动数字广告的机器学习。Claudia Perlich 在业界和学术界都有非常耀眼的成绩。最近获得了 Advertising Research Foundation（ARF）的 Grand Innovation Award，并被选为《纽约商业周刊》年度 40 位 40 岁以下人物名单。她还曾担任过 SIGKDD 2014 大会主席。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1.数据科学家当下面临的最低效的问题是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，我想声明的是我认为这并不是问题：事实上数据科学家 80% 的时间都花在准备数据上。这才是他们的工作！如果你对准备数据不感兴趣，那就不是一个好的数据科学家。任何分析的有效性几乎完全仰仗数据的准备程度，而与你最后选择的算法几乎无关。抱怨数据准备工作就像一个农民抱怨做任何与收成相关的事情，然后把灌溉、施肥、撒种的事交给别人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也可以说，数据准备工作难在原始数据收集上。设计一个收集有用且易于被数据科学消化的数据系统需要高超的技艺。对数据科学家来说，让系统内数据流的过程完全透明化也是需要高技巧的。这个过程需要做抽样、标注数据、匹配等工作。还不包括替换缺失值和过度规划化的工作。为数据科学创造一个有效的数据环境，需包括数据科学而且不能完全被工程工作所占有。数据科学并不总是能规范这种系统对细节的要求来完成一个干净利落的切换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是从更大范围来看，还有更重要的事情需要考虑。我认为到目前为止最大的问题是数据科学解决不相关的问题，浪费了大量时间和精力。原因通常是任何有这个问题的人在表达该问题时都缺乏对数据科学的理解，而且数据科学家最终解决任何他们认为可能是问题的问题，然后找到的方法不一定有帮助（广告常常都非常复杂）。一个典型的类别就是「定义不透彻（underdefined）」的任务：「在这个数据集中找出可操作的 insights！」。不过，大部分数据科学家并不知道要做哪个操作。他们也分不出哪些 insights 是琐碎的，哪些是有趣的？对于哪些跟风行事的人来说，也真没有什么好建议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;个人来讲，我觉得有必要提的问题是缺乏数据理解和数据直觉（事实上缺乏这个三个因素常常会让数据科学家很快就下出结论），而且怀疑是最影响效率的限制因素。这些因素影响效率的主要原因是找到正确的答案需要花费很长时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2.你最喜欢的机器学习算法是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;易于上手的逻辑回归（logistic regression，带有很多花里胡哨东西，比如机梯度下降、feature hashing 和 penalties）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我知道在深度学习风靡的时代，这答案有点奇怪。所以我先来讲下缘由：在1995-1998 年这段时间里 ，我用的是神经网络，到了 1998-2002 年这段时间我用的最多的是基于树的方法。从 2002 年开始，逻辑回归（一般的线性模型、包括分位数回归、泊松回归等等）逐渐深得我心。2003 年，我发表一篇机器学习的论文，展示了在 35 个数据集（那时候算是很大了）中三种逻辑回归分析方法与逻辑回归的比较结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长话短说，如果信噪比很高，树往往会赢。但是，如果信噪非常大，那么带有一个 AUC &amp;lt; 0.8 - logistic 的模型就是最好的选择，它总能击败基于树的方法。最终不太令人惊讶：如果信号太弱，高方差模型就完全没用了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以这到底意味着什么？在这种问题上，我倾向于处理低可预测水平的超级噪音。这个问题可以从非常确定性（国际象棋）的角度来考，也可以从非常随机（股票市场）的角度来考虑。（在有数据的情况下）一些问题仅仅是比其他问题更好预测一些。不是一个算法问题而是关于这个世界的概念陈述问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从这个角度来看，我感兴趣的大多数问题非常接近于股票市场。深度学习在另一个方向上效果很好——「这张图中有猫吗？」在一个不确定的世界中，偏置方差权衡（bias variance tradeoff）往往在有更多偏置的情况下是有利的——意味着你会想得到「简单的」非常受限的模型。而这就是逻辑回归的用武之地。我个人发现通过添加复杂的特征而不是尝试限制非常强大（高方差）的模型类别来增强一个简单的线性模型是更容易的。事实上，每个我赢过的数据挖掘比赛（KDD CUP 07–09）我都是用了线性模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性模型除了性能上的优势，还很稳健，而且往往只需要远远更少的人工处理（好吧，随机梯度下降和 penalties 会让其变得困难一点。当你想要在你不能花费 3 个月长的时间来构建完美的模型的行业进行预测建模时，这是极其重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最终，线性模型中的发生的情况更有可能得到是可以理解的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3.完全掌握 TensorFlow 需要什么样的数学背景？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这完全取决于你如何定义「掌握」。你是仅仅想用它调节一辆 Formula One 汽车的引擎，还是想要摘得国际汽车大奖赛的奖杯？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是，对于灵活使用数学与算法来解决某个问题这一方面，确实是有「掌握」一说的（如果你愿意，你还可以进行算法研究）。这是 Formula One 的技术人员常做的事情。对此，我指的是专业程度极高的数学。我个人不太喜欢做这一块，虽然我也曾学习过大量的高等数学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在应用方面 —— 当你做一个司机的时候，也有「掌握」这一概念。当你可能会使用 TensorFlow 的时候，你有好的灵感吗？你知道如何设计最好的数据表征才能使算法使用更加简单吗？如果 TensorFlow 的某个指标下表现良好，这是否代表它在其它指标下仍效果显著？以上这些例子都是考验你是否掌握 TensorFlow 的场景，而它们对使用者的经验、数据、以及直觉的要求，将远高于对单纯的数学的掌握。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么问题来了：你是想成为 Michael Schumacher（德国著名赛车手），还是想为他工作呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4.机器学习如何影响数字广告？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，在数字广告领域，机器学习无处不在 —— 曾经在 KDD（ACM SIGKDD 国际会议）有很多 ADKDD 研讨会（专注于在线广告）。当前，广告产业的数据日益丰富（虽然我不知是好还是坏），程序化的实时广告的兴起也为机器学习在这一领域的发展提供了充分的机会。作为回顾，你可以抽出一个小时，看一下我近期在 Institute of Advanced Study（高等研究院）的一个演讲：https://www.ias.edu/ideas/2015/perlich-data-video&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多数情况下，机器学习被应用于以下不同的广告产业组成部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;测量与分配（市场混合模型、观测数据的因果模型、用户倾向匹配等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;跨设备关联（基于用户模式、IP 重复等，预测两个设备属于同一个用户的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;意图预测（某个消费者在下个月购买某个新车型的可能性）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;广告印象层面的反应预测（点击或完成浏览的概率）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;欺诈检测（分辨机器与人、欺诈链接与点击等）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;用户洞察（观测一个在预测用户意图上表现良好的模型 —— 是否能够从中提取一些行为模式，并使用到创新设计中？）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;还有一些广告领域中的机器学习应用是备受争议的。Sweeney 教授的成果显示，机器学习算法会反应潜在的种族歧视，我近期的一些工作也发现，一些预测模型会「倒向信号所在的地方」—— 而在弱指标下这是十分危险的。假设现在我们要预测一个广告的点击量，结果发现预测你非常有可能点击手电筒应用十分简单。这不是因为你对手电筒感兴趣 —— 而是因为你在黑暗中行动笨拙。尽管这一模型在想高点击率优化上表现优异 —— 它只会将广告投放给那些几乎对该产品完全不感兴趣的人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5.随着产业内的机器学习越来越流行，数据科学将如何进化？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要回答这个问题，需要我们先思考一下数据科学与机器学习的关系。对我自己来说，数据科学本身包含了机器学习。从定义上看，机器学习指一个机器从数据中归纳知识的能力 —— 你可以把它称作学习或者推断。没有数据，机器就几乎无法学习。所以，如果有什么区别的话，机器学习在许多产业中的应用扩张，将会成为数据科学重现光彩的催化剂。机器学习的优劣与否，取决于其使用的数据，以及消化算法的能力。我的期望是，不断向基础的机器学习靠拢，将成为数据科学家的基本要求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，对我来说，最重要的数据科学技术之一是评估机器学习的能力。我认为，我们所从事的数据科学并不缺少酷炫的项目和迷人的算法；而我们仍旧有待了解的是事物背后的运作原理以及如何解决不标准的问题。对于机器学习的（学术）观点，我的主要忧虑之一是，目前人们持续地关注在样本表现中的简单结果。基本上 99% 的研究论文都是基于精确度而被采纳的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我从过去 12 年的工作中意识到：在多数应用领域中，学术评估几乎是没用的。在一些随机测试集中表现优秀的模型会变得一无是处。这个话题值得长篇讨论，但简要来说我对以下几点存疑：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人们经常使用的指标（将分类器的精确度定义为判断正确的百分比是罪魁祸首）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;一个模型在最开始预测的往往都是错的这一事实（多数是因为根本没有「对」的东西的数据）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt; 抛开使用背景评估模型 —— 你应当先基于预测采取行动，然后再评估结果是否有改进，而不是直接进行评估&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;样本数过大导致的种种问题 —— 人们基于现有的数据建立模型，而不是他们应有的数据，更加危险的是，人们往往还基于无代表性的样本评估模型。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在带有混合生成分布的对抗情形（adversarial situation）中，模型最终只能识别出「错误的」正类的困境（可以参看我在上文对广告点击量作为指标的评价）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;泄漏 —— 表明某一模型纯粹是某数据集合的衍生物，且该模型的真正表现是非常糟糕的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6.准备数据科学面试时，哪些资源是最好的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个答案的首先最明显的部分是明白你自己准备做什么。一般意义上都是这样，而不只是在数据科学领域。现实情况是今天有很多东西都被叫做数据科学，许多人也自命为数据科学家。所以首先你要知道你到底面试的是什么，那通常并不是很明显。你也许需要在面试之前问上几个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我已经看到数据科学的职位已经分化成了（多少有些夸张）多个类别：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找某种分析师，有某种擅长的技能实际上就足够了（你可能甚至不想要这样的工作）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找数据工程师，他们基本上最感兴趣的是你是否跟上了 Scala 编程的最新进展和知道你自己的版本控制方法。网上可能能找到一些标准的准备研讨会，我不知道。（这也不是我想要的工作。）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们实际上在寻找能够基于不同的数据大小设计有效的统计分析的人，其中会涉及到一些机器学习、预测建模、聚类，而不会过于在意你是用什么方法完成的（这就开始变得有意思了……）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们在寻找深度学习专家……&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;他们需要一匹独角兽——具备项目管理能力，能将一个业务问题翻译成一个「可解决的」数据驱动的解决方案。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦你搞清楚了你要做什么，你可能就要重新想想要不要应聘……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我通常在寻找第 3 类的申请者（有时候我也接受第 5 类）。不存在什么捷径「资源」能说服我你有这项工作所需要的能力。不幸的是，这确实涉及到个人的性格和经验。在 Kaggle 竞赛上花一些时间是获取经验的一个好方法。它们有许多数据集可以使用，你也可以从其他参与者那里学习经验。你所接触的数据集越不同，就越好！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，我还需要预测建模的专业知识，并且期望你熟悉来自《Elements of statistical learning》一书的概念。我需要你的能力足够为任何你所需要的事物编程出原型。你需要的技能包括数据拉取（API，SQ）、一些脚本编写的技能（Shell/Perl/Python）、一个建模的优质环境（Python 库/R/独立的实现），一些如何可视化/传达你的发现的想法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些发表资源能让你保持敏锐：KDNuggets KDD 大会（申请论文的好集合）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后是一点不太相关的提醒：尽管创业公司很有意思，但你不会想让你的第一份数据科学工作是一家创业公司的数据科学家……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7.你是如何学习机器学习的？学习机器学习时，你最喜欢那本书？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我在学校一开始就在数学和科学学科上表现不错，并不是因为我真正喜欢抽象的事物，而只是我喜欢理解事物和解决难题罢了。所以以数学作为职业就出局了，但对于我想要做什么，我并没有什么强烈的感觉。与此同时我的爸爸则推荐计算机科学作为一个延迟决定的解决方案，然后做点什么我可能会非常擅长的事！他在 92 年时认为计算机很快就将用到各种各样的地方，而且我可以在后面想想我究竟想做什么。（我会对今天的数据科学领域的申请说同样的话。）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1995 年，我在科罗拉多大学波尔得分校做了一年交换生，没有任何统计学背景的我迷迷糊糊地参加了人工神经网络的课程，完全不知道我在做什么。从此我和数据结下了不解之缘，剩下的就顺理成章了。在那里，我用德语写了另一篇硕士论文，将汽车的物理模型和该物理模型和被观察到汽车运动之间的残差分析的人工神经网络模型结合到了一起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我感觉我仍然很多东西要学（而且也不急着找工作），于是我开始寻找 PhD 项目，通过偶然的机会和朋友的关系，我在 1998 年加入了纽约大学斯特恩商学院信息系统的项目。在我看来，商学院的机器学习成果的优势在于专注解决实际的商业问题，而不是理论上的算法贡献。此外，也更加关注交流和说话的能力，因为我们都是作为下一代教 MBA 学生的教授而培养的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;幸运的是，在 2003 年，学术界还没有完全准备好将机器学习作为商学院的一项研究主题，所以我没有追求一项学术事业，而是加入了 IBM Watson 实验室的预测建模组。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面是我的一些「神圣的」引路人列表，按我遇到他们的顺序排列：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Duda, Hard, Stork「Pattern Classification」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bill Greene「Econometrics」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Hastie, Tibshirani, Friedman「The Elements of Statistical Learning」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Provost, Fawcett:「Data Mining for Business Intelligence」&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也在这里推荐几本轻松的书。一个好的数据科学家应该有一项非常好的技能：和随机性保持良好的关系。对此我推荐 Nassim Taleb 的「Fooled by Randomness」。同样，理解我们人类在处理信息中的偏见也对讲更好的故事而言是非常重要的，才能让我们自己不被数据愚弄。这方面我推荐 Duncan Watts 的一本书「Everything is Obvious Once you know the answer」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;8.作为一个数据科学家，你使用了哪些工具？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我是一个相当老派的人，而且因为我的事业在向管理岗位发展，也就更少写代码了，我已经控制好自己不「更新」我的工具集了。另外，因为我们不支持 X 的防火墙后面有非常敏感的数据，所以任何类型的图形界面都是很困难的，我的工作基本上都是通过命令行进行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我常使用 UNIX shell 命令对数据进行操作。其中必然包含：awk, sed, grep, sort, cut, cat, head, tail, uniq……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下一层的编译我使用的是 Perl。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;拉取数据主要是在我们 Hadoop 上的 Hive 前端上通过 SQL 完成。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我也用过 R、SAS、MATLAB 等工具——目前我大部分是使用 R 处理小事情（创建漂亮的图片等等）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了这些，我的态度是借用和窃取……我没有什么理由要自己实现任何机器学习算法。在这方面很多人都比我做得好。所以我已经积累了很多范围广泛的我发现的任何相关机器学习算法的独立可执行文件（UNIX）：Thorsten Joachims SVM 代码（http://svmlight.joachims.org/ ），Tree 学习的 FEST 库（https://github.com/n17s/fest ），John Langford 的 VowPal Wabbit（https://github.com/JohnLangford/vowpal_wabbit/wiki ）等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有时候我会发现一些我确实需要的工具 API。比如说，我发现了用于 NLP 的 Data Ninja（https://www.dataninja.net/ ）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;9.一个数据科学家需要必备一些特定领域知识吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;人们对这个问题争论很长时间了。我曾在 2013 年 KDD 的一个小组会议上做了几乎这同一主题的简短报告《The evolution of the expert（http://www.junglelightspeed.com/the-evolution-of-the-expert/ ）》。我过去尝试过许多回答这一问题的方法：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「如果你足够聪明，可以成为一个好的数据科学家，那么在大部分情况下，你都可以在一到两个月时间内学会任何你所需要的特定领域的知识。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「Kaggle 竞赛已经一遍又一遍地展示了好的机器学习算法胜过专家。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「当聘请数据科学家时，我更感兴趣的是那些在许多行业内的经验比我丰富的人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;或者我就让我的个人履历说话：我曾赢得了 5 次数据挖掘比赛，而我并不是一味乳腺癌、酵母基因组、电信客户关系管理、Netflix 影评或医院管理方面的专家。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所有这些可能都说明这个问题的答案是「No」。事实上，在通常的领域知识的解释上，我也会说「No」。但也有一些非常不同的地方：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我并不需要对一般意义上的领域了解太多，但我需要尽可能理解有关这些数据的创建及其含义的「一切」。这算是领域知识吗？不太算——如果你和一位普通的肿瘤学家交谈，你会发现他或她几乎不能解释你刚发现的 fRMI 数据的细节。你应该与之交谈的人可能应该是理解这些机器和其中的数据处理（比如校准）的技术人员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;10.广告拦截对用户有益吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从我非常个人的观点来看（许多比我聪明的人已经讨论过其经济影响了），短期内它对用户有益——它们可以减少页面加载时间，减少你的移动设备上的数据等等。但事实上内容商会受到伤害：而他们正是用辛劳的汗水为你提供你想要的内容的人。所以从长期来看，我很担心其影响不是人们所想要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果经济的广告模式失败了，因为广告拦截消除了免费内容的收入流，内容上可能会面临这样的选择（通常很糟糕）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;乞求——这就是维基百科正在做的事。问题就在于通过订阅和好心读者的捐助所获得的收入是否足以支撑一个健康的和独立的新闻环境。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更多内置广告渗透进内容之中。尽管 Steve Colbert 可能处在一个过于暗淡的位置上，但我认为这并不是我们想要的趋势。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;放弃独立和有信息的内容的概念，从别人那里复制信息，而不是为记者付钱。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;部分的问题在于现在我们已经习惯了免费的信息甚至娱乐（以及所有用来生产、分发和托管的基础设施）。但不能因为我们能免费得到它们，就意味我们应该这样做。我很乐意为无广告的高质量内容付费！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;11.进入科学技术圈子对于女性来说更难吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以我个人的经验来看，我会说并不会更难——事实上，在这个男性占大多数的技术领域，作为女性还存在一些明显的好处。之前的答案已经指出录取更偏向于女性。事实上，我并不喜欢这样的不平等——我可以看到它的好处，但我也厌恶它在暗示「否则女性就无法成功，因此女性并没有男性那么好。」下面的例外来自我 2014 年给出的关于这一主题的演讲：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一年前，我被要求担任世界上最大也是最负盛名的数据科学会议 SIGKDD 2014 (KDD) 的联席主席。我是该委员会中的唯一女性。很显然，选择我的决定并不是因为我是女性。换句话说，我很高兴这意味着确实有些男人确实在严肃对待性别平等，但另一方面，我有被骗的感觉，因为我并不确定我是否有资格得到它。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以当涉及到因为女性是少数派所以要支持女性时，我的感觉很复杂。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我总的看法是：女性的身份给我的事业带来的帮助超过其所带来的伤害。我还没遇见过什么人会让我觉得我的资格是毫无疑问因为我的性别。我从未标榜自己是数据科学领域的女性榜样，也没有哀叹女性的稀少，因为我从来没有担心过我的性别会限制我的能力或成就。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;事实上，女性参与到男性居多的领域中还有一个巨大的好处：人们会记得你。你已经和 90% 的典型男性区分开了。我去参加大会的时候，似乎有很多男人都认识我，甚至有时候我根本无法想起见过他们，更不要说记得名字了。事实上我们是很容易被记得的，而被记得是会很有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但被记住是一把双刃剑。一份不是最近的性别研究（http://consumerist.com/2011/01/25/sexy-news-anchors-distract-male-viewers/ ）发现当男性受试者看新闻时，他们可以很好地回忆起长得漂亮的女主持人，但却想不起她们说了什么。而当涉及到有吸引力的男性主持人时，他们可能没法回忆起他领带的颜色，但他们却可以回忆起在中东问题上的最新进展。这并不是因为他们真正相信女主持人读新闻的资格不够，只是生物学在作怪罢了——潜意识在帮倒忙。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以女性在男性主导的行业里确实面临着困境：我们如何通过我们的能力让人记得，而不仅仅是因为我们是女性这个事实？我们如何确保我们被叫上舞台是因为我们的思想而不是我们的性别？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;12.数据提取（data ingestion）可以自动化吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个大数据的时代，数据提取不得不实现某种程度的自动化——否则其它事情便无从谈起。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更有趣的问题是如何最好地实现它的自动化。以及数据准备（data preparation）的哪些阶段可以在消化（digestion）过程中完成。我非常强烈地认为我的数据应该尽可能地「原始（raw）」。所以，你应该，比如说，不要自动化处理缺失数据的方法。我宁愿知道它是缺失的，而不是被系统所取代的。同样，我也更喜欢维护最高粒度的信息——比如说一位客户访问的页面的所有 URL 地址 vs. 只保留主机名（不太理想，但还行） vs. 只保留一些用户目录。从个人的角度来看，有很多理由反对第一种情况——但 hashing 这样的工具可以中和其中一些担忧。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;让我们谈谈如何做：在自动化过程方面存在 3 个真正重要的部分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果全数据流过大就灵活地采样：如果你每天要处理 500 亿个事件——只是将它们塞进 Hadoop 系统倒还好——但后续操作却很繁琐。相反，除了 Hadoop 备份过程之外，有一个能将有特定价值的事件取出的过程是很好的。详情可参阅我们写的这篇博客文章：http://www.kdnuggets.com/2016/08/automated-data-science-digital-advertising.html&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态历史的注释：拥有所有的事件日志是很不错，但对于预测建模，我通常只需要有能获取实体历史的特征就行了。每一次都加入超过十亿行来创建历史是不可能的。所以部分的提取过程其实是一个注释过程，该过程能为每一个事件附加重要的信息。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;有统计测试该评估，看输入数据流的性质是否正在改变，当比如说一些数据源临时变暗时发送警报。一些相关内容请查看这里：http://www.troyraeder.com/papers/kdd12.pdf&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;13.如果想从数据科学家转向数据科学团队经理有哪些挑战？怎么做准备？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这十多年来，我一直在回拒让我管理数据科学团队的要求。而现在，我非常享受和一个团队进行互动、开大脑风暴会议、和某些人一起探究我的想法——但我相当不喜欢告诉别人该做什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人曾见过很多优秀的数据科学家不是很好的管理者——但这可能在任何领域都是一样。许多时候优秀的数据科学家会感到他们事业的唯一进展是开始领导一个团队。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我个人来看，我发现不自己做数据科学工作的前景是相当艰难的。我也有重大的信任问题——我超级怀疑我自己的结果——所以让其他人来做我的工作是难以接受的。我的忧郁的原因是工作成果的很多方面都依赖于数据准备过程中很多微小的细节。比如说：你删除重复项了吗？样本选择的任何细节都是至关重要的。除了非常了解你的团队的成员的长度和短处，以及知道什么人值得托付怎样的任务——这里要做一点准备。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为经理，一项非常重要的技能是与不同的业务部门互动，以确认需要做哪些工作，哪些工作优先。那需要了解数据科学的广阔图景以及很好的沟通技巧。作为准备，我建议你尽可能地参与到你目前所从事的项目的界定的过程中，并开始练习如何与企业的利益相关者交流。事实上，任何涉及到向不太技术的观众进行公开演讲的机会都对此会很有帮助。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 谷歌、微软合著论文：由知识引导的结构化注意网络</title>
      <link>http://www.iwgc.cn/link/2707040</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoy37z8zSoib7zfs5LbHwQyCHgWhE7V60Swra3d985epfcPGWZpfoicPcg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自然语言理解（NLU）是口语对话系统的一个核心组成部分。最近的循环神经网络（RNN）凭借其随时间保存序列信息的强大能力在 NLU 上取得了很好的结果。传统而言，NLU 模块根据话语的扁平结构标记话语的语义槽（semantic slot），其基本 RNN 结构是一个线性链（linear chain）。但是，自然语言展现的语言属性能为更好的理解提供丰富的、结构化的信息。这篇论文介绍了一种全新的模型——由知识引导的结构化注意网络（K-SAN：knowledge-guided structural attention network），这是一种广义的 RNN，再加入了由先前的知识引导的非扁平的网络拓扑。其有两个特点：1）可以从小型训练数据集中获取重要的子结构，让模型可以泛化到之前从未见过的测试数据；2）该模型可以自动找出对预测给定句子的语义标签至关重要的显著子结构，从而可是实现理解性能的提升。在航空旅行信息系统（ATIS）数据基准上的实验表明我们提出的 K-SAN 架构可以使用注意机制（attention mechanism）有效地从子结构中提取出显著的知识，其表现超过了当前最佳的基于神经网络的框架。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;点击「阅读原文」，下载论文↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>业界 | 量子计算未来布局：这18家公司走在了世界前列</title>
      <link>http://www.iwgc.cn/link/2707042</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自CB Insights&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、Cindy、Rick、吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子计算机很快就能真的有用了。世界上最大的几家公司正在尝试让这项技术走出实验室。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicicgGRHSzjquibiayCAjw0uIoNIiaetXdge0GaR06yia8GFTk50EA4qBTkbc0jpUg0oNfcv4lmcl6PuXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一台量子计算机利用一种叫量子比特（qubit）的亚原子粒子来加速解决复杂的计算。量子计算机近期有望解决包括从优化到量子加密通信在内的许多问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;根据CB Insight的投资、收购、和合作关系数据，我们列出了 18 家涉足商品化量子计算硬件和软件的企业。它们来自不同的行业，从科技行业巨头到国防承包商再到国家电信公司等等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们名单中出现的仅限于那些被报道近三年内在量子计算领域有独特重大技术突破的公司：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;空中客车（Airbus）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;阿里巴巴（Alibaba）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;博思艾伦咨询公司（Booz Allen Hamilton）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;英国电信公司（British Telecommunications）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌（Google）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;惠普（Hewlett Packard）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;IBM&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;英特尔（Intel）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;KPN&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;洛克希德·马丁（Lockheed Martin）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微软（Microsoft）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;三菱（Mitsubishi）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NEC&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;诺基亚（Nokia）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;NTT&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Raytheon&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;SK电讯（SK Telecom）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;东芝（Toshiba）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.空中客车公司（Airbus）希望量子计算机可以加强航空航天软件&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015年年末，Airbus 集团在威尔士（Wales）的 Newport 成立了一个团队来解决量子计算的问题。Airbus 的 Defence 和 Space 部门主要的任务将会是学习所有有关量子力学的技术，涵盖从密码术到计算。Airbus 不仅打算发展自己的量子计算硬件，还想要将现有的量子设备适用于具体的航空航天问题， 也就是那些需要处理和储存的大量数据（包括卫星流图象的分类和分析）或飞机超耐用材料的创造。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.阿里巴巴打算用量子计算机来开发更安全的电子商务和支持电子商务的数据中心&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 7 月，阿里巴巴的阿里云单位与中国科学院在上海建立了一个研究机构，称为阿里巴巴量子计算实验室（Alibaba Quantum Computing Laboratory）。该实验室的目标是为电子商务和数据中心研究量子安全技术。值得一提的是，同年，中国还发射了世界上第一个量子卫星，旨在发展安全的长距离量子通信。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.博思艾伦咨询公司（Booz Allen Hamilton）想要为政府和商业客户提供量子计算&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Booz Allen Hamilton 是发展中的群体的一部分，为了更好的数据-科学产品和服务，它正在竞争中寻找优势。为此，这家管理咨询公司说，它与政府和商业客户合伙开发实验项目和量子计算原型，并最终通过使用被称为量子退火设备（Quantum Annealers）的特殊设备来解决优化问题 。量子退火设备可以利用量子状态的粒子的自由演化来执行优化问题的计算。Booz Allen Hamilton 感兴趣的领域包技系统与网络优化、车辆路径、物流、工作计划、药品开发、制造业、系统设计和软件中复杂代码的验证&amp;amp;检验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4.英国电信（British Telecommunications/BT）正在研究量子计算用以保护其网络上传输的敏感信息&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;BT 与东芝公司、爱德华光网络公司（ADVA Optical Networking）和英国国家物理实验室一起合作，研究并实施量子加密，该技术是指光子——光粒子——可用来分配用于保护敏感数据的加密密钥，比如财务或健康记录数据。根据英国知识产权局专利情报组，截至 2013 年， BT 在量子保密通信技术领域优先专利和专利申请者数量方面排名全球第六。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5.谷歌希望通过量子计算来获得更好的人工智能和更好的复杂优化问题的解决方案&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌在量子人工智能实验室（Quantum Artificial Intelligence lab /QuAIL）拥有一台 D-Wave Systems 的量子计算机。该实验室由美国宇航局（NASA）以及位于加利福尼亚州芒廷维尤的美国宇航局艾姆斯研究中心（NASA Ames Research Center）里的大学空间研究协会（Universities Space Research Association）共同承办。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;D-Wave Systems Inc 是世界上第一个商用量子计算机公司——它与谷歌的交易是 D-Wave 历史上最大的一笔。谷歌及其合作伙伴拥有长达七年的最新 D-Wave 机器访问权限，期间新一代 D-Wave 系统将被安装在美国宇航局艾姆斯研究中心设备上以供使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;领导谷歌 QuAIL 工作的 Hartmut Neven 及其团队最近出版了一篇有关其 D-Wave 2X 计算机的论文，它展示了表明该机器的计算执行速度能够比一块经典的计算机芯片快 1 亿倍速的初步测试结果。早在 2013 年，该财团已利用 D-Wave 的机器在 Web 搜索、语音/图像模式识别、规划和行程安排、空中交通管理、机器人外太空任务等应用中进行量子计算的探索，并支持任务控制中心的操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2014 年，为了减少机器学习与人类智能之间的差距——且为了在人工智能的新兴领域中取得领先地位——谷歌开始利用其在 D-Wave 机器上的经验并专注于开发自己的量子硬件。谷歌为此雇佣了圣巴巴拉市加利福尼亚大学的一位物理学教授 John Martinis 及其团队，来建立谷歌的专属量子芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;6.惠普（Hewlett Packard/HP）专注于出售小规模的量子计算机或模拟器，以增加现实世界使用技该术的案例&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;惠普的量子信息处理组（Quantum Information Processing Group）位于英国的布里斯托尔的惠普实验室， 而且它是信息和量子系统实验室（Information and Quantum Systems Laboratory）的一部分。该实验室主要的重点领域包括计算、加密和通信。惠普实验室将它目前正在开发的东西称为「The Machine）」。这款计算机是惠普在建立量子「内存驱动计算」处理器方面的一个尝试，是惠普实验室最大的项目。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;7.IBM 正在致力于制造带有纠错保护的超导电路——保护量子信息免于被称作量子噪声的现象的干扰&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 研究院在其位于纽约约克镇高地的研究中心拥有一个量子计算小组。2015 年 4 月，他们宣布了一种能够检测出比特翻转（bit-flip）和相位翻转（phase-flip）误差的新电路。要想检测并纠正量子计算系统中可能存在的两类误差，这一突破对于克服该挑战来说是非常重要的。同年 12 月，IBM 被授予了一份 IARPA 补助金，以在逻辑量子比特项目（Logical Qubits program）中使用该技术。该项目的目标是主要通过建立一个可以扩展到更高维度的量子电路设计，来克服当前量子系统的局限性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;8.英特尔专注于利用量子计算来在先进制造业、电子工业和更好的系统架构设计中受益&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 9 月，英特尔向代尔夫特工业大学的量子研究所 Qutech 以及荷兰应用研究组织拨了 5000 万美元，用于 10 年合作期的工程支持供给。英特尔 CEO Brian Krzanich 发表了一篇博客，详细描述了公司在量子计算领域的战略利益，以及电子工业和制造业的专业知识在量子计算实践方面的相关性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;9.KPN 想要更好的后量子密码算法（post quantum cryptographic algorithms）来保护通信安全&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;KPN 是瑞士的一家固定电话和移动通信公司，它在其位于海牙和鹿特丹的 KPN 数据中心网络之间实现了端到端的量子密钥分配（Quantum Key Distribution/QKD）。KPN 正与 ID Quantique 合作，后者是一家专门从事量子加密的瑞士公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;10.洛克希德·马丁（Lockheed Martin）通过量子计算改进当前的软件校验/验证等等&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在与南加州大学的合作中，洛克希德·马丁联合设立了USC-洛克希德·马丁公司量子计算中心（QCC）。该中心的研究重点是利用隔热量子计算（adiabatic quantum computing）的力量，在这种计算中，问题可被编码成物理量子系统的最低能态（「最冷」），以寻找带有许多变量的特定问题的最优答案。除此之外，D-Wave Systems 曾在 2015 年宣布与洛克希德·马丁达成了多年协议以将该公司的 512 量子位 D-Wave Two 量子计算机更新成带有 1000 多个量子位的 D-Wave 2X 系统。这是自洛克希德·马丁 2011 年成为 D-Wave 的首位客户以来第二次系统升级。在 2014 年，Aerospace Concepts 公司与洛克希德·马丁 进行了一场合作研究，并最终合作组建了一家新公司 QxBranch——为金融、石油和天然气、航空航天和生物技术行业提供量子计算机产品和服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;11.微软正在为量子计算机创造专用软件和硬件&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的 QuArC 部门成立于 2011 年 12 月，其关注的重点是为可扩展的、容错的量子计算机的使用设计软件架构和算法。该机构值得关注的一项成就是 LIQUi|&amp;gt; ——一种用于量子计算的软件架构和工具套件。微软的 QuArC 组与全世界的许多大学都建立了紧密的合作关系，其中包括代尔夫特理工大学、Nils Bohr 研究所、悉尼大学、普渡大学、马里兰大学、苏黎世联邦理工学院和加州大学圣巴巴拉大学（UCSB）。在 2014 年，微软透露自己在 UCSB 的校园内有一个名叫 Station Q 的小组正在研究拓扑量子计算（topological quantum computing）——旨在改善量子状态的控制设计。在 QuArC 组的软件和算法工作的基础上，Station Q 是微软一项跨世界的工作：将全世界的数学家、计算机科学家、量子物理学家和工程师集合起来构建混合超导/半导体设备，以用于受控环境中的应用，其最终目标是创造一种可扩展的、容错的通用量子计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;12.三菱电机正在开发加密移动通信的专用量子加密设备&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;三菱电机（Mitsubishi Electric）声称其已经开发出了世界上第一个「one-time pad software」——一种用于移动电话的先进加密技术，以确保电话通信保持机密。此外，该公司已经参与到由日本情報通信研究機構主导的一个项目中，以实际应用其技术以测试在量子安全网络上移动通信的可行性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;13. NEC 和富士通希望向其客户提供长距离的量子加密通信&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2015 年 9 月，东京大学的纳米量子信息电子研究所（ Institute for Nano Quantum Information Electronics）与富士通和 NEC 集团合作宣布实现了 120 千米长距离安全通信的量子密钥分布，该系统使用了一个单光子发射器系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;14.诺基亚是贝尔实验室的母公司，而贝尔实验室是量子计算算法开发的先驱&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;贝尔实验室的一些研究者是量子计算的先驱，其中包括 Peter Shor（Shor’s Algorithm）和 Luv Grover（Grover’s Algorithm）。贝尔实验室的这个页面（https://www.bell-labs.com/our-research/disciplines/quantum-computingcommunications/ ）上可以看到他们在量子计算上的最新研究活动。诺基亚也与牛津大学和洛克希德·马丁建立了合作来探索量子技术在强化优化和机器学习方面的潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;15.日本电报电话公司（NTT）想要依赖光子处理信息的基于量子的计算机芯片&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NTT 基础研究实验室（Basic Research Laboratories）和 NTT 安全平台实验室（ Secure Platform Laboratories）正在合作探索超冷原子和量子信息处理。在 2014 年，这家公司和来自英国布里斯托大学的研究者开发了一款光学芯片，该芯片可以使用光子来测试量子计算领域的新理论，其目标是减少测试量子理论之前所需的资源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;16.美国雷神正在探索传感器、计算机、数据安全和图像技术的未来&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;雷神 BBN 科技是一个研发中心，是雷神公司的一部分。该研发中心建立了一个量子信息处理组，专注于使用量子现象进行传感、计算和成像。2012 年，雷神 BBN 获得 IARPA 赞助的量子计算科学项目的 220 美金。雷神的目标是在一个框架中融合量子计算的多个方面，从而更好的管理资源并评估性能。其他合作方包括 NEC、滑铁卢大学、墨尔本大学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;17.韩国 SK 电讯想要在韩国建立一个量子安全加密通信网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2016 年 3 月，SK 电讯宣布它完成 5 个不同国家测试网络进行量子通信的首次展示，覆盖范围共为 256 千米。SK 通信的量子加密系统从 2011 年开始开发，被认为最安全的加密方法之一，使用的是量子物理，而非如今普遍使用的基于数学的加密算法。参与该项目的公司包括 Wooriro Co,Ltd，HFR Inc，韩国安全研究所，电子和电话通讯研究所，首尔大学，KAIST，韩国大学，韩国光州科学技术院，量子信息通信研究协会。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;18.东芝在追求量子秘钥分布和安全通信网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;东芝的原型量子秘钥分布系统为基于光导纤维计算网络上的密码学应用提供数字秘钥。尤其是东芝在 2015 年宣布来自东芝的生命科学分析中心的基因组数据被提名被一个量子计算系统加密，并被发送到日本东北大学的Medical Megabank Organisation。东芝掌握着世界上最大的量子 IP portfolios 之一。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Thu, 15 Sep 2016 17:53:53 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | 语音识别新里程碑：微软新系统词错率低至6.3%（附论文）</title>
      <link>http://www.iwgc.cn/link/2688676</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Microsoft&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Richard Eckel&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.75em; text-align: justify;"&gt;&lt;span&gt;微软研究者在追求计算机像人类一样理解语音的道路上取得了新的里程碑。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJjNjPliaNXrYLCWIzgh3GPWZfykoj3aCibWSTHtHDOqcLop2xgYguv17Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;黄学东&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软首席语音科学家黄学东在产业标准 Switchboard 语音识别任务的最新基准评估中报告出了这样一成果，微软研究者取得了产业中最低的 6.3% 的词错率（WER）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这周二发表的一篇研究论文中，黄学东说：「我们最好的单个系统在 NIST 2000 Switchboard 集上取得了 6.9% 的词错率。我们相信这是取得最好表现的不基于系统结合的单个系统。在 Switchboard 测试数据上，数个声学模型的结合将前沿成果推进到了 6.3% 的词错率。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上周，在旧金山举办的国际语音交流和技术大会 Interspeech 上，IBM 宣称他们取得了 6.6% 的词错率。20 年前，最好的研究系统的词错率是 43%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;黄说：「这一新的里程碑得益于过去 20 年中由来自不同组织的人工智能社区开发出的各种新技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJOlG27zVBQPTViaMDbuRW8XRLvRicR5q4wnG1829AsTWrwJCS73WnQ4XQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;IBM 和微软都提到了深度神经网络时代的到来，其受到了大脑生物处理方法的启发，也成为了语音识别技术进步的关键推动力。计算机科学家已经尝试了数十年去训练计算机系统做图像识别、语音理解这样的任务，但到目前为止，这些系统的准确度仍然不尽如人意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络构建于一系列的计算层之中。今年早些时候，微软研究员利用一个深度残差神经网络（deep residual neural network）系统赢得了 ImageNet 计算机视觉挑战赛，该系统使用了一种新型的交叉层网络连接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJFApmqAsjias8FPVwEibWAEG0IYsdXfvIgcib4RaiajSMnvic8iaQH7KnKeaw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Geoffrey Zweig&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的 Speech &amp;amp; Dialog 研究组的首席研究员兼管理者 Geoffrey Zweig 领头此次的 Switchboard 语音识别工作。他在微软带领新型训练算法、高度优化的卷积和循环神经网络模型、CNTK 这样的工具等内容的开发。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文：微软 2016 对话语音识别系统（The Microsoft 2016 Conversational Speech Recognition System）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJboMsrKleE606EJcBibpyFLwmIuDLOuANenxkP725frLicA0OXwFWK0og/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们描述了微软的对话语音识别系统，在该系统中我们结合了近期在基于神经网络的声学和语言模型上的进展，推进了在 Switchboard 识别任务上的顶尖成果。受到机器学习集成技术（machine learning ensemble techniques）的启发，该系统使用了一系列卷积和循环神经网络。I-vector 建模和 lattice-free MMI 训练为所有声学模型架构带来了显著的提升。使用了多个前向和反向运行 RNNLM 的语言模型重新计分（Language model rescoring）与基于后验的词系统结合为系统性能带来了 20% 的增益。最好的单个系统使用 ResNet 架构声学模型和 RNNLM rescoring，在 NIST 2000 Switchboard 任务上实现了 6.9% 的词错率。结合系统取得了 6.3% 的词错率，代表了在这一基准任务上对先前成果的改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近年，由于对卷积和循环神经网络的精心设计和优化，在降低语音识别错误率上我们已经看到了快速发展。尽管我们对基础架构已经很好地认识一段时间了，但它近期才成为了进行语音识别的最好模型。惊人的是，对声学模型和语言模型而言都是如此。相比于标准的前馈 MLP 或 DNN，这些声学模型有能力对大量带有时间不变性的声学环境建模，而且卷积模型还能应对频率不变性的情况。在语言模型中，循环模型通过对连续词表征（continuous word representations）的归纳能力，在传统的 N-gram 模型上实现了进步。同时，集成学习（ensemble learning）已经在多种神经模型得到了普遍的应用，从而通过减少偏差和方差改进稳健性。在此论文中，我们广泛地使用模型的集成，同时也改进单个组件模型，从而推进在对话电话语音识别（CTS）中的进展，CTS 从上世纪 90 年代就已经成为了检验语音识别任务的一项基准。这一系统的主要特征包括：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;对卷积神经网络和长短期记忆（LSTM）网络这两种基础声学模型架构的集成，每个架构也有多种变体；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 LACE 卷积神经网络中的一个注意机制，其可以有区别地为不同距离的语境赋予权重；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Lattice-free MMI 训练；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在所有模型中使用基于 i-vector 的改编版本；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在前向和反向过程中都运行带有多个循环神经网络语言模型的 language model rescoring；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;融合网络系统组合与最好系统子集搜索的耦合，这正是在有许多候选系统的情况下所需的&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该论文在其他部分对该系统进行了详细描述。Section 2 描述了 CNN 和 LSTM 模型。Section 3 描述了我们对 i-vector 改编版的部署。Section 4 展现了 lattice-free MMI 训练过程。语言模型 rescoring 是该系统的一个重大部分，在 Section 5 中有描述。实验结果呈现在 Section 6 中，随后是对相关工作和结论的讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ1I6omFmQMwEgcia2b57iasnc6Cy1NzIibLDPwFKglVpJCzYHE6dvmbB9w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;表 4. 在 eval 2000 set 上的来自 i-vector 和 LFMMI 的性能改进&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJ8OcjXl8BK8b2IZ00icLh92BTQbRDApNKibSy6np2RGCl1nsclMETPKGg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;表 5. 在 eval 2000 set 上不同声学模型的词错率。除非特别标注，所有的模型都在 2000 小时的数据上进行训练，有 9000 个 senones（聚类的结果）。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>业界 | MIT推出并行计算编程语言Milk：为大数据应用提速</title>
      <link>http://www.iwgc.cn/link/2688677</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自MIT&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Larry Hardesty&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、杜雪&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; color: rgb(62, 62, 62); line-height: 25.6px; white-space: normal; box-sizing: border-box !important; word-wrap: break-word !important; background-color: rgb(255, 255, 255);"&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;在今天的计算机芯片中，内存管理（memory management）是基于计算机科学家所称的局部性原理（principle of locality）：如果一个程序需要存储在某个内存位置的一个数据块，它可能也会需要这个数据块附近的数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这种假设在大数据的时代已被打破，现在的程序常常需要在许多巨大的数据集上任意离散的少量数据上进行操作。因为从它们的主内存区块中读取数据是今天芯片的主要性能瓶颈，所以不得不进行的更为频繁的读取会极大地减慢程序的执行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本周，在并行架构和编译技术国际会议（International Conference on Parallel Architectures and Compilation Techniques）上，来自 MIT 计算机科学和人工智能实验室（CSAIL）的研究者展示了一种名为 Milk 的新型编程语言，该语言可让开发者在需要处理大型数据集中离散数据点的程序中更高效地管理内存。研究人员在一些常用算法上对该语言进行了测试，发现使用这种新语言编写的程序的速度可达到用已有语言所编写的程序的 4 倍。但研究者相信进一步的工作可以让其速度得到更大的提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;电气工程和计算机科学教授 Saman Amarasinghe 说，今天的大数据集给已有的内存管理技术带来问题的原因不仅仅是因为它们很大，更多的则是因为它们是稀疏的（sparse）。也就是说，解决方案的规模并不总是与问题的规模成正比。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「在社会环境中，我们习惯了应对更小的问题。」Amarasinghe 说，「如果你看看这栋楼（CSAIL）里面的人，你能看到我们全部是互连的。但如果你从整个行星的规模上来看，我的朋友的数量也不会扩大规模。这颗行星上有几十亿人，但我的朋友仍然只有几百人。突然你就有了一个非常稀疏的问题。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Amarasinghe 说，类似地，比如说有一个有 1000 位客户的在线书店，可能会为其访客提供 20 本最受欢迎的书的清单。然而，这并不意味着，如果这家在线书店的客户达到 100 万了，它就会为其访客提供 20,000 本最受欢迎的书的清单。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;本地思考（Thinking locally）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天的计算机芯片并没有为稀疏数据进行优化——其实事实情况正好相反。因为从芯片的主内存区块读取数据的速度很慢，所以现代芯片的内核或处理器中都有自己的「缓存（cache）」——一种相对小的、本地的且高速的内存区块。内核不会一次只从主内存中读取单个数据项，而是会一次读取一整个数据块。而这个数据块是通过局部性原理进行选择的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;局部性原理（principle of locality）的工作方式很容易理解。以图像处理任务为例：如果一个程序的目的是在图像上应用一种视觉滤镜（visual filter）而且一次只能处理图像上的一块，那么当内核请求一个块时，它应该就还会收到其缓存所能容纳的所有相邻的块，这样它就可以一块接一块地处理，而不需要再取更多数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但如果该算法的目标是从在线零售商数据库的 200 万种书中取出仅仅 20 种，那这种方法就不管用了。如果它请求与某一种书相邻的数据，很有可能其相邻的 100 种书的数据都是没有关联的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从主内存中一次只读取一个数据项是非常低效的。「这就像是，每次你想要一勺麦片时，你都需要打开冰箱、打开牛奶盒、倒出一勺牛奶、盖上牛奶盒、将它放回冰箱。」Vladimir Kiriansky 说，他是电气工程和计算机科学的博士生，同时也是这篇新论文的第一作者。Amarasinghe 和 Yunming Zhang 是他的合作者，Zhang 也是一位电气工程和计算机科学的博士生。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;批处理（Batch processing）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Milk 只是简单地为 OpenMP 增加了一些命令；OpenMP 是一种 C 或 Fortran 等语言的扩展，可以帮助用来更轻松地为多核处理器编写代码。使用 Milk，程序员可以在任何指令附近插入几行代码，其可以在整个大数据集中迭代，寻找相对较少数量的项。Milk 的编译器（将高级代码转换成低级指令的程序）然后可以据此找到管理内存的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用 Milk 程序时，如果一个内核发现它需要一个数据项，它不会发出请求从主内存中读取它（以及其相邻的数据）。它会将该数据项的地址加入到一个本地存储的地址列表中。但这个列表足够长时，该芯片的所有内核都会池化（pool）它们的列表，然后将这些地址按临近排布的形式组合到一起，并将它们重新分配给内核。如此一来，每一个内核都只请求了它知道自己需要的数据项，而且可以高效地检索得到。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这种描述的层面较高，但实际上的细节会复杂得多。事实上，大部分现代计算机芯片都有多级缓存，一级比一级大，但效率也因此更低。Milk 编译器不仅必须跟踪内存地址列表，还要跟踪存储在这些地址中的数据，而且它常常将这两者在各级缓存之间进行切换。它也必须决定哪些地址应当被保留（因为可能需要被再次访问），哪些应当被丢弃。研究者希望能够进一步提升这种能够编排这种复杂的数据芭蕾舞的算法，从而进一步提升性能表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「今天许多重要的应用都是数据密集型的，但不幸的是，内存和 CPU 之间不断增大的性能鸿沟意味着当前的硬件还没有发挥出它们的全部潜力。」斯坦福大学计算机科学助理教授 Matei Zaharia 说，「Milk 通过优化常见编程架构中的内存访问来帮助解决这一鸿沟。这项成果将关于内存控制器设计的详细知识和关于编译器的知识结合了起来，能为当前的硬件实现良好的优化。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Michael Jordan最新论文：少于单次通过，随机受控的随机梯度方法</title>
      <link>http://www.iwgc.cn/link/2688678</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：Lihua Lei、Michael I. Jordan&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜雪、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibH9vW38W2lryaKibvbIIIiaJXYZeFoiaGicaLlOKsFbfBny7eINMgQqgE5zvricr0TziaO5OpSKC3cJLpw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们为基于梯度的优化开发并分析了一套程序，我们称之为随机受控的随机梯度（SCSG: stochastically controlled stochastic gradient）。作为 SVRG 算法家族的一员，SCSG 利用梯度在两个级别上估算。与这家族已有的其他算法不同，SCSG 的计算成本和通信成本不需要线性扩展样本量 n；事实上，当目标精确度较低时，这些成本是独立于 n 的。一个在 MNIST 数据集上的 SCSG 的试验评估显示：只需要带有 2.6 MB 内存的日常所用的机器和 8 次磁盘访问，它就能在这个数据集上生成精确的结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 14 Sep 2016 13:34:50 +0800</pubDate>
    </item>
  </channel>
</rss>
