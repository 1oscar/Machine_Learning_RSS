<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>重磅 | 谷歌翻译整合神经网络：机器翻译实现颠覆性突破（附论文）</title>
      <link>http://www.iwgc.cn/link/2869136</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Google Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：Quoc V. Le、Mike Schuster&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;昨日，谷歌在 ArXiv.org 上发表论文《Google`s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》介绍谷歌的神经机器翻译系统（GNMT），当日机器之心就对该论文进行了摘要翻译并推荐到网站（www.jiqizhixin.com）上。今日，谷歌 Research Blog 发布文章对该研究进行了介绍，还宣布将 GNMT 投入到了非常困难的汉语-英语语言对的翻译生产中，引起了业内的极大的关注。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;十年前，我们发布了 Google Translate（谷歌翻译），这项服务背后的核心算法是基于短语的机器翻译（PBMT:Phrase-Based Machine Translation）。自那时起，机器智能的快速发展已经给我们的语音识别和图像识别能力带来了巨大的提升，但改进机器翻译仍然是一个高难度的目标。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，我们宣布发布谷歌神经机器翻译（GNMT：Google Neural Machine Translation）系统，该系统使用了当前最先进的训练技术，能够实现到目前为止机器翻译质量的最大提升。我们的全部研究结果详情请参阅我们的论文《Google`s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》（见文末）[1]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;几年之前，我们开始使用循环神经网络（RNN：Recurrent Neural Networks）来直接学习一个输入序列（如一种语言的一个句子）到一个输出序列（另一种语言的同一个句子）的映射 [2]。其中基于短语的机器学习（PBMT）将输入句子分解成词和短语，然后很大程度上对它们进行独立地翻译，而神经机器翻译（NMT）则将整个输入句子视作翻译的基本单元。这种方法的优点是：相比于之前的基于短语的翻译系统，这种方法所需的工程设计更少。当其首次被提出时，NMT 在中等规模的公共基准数据集上就达到了可与基于短语的翻译系统媲美的准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自那以后，研究者已经提出了很多改进 NMT 的技术，其中包括模拟外部对准模型（external alignment model）来处理罕见词 [3]，使用注意（attention）来对准输入词和输出词 [4] 以及将词分解成更小的单元以应对罕见词 [5,6]。尽管有这些进步，但 NMT 的速度和准确度还没能达到成为 Google Translate 这样的生产系统的要求。我们的新论文 [1] 描述了我们怎样克服了让 NMT 在非常大型的数据集上工作的许多挑战，以及我们如何打造了一个在速度和准确度上都已经足够能为谷歌的用户和服务带来更好的翻译的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYYnia3jZRmI5bMIIluWHibScueHom3bXNCS0rORlZiaEoRuzsvkGVYc2TA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;来自对比评估的数据，其中人类评估者对给定源句子的翻译质量进行比较评分。得分范围是 0 到 6，其中 0 表示「完全没有意义的翻译」，6 表示「完美的翻译」。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下面的可视化图展示了 GNMT 将一个汉语句子翻译成英语句子的过程。首先，该网络将该汉语句子的词编码成一个向量列表，其中每个向量都表征了到目前为止所有被读取到的词的含义（「编码器（Encoder）」）。一旦读取完整个句子，解码器就开始工作——一次生成英语句子的一个词（「解码器（Decoder）」。为了在每一步都生成翻译正确的词，解码器重点注意了与生成英语词最相关的编码的汉语向量的权重分布（「注意（Attention）」，蓝色链接的透明度表示解码器对一个被编码的词的注意程度）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYKGC1GMI9FibcKU7hc2dia5qJzKVUg9B1FULZKcIs3wvxjCOsk7o57QfQ/0?wx_fmt=gif"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用人类评估的并排比较作为一项标准，GNMT 系统得出的翻译相比于之前的基于短语的生产系统实现了极大的提升。在双语人类评估者的帮助下，我们在来自维基百科和新闻网站的样本句子上测定发现：GNMT 在多个主要语言对的翻译中将翻译误差降低了 55%-85% 以上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYoD35dMSt4n4vib0HZUT5k5xxh1ohicNClItTUycqMiaW7lahOJFBYzVMQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;我们的系统产出一个翻译案例，其输入句子采样自一个新闻网站。这个地址（https://drive.google.com/file/d/0B4-Ig7UAZe3BSUYweVo3eVhNY3c/view?usp=sharing）可以看到更多随机采样自新闻网站和书籍的输入句子翻译样本。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天除了发布这份研究论文之外，我们还宣布将 GNMT 投入到了一个非常困难的语言对（汉语-英语）的翻译的生产中。现在，移动版和网页版的 Google Translate 的汉英翻译已经在 100% 使用 GNMT 机器翻译了——每天大约 1800 万条翻译。GNMT 的生产部署是使用我们公开开放的机器学习工具套件 TensorFlow 和我们的张量处理单元（TPU：Tensor Processing Units），它们为部署这些强大的 GNMT 模型提供了足够的计算算力，同时也满足了 Google Translate 产品的严格的延迟要求。汉语到英语的翻译是 Google Translate 所支持的超过 10000 种语言对中的一种，在未来几个月，我们还将继续将我们的 GNMT 扩展到远远更多的语言对上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器翻译还远未得到完全解决。GNMT 仍然会做出一些人类翻译者永远不出做出的重大错误，例如漏词和错误翻译专有名词或罕见术语，以及将句子单独进行翻译而不考虑其段落或页面的上下文。为了给我们的用户带来更好的服务，我们还有更多的工作要做。但是，GNMT 代表着一个重大的里程碑。我们希望与过去几年在这个研究方向上有所贡献的许多研究者和工程师一起庆祝它——不管是来自谷歌还是更广泛的社区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Google Brain 团队和 Google Translate 团队都参与了该项目。Nikhil Thorat 和 Big Picture 也帮助了该项目的可视化工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;论文：Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYT2r1HkDqWO8ianIib4y3rsKrCDj3oq8IicQMlqe2AkxEibuSfNp4D582gA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：神经机器翻译（NMT: Neural Machine Translation）是一种用于自动翻译的端到端的学习方法，该方法有望克服传统的基于短语的翻译系统的缺点。不幸的是，众所周知 NMT 系统的训练和翻译推理的计算成本非常高。另外，大多数 NMT 系统都难以应对罕见词。这些问题阻碍了 NMT 在实际部署和服务中的应用，因为在实际应用中，准确度和速度都很关键。我们在本成果中提出了 GNMT——谷歌的神经机器翻译（Google's Neural Machine Translation）系统来试图解决许多这些问题。我们的模型由带有 8 个编码器和 8 个解码器的深度 LSTM 网络组成，其使用了注意（attention）和残差连接（residual connections）。为了提升并行性从而降低训练时间，我们的注意机制将解码器的底层连接到了编码器的顶层。为了加速最终的翻译速度，我们在推理计算过程中使用了低精度运算。为了改善对罕见词的处理，我们将词分成常见子词（sub-word）单元（词的组件）的一个有限集合，该集合既是输入也是输出。这种方法能提供「字符（character）」-delimited models 的灵活性和「词（word）」-delimited models 的有效性之间的平衡、能自然地处理罕见词的翻译、并能最终提升系统的整体准确度。我们的波束搜索技术（beam search technique）使用了一个长度规范化（length-normalization）过程，并使用了一个覆盖度惩罚（coverage penalty），其可以激励很可能能覆盖源句子中所有的词的输出句子的生成。在 WMT' 14 英语-法语和英语-德语基准上，GNMT 实现了可与当前最佳结果媲美的结果。通过在一个单独的简单句子集合的人类对比评估中，它相比于谷歌已经投入生产的基于短语的系统的翻译误差平均降低了 60%。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[1] Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean. Technical Report, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[2] Sequence to Sequence Learning with Neural Networks, Ilya Sutskever, Oriol Vinyals, Quoc V. Le. Advances in Neural Information Processing Systems, 2014.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[3] Addressing the rare word problem in neural machine translation, Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[4] Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. International Conference on Learning Representations, 2015.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[5] Japanese and Korean voice search, Mike Schuster, and Kaisuke Nakajima. IEEE International Conference on Acoustics, Speech and Signal Processing, 2012.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;[6] Neural Machine Translation of Rare Words with Subword Units, Rico Sennrich, Barry Haddow, Alexandra Birch. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016.&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 28 Sep 2016 11:01:38 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | 日本科学家开发精密控制量子纠缠态的新技术，成功率提高60倍（附论文）</title>
      <link>http://www.iwgc.cn/link/2869137</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自phys.org&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;30 年前人们对量子计算机的想象是，它有潜力能快速精确地完成一些对人类和传统计算机来说不可能的事情。但是一个很大的问题是：微尺度量子效应很容易崩溃，以至于可靠供应能量的计算机一直无法真的实现。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在，日本的一组科学家或许已经克服了这个障碍。他们用激光开发出一个精确持续的控制技术，其持续维持「量子比特」生命周期的成功率要比之前的技术高上 60 倍，而量子比特是量子计算机编码的基本单元。尤其是这些研究者已经展示出他们能持续创造一个纠缠状态的量子行为——纠缠超过一百万个不同的物理系统。在他们数据存储的调查范围内，这是一个世界记录。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这一成果非常重要，因为纠缠的量子粒子，比如原子、电子和光子，都是量子信息处理的来源，而这些是由微小尺寸下的量子行为创造出来的。利用它们开创了一个崭新的信息技术时代。在叠加和纠缠这些行为中，量子粒子能同时处理巨量的计算任务。他们的研究报告发表在本周的 APL Photonics 期刊上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「在量子信息处理中，量子比特的生命周期有一个问题。我们已经解决了这个问题，而且我们能在我们想要的任何时间段内持续进行量子信息处理，」东京大学工程学院应用物理系的 Akira Furusawa 解释道，他是该项研究的领导人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「这一成果最困难的地方是被压缩（squeezed）的光束之间的连续相位锁定，但是我们已经解决了这个问题。」基于集成电路和硅芯片的计算机是当下主导的信息处理技术，而量子计算机被认为继它们之后的下一代计算。目前计算机使用一长串的 0 和 1，也被称为 bit，来处理信息。相比之下，量子计算机通过利用量子力学的强大力量，能编码在量子状态下被称为量子比特的 0 和 1。量子比特有两种不寻常的配置方式：「叠加」和「纠缠」。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;量子系统可以同时以好几种状态存在，比如衰变和未衰变两种状态的叠加。粒子也表现出纠缠的量子行为，这是量子之间的亲密性质，能将它们完美地结合在一个共同的存在中，即使它们之间的距离非常远。用爱因斯坦的话说，就是鬼魅。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来在迈向量子计算粒子的道路上，Furusawa 设想要创建二维和三维的晶格状的纠缠状态。「这将会使我们实现拓扑量子计算，一种非常强大的量子计算，」他说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文：Invited Article: Generation of one-million-mode continuous-variable cluster state by unlimited time-domain multiplexing&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic8AYNx1rVQyUcIs8bbyDhYYZZwCNamicKgSnia4hicD38tDOZVFkiapzLcW9wk2txFcbA0DNYWicPKPCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：在最近的量子光学的连续可变实验中，完全不可分的光模式的数量通过引入一个复用方案（不管是时域还是频域）而实现了急剧的增长。其中，Yokoyama et al.[Nat. Photonics 7, 982 (2013)] 报告了在时域复用进行修改的实验的成果，我们的研究表明其可以连续生成超过一百万种模式的完全不可分的光模式（fully inseparable light modes）。我们所得到的多模式状态可用作双轨连续可变集群状态（dual-rail continuous variable cluster state）。我们通过光学系统的连续反馈控制规避了之前的光学相位漂移（optical phase drifts）问题，这个问题将完全不可分光模式的数量限制在了大约一万。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 28 Sep 2016 11:01:38 +0800</pubDate>
    </item>
    <item>
      <title>招聘｜图普科技第二季谜题招聘活动正式启动</title>
      <link>http://www.iwgc.cn/link/2869138</link>
      <description>&lt;blockquote style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;p&gt;&lt;span&gt;今年6月份的时候，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716316&amp;amp;idx=4&amp;amp;sn=3c38c2d65d2f3d58f4e267528ab10092&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650716316&amp;amp;idx=4&amp;amp;sn=3c38c2d65d2f3d58f4e267528ab10092&amp;amp;scene=21#wechat_redirect"&gt;机器之心发布了一期图普科技以Mario为主题的深度学习谜题招聘活动&lt;/a&gt;，在DL爱好者圈内引发了不少讨论。现在，图普科技第二期谜题招聘活动已经开启，本次我们准备了彩蛋更多的DL招聘海报，同时，Node.js方向的谜题也加入了本期招聘。&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;深度学习招聘&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2V9TFXKwdEurJ1nj7RibZ3xVrZxXpQkt1errOZtbVVXaNkVj0nMh8b0A/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;你想要成为谁&lt;span&gt;，你想要去到哪，还剩&lt;/span&gt;多少勇气？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一张门的背后&lt;span&gt;，&lt;/span&gt;一条路的尽头&lt;span&gt;，&lt;/span&gt;还有多少可能？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「每个梦都像任意门&lt;span&gt;，&lt;/span&gt;往不同世界&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; &lt;/span&gt;而你的故事&lt;span&gt;，&lt;/span&gt;现在正是起点。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;Node.js招聘&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2uPkajTibgM0iaGM5q2ZjsHYltqbfdiciaIQRFQabJmojFazBGiakHicp6S0Q/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每棵参天的大树，都萌芽于一颗不安分的种子；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个梦想的实现，都始于一颗不忘勇气的初心。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「&lt;/span&gt;&lt;span&gt;小小的天 留过的泪和汗&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总有一天我有属于我的天&lt;span&gt;」&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;活动说明&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;＊前10名完成谜题的挑战者，直通技术终面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;＊前60名完成谜题的挑战者，优先获得面试。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;＊全职/实习皆可，&lt;span&gt;欢迎应届生/非应届生前来挑战。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;＊点击阅读原文开始答题，推荐使用Chrome浏览器。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;－END－&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 28 Sep 2016 11:01:38 +0800</pubDate>
    </item>
    <item>
      <title>专栏 | 基于行为事件的客户画像理论基础之哈耶克统一意识表达框架</title>
      <link>http://www.iwgc.cn/link/2869139</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心专栏&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者：袁峻峰&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文是文章《我们为什么要这样联想|用哲学论证客户画像体系的复杂性》中 &amp;nbsp;[4] 提出基于行为事件的客户画像的理论探讨。客户历史行为事件构建客户画像可以认为是「哈耶克将『自我』理解为能够统一表达全部意识事件的时空框架」[3] 的一种应用。了解更多内容可浏览文章《&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718634&amp;amp;idx=5&amp;amp;sn=1fe554973deb996d761ce3af40b37edd&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650718634&amp;amp;idx=5&amp;amp;sn=1fe554973deb996d761ce3af40b37edd&amp;amp;scene=21#wechat_redirect" style="font-size: 12px; text-decoration: underline; color: rgb(136, 136, 136);"&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;专栏 | 基于客户行为事件的跨领域统一推荐模型探讨&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;》。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在前文 [4] 中，基于行为事件的客户画像的想法源自「每个插曲，每一个决心，每一种不合时宜的行动，都象征着唐吉坷德」[2]. 仅凭福柯大师的一句话，似乎还是缺乏理论基础。虽然我们常常以特定行为去标记他人「原来你是这样的人！」；虽然孔夫子也说过「观其言而察其行」；但这些都不是系统的理论基础。在寻找模型的理论基础过程中，读到了汪丁丁教授「哈耶克《感觉的秩序》导读」[3], 终于找到基于行为事件的客户画像的理论基础, 那就是哈耶克的基于事件的统一意识表达框架！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;一、哈耶克基于事件的统一意识表达框架&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然业界的客户画像基本上都是基于标签体系。特别是前些日子的百度世界大会上大力的推荐其客户画像能力。给用户打了 60 多万个标签，做到千人千面，在不同领域取得的很棒的应用效果。和福柯认为特征体系是存在随意性，特征的确认是困难的相一致。哈耶克也认为，意识是难以定义的，与之类似的问题，感知的特征也是难以确认的。所以我们不应纠结于意识是什么而更应关心意识做了什么「Not asking what consciousness 『is』but by merely inquiring what consciousness does」[1]。哈耶克认为这是可以通过有意识过程行为以及无意识过程行为观察的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些过程行为是指发生在我们身上用于描述自我与其他人不同的有意识以及无意识行为事件。在此基础上，哈耶克构建了「统一表达全部意识事件的时空框架」[3]( Common Spatio-Temporal Framework)，其认为所有可重复的、想象的、过去的、可能的事件都关联到连续的「自我」意识表达。借此框架可以解决以下问题：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;通过行为事件标记一个人，他是什么，做过什么。「Be able to 『give an account』 of what he is or has been doing」.[1]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;「借助于意识事件的统一表达框架，行为主体得以『想象』和『预期』未来事件的样式及后果」[3].&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;并且哈耶克认为事件是通过注意力被感知的，这符合行为经济学的有限理性假设，即只有被感知的事件特征会被用于预期。而不论是主体注意力对事件的特征属性提取，还是预期过程都是基于主体历史的意识经验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如汪丁丁教授总结的「在哈耶克看来，对人类这样具有『有限理性』的动物而言，没有』客观』知识，只有』主体间客观』（Intersubjective）的知识过程。其次，』知识』不是静态的一堆观念，它们是』过程』。」[3] 所以我们可以认为，以过程的视角，通过个人历史行为事件数据去构建客户画像是与哈耶克基于事件的统一意识表达框架的相一致的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;二、场景中的事件&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&amp;nbsp;如何将哈耶克所描述的有意识无意识个体行为事件与当下大数据中个体行为事件相结合呢？我们知道在行为经济学以及心理学中，情景记忆（Episodic Memory）是构成自我意识至关重要的一种长期记忆。而互联网中场景化，特别是互联网金融的目标之一是将金融服务融入各个场景中。自然，数据也是有场景化特征的。如果我们不再特意区分线上场景事件、线下场景事件、有意识行为事件、无意识行为事件，那么在哈耶克意识事件统一表达框架下的行为主体未来事件后果预测，也可以应用为在基于行为事件的客户画像体系下场景事件的预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;三、结论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文简单的介绍了哈耶克的统一意识事件框架，并将其视为基于行为事件的客户画像体系下跨领域统一推荐模型理论基础。并结合行为经济学以及实践中数据特性，将意识事件进一步理解应用为基于场景事件，为之后具体应用提供可能性。欢迎各位同业讨论相关应用案例可行性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[1] 哈耶克．The Sensory Order: Inquiry into the Foundations of Theoretical Psychology[M]．1952.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[2] 米歇尔•福柯, 莫伟民 译．词与物 [M]．上海三联书店. 2002.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[3] 汪丁丁. 哈耶克《感觉的秩序》导读 [OL].&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;[4] 袁峻峰. 我们为什么要这样联想|用哲学论证客户画像体系的复杂性 [OL]. 大数据文摘 (公众号). 2016-09-14.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;作者介绍&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：袁峻峰，花名观妙，蚂蚁金服人工智能部，复旦金融学硕士，FRM 金融风险管理师。10 年以上从事金融IT相关领域工作经验：国内银行间市场金融产品（包括衍生产品）的量化分析、市场风险管理以及相关系统实现。目前从事并关注于金融领域机器学习相关主题与应用，欢迎探讨, 邮箱yuanjunfeng_fr@163.com &amp;nbsp;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;本文版权属于袁峻峰，仅代表个人观点。如需转载请联系作者（微信号 jake-80 ）&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 28 Sep 2016 11:01:38 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Facebook 开源人工智能环境CommAI-env，目标是实现人机之间的语言交流（附论文）</title>
      <link>http://www.iwgc.cn/link/2854955</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Facebook Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Facebook 今天开源重磅项目 CommAI-env，这是一个开发基于通信的人工智能系统的平台，本文整合了该项目的 README.md 文件和相关论文的摘要。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2oJ45SVbGgYCqUTQpUoDc0UUe4qpQYkmBjNKZKWC2oWHhyf51kM3Qhw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;开源地址：https://github.com/facebookresearch/CommAI-env&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env（基于通信的人工智能环境（Environment for Communication-based AI））是一个用于训练和评估人工智能的平台，该平台已经在论文《A Roadmap towards Machine Intelligence》中进行过了描述。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2UveKaNObbFpNSI1IxW7HVCydHwZn1YQNhHoCyib9ZgWmnuyHdxm68pQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;介绍&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env 是一个用于训练和测试人工智能系统（Learner（学习器，可以系统开发者所选择的任意语言编程））的平台，其使用了一个基于通信（communication）的设置，其中它可以通过一个 bit 层面的接口与 Environment（环境）进行交互。该 Environment 会要求 Learner 去解决一些基于通信的 Task（任务），并为其已经成功完成的每一个任务实例分配一个 Reward（奖励）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该 Learner 会以随机的顺序处理所有任务中的多个实例，而且其必须尽可能多地解决它们以获得最大的奖励。目前可以实现的任务包括计数问题、Learner 必须学习记忆项的列表并回答有关问题的任务、或通过基于文本的引导方案按引导指令执行任务（任务的详情描述请参阅该文档：https://github.com/facebookresearch/CommAI-env/blob/master/TASKS.md）。其任务范围现已开放：我们正在扩展任务范围，我们也邀请其他人能够参与进来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CommAI-env 的最终目标是提供一个可用于从头开始训练 Learner 的环境，从而使其能够实现通过语言的与人类的真正交互。尽管这样的任务可能看起来微不足道（但你可以试试在我们所支持的混杂的模式中解决它们——你的英语知识根本没用！），但我们认为其中大部分任务都超出了现有的基于学习的算法的能力；而要让一个 Learner 能够解决所有这些任务，就必须要在与人类交互所需的通信智能（communicative intelligence）上取得重大的进展，并且还要能从人类老师那里获得进一步的学习。注：我们并不是说 CommAI-env 中的任务覆盖了一个智能通信代理所应该处理的所有技能。我们是说：为了解决 CommAI-env，智能代理必须要有非常宽泛的学习能力，这样它应该才可以通过与人类交互和其它方式获得它快速所需的所有进一步的任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以下是 CommAI-env 有别于其它目前用于训练和测试人工智能系统的环境和数据集（比如，OpenAI Gym、Allen AI Science Challenge、MazeBase 或 bAbI）的一些基本特征，这些特征被设计出来的目的是为了鼓励开发者开发快速的、通用的、基于通信的 Learner。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CommAI-env 的关注重点完全是基于通信的任务，其中所有的通信都是通过 Learner 和 Environment 之间的一个共同的 bit 层面的接口实现的。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在单个 CommAI-env 会话（session）中，Learner 会被暴露在许多种任务中，所以它必须学会识别不同的任务，并将不同的技能合适地应用到这些任务上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;许多任务都是渐进的，在这个意义上，解决其中一个或多个任务应该能让其它任务的解决更简单，只要 Learner 有对数据和算法的长期记忆（例如，一旦一个 Learner 解决了基本的计数任务以及如何将物体和属性关联起来，那么计数一个物体的属性就会更容易）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;训练和测试阶段并没有明显的区分：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: circle;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;一方面，Learner 应该不只是记忆一个固定任务集合的解决方案，而且还要学习如何将其泛化到其所遇到的新任务上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;另一方面，就和人类一样，Learner 应该只需要少数几次遭遇后就能解决基本的问题：因此，学习的速度应该被考虑在评估之中。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们计划当该平台足够成熟时开展一个基于 CommAI-env 的比赛。为此，CommAI-env 的评估配置的任务集合将不同于开发版本中所包含的任务。这些任务不同的方面可能包括：它们可能基于不同的（自然和人工）语言、它们可以需要学习 Learner 周围的新物体和新位置、它们可能需要重新组合在开发过程中所学到的技能、等等。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;运行该环境有以下两步:&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;# Creating a configuration file (for instance, by copying the full training set)&lt;br/&gt;cp task_config.sample.json tasks_config.json&lt;br/&gt;# Running the environment, in the simplest case, just providing the configuration file as an argument&lt;br/&gt;python run.py tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;默认情况下，该环境会在人类模式下运行（见下方）。如果你想用一个给定的算法来运行该环境，请见下方相应的部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;配置&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，你应&lt;/span&gt;&lt;span&gt;该创建一个配置文件标明要投给 Learner 的任务和命令。你可以从复制对应全部训练集的文件开始，如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;cp tasks_config.sample.json tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人类模式&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了在控制台接口上运行这个系统，在这个系统中你可以冒充该&lt;span&gt; Learner &lt;/span&gt;，运行如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;python run.py tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这会为你提供一个基于控制台的用户界面，与一下运作的环境互动。每次当该环境看上去静了下来并期待&lt;span&gt; Learner &lt;/span&gt;回答时，控制就转换到能输入字符串返回环境的用户手中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两者之间的沟通，当下的时间和积累的奖励都会显示在屏幕上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了更好地理解该学习算法面对的问题的种类，你可以使用该 scrambled flag 运行该环境，它会用随机的伪词（pseudo-word）替换所观察到的词汇中的每一个词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;警示：注意人类模式关于输入有两个来自教授者的假设。第一个假设是关于字符编码。虽然输入事实上达到了位（bit）级别，但是对于人类用户来说，阅读一长串 bit，还是会不舒服，所以我们在将它们投放到屏幕上之前将它转换成了字符串。第二个假设是话轮转换机制（turn-taking convention），在这个假设下，当环境已经产生了两个连续的空间后，我们将控制交给人类。学习算法不能安全地假设这些机制（convention），因为它们可以在后续的任务迭代中被修改。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;指定学习算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用一个给定的学习算法来运行该环境，你可以使用学习器 class 的完全限定名称 -l 或 --learner flag。例如，你可以使用任何一个样本学习器：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;&lt;span&gt;learners.sample_learners.SampleRepeatingLearner&lt;/span&gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;&lt;span&gt;learners.sample_learners.SampleMemorizingLearner&lt;/span&gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;限定一个学习算法需要定义两个功能：next 和 reward.next 从该环境中接收一个 bit，并且应该回到&lt;span&gt; Learner &lt;/span&gt;说出的下一个 bit。reward 告知&lt;span&gt; Learner &lt;/span&gt;一个给定的接收奖励。在 Python 中，你可以从下面的代码片断开始创建一个&lt;span&gt; Learner &lt;/span&gt;：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;from learners.base import BaseLearner&lt;br/&gt;class MySmartLearner(BaseLearner):&lt;br/&gt; &amp;nbsp; &amp;nbsp;def reward(self, reward):&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# record receiving a rewarddef next(self, input_bit):&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# figure out what should be# the next bit to be spikenreturn next_bit&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用编程语言 X 定义一个&lt;span&gt; Learner &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;也可以用其他任何一种编程语言俩定义学习算法。为此，我们要包含一个learner.base.Remotelearner，它在一个任意的&lt;span&gt; Learner &lt;/span&gt;二进制与该环境之间建立一个 zeromq socket 接口。该环境作为一个服务器。为了方便，该用户能指定命令来发布这个学习者，所以它是在同一个过程中与环境一起发布的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当该 Session 创建后，该环境和学习器就被初始化了：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;该学习器通过向环境发送一个招呼「hello」来开始。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;循环：接受奖励，接受环境 bit，发送回复 bit&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;#include &amp;lt;string.h&amp;gt;&lt;br/&gt;#include "zmq.h"int main()&lt;br/&gt;{&lt;br/&gt; &amp;nbsp; // This is an example of a silly learner that always replies with '.'char reply[1];&lt;br/&gt; &amp;nbsp; int n = 0;&lt;br/&gt; &amp;nbsp; const char* response = "00101110"; &amp;nbsp;// '.' utf-8 code// connectvoid *context = zmq_ctx_new();&lt;br/&gt; &amp;nbsp; void *to_env = zmq_socket(context, ZMQ_PAIR);&lt;br/&gt; &amp;nbsp; int rc = zmq_connect(to_env, "tcp://localhost:5556");&lt;br/&gt;&lt;br/&gt; &amp;nbsp; // handshakezmq_send(to_env, "hello", 5, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; // talkwhile (true)&lt;br/&gt; &amp;nbsp; {&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// receive rewardzmq_recv(to_env, reply, 1, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// receive teacher/env bitzmq_recv(to_env, reply, 1, 0);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;// reply&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;reply[0] = response[n % strlen(response)];&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;zmq_send(to_env, reply, 1, 0);&lt;br/&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;n += 1;&lt;br/&gt; &amp;nbsp; }&lt;br/&gt;&lt;br/&gt; &amp;nbsp; zmq_close(to_env);&lt;br/&gt; &amp;nbsp; zmq_ctx_destroy(context);&lt;br/&gt;&lt;br/&gt; &amp;nbsp; return 0;&lt;br/&gt;}&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用 Learner Binary Run Session：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;python run.py tasks_config.json -l learners.base.RemoteLearner \&lt;br/&gt;--learner-cmd "/my/learner/binary"&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;控制台视角&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;鉴于人类模式，默认视角展示了一个控制台界面，你能观察正在进行的两部分之间的通信，当运行一个自动学习算法时，该视角默认为一个更简单的存在，从而让算法更快的运行。然而，仍然有可能通过参数 -v ConsoleView，或等效的 --view ConsoleView 来返回到控制台视角。例如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;python run.py -l learners.sample_learners.SampleRepeatingLearner -v ConsoleView tasks_config.json&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要求：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Python 2.6+&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;zeromq (for remote learners)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;论文标题：通向机器智能的路线图&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2gFia8vkeaOMlN5Nk0Gn1OtibjwPicOfJMEPqKiaDgKvsD5zDzvZEwAVYQQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;span&gt;智能机器的发展是计算机科学领域中面临的最大挑战之一。在这篇论文中，我们提出了这些机器应该有的一些基本性质，尤其是沟通和学习上。我们探讨了一个用于逐步教一台机器学习基于自然语言的交流的环境，将它作为与人类用户实现更加复杂的互动前提。我们也提出一些猜想，是关于该机器应该支持的算法，以便于能便捷地从该环境中学习。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Sep 2016 11:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | PaddlePaddle之后，百度开源深度学习硬件基准DeepBench</title>
      <link>http://www.iwgc.cn/link/2854957</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Baidu Research&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲、老红、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;谷歌、微软、Facebook 等传统的人工智能技术巨头之外，百度近来也加入到了技术开源的浪潮之中，继 PaddlePaddle 之后，百度又宣布开源了一项深度学习基准 DeepBench。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;开源地址：&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;https://github.com/baidu-research/DeepBench&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;1.DeepBench&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 是一个开源的基准工具，用来测量深度神经网络训练中的基础操作的表现。使用神经网络库，这些操作在不同的硬件平台被执行。如今测试基准工具 DeepBench 在 github 上已经开源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 的主要目标是 benchmark 对深度学习在不同硬件平台上而言很重要的运算。尽管深度学习背后的基础计算已经被很好的理解了，但在实践中它们被使用的方式惊人的不同。例如，矩阵相乘运算基于被相乘矩阵的大小和 Kernel 实现，可能是 compute-bound，也可能是 bandwidth-bound, 或者 occupancy-bound。因为每个深度学习模型带着不同的参数使用这些运算，面向深度学习硬件和软件的优化空间还是很大的，也是不足的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 试图解决这个问题，「在被用于训练深度神经网络时，在基础运算上哪种硬件提供最好的表现？」我们在低层次上详细说明了这些运算，建立深度学习处理器的团队很适合在硬件模拟中使用 DeepBench。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.1 DeepBench 适合用在哪里？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习生态系统包含不同的模块。我们想要强调 DeepBench 适合用于该生态系统的哪部分。在下面的图表中，描述了关于深度学习的软件和硬件组件。在最顶端是百度的 PaddlePaddle、Theano、TensorFlow、Torch 等这样的深度学习框架，这些框架使得我们能够建立深度学习模型。它们包含层（layer）这样的基础建筑模块，可通过不同的方式连接从而创造模型。为了训练这些模型，框架使用英伟达的 cuDNN 和英特尔的 MKL 这样的基础神经网络库。这些库执行矩阵相乘这样的用来训练深度学习模型的运算。最后，在英伟达 GPU 或英特尔 Xeon Phi 处理器这样的硬件上训练这些模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2e9M6RfMFXubcjvko6xfibLiccfnxQO07Ncfc327Gc0ibXiby4UQlJwn8Ww/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 使用神经网络库 benchmark 基础运算在不同硬件上的表现。它对建立应用的深度学习框架或深度学习模型没用。我们不能测量使用 DeepBench 训练整个模型所需要的时间。为不同应用建立的模型的表现特性彼此间差别很大。因此，我们要 benchmark 涉及到深度学习模型训练中的潜在运算。benchmark 这些运算有助于提高硬件供应商的 意识，也有助于软件开发者了解深度学习训练的瓶颈。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.2 方法论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepBench 包括一系列的基础操作（稠密矩阵相乘、卷积和通信）以及一些循环层类型。在开源的代码中有一个 Excel 表格描述了所有的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前向和后向的运算都会被测试。该基准的第一代版本将注重在 32 位浮点算法中的训练表现。未来的版本可能扩展到注重推理工作负载（inference workloads）和更低精度的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即使存在更快的独立库或公开了更快的结果，我们也将使用供应商提供的库。大部分用户将默认使用供应商提供的库，而且这种库更代表用户的体验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.3. Entry&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在释放在 4 个硬件平台上的结果：英伟达的 TitanX、M40、TitanX Pacal 和英特尔的 Knights Landing。硬件供应商或独立用户可运行大致的基准，并将结果输入到表格中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;2. 运算的类型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.1 稠密矩阵相乘&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;稠密矩阵相乘存在于大部分深度神经网络中。它们被用于执行全连接层和 vanilla RNN，以及为其他类型的循环层建立基石。有时它们也被用作快速执行新类层（在这里面自定义的代码不存在）的方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当执行 GEMM 运算 A * B = C 时，A 和 B 中的一个或两个都能被随意的换位。描述一个矩阵问题的常用术语是 triple（M,N,K）, 该术语描述了矩阵的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2uGysicfIFOj1QHCpoMFePBv7fDkO4wMDufEMGsbs5j5XQlYITrr05EQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.2 卷积&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积构成了网络中在图像和视频操作上的绝大多数的浮点计算，也构成了语音和自然语言模型网络中的主要部分，从模型表现角度来看，它可能也是唯一最重要的层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积有 4 或 5 维的输入和输出，为这些维提供了大量可能的排序。在改基准的第一代版本，我们只考虑了在 NCHW format 中的表现，即数据是在图像、特征映射、行和列中展示的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有很多计算卷积的技术对不同大小的过滤器和图像来说都是很好的选择，包括：direct approaches、基于矩阵相乘的方法、基于 FFT 的方法以及基于 Winograd 的方法。在该基准的第一代版本，我们没考虑不同方法的准确率，因为普遍共识是 32 位浮点计算对它们每个方法而言都是足够准确的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.3 循环层（recurrent layers）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环层总是由之前的运算与一元（unary）或二元（binary）运算这样的简单计算结合而成的——这些简单运算不是计算密集型的，通常只需占据总体运算时间的一小部分。然而，在循环层中，GEMM 和卷积运算相对较小，所以这些更小运算的成本变得有极大影响。如果开始计算就有一个很高的固定成本，那上述内容就尤其准确。也可以为循环矩阵使用额外的存储格式，因为转换成一个新的存储格式的成本可被分摊到循环计算的许多步骤上。如果能做到这一点，那么从一个自定格式转换或转换成一个自定义格式的时间应该被包含在整体时间之内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在一个时间步骤内和跨序列的时间步骤上，这些因素会导致很多优化的可能性，因此测定运算的原始性能并不一定能够代表整个循环层的的性能。在这样的基准上，我们仅关注一个循环层，即使还存在其它更多的优化机会（如果考虑它们的层叠（stack））。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;输入的计算不应该被包含在用于循环层计算的时间内，因为其可以作为一个大的乘法计算，然后被实际的循环运算所消化。所以在 h_t = g(Wx_t + Uh_t-1) 中，Wx_t 对于所有 t 的计算时间不应被包含在循环层的时间内。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向计算应该在考虑权重而非输入的基础上计算更新（update）。所有的循环工作完成以计算权重更新，所以同时考虑输入来计算更新会掩盖我们想要测定的内容。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;vanilla RNN 的非线性应该是一个 ReLU。LSTM 的内在非线性应该是标准运算——门（gate）是 S 型函数，激活（activation）是双曲正切函数。LSTM 不应该有窥视孔连接（peephole connections）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2.4. All-Reduce&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在的神经网络通常在多 GPU 或多系统与 GPU 并行的情况下训练，这主要有两个技术分类：同步和异步。同步技术依赖于保持参数和所有模型实例的同步，它通常要保证在优化步骤执行前，所有模型实例有一些梯度的备份。最简单运行这些计算结果的 Message Passing Interface (MPI) 被称为 All-Reduce。有很多可以执行 All-Reduce 的方法，我们可以依靠数字的排列、数据的大小和网络的拓扑结构来执行。这种基准测试的方式在执行时是没有限制的，所以它的结果是不确定的。异步的方法则非常的不同，在这个版本的基准测试中我们不会测试这些方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了评估 All-Reduce，我们使用了下面的库和基准：NVIDIA's NCCL Ohio State University (OSU) Benchmarks&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NCCL 库包括一组标准的通信程序。这个库支持在单个节点任意数量的 GPU 运行，并且它能在单个或多个进程中运行，但 NCC 程序不支持多节点下的 All-Reduce。为了能够在多节点下评估 All-Reduce，我们使用了 OSU 下的 benchmark。我们在三个执行过程中 (NCCL single process, NCCL MPI, OpenMPI) 报告了最短的延迟。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Nvidia 8 GPU 系统的拓扑结构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个节点都有两个 GPU 插槽，而每个插槽都有一个 PCIe root complex。&lt;/span&gt;&lt;span&gt;每个节点都有两个 GPU 插槽，而每个插槽都有一个 PCIe root complex。每一套插槽都有两个 PLX 开关，它们通过 16 个 PCIe v3 的 lanes 各自连接到 CPU 插槽中。每个 PLX 插槽有两个 GPU，所有的 GPU 通过 16 个 PCIe v3 的 lanes 进行同时通信。这两个 CPU 插槽通过 Intel 的 QPI 连接，而跨界点的互联则是通过 InfiniBand FDR。下图显示了一个原理图的节点，在图中，所有的设备均由同一个虚线框内的 PCL root 连接。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t22b2ic5ZGyos6e4fGLUWBR3jtbX331VTibN2TNGVnXsvuibRdK35b6ibuyg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;英特尔 Xeon Phi 和 Omni-Path 系统的拓扑结构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MPI_AllReduce 时间是在英特尔 Xeon Phi 7250 处理器上测定的——在英特尔内部的带有 fat-tree 拓扑的 Intel® Omni-Path Architecture (Intel® OPA) series 100 fabric 结构的 Endeavor 集群上，使用了 Intel MPI 5.1.3.181。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3. 结果&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这部分，我们记录了一些运算的表现。下面这些结果是随机挑选的，它们只是为了演示几个应用的运算表现。下面的结果仅包括了特定操作和参数下最快的处理器的时间和浮点运算速度。完整的结果可以在库里查看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些软件库（例如 cuDNN 和 OpenMPI）和一些硬件系统的细节同样在 github 的库里适用。如有问题，请随时和我们联系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦更多硬件平台的结果被发现可以适用，它们都会被加到库里面来。我们也欢迎所有硬件厂商为此贡献结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.1. GEMM Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t26OqvhvvTQEVm395lDtHE2VCYX0NH2gbKHxGic592ibxnQHjSlW7Cu3Lg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.2. Convolution Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t25oxl1fFBhibRCwSNRSETMqSwicHaY78Igxqx4GP3cRKMuqSUZCFFUibwQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.3. Recurrent Ops Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;周期性的操作内核仅能在 NVIDIA 的硬件上运行，而周期性的标准检查程序也将很快可以在 Intel 的硬件上运行。在今年十月份我们将会得到这些结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2FXbgGkpzRLSdNEVqfauH5psbhGLsHQ6Aplm11gQNMv8B5A0l3AWCDA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.4. All-Reduce Results&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为我们仅仅只有一个 Pascal GPU，所有我们不能在 NVIDIA's TitanX Pascal GPU 运行 All-Reduce benchmark。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8UiaacpTJM0C6dlzmbWH7t2jUqWWxibH4n3dffVFS4Y9gu7uAGMxLjvxIJ4SqLL8ibaqZzicjR1jOaTQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们欢迎来自各界的贡献，具体体现在如下两个方面：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 深度学习研究者/工程师：如果你是一个正在从事新的深度学习应用的研究者或者工程师，你可能会在训练你的模型的时候有不同的运算结果和工作量。而我们对于那些能够逆向影响你模型表现（速度）的底层运算结果非常感兴趣。请将这些运算结果和工作量反馈给我们。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 硬件供应商：我们也非常愿意接受来自其他硬件硬件供应商的贡献。我们对 benchmark results 的接受态度十分开放，无论你是大公司还是基于深度学习训练的小型创业公司，都请将你们的 benchmark results 反馈给我们！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 27 Sep 2016 11:54:34 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | Deep Learning School第二天：Yoshua Bengio压轴解读深度学习的基础和挑战</title>
      <link>http://www.iwgc.cn/link/2841694</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;机器之心整理&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;今天早上，Deep Learning School 第二天的讲座结束（点击《&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719393&amp;amp;idx=2&amp;amp;sn=22daaa88f7737c21587ba1e8bb217af7&amp;amp;chksm=871b00dfb06c89c92a1cac99880d347d9169368eea2a239df5ed26f58ddb9d5a4fe07af02aee&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650719393&amp;amp;idx=2&amp;amp;sn=22daaa88f7737c21587ba1e8bb217af7&amp;amp;chksm=871b00dfb06c89c92a1cac99880d347d9169368eea2a239df5ed26f58ddb9d5a4fe07af02aee&amp;amp;scene=21#wechat_redirect"&gt;视频 | Deep Learning School 第一天：4 小时的深度学习盛宴&lt;/a&gt;》观看第一天的精彩内容）。John Schulman、Yoshua Bengio 等人分别作了关于深度学习的主题演讲，直播时长达到 10 个小时（如今公开的视频只有 4 个小时）。机器之心对第二天的讲座进行了整理，尤其重点介绍了 Yoshua Bengio 的讲演（PPT 截图，最后一位演讲者），读者可通过此篇文章介绍选择性的观看视频内容。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;第二天讲座观看地址：https://www.youtube.com/watch?v=9dXiAecyJrY&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;第一位演讲者：John Schulman（OpenAI 研究科学家）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：策略梯度和 Q-学习：上升到力量、对抗和统一（Policy Gradients and Q-Learning: Rise to Power, Rivalry, and Reunification）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先概述一下深度强化学习中目前最好的研究成果，其中包括最近的在视频游戏（如 Atari）、棋盘游戏（如 AlphaGo）和模拟机器人上的应用。然后我会给出这些成果背后的两种核心方法的教学介绍：策略梯度 和 Q-学习。最后，我将给出一个新的分析以说明这两种方法有多么类似。本演讲的主题将不仅是问「什么有效？」，而且还有「它在什么情况下有效？」以及「它为什么有效？」；另外还要找到这些问题的可用于调节具体的实现和设计更好的算法的答案。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第二位演讲者：Patrice Lamblin&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：Theano 教学（Theano Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Theano 是一个 Python 库，允许在 CPU 或 GPU 上高效地定义、优化和评估涉及多维数组的数学表达式。自 Theano 诞生以来，它就一直在深度学习社区最受欢迎的框架之一，而且还有许多用于深度学习的框架也是基于它而构建的，其中包括 Lasagne、Keras、Blocks 等等。这个教程将首先关注 Theano 背后的概念以及如何构建和评估简单的表达，然后我会介绍如何定义和训练更为复杂的模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;第三位演讲者：Adam Coates（百度硅谷人工智能实验室（Silicon Valley AI Lab）主任）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：用于语音的深度学习（Deep Learning for Speech）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：传统的语音识别系统具有大量模块，其中每一个都需要单独的艰难的工程开发。现在深度学习已能创造能执行大多数传统引擎的「端到端」的任务的神经网络，这能极大地简化新型语音系统的开发，并开启了实现人类水平的表现的大门。在本教程中，我们将概览一遍类似百度的「Deep Speech」模型的端到端系统的开发步骤。我们将会将这些片段组合起来成为一个最先进的语音系统的「比例模型」——现在已经在驱动生产型的语音引擎的神经网络的小型版本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第四位演讲者：Alex Wiltschko（&lt;strong&gt;Twitter 的 Advanced Technology 部门的研究工程师，Whetlab 联合创始人）&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TxxoCK1Vtxmck1ANLiaooTqzkMjEAhj07WiaZd5Fj5XxOafhwqVAiad8hg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：Torch 教学（Torch Tutorial）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：Torch 是 Lua 语言环境中一个用于科学计算的开放平台，其关注的重点是机器学习，尤其是深度学习。Torch 为 GPU 计算提供了一流的支持，而且是以一种清晰的、交互式的和必需的风格，这是 Torch 和其它阵列库之间的显著不同点。尽管 Torch 从广泛的行业支持中获益匪浅，但却是社区所有的和社区开发的生态系统。包括 Torch NN、TensorFlow 和 Theano 在内的所有神经网络库都依靠自动微分（AD：automatic differentiation）来管理复杂函数组件的梯度计算。我将介绍一些自动微分的广义背景，这是基于梯度的优化的基础概念，还将展示 Twitter 通过 torch-autograd 对自动微分的灵活实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TC3YoRPw4yN5RLSAVt8KfLFGujgU69Uw9llc5nLia22joF8KGmFadAGA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TNOTusXyrib5xyCoOKG8zudqgmKctnaxhSiaTRfYvkZacuicfUlK8Q7yJw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Torch 得到了很多企业组织的支持，发展非常快&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T2KxWB4ox6v5MF3G8PQ9fq9ia468QLEuxTGmOuQXuXibSF8kW8GcGMmxw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;各种框架在产业界-研究界坐标上的位置&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第五位演讲者：Quoc Le（&lt;strong&gt;谷歌研究科学家，博士导师是吴恩达教授&lt;/strong&gt;）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TEwNARerhiakRfySaNNII66zI8BS6LwHYVUQcEdEnkBEeDWJ5OJp99gg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：用于自然语言处理和语音的序列到序列学习（Sequence to Sequence Learning for NLP and Speech）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：我将首先提出序列到序列（seq2seq）学习和注意模型（attention model）的基础，以及它们在机器翻译和语音识别上的应用。然后我将讨论带有指针（pointer）和函数（function）的注意。最后我会描述强化学习可以在 seq2seq 和注意模型中发挥怎样的作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TGwB8LEfxBSick8icnCMvkDxuYoTVAjiaUPaeial2kibzxevFvOGJDPAoLNA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;带有注意的序列到序列（seq2seq）&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前在许多翻译任务重表现最好的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 使用词分割或词/字符混合（而不只是词）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 梯度裁剪以阻止梯度爆炸&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 使用 LSTM&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T38aj7jicmTxSV2ulKBlwGGM94vCQHxyByUXTUkxZ21pljb9toRuvodg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;用于语音的 seq2seq&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;第六位演讲者：Yoshua Bengio&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TMPVRapf75Xzo5dibYlXUPGcGxbcHyWeaCC8LgKNz2EHlPp7BwhfSedg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;演讲主题：深度学习的基础和挑战（Foundations and Challenges of Deep Learning）&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简介：为什么深度学习的效果这么好？未来还面临着什么挑战？这个演讲将首次探讨深度学习成功的关键因素。首先，根据 no-free lunch 定理，我们将讨论深度网络获取抽象的分布式表征的表达力。其次，我们将讨论我们的实际优化神经网络的参数的惊人能力——尽管存在非凸性（non-convexity）。然后我们将思考一些未来的难题，包括核心的表征问题——理解变化的基本解释因素，尤其是对于无监督学习，为什么这对于将强化学习带向下一阶段来说是非常重要的，以及仍然存在挑战性的优化问题，比如长期依赖的学习，理解深度网络的优化全局，以及为什么生物大脑的学习方式的秘密仍然值得从深度学习的角度来破解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TWEiaFLSNg9bAEugbhhTnsSuT2otrY988fWgjHibfA1uMDufNMNS5dHfA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TtXNs88IuaV7jicsBjpwiaSCLAUKnlht1I0Got83p5xCTVRbiczdKydTQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P2: 深度学习、人工智能和天下没有免费的午餐&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要用机器学习实现人工智能，你需要这五大关键成分：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1. 许多许多数据&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2. 非常灵活的模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3. 足够的计算机算力&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4. 可以克服「维度的诅咒」的强大的先验知识&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;5. 在计算上高效的推理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TnGHXARBBWqpv1rDfLibqrb6ZHzVtTk0YxqBdUtseeG7GAZTy7icFmjoQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P3:绕过维度的诅咒&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们需要为我们的机器学习模型构建组合性，正如人类语言通过组合来表达复杂想法的含义一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;组合性的使用能够给表现力带来巨大的提升&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分布式表征/嵌入：特征学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度架构：多层特征学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;先验的假设：组合性可用于有效地描述我们周围的世界&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TfyTdPLqEIfmANzRUcCYrOicdrgtHEOnXmyAibnrDXEMYFjxIRmRbyDibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P4: 非分布式表征&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;聚类、n-grams、最近邻、RBF SVM、局部非参数密度估计 &amp;amp; 预测、决策树等&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个可以区分开的区域的参数&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可区分的区域的序号和参数的序号成线性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T4jiaYs0BiamChTj09HpbfibLgqzlOkVDHRBp3MfYBLv033ooic8NQVp1Yw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P5:对分布式表征的需求&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因子模型、PCA、RBM、神经网络、稀疏编码、深度学习等&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;每个参数都可以影响很多区域，而不只是局部区域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可区分的区域的序号几乎与参数序号成指数增长&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;非局部地归纳成从未被见过的区域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T9kjk5ORiahEWJY0YxfCyxH8y4rrCGe8Erls8QWI49ibFyXZA7zwich4sg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P7：无需了解其它特征的指数级大量的配置就能发现每个特征&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TpCLnHRKHHPwSvxa3icl2hPBjDcGKE6GxibJtFPd4NFnGmdwjSIHyTicQw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P6：隐藏单元发现语义上有意义的概念&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Zhou et al &amp;amp; Torralba, arXiv1412.6856, ICLR 2015&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;训练神经网络识别地方，而非物体&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Tj9mRLGgIUSYvQFvHsDTxwbTiawgicur3om1f9xdY3eV0AtacuVnxJ3aA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P8：分布式表征的巨大优点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Twtgx4we7P9bEQBvC0StJ6UzvMme4Fh6iblPgG39ib1iacgAachvibSe1yA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P9：深度的先验知识是非常有用的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TsAia3aXsqLqv8ySibIdjub9QYtjKiaUvnoqQXlZx8VXCgJX46mGLyPiaVQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P10：「浅的」计算机程序&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TKb40WZCONpPaaZZwYQJgUt2CTqPYeEEsPM2vkNSAiabGQWJR20QbrdA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P11：「深的」计算机程序&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TiaFFHkamjpLU6GWT9ffJoLwZINWEfsMKMJ35Ld58C8c2Knd1VssJhfg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P12：深度的巨大优越性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TyUwltmPbVpj61gvVQ0MV59x4yc5dnwiaWaIAzQI0TNibicVUpBoa1giblg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P13：揭开神秘：神经网络的局部极小值→不需要凸性&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TSEUFzGpxVdDquA7GPmmibaWH3kIpaopquW6IibOibPAIm2qMp2LlTLwKA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P14：鞍点（saddle points）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;低维情况局部最小值占主导，高维情况鞍点占主导&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大多数局部最小值接近底部（全局最小误差）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TgPDdZFm0HMoIKu8kdVtOBVE8QVqacicIibpdic9I8x5tautbsJXLQaGYw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P15：低指数临界点&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Choromanska et al &amp;amp; LeCun AISTATS 2015, The Loss Surface of Multilayer Nets 表明 deep rectifier nets 类似于球形自旋玻璃模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大型模型的低指数临界点汇聚在全局极小值之上的一个带中&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Txv3qR2uCX2qzCHQViboZeWrKOLrsb5mHuVHqgk0PsNqbFTL3bYprvZg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P16：深度学习：超越模式识别，迈向人工智能&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Th4QLe6usPyfdK0gJIXDibYldATicvrOY4pJia7AketTe8NHZsql8gXlKw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;P17:用于深度学习的注意机制&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TJostTtrBicoLhoyv29Uxl7OUPYfVoHqu7b8Ybjle3nIcD3IkyKgKWVQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P18：使用循环网络和注意机制的端到端机器翻译&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TKvsyfiaW2wXGcB2us9BJiayC2vy3hwUIvB5Smj3A8o8m2CoPe9lzEjBw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P19：IWSLT 2016——Luong &amp;amp; Manning（2016）TED 演讲机器翻译（英语-德语）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T7ibjCibGIePIwIP6tJheZMkmLWLVLZlhXnvSNIKvfsiaTPY1tbUQmOxDg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P20：下一个前沿：推理和问答&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TQYloH9rpTtBDxdwO8FIUWCn5LfOArfhNeBrHZmviacfibJnrRXVBiblhw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P21：使用梯度下降学习长期依赖是很困难的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TIYhWfPMibDJPSkanwHO5ZeBFnSyiaah1RDNxTTLAG70ib1gXgAkibU3FKQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P22：延迟和分层以更进一步&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TNktj2AibCiaj98wYhGoSibBNvMvAQNEyibx7Z3dfwhtTuvicfDsGgxgXUibQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P23：用于记忆获取的记忆机制能实现推理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经图灵机（Graves et al 2014）和记忆网络（Weston et al 2014）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;使用一种形式的注意机制来控制一个记忆的读写权限&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该注意机制在注意焦点的位置上输出一个 softmax&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TJ6nuU0Qibf3vTVAj4iaupNO1LZQENUMCAPonsWUQmiatbwFruibSadCwQQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P24：大型记忆网络&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;记忆 = 状态性质&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基于记忆的网络是特殊的 RNN&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个存储在外部存储中的心智状态可以维持任意长时间，直到其被重写（部分或不）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;遗忘 = 消失的梯度&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;记忆 = 高维状态，避免或减少对遗忘/消失的需求&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T59UtbhYFykMAeibz1o3ia5Doib0LRYP8boRT3GsXqbgNTVsQFAGmg1Ncw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P25：下一个大挑战：无监督学习&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近来的进步基本上集中于监督深度学习领域&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在无监督深度学习领域存在真正的技术难题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可能存在的好处：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;利用巨量无标注的数据&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;回答关于被观察到的变量的新问题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正则化矩阵-迁移学习-领域适应&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;优化更简单（局部训练信号）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结构化的输出&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对没有给出模型或领域模拟器的强化学习是必需的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TTcjsbcTFFmd6JfCMN3deDDyVPpZ7nryDn3zKPkaGlTewAJ9F9kRm3w/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P28：不变性和 disentangling&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T82qQSx9iaXfibgFJLiadkd0wNwkxBje2jwvOmmBfhDjT7CvymDF0SLNSw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P29：学习多层次的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习的最大好处是允许实现更高程度的抽象&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;更高程度的抽象能够解开不变性的因子，这能带来远远更为简单的泛化和迁移&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TABpzlVrjVTfBL0TfsMGViba7Q7nb7uSIwO5KWicpn7DT1K98L19H7lcQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;P30：追寻机器大脑和生物大脑的学习的关键原理&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在整个大脑（或整个皮层）的网络层面上的联合学习仍然是一个秘&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向传播确实很赞，但我们目前还不清楚其所对应的生物机制是什么，也不知道怎么将其泛化到无监督学习上&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们在弥合反向传播、玻尔兹曼机和 STDP 之间的鸿沟上实现了一些进展：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 《连线》长文揭秘微软Project Catapult：人工智能时代押注FPGA</title>
      <link>http://www.iwgc.cn/link/2841695</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Wired &lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、李亚洲、虞喵喵&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;可编程的芯片将主导未来的互联网世界，几大科技巨头都认识到这一点。微软也不例外，这家做了四十年软件的公司，也在向硬件进军，而 FPGA 成为了微软未来竞争的押注。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2012 年 12 月的某一天，Doug Burger 站在 Steve Ballmer 面前，尝试着预测未来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Ballmer，微软的那个大嗓门 CEO，坐在&lt;span&gt;微软在西雅图郊外的蓝天研发实验室（blue-sky R&amp;amp;D lab）基地&lt;/span&gt; 99 号楼一层的演讲室。桌子排成 U 型，Ballmer 被他的高级助围住，开着笔记本电脑。Burger，一位四年前加入微软的计算机芯片研究员，正在为高管们描绘一个新想法，Project Catapult。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TrCpaWK6jeaOhkBibBRfdoTJgUqXK2VYZYpmv2DzvyuTBRzNr8NP9FHw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Doug Burger&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Burger 解释道，技术世界正在迈向一个新轨道。未来将是少数几家互联网巨头运作着几个巨型互联网服务，这些服务非常复杂并与之前的服务非常不同，所以这几家公司不得不打造全新的架构来运行它们。不仅仅是驱动这些服务的软件，巨头们还得造出硬件，包括相应的服务器和网络设备。Project Catapult 将会为微软所有的服务器——几百万台——提供专用芯片，这些芯片可以用来为特定的任务重新编程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是还没等待 Burger 介绍到这个芯片时，Ballmer 突然抬起了头。Ballmer 刚到微软研究时，就说他希望看到研发中心有新进展，而不是一个战略简报。「他开始拷问我，」Burger 说。微软花了 40 年建立起像 Windows、Word 和 Excel 这样的 PC 软件，然而它才发现自己只是刚刚涉足互联网。微软还没有编程计算机芯片所必须的工具和工程师，这是一项困难、耗时、专业且有些奇怪的任务。微软编程计算机芯片听起来就像是可口可乐要做鱼翅汤了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TyE2pibbyQwQuCAOPwX0QiaX4zUe0sn4uNBpbTFtuqW9iaWiaGVlrYsibx5Q/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Project Catapult 目前的样子&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Burger，着装整齐，轻微秃头，能冷静的分析问题，就像很多优秀的工程师一样。他转过身，告诉 Ballmer 像谷歌和亚马逊这样的公司一直正在超这个方向发展。他说世界上的硬件制造商不会提供微软需要用来运行线上服务的硬件。他说，如果微软不打造自己的硬件，就会落后。Ballmer 听完后并不买账。但是过了一会儿，另一个声音参与到这场讨论中来。这个声音来自陆奇，他管理者 Bing，微软的搜索引擎。两年来，陆奇的团队一直在和 Burger 讨论可再编程芯片的事情。Project Catapult 不仅仅是一种可能，陆说：他的团队已经开始着手做了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;今天，微软已经有了现场可编程门阵列（field programmable gate arrays，FPGA），Burger 和陆都相信这个可编程芯片可以改变世界。FPGA 目前已支持 Bing，未来几周，它们将会驱动基于深度神经网络——以人类大脑结构为基础建模的人工智能——的新搜索算法，在执行这个人工智能的几个命令时，速度比普通芯片快上几个数量级。有了它，你的计算机屏幕只会空屏 23 毫秒而不是 4 秒。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TkjQ3SLx21muPPCJ3dr17ibt4GuZUnRV7X5ueysdj7FLSiciatkKKymSoA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Catapult 团队成员 Adrian Caulfield, Eric Chung, Doug Burger, 和 Andrew Putnam&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 不只是要追赶 Google。Project Catapult 暗示着改变未来全球系统运作的方式。从美国的亚马逊到中国的百度，所有的互联网巨头都在用硅替代他们的标准服务器芯片——中央处理单元，也叫 CPU，这些硅制成的芯片可以让它们跟上人工智能的快速变化。微软现在每年花在硬件上的钱在 50 亿到 60 亿美元，以维持其线上帝国的运转。所以这样的工作「再也不仅仅是研究了，」Satya Nadella 说道，他在 2014 年接任了微软 CEO 一职。「它有极为重要的优先性。」也就是 Burger 当年在 99 号大楼中要解释的，并让他和他的团队耗费多年，克服种种挫折，不断重新设计，与体制对抗，最终实现的一种新的全球超级计算机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;一种全新的古老计算机芯片&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2010 年 12 月，微软研究院 Andrew Putnam 离开西雅图度假，回到了位于科罗拉多斯普林斯的家中。当时正是圣诞节前两天，他还没开始大采购。正在他开车去商场的路上，电话突然响了，另一端正是他的老板 Burger。Burger 当时打算节后面见 Bing 高管，但他需要一份能在 FPGA 上运行 Bing 机器学习算法的硬件设计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Putnam 找到最近的星巴克开始规划设计，这大约花了他 5 个小时，所以他仍有时间去购物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当时 Burger 45 岁，Putnam 41 岁，两人过去都是学者。Burger 曾在特克萨斯大学奥斯汀分校担任计算机科学教授，他在那里工作了 9 年，专攻微处理器，还设计了一款名为 EDGE 的新型芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Putnam 曾在华盛顿大学工作 5 年，担任研究员并主要从事 FPGA 研究。当时可编程芯片已经存在了好几十年，但它们大多被当作处理器的一部分。2009 年 Burger 将 Putnam 挖到微软，两人开始探索用可编程芯片提升线上服务速度的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TH5cctXxQiboTQiaZRUC1zQvFb4PickWYnJpjs3n0E0HzWgCDfDEseEv4g/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Project Catapult V1，即 Doug Burger 团队曾在微软西雅图数据中心测试过的版本。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的搜索引擎是一个依靠成千上万台机器运行的在线服务。每台机器都需要靠 CPU 驱动，尽管英特尔等公司不断改进它们，这些芯片还是跟不上软件更新的脚步。很大程度上，是因为人工智能浪潮的来临。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 搜索等服务已经超出了摩尔定律预言的处理器能力，即每 18 个月处理器上晶体管的数量翻一倍。事实还证明增加 CPU 并不能解决问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但同时，为新出现的需求制造专用芯片，成本是非常昂贵的。恰好 FPGA 能弥补这个不足，Bing 决定让工程师制造运行更快、比流水线生产的通用 CPU 能耗更少、同时可定制的芯片，从而解决不断更新的技术和商业模式变化所产生的种种难题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;圣诞节后的会面中，Burger 为必应高管们拿出了一套用 FPGA 提升搜索速度，同时功耗较低的方法。高管们不置可否。在接下来的几个月中，Burger 团队根据 Putnam 圣诞节时画出的草图构建了原型，证明其运行必应的机器学习算法时速度可以提升 100 倍。「那时他们才表现出浓厚兴趣」，当时的团队成员、现瑞士洛桑联邦理工学院院长 Jim Larus 告诉我们，「但同样也是艰难时光的开始。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;原型是一个使用六个 FPGA 的专用盒，由一整个机架的服务器共享。如果盒子吱吱作响，表明它们需要更多 FPGA——考虑到机器学习模型的复杂性需求会越来越大——这些机器就会停止工作。必应的工程师非常厌恶这件事。「但他们没错，」Larus 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正是这个原型吸引了陆奇。他给了 Burger 足够的资金，可以在 1600 台服务器上装配 FPGA 并进行测试。在中国和台湾硬件制造商的帮助下，团队花费半年时间制造出了硬件产品，并在微软数据中心的一组机架上进行测试。但一天晚上灭火系统出现了问题。他们花了三天时间修复机架——它仍能工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2013 年到 2014 年的几个月中，测试显示必应「决策树」机器学习算法在新芯片的帮助下，可以提升 40 倍运行速度。2014 年夏天，微软公开表示要很快要将这些硬件应用到必应实时数据中心。但是在那之后，微软暂停了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;不只是 Bing 搜索&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Bing 在前几年一直是微软线上发展的核心，但到 2015 年，公司有了其他两个主要的在线服务：商务应用套件 Office 365 和云计算服务 Microsoft Azure。和其他竞争者一样，微软高层意识到运营一个不断成长的在线帝国的唯一有效方法是在同样的基础上运营所有的服务。如果 Project Catapult 将转变微软的话，那 Bing 也不能被排除在外。它也要在 Azure 和 Office 365 内部工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;问题是，Azure 高官们不在乎加速机器学习，他们需要联络的帮助。Azure 数据中心的流量跳动增长的太快，服务的 CPU 不能跟上脚步。最终，Azure 首席架构师 Mark Russinovich 这样一批人看到了 Catapult 能帮助解决这些问题，但不是为 Bing 设计的那种解决问题的方式。他的团队需要可编程的芯片，将每个服务器连接到主要网络，如此他们在数据流量到达服务器之前就能开始处理了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6T7BMwMRPcEESVcEoH5Scxk7oQSI5XIWkE7TSicd9vLt1oqntAgGKvYTg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;FPGA 架构的第一代原型是一个被一架服务器共享的单个盒子（Version 0）。然后该团队转向为每个服务器设计自己的 FPGA（Version 1）。然后他们将芯片放到服务器和整体网络之间。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，研究 FPGA 的这伙人需要重新开发硬件。在第三代原型中，芯片位于每个服务器的边缘，直接插入到网络，但仍旧创造任何机器都可接入的 FPGA 池。这开始看起来是 Office 365 可用的东西了。最终，Project Catapult 准备好上线了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Larus 将许多重新设计描述为噩梦，这不是因为他们需要建立新的硬件，而是他们每次都需要重新编程 FPGA。他说，「这非常的糟糕，要比编程软件都糟糕，更难写、难纠正。」这是一项非常繁琐的工作，像是改变芯片上的小逻辑门。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;既然最终的硬件已经有了，微软还要面对每一次重新编程这些芯片时都会遇到的同样挑战。「这是一个看世界思考世界的全新视角，」Larus 说。但是 Catapult 硬件的成本只占了服务器中所有其他的配件总成本的 30%，需要的运转能量也只有不到 10%，但其却带来了 2 倍原先的处理速度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个布局非常大。微软 Azure 用这些可编程的芯片来路由、加密和压缩数据。Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。而且据微软的一名员工说，Office365 正在尝试在加密和压缩上使用 FPGA 以及机器学习——这一举措将惠及其 2310 万用户。最终，Burger 说道，这些会驱动所有的微软服务。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;这真的起作用吗？&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Peter Lee 说，「这仍然使我迷惑。我们让公司做这些事。」Lee 监管着微软内部一个被称为 NExT 的组织，NExT 是 New Experience and Technologies 的缩写。在 Nadella 接任 CEO 之后，他个人推动了 NExT 的创建，代表了从 Ballmer 十年统治的重大转变。该组织的目标是培养能在近期实现的研究，而不是远期研究，这能改变微软如今的进程，而非多少年后的进程，就像增强现实设备 HoloLens 一样。当然也包括 Project Catapult。Burger 说，「起跳点就在前面，来自于非 CPU 技术。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TYPa0vfLTXOrXy4ZNmsXicboiasUic4COd58S2NJiaow8ylrzeer5nCFTQA/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Peter Lee&lt;/span&gt;&lt;/em&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所有的互联网巨头，包括微软，如今都在用图像处理单元增补 CPU，GPU 可为游戏和其他高度视觉化的应用渲染图像。例如，当这些公司训练神经网络识别图像中的人脸时（输入百万张图片），GPU 可处理很多的计算。像微软这样的巨头也使用可替代的硅片在训练后执行神经网络。而且，即使定制芯片异常昂贵，谷歌在设计执行神经网络的处理器上也走得相当远了，也就是他们设计的 TPU。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 TPU 上，谷歌为追求速度牺牲了长期灵活性。也就是说，在识别进入智能手机的指令时，TPU 想要消除所有的延迟。但问题是如果神经网络模型改变的话，谷歌必须要建立新的芯片。但在 FPGA 上，微软在打一场长久战。尽管在速度上比不上谷歌的定制芯片，微软可在需要的时候重新编程芯片。微软不只能为新的人工智能模型编程，也能为任何任务重新编程。而且这些设计在接下来几年如果有用，微软能一直采用这种 FPGA 的程序，并建立专用芯片。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TVv0iaIQiceDuZZlqKyQc54MYlWMR9KXWDD4UQEQDz2CicwViaGoTCWxPXQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;该硬件的新版本 V2，是一张能插入微软任一服务器终端的芯片，并能直接连接到网络。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;微软的服务很广，也使用如此多的 FPGA，如今他们正在改变全球芯片市场。FPGA 出自一家名为 Altera 的公司，英特尔副总裁 Diane Bryant 告诉我为什么英特尔会在去年夏天收购 Altera，这是一笔价值 167 亿美元的收购，也是芯片制造商史上最大的一笔收购。她说，到 2020 年，所有主要的云计算公司的 1/3 的服务器将使用 FPGA。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是科技名词缩写之间的纠缠：GPU、CPU、TPU、FPGA。但它们也是将成为关键的代名词。在云计算上，微软、谷歌、亚马逊这些公司驱动着世界上很大一部分技术，以至于这些可选择的芯片将驱动大范围的 app 和在线服务。Lee 说，直到 2030 年，Project Catapult 将继续扩展微软全球超级计算机的能力。在这之后，他说，微软就能转向到量子计算了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;之后当我们谈到手机时，Nadella 也告诉了我同样的事。他们读取自同样的微软蓝图，正在触摸量子技术驱动的超快计算机的未来。想象建立量子机器多么的难，就像白日梦一样。但在几年前，Project Catapult 也如同白日梦一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;原文链接：https://www.wired.com/2016/09/microsoft-bets-future-chip-reprogram-fly/&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>学界 | 伯克利大学和Adobe开源最新的深度学习图像编辑工具 iGAN（附论文）</title>
      <link>http://www.iwgc.cn/link/2841696</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Github&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;近日，伯克利和 Adobe 在 Github 上开源了新的深度学习图像编辑工具 iGAN。这是在 ECCV 2016 接收的的论文 Generative Visual Manipulation on the Natural Image Manifold 中作者们介绍的工具。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=m0331056q04&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Github 开源介绍&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;给定一些用户笔触（stroke），我们的系统能够实时产生最佳满足用户编辑的逼真图像样本。我们的系统基于 GAN、DCGAN 这样的深度生成模型。该系统有以下两个目标：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6TIfuibMiaIRNwTBq9cwOe5rGpe7dibdZwa2JpBia1rbxkUHKK8rPOhA2Zgw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;受笔触效果颜色和形状的启发，系统中的智能绘画界面能自动生成图像&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;系统中的交互视觉 debugging 工具能理解并可视化深度生成模型。通过与该生成模型交互，开发者能理解该模型可生成什么视觉内容，也有助于理解模型的缺陷。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们正在研究支持更多的生成式模型（例如，变自编码器）和更多的深度学习框架（比如，TensorFlow）。欢迎提议更多的新变化或贡献更多的新特征（比如，TensorFlow 分支）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开源代码地址：&lt;span&gt;https://github.com/junyanz/iGAN#igan-interactive-image-generation-via-generative-adversarial-networks&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：在自然图像流形上的生成式视觉操作（Generative Visual Manipulation on the Natural Image Manifold）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8vCJKt3aAjIhDWUc4P0N6Tz2EWoOSx5lOEabG5uwpQ07j9jkkAGOWvqPicCVaZicmw5YLmMeqpHYHg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：真实图像流形上的操作一直具有挑战性，因为它需要以一种用户可控的方式调整图像外貌，还要保留结果的真实性。除非用户有相当好的艺术技能，不然在编辑时候很容易减少自然图像的流形。在此论文中，我们提出使用生成式对抗网神经网络直接从数据中学习自然图像的流形。然后，我们定义了一类图像编辑操作，并依赖一直学习到的流形束缚它们的输出。该模型能自动调整输出，保持所有的编辑都是尽可能真实的。我们所有的处理方法都依据约束最优化来表达，几乎是实时的情况下被应用。我们在真实图像形状和颜色操作任务上评估该算法。该方法可进一步用于将一张图像改变为类似的一张，也可基于用户的涂鸦乱画生成新的图像。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;点击阅读原文，下载论文↓↓↓&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 26 Sep 2016 13:26:16 +0800</pubDate>
    </item>
    <item>
      <title>干货 | 深度学习名词表：57个专业术语加相关资料解析（附论文）</title>
      <link>http://www.iwgc.cn/link/2831065</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自wildml&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;本文整理了一些深度学习领域的专业名词及其简单释义，同时还附加了一些相关的论文或文章链接。本文编译自 wildml，作者仍在继续更新该表，编译如有错漏之处请指正。文章中的论文与 PPT 读者可点击阅读原文下载。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;激活函数（Activation Function）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了让神经网络能够学习复杂的决策边界（decision boundary），我们在其一些层应用一个非线性激活函数。最常用的函数包括 &amp;nbsp;sigmoid、tanh、ReLU（Rectified Linear Unit 线性修正单元） 以及这些函数的变体。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adadelta&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adadelta 是一个基于梯度下降的学习算法，可以随时间调整适应每个参数的学习率。它是作为 Adagrad 的改进版提出的，它比超参数（hyperparameter）更敏感而且可能会太过严重地降低学习率。Adadelta 类似于 rmsprop，而且可被用来替代 vanilla SGD。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Adadelta：一种自适应学习率方法（ADADELTA: An Adaptive Learning Rate Method）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福 CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adagrad&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adagrad 是一种自适应学习率算法，能够随时间跟踪平方梯度并自动适应每个参数的学习率。它可被用来替代vanilla SGD (http://www.wildml.com/deep-learning-glossary/#sgd)；而且在稀疏数据上更是特别有用，在其中它可以将更高的学习率分配给更新不频繁的参数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福 CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Adam&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Adam 是一种类似于 rmsprop 的自适应学习率算法，但它的更新是通过使用梯度的第一和第二时刻的运行平均值（running average）直接估计的，而且还包括一个偏差校正项。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Adam：一种随机优化方法（Adam: A Method for Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;仿射层（Affine Layer）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;神经网络中的一个全连接层。仿射（Affine）的意思是前面一层中的每一个神经元都连接到当前层中的每一个神经元。在许多方面，这是神经网络的「标准」层。仿射层通常被加在卷积神经网络或循环神经网络做出最终预测前的输出的顶层。仿射层的一般形式为 y = f(Wx + b)，其中 x 是层输入，w 是参数，b 是一个偏差矢量，f 是一个非线性激活函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;注意机制（Attention Mechanism）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;注意机制是由人类视觉注意所启发的，是一种关注图像中特定部分的能力。注意机制可被整合到语言处理和图像识别的架构中以帮助网络学习在做出预测时应该「关注」什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：深度学习和自然语言处理中的注意和记忆（http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Alexnet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Alexnet 是一种卷积神经网络架构的名字，这种架构曾在 2012 年 ILSVRC 挑战赛中以巨大优势获胜，而且它还导致了人们对用于图像识别的卷积神经网络（CNN）的兴趣的复苏。它由 5 个卷积层组成。其中一些后面跟随着最大池化（max-pooling）层和带有最终 1000 条路径的 softmax (1000-way softmax)的 3个全连接层。Alexnet 被引入到了使用深度卷积神经网络的 ImageNet 分类中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自编码器（Autoencoder）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自编码器是一种神经网络模型，它的目标是预测输入自身，这通常通过网络中某个地方的「瓶颈（bottleneck）」实现。通过引入瓶颈，我们迫使网络学习输入更低维度的表征，从而有效地将输入压缩成一个好的表征。自编码器和 PCA 等降维技术相关，但因为它们的非线性本质，它们可以学习更为复杂的映射。目前已有一些范围涵盖较广的自编码器存在，包括 降噪自编码器（Denoising Autoencoders）、变自编码器（Variational Autoencoders）和序列自编码器（Sequence Autoencoders）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;降噪自编码器论文：Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;变自编码器论文：Auto-Encoding Variational Bayes&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;序列自编码器论文：Semi-supervised Sequence Learning&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;平均池化（Average-Pooling）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;平均池化是一种在卷积神经网络中用于图像识别的池化（Pooling）技术。它的工作原理是在特征的局部区域上滑动窗口，比如像素，然后再取窗口中所有值的平均。它将输入表征压缩成一种更低维度的表征。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;反向传播（Backpropagation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;反向传播是一种在神经网络中用来有效地计算梯度的算法，或更一般而言，是一种前馈计算图（feedforward computational graph）。其可以归结成从网络输出开始应用分化的链式法则，然后向后传播梯度。反向传播的第一个应用可以追溯到 1960 年代的 Vapnik 等人，但论文 Learning representations by back-propagating errors常常被作为引用源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：计算图上的微积分学：反向传播（http://colah.github.io/posts/2015-08-Backprop/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;通过时间的反向传播（BPTT：Backpropagation Through Time）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过时间的反向传播是应用于循环神经网络（RNN）的反向传播算法。BPTT 可被看作是应用于 RNN 的标准反向传播算法，其中的每一个时间步骤（time step）都代表一个计算层，而且它的参数是跨计算层共享的。因为 RNN 在所有的时间步骤中都共享了同样的参数，一个时间步骤的错误必然能「通过时间」反向到之前所有的时间步骤，该算法也因而得名。当处理长序列（数百个输入）时，为降低计算成本常常使用一种删节版的 BPTT。删节的 BPTT 会在固定数量的步骤之后停止反向传播错误。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Backpropagation Through Time: What It Does and How to Do It&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分批标准化（BN：Batch Normalization）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分批标准化是一种按小批量的方式标准化层输入的技术。它能加速训练过程，允许使用更高的学习率，还可用作规范器（regularizer）。人们发现，分批标准化在卷积和前馈神经网络中应用时非常高效，但尚未被成功应用到循环神经网络上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：分批标准化：通过减少内部协变量位移（Covariate Shift）加速深度网络训练（Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用分批标准化的循环神经网络（Batch Normalized Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;双向循环神经网络（Bidirectional RNN）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;双向循环神经网络是一类包含两个方向不同的 RNN 的神经网络。其中的前向 RNN 从起点向终点读取输入序列，而反向 RNN 则从终点向起点读取。这两个 RNN 互相彼此堆叠，它们的状态通常通过附加两个矢量的方式进行组合。双向 RNN 常被用在自然语言问题中，因为在自然语言中我们需要同时考虑话语的前后上下文以做出预测。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：双向循环神经网络（Bidirectional Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Caffe&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Caffe 是由伯克利大学视觉和学习中心开发的一种深度学习框架。在视觉任务和卷积神经网络模型中，Caffe 格外受欢迎且性能优异&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;分类交叉熵损失（Categorical Cross-Entropy Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分类交叉熵损失也被称为负对数似然（negative log likelihood）。这是一种用于解决分类问题的流行的损失函数，可用于测量两种概率分布（通常是真实标签和预测标签）之间的相似性。它可用 L = -sum(y * log(y_prediction)) 表示，其中 y 是真实标签的概率分布（通常是一个one-hot vector），y_prediction 是预测标签的概率分布，通常来自于一个 softmax。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;信道（Channel）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习模型的输入数据可以有多个信道。图像就是个典型的例子，它有红、绿和蓝三个颜色信道。一个图像可以被表示成一个三维的张量（Tensor），其中的维度对应于信道、高度和宽度。自然语言数据也可以有多个信道，比如在不同类型的嵌入（embedding）形式中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;卷积神经网络（CNN/ConvNet：Convolutional Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;CNN 使用卷积连接从输入的局部区域中提取的特征。大部分 CNN 都包含了卷积层、池化层和仿射层的组合。CNN 尤其凭借其在视觉识别任务的卓越性能表现而获得了普及，它已经在该领域保持了好几年的领先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n类——用于视觉识别的卷积神经网络（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：理解用于自然语言处理的卷积神经网络（http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;深度信念网络（DBN：Deep Belief Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DBN 是一类以无监督的方式学习数据的分层表征的概率图形模型。DBN 由多个隐藏层组成，这些隐藏层的每一对连续层之间的神经元是相互连接的。DBN 通过彼此堆叠多个 RBN（限制波尔兹曼机）并一个接一个地训练而创建。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：深度信念网络的一种快速学习算法（A fast learning algorithm for deep belief nets）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Deep Dream&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这是谷歌发明的一种试图用来提炼深度卷积神经网络获取的知识的技术。这种技术可以生成新的图像或转换已有的图片从而给它们一种幻梦般的感觉，尤其是递归地应用时。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：Github 上的 Deep Dream（https://github.com/google/deepdream）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：Inceptionism：向神经网络掘进更深（https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Dropout&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Dropout 是一种用于神经网络防止过拟合的正则化技术。它通过在每次训练迭代中随机地设置神经元中的一小部分为 0 来阻止神经元共适应（co-adapting），Dropout 可以通过多种方式进行解读，比如从不同网络的指数数字中随机取样。Dropout 层首先通过它们在卷积神经网络中的应用而得到普及，但自那以后也被应用到了其它层上，包括输入嵌入或循环网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Dropout: 一种防止神经网络过拟合的简单方法（Dropout: A Simple Way to Prevent Neural Networks from Overfitting）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：循环神经网络正则化（Recurrent Neural Network Regularization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;嵌入（Embedding）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个嵌入映射到一个输入表征，比如一个词或一句话映射到一个矢量。一种流行的嵌入是词语嵌入（word embedding，国内常用的说法是：词向量），如 word2vec 或 GloVe。我们也可以嵌入句子、段落或图像。比如说，通过将图像和他们的文本描述映射到一个共同的嵌入空间中并最小化它们之间的距离，我们可以将标签和图像进行匹配。嵌入可以被明确地学习到，比如在 word2vec 中；嵌入也可作为监督任务的一部分例如情感分析（Sentiment Analysis）。通常一个网络的输入层是通过预先训练的嵌入进行初始化，然后再根据当前任务进行微调（fine-tuned）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度爆炸问题（Exploding Gradient Problem）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度爆炸问题是梯度消失问题（Vanishing Gradient Problem）的对立面。在深度神经网络中，梯度可能会在反向传播过程中爆炸，导致数字溢出。解决梯度爆炸的一个常见技术是执行梯度裁剪（Gradient Clipping）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;微调（Fine-Tuning）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Fine-Tuning 这种技术是指使用来自另一个任务（例如一个无监督训练网络）的参数初始化网络，然后再基于当前任务更新这些参数。比如，自然语言处理架构通常使用 word2vec 这样的预训练的词向量（word embeddings），然后这些词向量会在训练过程中基于特定的任务（如情感分析）进行更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度裁剪（Gradient Clipping）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度裁剪是一种在非常深度的网络（通常是循环神经网络）中用于防止梯度爆炸（exploding gradient）的技术。执行梯度裁剪的方法有很多，但常见的一种是当参数矢量的 L2 范数（L2 norm）超过一个特定阈值时对参数矢量的梯度进行标准化，这个特定阈值根据函数：新梯度=梯度*阈值/L2范数（梯度）{new_gradients = gradients * threshold / l2_norm(gradients)}确定。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GloVe&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Glove 是一种为话语获取矢量表征（嵌入）的无监督学习算法。GloVe 的使用目的和 word2vec 一样，但 GloVe 具有不同的矢量表征，因为它是在共现（co-occurrence）统计数据上训练的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：GloVe：用于词汇表征（Word Representation）的全局矢量（Global Vector）（GloVe: Global Vectors for Word Representation ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GoogleLeNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GoogleLeNet 是曾赢得了 2014 年 ILSVRC 挑战赛的一种卷积神经网络架构。这种网络使用 Inception 模块（Inception Module）以减少参数和提高网络中计算资源的利用率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用卷积获得更深（Going Deeper with Convolutions）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;GRU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GRU（Gated Recurrent Unit：门控循环单元）是一种 LSTM 单元的简化版本，拥有更少的参数。和 LSTM 细胞（LSTM cell）一样，它使用门控机制，通过防止梯度消失问题（vanishing gradient problem）让循环神经网络可以有效学习长程依赖（long-range dependency）。GRU 包含一个复位和更新门，它们可以根据当前时间步骤的新值决定旧记忆中哪些部分需要保留或更新。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Highway Layer&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Highway Layer　是使用门控机制控制通过层的信息流的一种神经网络层。堆叠多个 Highway Layer 层可让训练非常深的网络成为可能。Highway Layer 的工作原理是通过学习一个选择输入的哪部分通过和哪部分通过一个变换函数（如标准的仿射层）的门控函数来进行学习。Highway Layer 的基本公式是 T * h(x) + (1 - T) * x；其中 T 是学习过的门控函数，取值在 0 到 1 之间；h(x) 是一个任意的输入变换，x 是输入。注意所有这些都必须具有相同的大小。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：Highway Networks&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ICML&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即国际机器学习大会（International Conference for Machine Learning），一个顶级的机器学习会议。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ILSVRC&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即 ImageNet 大型视觉识别挑战赛（ImageNet Large Scale Visual Recognition Challenge），该比赛用于评估大规模对象检测和图像分类的算法。它是计算机视觉领域最受欢迎的学术挑战赛。过去几年中，深度学习让错误率出现了显著下降，从 30% 降到了不到 5%，在许多分类任务中击败了人类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Inception模块（Inception Module）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Inception模块被用在卷积神经网络中，通过堆叠 1×1 卷积的降维（dimensionality reduction）带来更高效的计算和更深度的网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用卷积获得更深（Going Deeper with Convolutions）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Keras&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Kears 是一个基于 Python 的深度学习库，其中包括许多用于深度神经网络的高层次构建模块。它可以运行在 TensorFlow 或 Theano 上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;LSTM&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;长短期记忆（Long Short-Term Memory）网络通过使用内存门控机制防止循环神经网络（RNN）中的梯度消失问题（vanishing gradient problem）。使用 LSTM 单元计算 RNN 中的隐藏状态可以帮助该网络有效地传播梯度和学习长程依赖（long-range dependency）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：长短期记忆（LONG SHORT-TERM MEMORY）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：理解 LSTM 网络（http://colah.github.io/posts/2015-08-Understanding-LSTMs/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程，第 4 部分：用 Python 和 Theano 实现 GRU/LSTM RNN（http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最大池化（Max-Pooling）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;池化（Pooling）操作通常被用在卷积神经网络中。一个最大池化层从一块特征中选取最大值。和卷积层一样，池化层也是通过窗口（块）大小和步幅尺寸进行参数化。比如，我们可能在一个 10×10 特征矩阵上以 2 的步幅滑动一个 2×2 的窗口，然后选取每个窗口的 4 个值中的最大值，得到一个 5×5 特征矩阵。池化层通过只保留最突出的信息来减少表征的维度；在这个图像输入的例子中，它们为转译提供了基本的不变性（即使图像偏移了几个像素，仍可选出同样的最大值）。池化层通常被安插在连续卷积层之间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;MNIST&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MNIST数据集可能是最常用的一个图像识别数据集。它包含 60,000 个手写数字的训练样本和 10,000 个测试样本。每一张图像的尺寸为 28×28像素。目前最先进的模型通常能在该测试集中达到 99.5% 或更高的准确度。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;动量（Momentum）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动量是梯度下降算法（Gradient Descent Algorithm）的扩展，可以加速和阻抑参数更新。在实际应用中，在梯度下降更新中包含一个动量项可在深度网络中得到更好的收敛速度（convergence rate）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：通过反向传播（back-propagating error）错误学习表征&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层感知器（MLP：Multilayer Perceptron）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层感知器是一种带有多个全连接层的前馈神经网络，这些全连接层使用非线性激活函数（activation function）处理非线性可分的数据。MLP 是多层神经网络或有两层以上的深度神经网络的最基本形式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;负对数似然（NLL：Negative Log Likelihood）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参见分类交叉熵损失（Categorical Cross-Entropy Loss）&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经网络机器翻译（NMT：Neural Machine Translation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NMT 系统使用神经网络实现语言（如英语和法语）之间的翻译。NMT 系统可以使用双语语料库进行端到端的训练，这有别于需要手工打造特征和开发的传统机器翻译系统。NMT 系统通常使用编码器和解码器循环神经网络实现，它可以分别编码源句和生成目标句。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：为统计机器翻译使用 RNN 编码器-解码器学习短语表征（Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;神经图灵机（NTM：Neural Turing Machine）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NTM 是可以从案例中推导简单算法的神经网络架构。比如，NTM 可以通过案例的输入和输出学习排序算法。NTM 通常学习记忆和注意机制的某些形式以处理程序执行过程中的状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：神经图灵机（Neural Turing Machines）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;非线性（Nonlinearity）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;参见激活函数（Activation Function）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;噪音对比估计（NCE：noise-contrastive estimation）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;噪音对比估计是一种通常被用于训练带有大输出词汇的分类器的采样损失（sampling loss）。在大量的可能的类上计算 softmax 是异常昂贵的。使用 NCE，我们可以将问题降低成二元分类问题，这可以通过训练分类器区别对待取样和「真实」分布以及人工生成的噪声分布来实现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：噪音对比估计：一种用于非标准化统计模型的新估计原理（Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用噪音对比估计有效地学习词向量（Learning word embeddings efficiently with noise-contrastive estimation）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;池化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;参见最大池化（Max-Pooling）或平均池化（Average-Pooling）&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;受限玻尔兹曼机（RBN：Restricted Boltzmann Machine）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RBN 是一种可被解释为一个随机人工神经网络的概率图形模型。RBN 以无监督的形式学习数据的表征。RBN 由可见层和隐藏层以及每一个这些层中的二元神经元的连接所构成。RBN 可以使用对比散度（contrastive divergence）进行有效的训练，这是梯度下降的一种近似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;第六章：动态系统中的信息处理：和谐理论基础&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：受限玻尔兹曼机简介（An Introduction to Restricted Boltzmann Machines）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;循环神经网络（RNN：Recurrent Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RNN 模型通过隐藏状态（或称记忆）连续进行相互作用。它可以使用最多 N 个输入，并产生最多 N 个输出。比如，一个输入序列可能是一个句子，其输出为每个单词的词性标注（part-of-speech tag）（N 到 N）；一个输入可能是一个句子，其输出为该句子的情感分类（N 到 1）；一个输入可能是单个图像，其输出为描述该图像所对应一系列词语（1 到 N）。在每一个时间步骤中，RNN 会基于当前输入和之前的隐藏状态计算新的隐藏状态「记忆」。其中「循环（recurrent）」这个术语来自这个事实：在每一步中都是用了同样的参数，该网络根据不同的输入执行同样的计算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：了解 LSTM 网络（http://colah.github.io/posts/2015-08-Understanding-LSTMs/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：循环神经网络教程第1部分——介绍 RNN （http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;递归神经网络（Recursive Neural Network）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;递归神经网络是循环神经网络的树状结构的一种泛化（generalization）。每一次递归都使用相同的权重。就像 RNN 一样，递归神经网络可以使用向后传播（backpropagation）进行端到端的训练。尽管可以学习树结构以将其用作优化问题的一部分，但递归神经网络通常被用在已有预定义结构的问题中，如自然语言处理的解析树中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用递归神经网络解析自然场景和自然语言（Parsing Natural Scenes and Natural Language with Recursive Neural Networks ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ReLU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即线性修正单元（Rectified Linear Unit）。ReLU 常在深度神经网络中被用作激活函数。它们的定义是 f(x) = max(0, x) 。ReLU 相对于 tanh 等函数的优势包括它们往往很稀疏（它们的活化可以很容易设置为 0），而且它们受到梯度消失问题的影响也更小。ReLU 主要被用在卷积神经网络中用作激活函数。ReLU 存在几种变体，如Leaky ReLUs、Parametric ReLU (PReLU) 或更为流畅的 softplus近似。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：深入研究修正器（Rectifiers）：在 ImageNet 分类上超越人类水平的性能（Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：修正非线性改进神经网络声学模型（Rectifier Nonlinearities Improve Neural Network Acoustic Models ）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：线性修正单元改进受限玻尔兹曼机（Rectified Linear Units Improve Restricted Boltzmann Machines &amp;nbsp;）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;残差网络（ResNet）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度残差网络（Deep Residual Network）赢得了 2015 年的 ILSVRC 挑战赛。这些网络的工作方式是引入跨层堆栈的快捷连接，让优化器可以学习更「容易」的残差映射（residual mapping）而非更为复杂的原映射（original mapping）。这些快捷连接和 Highway Layer 类似，但它们与数据无关且不会引入额外的参数或训练复杂度。ResNet 在 ImageNet 测试集中实现了 3.57% 的错误率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于图像识别的深度残差网络（Deep Residual Learning for Image Recognition）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;RMSProp&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;RMSProp 是一种基于梯度的优化算法。它与 Adagrad 类似，但引入了一个额外的衰减项抵消 Adagrad 在学习率上的快速下降。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;PPT：用于机器学习的神经网络 讲座6a&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;序列到序列（Seq2Seq）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;序列到序列（Sequence-to-Sequence）模型读取一个序列（如一个句子）作为输入，然后产生另一个序列作为输出。它和标准的 RNN 不同；在标准的 RNN 中，输入序列会在网络开始产生任何输出之前被完整地读取。通常而言，Seq2Seq 通过两个分别作为编码器和解码器的 RNN 实现。神经网络机器翻译是一类典型的 Seq2Seq 模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：使用神经网络的序列到序列学习（Sequence to Sequence Learning with Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;随机梯度下降（SGD：Stochastic Gradient Descent）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随机梯度下降是一种被用在训练阶段学习网络参数的基于梯度的优化算法。梯度通常使用反向传播算法计算。在实际应用中，人们使用微小批量版本的 SGD，其中的参数更新基于批案例而非单个案例进行执行，这能增加计算效率。vanilla SGD 存在许多扩展，包括动量（Momentum）、Adagrad、rmsprop、Adadelta 或 Adam。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于在线学习和随机优化的自适应次梯度方法（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：斯坦福CS231n：优化算法（http://cs231n.github.io/neural-networks-3/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;技术博客：梯度下降优化算法概述（http://sebastianruder.com/optimizing-gradient-descent/）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Softmax&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Softmax 函数通常被用于将原始分数（raw score）的矢量转换成用于分类的神经网络的输出层上的类概率（class probability）。它通过对归一化常数（normalization constant）进行指数化和相除运算而对分数进行规范化。如果我们正在处理大量的类，例如机器翻译中的大量词汇，计算归一化常数是很昂贵的。有许多种可以让计算更高效的替代选择，包括分层 Softmax（Hierarchical Softmax）或使用基于取样的损失函数，如 NCE。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TensorFlow是一个开源 C ++ / Python 软件库，用于使用数据流图的数值计算，尤其是深度神经网络。它是由谷歌创建的。在设计方面，它最类似于 Theano，但比 &amp;nbsp;Caffe 或 Keras 更低级。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Theano&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Theano 是一个让你可以定义、优化和评估数学表达式的 Python 库。它包含许多用于深度神经网络的构造模块。Theano 是类似于 TensorFlow 的低级别库。更高级别的库包括Keras 和 Caffe。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;梯度消失问题（Vanishing Gradient Problem）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;梯度消失问题出现在使用梯度很小（在 0 到 1 的范围内）的激活函数的非常深的神经网络中，通常是循环神经网络。因为这些小梯度会在反向传播中相乘，它们往往在这些层中传播时「消失」，从而让网络无法学习长程依赖。解决这一问题的常用方法是使用 ReLU 这样的不受小梯度影响的激活函数，或使用明确针对消失梯度问题的架构，如LSTM。这个问题的反面被称为梯度爆炸问题（exploding gradient problem）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：训练循环神经网络的困难之处（On the difficulty of training Recurrent Neural Networks）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;VGG&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;VGG 是在 2014 年 ImageNet 定位和分类比赛中分别斩获第一和第二位置的卷积神经网络模型。这个 VGG 模型包含 16-19 个权重层，并使用了大小为 3×3 和 1×1 的小型卷积过滤器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：用于大规模图像识别的非常深度的卷积网络（Very Deep Convolutional Networks for Large-Scale Image Recognition）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;word2vec&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;word2vec 是一种试图通过预测文档中话语的上下文来学习词向量（word embedding）的算法和工具 (https://code.google.com/p/word2vec/)。最终得到的词矢量（word vector）有一些有趣的性质，例如vector('queen') ~= vector('king') - vector('man') + vector('woman') （女王~=国王-男人+女人）。两个不同的目标函数可以用来学习这些嵌入：Skip-Gram 目标函数尝试预测一个词的上下文，CBOW &amp;nbsp;目标函数则尝试从词上下文预测这个词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：向量空间中词汇表征的有效评估（Efficient Estimation of Word Representations in Vector Space）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：分布式词汇和短语表征以及他们的组合性（Distributed Representations of Words and Phrases and their Compositionality）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;论文：解释 word2vec 参数学习（word2vec Parameter Learning Explained）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 25 Sep 2016 14:11:02 +0800</pubDate>
    </item>
  </channel>
</rss>
