<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>机器之心</title>
    <link>http://www.iwgc.cn/list/670</link>
    <description>人与科技的美好关系</description>
    <item>
      <title>专访 | 机器之心独家对话田渊栋：无监督学习具有超过人类的发展潜力</title>
      <link>http://www.iwgc.cn/link/3111768</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心原创&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;记者：赵云峰、赵巍&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;编辑：老红&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;两年前，知乎作者「谢熊猫君」将「waitbutwhy」上的 The AI Revolution 译为中文。他巧妙引入「吓尿指数」，人工智能也被披上了「玄学」外衣。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;半年前，AlphaGo 对阵李世乭。这场「人机大战」让「人工智能」这一并不新鲜的名词，第一次获得了大规模的主流关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;兴奋、怀疑与惶恐，比挑战人类智力「试金石」更让人不知所措的，是站在改变「临界点」上的迷茫。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「比赛期间有一张让人印象深刻的照片。照片中一边是需要千台机器的 AlphaGo，另一边是李世乭和一杯咖啡。大自然的鬼斧神工一直让人肃然起敬，而这其中最杰出的造物，莫过于我们人类自己。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;说出这句话的人是田渊栋。在 AlphaGo 和李世乭人机大战正酣之时，这位人工智能领域杰出的华人专家因其主导开发了 Facebook 的人工智能围棋研究项目 DarkForest 以及在知乎上独到而专业的比赛点评，迅速为大众所熟知。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibSRMYoPOcvDfyajbARlTgWu1Uc03tjWqUzkzDhKwHyzJtk0er6N0tIbnWT0ZfGI7lribpEXpuTlXA/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卡耐基梅隆大学机器人系博士、前谷歌无人驾驶汽车项目组研究员、现 Facebook 人工智能组研究员，多重身份的加持和前沿、专业的研究为田渊栋吸引了相当多的目光。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们要做的，不是成为高谈阔论的事前事后评论人，而是去当那一两个先行者。他们才是明白事实真相，才是真正改变历史轨迹的人。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了前沿研究与技术创新，他还保持了长期的写作习惯。除了早期的个人博客和现今的知乎专栏，田渊栋甚至还完成过一部超过 30 万字的小说，这在以理工科为代表的前沿科技领域是极为罕见的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;近期，机器之心对田渊栋进行了一次独家专访。关于人工智能、个人经历以及前沿技术研究的进展，田博士分享了诸多鲜为人知的故事和观点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;本文目录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ol class=" list-paddingleft-2" style="list-style-type: decimal;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;写小说的人工智能科学家&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;从交大人工智能论坛版主到微软研究院&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;人工智能不该被过度炒作&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;沟通和交流能力是研究的重要组成部分&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;从 Google 到 Facebook 的身份转变&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关于未来人工智能行业的一些思考&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;关于国内人工智能的发展&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;田渊栋的学习方法论&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;一、写小说的人工智能科学家&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们注意到您非常喜欢写作，以前也写过小说，这是您的业余爱好吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，这个也算是我在硕士和博士期间的一个爱好，主要的成就是写过一部大概 30 万字的长篇小说，还有一些中篇和短篇。当然，长篇小说毕竟读得人不多，后来就改成写博客，大家还是愿意看的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您对文字的爱好，是因为受家庭影响吗？还是您从小就有偏向文科？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我以前受到高中班主任的影响，对历史有兴趣，愿意看些东西，当然写作还是高考作文这种水平。大概 06、07 年的时候，网络小说开始流行，和大家一样我也喜欢看。看多了，我这个人就喜欢动手，自己写写试试。一开始写的时候真的不好写，挤不出几个字来，写的是全是大段对话。但是慢慢就知道怎么写了，时间长了，越写越顺。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：都是科技题材的内容吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：都有，玄幻加科技，就是大杂烩嘛。我不是商业写作，所以主角不打怪升级。主要是掺杂了一些个人经历，把自己想写的人物和事情写出来，小说嘛，题材其实无所谓，发生在火星上还是地球上都一样，但人物很重要，是灵魂。这部长篇小说写了五年，一开始是零零散散的写，然后串起来，最后集中精力花三个礼拜把它全部写完，现在回想起来，那段时间太有意思了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您这个领域跨度太大了。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：没有，这个也就是业余兴趣，现在比较忙，以写博文和杂文为主。写小说这个经历对我的锻炼很大，一方面在写人物的时候，要站在人物角度见他所见想他所想，要让人物活起来，这个对于习惯从自我出发的人来说是很好的历练；另一方面语感有很大提高，有了之前的积累，现在写杂文和博文，自然而然会觉得这个地方这么写，会让读者看得顺眼。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您现在的状态是以论文为主吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：是的，学生时代相对来说空闲一点，也是积累和摸索阶段。现在是当打之年，当然是以论文为主，人生的好时光没有多少的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;二、从交大人工智能论坛版主到微软研究院&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您提到过您对数理化全有兴趣，最后转到计算机。那您在本科的时候读什么专业呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我是计算机专业的。当时我进了交大的联读班，一开始不分专业上基础课，比如说数学物理化学课，还有通信的相关课程，到两年之后再选专业。现在我相信很多学校也开始做这方面的尝试了。比如说第一年不选专业，让你自己去选什么科。我觉得这样对于一个人的发展来说，特别是对学术有喜爱的人来说，是比较好的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您当年本科读完了，就到美国去读博士了？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我当时在交大读了研究生，然后再出国。那个时候我基本上花了一半时间在微软亚洲研究院。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那个时候已经在做了？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，在做这方面的。一开始是做人脸嘛，然后做一些比较广泛的图像识别，图像课程的一些问题，然后就申请了美国的博士。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：这个方向当时是您在交大的导师帮您选的，还是研究院的，还是您自己的兴趣？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得我真的要感谢我在交大的导师张丽清教授，他给了我自由的发展空间。我说我要去微软亚研院实习半年，一般老师不会同意的。他说：「没关系，去吧」，非常支持。我在交大时做计算机视觉，研究院那边也是做图像识别的，具体来说是人脸识别。当时我想着能去研究院很好了，非常向往，做什么方向也无所谓。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那个时候机器学习有重视，但是没有现在这么热，是吧？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，那个时候是这样的，学术归学术，系统归系统，两边分开。机器学习的能力已经开始体现出来了，比如说在特定问题如人脸检测上有很好的解决方案；但是更复杂的物体检测则远远不及人的能力，大家都在讨论什么才是好的视觉表示。那一波其实持续了很长时间，从 01 年开始一直持续到大概 07-08 年。那时我觉得机器学习有用，但没有像现在这样有广泛的应用。那时基本上是人工设计特征，再让计算机跑个线性模型就完事了。特征还是要人自己去找。现在就完全不一样了，因为数据量大了，又有深度学习的框架，可以让计算机自己去学到好的特征，效果也好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了去亚研院之外，硕士阶段我主要在数学上打下了基础。我当上了交大 BBS 数学版版主，经常去回答板上提出的各种问题，不能回答的话就会去查资料。作为版主，回答不了问题是会有很遗憾的感觉的，这样就产生一种压力，通过这种方式，我强迫自己不断地学习。时间长了之后就慢慢习惯。另一方面我还开讨论班，我说我主动来讲机器学习和模式识别的一些数学模型，这样大家来听，我就得要准备，准备多了，基础就扎实了。研究生阶段还选了一些其它系的课，比如说广义相对论还有随机过程，一般人不会这么做，但我有兴趣。这样基础就打下了，以后看别的文献就会方便一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：听上去，好像您在学生时代的时候，就已经应该是交大学生团体里面的机器学习，人工智能的一个先锋人物了。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：可以算吧。我那时还是人工智能版的版主。版上那时有很多非常有趣的讨论。当然那时候的讨论，现在看起来可能比较幼稚。不过既然是出于兴趣，也不怕人笑话。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那个时候您比较确定自己会读这个方向，是吧？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;：至少确定将来会做人工智能这一块吧。有兴趣的原因是，我觉得很多问题没有解决。当时我在版里说，人工智能感觉上就像化学史上「燃素说」和「氧化说」争鸣时的状态，还没有系统性的理解，还在黎明前夜。大家现在都在那边低头调参数加特征，只知其然却不知所以然。将来肯定有很多理论框架，但是哪个是对的，现在毫无头绪。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这个就是机会。现在回过头来看，我想的是对的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、人工智能不该被过度炒作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您觉得人工智能现在的状态呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：还是那样，还是比较浅层的。当然我们现在有机器也有数据，效果肯定比以前好很多。但是理论这一块，现在还没有太大的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：所以您专门写文章呼吁不要对人工智能过度炒作，目前理论上的挑战还是非常的艰巨。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，还是有很多问题。当然了，还存在一种可能，现在机器多了数据多了，不用管理论，一路做应用做到底。在理论还没有掌握之前，应用已经超过人的水平，都是有可能的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那对围棋的研究，你还会继续下去吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这块我们还会再做一点，但是现在主要是开一些其他的方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那你现在最主要的兴趣是在视觉和在语言处理这方面的这个方向吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这些方向都会有涉及。但现在时代不同了，不应该把自己限制在视觉或者某个特定方向。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：不是一个专门的应用。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，因为感觉上自然语言处理、图像、语音，这些基本上都是应用了。所以说如果必要的话，其实可以在这个中间进行切换，或者做一些交叉的方向。以前做这三个方向，可能需要大量的领域知识，特别是做自然语言处理，要学以前语言学的文献。要做分词，比如说每个词给一些词性。要做一些语法的分析、语素的分析，有很多很多的步骤。但现在的趋势是从头到尾都让机器学。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：就是他们说的 end-to-end。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：是的，end-to-end 端对端的学习。比如说自然语言这一块，并没有比以前的效果好太多，但整个流程变得很简单方便，将来进步的速度可能就会变快。比如机器翻译里面，你把一个句子，直接通过神经网络翻译成另外一个语言的句子，这样就比以前快。以前可能要分词呀，词性标注呀，对每个词找到另外一个语言对应的词或者词组，找到之后再重新排列一下，最后才产生一个句子。要通过几个步骤，但是现在在概念上，只要一步就算出来。【注：现在基于神经网络的翻译系统确实比以前好很多了 】。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么会出现端对端呢？我觉得主要是因为神经网络这个模型的优点。神经网络模型是个非常灵活可扩充的模型，随便连一下，然后做后向传递就可以了。大家一开始没有意识到它的厉害，觉得做这个系统得要分几步吧，神经网络只是其中一步，前面和后面还是通过传统方式来做比较安全。后来大家就慢慢意识到，为什么不用神经网络把整个系统打通？那样的话，又省时效果也会更好。自然而然，大家都会思考端对端的思路。我觉得现在基本上端对端的效果，主要体现在整个迭代的速度上，从设计模型到训练，到看到结果，到修改模型这样一个循环的速度会很快，效果也通常会变得更好。人优化参数的时候，可能半小时优化一次，看看结果如何；机器优化参数，可能一秒就优化几百次。所以这个时间的改进是数量级上的改进。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据集的获得，现在主要是在网上花钱，人工标注。比如说一张图几块钱的，然后让人去做。就发动群众的力量嘛，看大家有没有空。有空闲着无聊了，就标注两张。这样把力量汇集起来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：像您在 Facebook 做的，Facebook 有那么多的图片，然后底下还会有人可能对这个 pictures 做一个评论。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那这种东西，你们把它拿来用吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个是有用的，但是具体怎么用，我们现在还在商讨中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：因为它没有那么准确？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：是的。而且大量的图片，下面的评论可能是杂的、乱的。比如说我们所有的话，下面都可以写一个赞呀。这个评论，其实跟这张图没有关系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：需要比如说去噪音这种方式去解决。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，可能有多少话，一开始说得跟图片有关。但是后面说两句，说到某个人身上，扯远了，离题了，这句话就跟这张图没有关系了，所以这个其实都很难，现在还没有办法做，还需要好好研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我记得前一阵华为他们那边做了一个小对话的系统，然后进行了简单的归纳。它用的数据其实在微博上取下来的，但是它那个数据像您说的也很乱，它会有一些规则。比如说第多少条回复以后，肯定就绕得不知道哪儿去了，肯定不能要了，还有常见的一些感叹的词语。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;：对，肯定不一样。相对来说，你可能需要把剩下的句子提关键词。然后把关键词作为这个图的标注，这是一种方法。或者做一些简单的语音分析。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：它还有一点，田博士您看到一张图，我们人可以标注它。但是实际上这张图有好多种标注方式而且都是准确的，因为看的角度不同。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那在这种标注数据拿给你的时候，一个图会给你多少种标注呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个其实不同领域，有不同的方法。比如说问答系统，可能有一个问题有一个回答。问题不同，回答又不同。所以一张图里面有三个问题，那么就有三个回答。或者一张图有三个问题，有三十个回答。每十个回答对应于一个问题，这是可以的。然后你有这些数据之后，你想办法找到一个比较好的模型去归纳这些数据，这是一种。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如说还有一些，一张图有几千个标注。一张图里面你可以标注很多属性，里面有猫，有人，有天空，有大地，可能是外景，或者可能是晚上。像这种，每张图上有很多属性，这种也可以拿来的。这种不同的标注方法，目标是不一样的。比如说你做问答系统的话，问题和回答必须成对出现的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为这个回答非常依赖问题，如果你没看见图，只看到问题。然后回答的话，其实正确率挺高的，因为可以猜出来。所以你就会发现在不同的情况下，需要的标注是不一样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：这种数据上的处理，不仅需要强有力的技术，还需要更多的思考。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，所以这一块的思考需要很大很大的力气。有可能一个数据做得不好的话，它的标注出了问题，或者它的采集过程出了问题，就不能用了。现在有很多机构都在做数据集，想办法通过数据来取得进步。做完数据处理之后，大家都会有一个客观的标准来评判他的算法怎么样。然后在数据上提高自己的算法性能，从而达成整个领域的提升。通过衡量数据上的表现，来衡量整个领域的进展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那这个趋势看来也是一种需要了，在学术圈，包括像公司这样的级别，尽量去制造好的学习数据，可能会在深度学习这一块取得极大的突破。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这是一个方面，另外一个方面在算法这一块，我们希望深度学习用更少的数据达到相同的效果。这两方面都在做的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：小数据这件事，大家很关注。你觉得现在有什么突破口，或者什么思考方法？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在主要做的是：你先在大数据上，训练一个模型。然后在小数据集上做微调。这样的话，你所要学习的权值数目就变少了。如果这两个问题本身也有相关性，这样就比较容易。或者你把少部分具有足够的健壮性的数据，加上大量的弱标注的数据放在一起训练，这样也是可以的。或者把小数据通过增广变成大数据，比如说旋转缩放图像，里面的物体属性标注保持不变。这样的话，数据增加了对模型的训练过程会有好处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当然，这些都是权宜之计。真正要解决这个问题的话，需要对深度学习的机制要有很明白清晰的了解。这个很难，还没有办法做出来，大家还在做。之前我去清华做演讲的时候，跟姚教授也在聊，他也觉得这是非常难的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：他们现在也在关注？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，他们现在也关注这个。神经网络的训练是一个非凸的优化问题，目前传统的方法没有办法解决它。没有对它的本质理解，可能没有办法真正解决神经网络训练过程中的疑难杂症。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：说到这个，有一个大家都在讨论的问题，就是神经网络它的高效性，有一点像黑箱子，里面真正的数学原理大家还不是很清楚。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您对这方面的研究感兴趣吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这方面跟我的博士论文是很有关系的。虽然说大家可能因为围棋的工作认识了我，但是我在博士阶段是做理论的，研究如何获得非凸问题的最优解。一般情况下这个问题做不了，但在某些特定情况下是有可能的。我当时做的是如何对齐两张扭曲的图像。对齐是非凸的，局部最小值的分布和图像内容有关，图像里有重复结构，比如说一栋建筑物里有很多窗，那么就对应非常多的局部最小值。那么这个怎么办呢？一种方法是说我们干脆不优化了，就直接把图像用各种已知的扭曲参数生成出来，存到数据库里。然后新的扭曲图像拿进来之后，我就查那个数据库，就可以得到我想要知道的参数。但是这个办法的缺点是需要要非常多的数据，才能够保证得到的参数是准确的。另外一个方法就是传统优化算法，不管它是不是非凸的，我们用梯度下降迭代，但这样可能会陷入局部最小值。我发现了一个折中的方案，结合迭代算法和数据，做一个数据驱动的迭代算法，这种情况下，可以证明用更少的样本达到全局最优解。其中原因就是这个特定的非凸问题有一些特殊的群结构。这就是我博士毕业论文的主要工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我当然希望在深度学习上也能看到一些特殊结构，从而揭示它的秘密。但这个仍在探索中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、沟通和交流能力是研究的重要组成部分&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您的导师对您的影响好像很大，特别是在写作方面。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：他的写作和演讲的技巧是很好的。我觉得他对我在博士期间的成长是非常有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：就是他的沟通、交流能力好像很强。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，他是印度人嘛，大家都知道印度人这方面的能力比较强，他就是这样的。其实我之前是比较内向的，可能大家都有这种刻板印象，认为中国好学生比较安静一点，不愿意说话，比较内向。但是你出国了之后，你会觉得这两个属性不是连在一起的。你可以成为一个好学生，然后你也可以愿意和别人聊天。这些都可以做到的，都可以培养的。比如说一开始上台演讲的时候，有一种恶性循环，上台不知道怎么说，不敢上台。你上台又不知道怎么说，又不敢上台，那你到时候就不敢上台了。一开始要打破这个恶性循环，就要准备非常丰富的，非常好的演讲。第一个演讲说好了，觉得自己有信心了之后，再往上走，就一点点变得非常非常自然了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：对，对交流还是有很多的思考。那我个人有一个感触，不知道您是不是认同。就是中国的理科方面的学生，如果有一些追求的话，一定要对语言非常深的深钻。尤其是英语，我觉得国内好像对这个重视不太够。大家好像觉得，大家说论文的英语语言本身不是很复杂。但是我觉得真的，也提到您刚才一个话题。可能你整个思维方式的形成，不仅仅是通过读论文，可能是通过读专著呀，通过读科普的著作呀，跟其他英语的 speaker 进行交流，学术沟通呀。这里面实际上要求你非常强的语言能力，我感觉到您好像是咱们国内华裔学生里面，对这个是有足够重视的。好像有一些学者，尤其在国内没有国外留学经验的人，不知道这个东西价值有多大。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：英语只是特定语言，我指的是表达和交流能力，这个价值是非常非常大。中国有句老话叫「酒香不怕巷子深」，其实在现代社会不完全是这样。越是好的东西，越是要说出来，一定要广播，要想办法让大家都知道，才能让别人欣赏你。每年投稿在各大杂志和会议上的文章，基本上以千为单位了，加在一起肯定要上万了。你的文章能否脱颖而出，是一个很大的问题。当然了，如果你做了一个世界上没有人做出来的问题，或者你的效果比别人好太多，那不必多言，大家都觉得你非常厉害对吧。但是很多情况下，你的工作并不能达到世界第一，也有很多工作是分析现有问题，或者表达一个新的思路或者观点，不是硬拼性能的。像这种文章就要靠说了，要靠组织和表达清楚的语言，不然的话，别人看了一头雾水不知道你说什么。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;国外有好多的教授，其实这方面的功底是非常深的。比如说咱们 CMU 有个教授，一篇文章开篇引了福尔摩斯的话。福尔摩斯说：「没有数据支持的任何推理，都是不成立的。」然后他就举例说明数据的重要性。这样的文章，不一定有算法上的贡献，但是他们对别人思维的改变，其实起很大的作用，让别人觉得他这样的思路可能是对的，从而改变自己整个的研究路线。我刚去的时候不适应，觉得这种软文有什么好看的，只会用个最近邻方法，一点技术含量也没有；现在发现这不是吹牛，是对大方向的重要把握。现在深度学习来了，数据更多了，他在文章中提倡的，完全是符合潮流的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您现在做研究的时候思考，用英语在做吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：思考并不是依赖于某种语言。英语用得多些，因为这个领域中文有很多词可能还是得翻成英语。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：甚至超越语言的一种。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，图像呀，或者一种内在的东西。然后你想到了之后，通过你内部的思考表达出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：可能这是一种，有点像神经网络，它是跨越语言的。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，拿神经网络的术语来说，它们都映射到同样一个内部表示，然后再翻译过去。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、从 Google 到 Facebook 的身份转变&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：当时田博士您在谷歌无人驾驶的项目里面做过一段时间，后来转到 Facebook。实际上在很多人眼里，谷歌已经是天堂般研究这样一个地方。您怎么会转到 Facebook？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得主要还是因为谷歌是一个比较大的公司嘛。并不是说谷歌每个人都可以做你想做的事情。要看你在哪个组，你是什么地位，你做什么样的方向。大公司有一个问题，去得晚的话，你可能只能做螺丝钉。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：有一点排资论辈的感觉吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：其实谷歌已经非常不排资论辈了，已经很开放了，但是还是会存在这样的问题。因为没有办法，无人车已经做了很多年了嘛。你进去之后，东西都做好了，只要修补就行了，你想要搞些有趣的，条件不允许。一开始觉得挺有意思的，但是时间长了，你会觉得没意思。而且还有一个问题，无人车比较保密，想要发表自己的工作就很难。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：有点受限制。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，我又是一个比较喜欢写博客的人，你让我这个话不能写，那个话不能写，那怎么写呢。我之前写过一篇有关无人车的博文，不过那篇博文没有涉及到任何细节。后来还是觉得 Facebook 相对来说更公开一点，所以就跳走了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您觉得 Facebook 的企业文化有哪些非常值得我们国内企业学习的地方？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得它就比较扁平嘛，小扎就坐在我后面 6、7 米的地方，6、7 米都没有，基本上我后面是一个很大的区域。就是 CEO、CTO 还有 COO 都在后面坐着嘛。去年我的实习生周博磊还被 COO 雪莉点到了，雪莉带着访问者问他在做什么工作，他回答得非常好。感觉上高层都对人工智能很感兴趣。Facebook 总的来说就是比较开放的环境，很多时候比较随意吧，没有那么严格的上下级。在 Facebook 里面，你也看不见别人的级别。相对来说人和人之间平等一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：那你们这个深度学习研究人员和公司的高层坐得这么近，是不是因为高层也是把你们最重视的一个。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有可能是吧，但是这个我也不好说。我觉得这个是公司的安排嘛，所以我不会有什么特别的评论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那他有没有时候会主动地过来，问问你现在在忙什么呀？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：他还是比较忙的。每个人都有自己的职责嘛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：LeCun 是这个领域的元老，离 Zuckerberg 比较近，就想说有没有从他身上得到一些启发。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：LeCun 是一个非常开放的一个人。感觉我们整个组非常民主，你想做什么都可以。如果你愿意做的话，也没有人管你。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：方向上给你很大的支持。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：就像为什么会做围棋嘛，对吧，就是很奇怪的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：这个是你自己选的吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：自己选的，围棋是自己选的，然后一开始数据集呀，整个东西都是我和实习生自己弄的，然后 DarkForest 的名字也是我自己起的。这个名字比较酷。我们组也是比较开放的，都没有管，说你这个名字一定要跟 Facebook 有关。当时也没有多少人看好这个方向，只是一个试验。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：所以你们的研究不需要直接跟公司的业务产品相关吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：能有产品的话，那当然最好，但是还是以研究为主。而且这次我选围棋也证明了眼光是对的。我当时对它有兴趣，是因为看到了两篇文章，当时大家都没有引起重视，就只有圈里人知道。我看了一下觉得这个东西有点意思。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：就是你的文章可以引用的地方。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，我觉得这个方向，将来会有一些突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那当时你意识到他们进度会这么快吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：当然没有意识到那么快，只是觉得这个方向可能有前景。当时还做了还挺多项目的，没有吊在一棵树上。做研究的风险都很高，所以你必须分几个不同的项目同时做，看哪个项目比较好。这个围棋项目它的效果是不错的，那么就花时间在上面。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那除了给你们很多的自由度之外，你觉得他（LeCun）给你最大的帮助是什么呢？或者是收获？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：他会有一些比较大的想法和观点分享给大家。比如说他觉得对抗式学习是一个比较重要的方向。他会经常说嘛，让大家觉得这个东西挺重要，这么做可能是有道理的。用这种方式来影响大家。不过他也没有说一定要做这个，一定要做那个，没有。他是个比较宽厚的长者，和大家聊聊。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：经常会跟你们沟通吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：还比较多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那你们内部有各种，像研究人员之间的交流，小组这种讨论吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：你想要研究，想交流很容易嘛，因为大家都坐很近。你可以发个信息过去。或者说直接到他座位上随便聊聊，大家讨论一下。这个还挺重要的，特别是你要做别的方向自己不熟悉的话。你一个做图像的人，突然去做自然语言这一块，那么你对自然语言理解这一块的文献，肯定不那么熟悉。你问别人一个想法，别人会告诉你这个东西做过了。这样的话，你可以慢慢知道这个方向，它的现状怎么样，然后接下来要怎么做，什么地方做过，有哪些地方还没有做过。通过这样的交流方式，你会很快的知道什么东西是应该做的。研究这一块，对方向的确定是很重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我看您对研究的方法论，自己非常有成熟的一个看法。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个也是慢慢总结出来的，碰过钉子嘛，很多时候你都知道了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们看过一篇文章，就是 LeCun 接受采访的时候，说了一个最不喜欢的对深度学习的描述，就是它像大脑一样的过程。后来记者让他能不能用 8 个单词去描绘一下，然后就想说您能不能用简单的一句话去描述一下深度学习？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;：我觉得就是神经网络嘛，现在目前为止还是神经网络为主。就是通过神经网络的多层处理，把数据从一开始的红蓝绿这种非常简单的特征，通过一点点的自组织，变成比较复杂的特征，就是这样一个过程。当然这个想法老早老早就有了，只是最近才在实际数据集上产生了很好的效果，受到了大家的关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，计算上的神经网络和生物上的神经网络其实没有太大的关系，神经网络里的节点只是对神经元做了最简单的抽象。其实神经元结构太复杂了，一个含各种参数的微分方程，要能快速模拟上亿的神经元，代价很大；另一方面，就算模拟出来效果好，也不知道是哪个原因导致的，反而会拖累对本质的理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我看您博客里面提到科技树这样一个概念。能不能以科技树的形式给大家梳理一下人工智能，或者图像识别这样一个大体的框架。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我在博客里写的科技树，是一个比方。你看科技树的发展，一开始枝繁叶茂，大家都觉得很有希望，可是发展一会儿就停下来了。等大家没兴趣的时候，过了几年，在某个很不起眼的地方，突然就出现一个突破。所以做一个研究员嘛，最重要的是要于无声处听惊雷，就是不能人云亦云，要静下心来找到别人没看见的方向，然后把它挖深，证明这个方向是有效的。一旦大家都觉得这个方向对，大家冲过来接你的棒了，你就是成功的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;六、关于未来人工智能行业的一些思考&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：未来深度学习，包括整个人工智能面临的一个挑战，就是非监督系统学习。现在做得最好的监督系统学习，有些人觉得稍微过度，您是认可的吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，这个话我觉得是有道理的。监督学习相对成熟些，但是需要大量的样本，往往是样本翻倍，性能才涨一点点。很多时候，对一个系统而言，光用样本把它的性能提上去就很难。非监督学习要是效果好了，对样本的需求就会少很多。比如说吧，我本来可以对围棋的每一步做一个标签，这步是好棋，这步是臭棋。但是也可以给最后输赢的结果，把这个结果反向传递回去，让算法自己发现哪一步是好棋，哪一步是臭棋。那这样的话，你输入信号变少了，就是一种半监督学习的方法；另一方面，机器也就有了超过人类的潜力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那这种东西也是你们在 Facebook 关注的吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我还是比较关注的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那你会花一些时间专门攻这方面的研究吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：肯定会看一些文章嘛，然后看看有什么东西可以做的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：会有一些 paper 出来吗？有一些计划吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在在做，但是能不能出文章，这个不知道，肯定是要边做边看看有什么有趣的。一开始你不熟悉这个领域，你肯定先看文章，然后再选题，然后再看有什么东西可以做。你看多了之后，会慢慢的发现，噢，原来这个有问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：去年有一篇论文【Human-level concept learning through probabilistic program induction&lt;/span&gt;&lt;span&gt;】讲到小数据集做得比较好的，甚至它自己在个别的案例上已经超越了（深度学习）。您是怎么评价他这种研究方式和思维方式？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：他那篇文章用图模型做 One-shot learning，和深度学习作了对比，在生成手写字母这个任务上，在小数据集上比深度学习要好。图模型在推理上比较自然，解释性也比较强，这个是大家公认的。但是相伴地就有另一个问题，就是说设计的模型一定要对，像他写字的模型可以设计得正确，但是对于复杂的真实世界，建一个包罗万象的模型就很困难，未必有深度学习的能力强，计算机视觉这个领域，大家都做了二三十年的模型了，结果还是被卷积神经网络超过了。所以说两者现在各有所长，深度学习长于感知，图模型长于推理，如果我们能把它们连起来会是个很大的突破，是值得我们去发现的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：所以您也比较看好这个方向，是吧？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：图模型和深度学习如果能够很深地结合起来的话，会是一个很好的方向，现在还是比较浅。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：所以您自己在这方面愿意做一些探索？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：都会看，我肯定不会现在下定论。先了解一下，一点点了解完之后。发现这个有意思，然后再去做。很多时候研究员做的事情，是介于了解、探索、研究之间的。你不知道在看这篇文章的时候，是为了做这个方向呢，还是属于好奇呢，还是审稿呢。所以很多时候你无法界定自己的工作。文章看多了，自然会有一些想法，如果想法有意思，就愿意花时间在上面，然后你就变成从事这个研究方向的人。做研究不像通常的工作，有个老板和你说具体要做什么。可能今天看文章，明天推公式，后天写程序，大后天发现全错了从头再来，自己得分配时间，得要找找准方向。所以啊，这个都不好说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：之前有一些深度学习比较小的突破，像注意力模型呀，记忆模型呀，还有深度神经网络简单通俗地解释一下，给一些对这个不太专业的读者，或者做一个形象的比喻。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个你看一些文章就可以了，很容易懂的。比如说注意力模型吧，看一张图，先看左边，再看右边，最后得出图里有什么的结论，和人的行为一样。听起来很有道理吧，但是实际上训练完，往往计算机看一眼就知道图里有什么，猜功太好，让它多看几眼没什么用，可有些情况下又是有用的。所以说实际机制未必和文章描述得一样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：刚才有一点涉及到，正好田博士对物理也非常的有了解。他刚才说很多非常非常多经典的东西，其实当时田博士您记得从经典物理过渡到量子物理，几个地方都在开花。像波粒、活动方程呀，这些东西都在。然后促使了这个量子力学突飞猛进的进展。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您觉得现在深度学习的状态和当时从经典力学过度到量子力学那种，比较非常大的状态，能是一种状态吗？还是您个人认为深度学习还是比较平稳，比较缓慢的发展。因为现在媒体对这个的炒作也很热，好像有一点新的科技时代的降临。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：不好说吧。量子物理怎么建立的呢？二十世纪初的时候，一个很大的问题是如何建模黑体辐射，一个东西加热到一定温度，会发出什么频率的光。物理学家们提出两种模型，各对了一半，就是拼不起来。然后大家深挖下去，作了夸张的假设，找到量子的方法去解释。相对论也是一样的，一开始大家用以太去解释光速不变，被干涉实验推翻，后来找到狭义相对论，认识到洛伦兹变换是绝对的。这两个都推翻了经典的直觉假设，刷新了大家对世界的认知。对物理来说，从不承认这些假设，到承认这些假设，是一个大突破；认知刷新，是一个大突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们现在不一样，是工科不是理科，更多是一种经验的东西，也更看重经验的结果。比如说吧，因为数据集不同，模型不同，经验的结果往往是模糊的，渐近的，慢慢地大家意识到这样是对的。这就不像物理学有个明确的分界线，控制完变量后，一个假设一个公式把现象阐述得很清楚，一个实验对不对，改变整个认知，然后宣告胜利。另一方面，你可能对深度学习的认知有突破了，但那时系统性能已经超越人类了，没有人在意。这两点都会让圈外人觉得发展相对平缓，没有像物理学这样的。当然，从人工设计特征到让机器自动发现特征，这是一个比较大的认识上的突破。但是就算如此，大家好像也没有把它当成是革命，而只是默默地记下了继续往前走。也许以后历史学家们会记录成突破吧，就像我们看二十世纪初那样；但是目前看来，身在局中的我们，并不一定会感觉得到，所以大家也不要期望太高嘛。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：发生得太静悄悄了。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，有可能某个人某一天宣布，深度学习是这样起作用的，认识上有了突破。然后圈外人觉得，我已经用上了，用上语音识别了，用上图像理解了，用上问答系统了，没有人管了。对他们来说，是一个很平稳的过渡嘛——软件变得越来越牛了。所以这个不像是物理，这个不一样的。物理那边，非常看重对事物的深刻理解。物理是理科，它的目标是发现。为了更新的发现，全世界可以砸钱下去不求回报。而我们这边，总的目标是做一个很好的系统给大家用，AlphaGo 战胜了李世石，大家把它当大新闻，就算世界上没人知道 AlphaGo 是如何算出好招的，也没有关系，没人管。当然，我个人非常喜欢好的理论，如果对深度学习有一个非常好的突破性理解的话，我会非常非常开心。虽然难，但我相信它迟早会发生的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：明白，因为提到一个人工智能进展的问题。您之前写文章，提出大家不要对人工智能有过度的热捧。就是说您觉得现在发展的，它现在最大的瓶颈是什么？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有很多，比如说小数据，非监督学习，比如说对整个深度学习的原理不理解，大家现在就是摸瞎调参数，看怎么样。没有对这个模型有本质的理解，这个其实是一个比较大的问题，这个是需要突破的。我之前说了嘛，这个突破可能对大众来说没有太大的意义，大家都觉得用上了，就用上了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：如果我们接下来要在这个无监督学习方面实现一些突破的话，有没有哪些您认为比较好的路径？比如说您刚才说的深度学习和图模型的结合。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们有注意到您之前开发过图像的大系统。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;机器之心：我们可能把它看作是图像和自然语言处理的结合，它们这个结合的时候，它的重点在哪个地方？怎么给它结合在一起？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在还是比较浅的结合，把两边的特征连在一起，或者放进模型里面混合下，就完事了。更深的结合现在还在研究中。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您在这个图像和自然语言处理结合的点，是不是有写论文的计划？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：现在在做，但是还早，可以回答一些问题，刚用的人可能会觉得很惊艳，但是用多了就知道它弱在哪里，离真正能用还早。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您怎么看待以对话引擎切入的工具，它是不是会取代我们的 App？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个我也不好评论，我觉得挺好的，可能是一个很好的入口吧，通过更自然的方式来跟别人交流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我们之前看到一篇文章，是科技公司对人才的激烈争夺，您是如何看待这个现象？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得这个对我们来说是好事，对吧，工资肯定会提高。另一方面，这也表明现在人才越来越重要了，以后人工智能能够自动化很多事情，有这方面的人才，能把人工智能运用得好，几个人的小公司能做到跟以前大公司一样，甚至超越，这都是有可能的。技术越发达，可能最后的效果就越好，以一当千当万，都不是天方夜谭。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：刚才我们聊的有监督学习、强化学习，最后到无监督学习。如果这个过程发展得很顺利的话，我们能够期待这个系统或者机器，能够做一些那些我们现在还不到的事情？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：如果这些阶段都能做完的话，那基本上就差不多了。因为人也从无监督中学习，一个婴儿通过有限的监督学习慢慢学到很多技能，对吧。这几块如果能做出来的话，确实会有很大的突破。人脑的核心技术肯定是大大领先现在人类掌握的核心技术，但是工程上仍然有很多可以改进的地方，你要相信进化出来的东西，它是会有很多缺陷的。我们现在就像是原始人去研究一辆二战坦克，怎么看都觉得科技逆天；但等到了我们会造坦克了，改进的路子马上就会想到的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;七、关于国内人工智能的发展&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：国内的研究水准，还有国内整个产业环境都不如美国，那您觉得中国有很大机会可以是人工智能存在的地方吗？还是我们只能做一个舶来品拿来应用，您是怎么看待的？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个问题太大了，说实在的，我也不是太了解国内的很多情况。所以我也不好说，首先第一肯定咱们中国人是非常聪明的。我觉得大家如果有信心，有恒心的话，确实能够做到很好的水平。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那像您在清华，还有在交大，您的同行在沟通的时候。您觉得他跟在美国这个领域同行沟通的时候，还是有明显的差距吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：可能还有相当的差距。这次围棋大热，国内有一些像刘知青教授他们在做。但是除此之外，也没有太多的人在做这个东西。另外围棋本身有深厚的人文背景，两个因素综合起来，问一些比较宽泛的问题，也很正常。所以我想这次回来一方面是探亲，另一方面做一些报告给大家科普一下。我觉得我有资格去说这个东西，因为我正在做。当然很多不一定说得对，只是和大家探讨下。总得来说我觉得国内做得还挺好的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您在深度学习这个领域里面，跟国内的学者沟通的时候，会觉得有什么差异吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：国外可能更细一点。国外交流的时候，大家都对问题有了解，会谈到很细的内容，会说「这个东西我没有理解，我不知道，我回去查一下资料。我得做了实验才告诉你答案」。但是国内问的问题就比较大一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：因为可能还没有那么深入地了解这个领域，是吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：可能是吧，特别是围棋这一块。当然也有可能国内大家都愿意问比较大而宽泛的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那从论文的发表，现在的数目和质量来说，您觉得美国有多大的差距呢？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个还没有仔细的研究。原创性的，有大跳跃的文章相比还是国外多一点，但是国内跟进很快。大概是这样。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：你有时候会有国内的某个研究机构出来的文章，让你觉得写得非常漂亮这种感觉吗？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：不错的很多啊，何恺明的 ResNet 大家都在用，做得好管你国内国外，大家都会用的。如果钻研某个方向，国内做到和国外差不多水平甚至更高，非常正常。国内这种工作的强度，国外是不可想像的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那国内如果有深度学习方面，有浓厚这个热情和兴趣的学生，他想读一个本科。你推荐他哪些院校呢？首先您的母校交大，对吧？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：如果出国留学，你推荐哪几个学校？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得 CMU 其实是很好的一个学校，我觉得卡耐基梅隆大学的一贯风格是做事做得很细，然后大家都很认真，愿意把一些事情做好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;八、田渊栋的学习方法论&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：之前您写过一些科研的总结，还有博士的过程。我们发现那些文章的归纳能力特别强。有主线，有要点，非常注重系统性和方法论，这个东西是怎么养成的，或者对于其他的研究人员或者技术人员怎么帮他们更好地做到这一点。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个其实我自己的经历比较特殊嘛，我之前说过，我自己写过小说的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：和这个有关系？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有关系的。我写过长篇小说，写长了之后，会有一些问题，比如说角色的把握和剧情的走向。你写下来发现这个角色和之前相比，性格走样了，说的话做的事不像他/她应该做的了。这时候再写下去就越来越糟糕，这时候就要多想想，有些段落虽然写得精彩，但于全局无益的话就得要忍痛割爱。然后反复读，再找到正确的路子写下去。像这样写多了的话会有感觉，会避坑，然后会有一些自己在方法上的总结。写博文也是一样的，一开始一泻千里，东一点西一点，然后收束了，归类了，有些大段大段的直接删掉，迭代几次之后，发表出来的才让人读着舒服。所以这个对我来说是比较特殊的经历。总的来说，我走过很多弯路，走弯路走多了，你才知道什么地方是对的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：靠经验积累。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：靠经验积累，如果大家想读博士的话，那还是要通过自己的经验积累，别人说的话再多，都没有自己的教训深刻。不要怕犯错。几个比较简单的经验，动作要快，不要怕犯错，多试几个方向。然后从错误中慢慢总结，知道更多的东西。我觉得现在最重要的是一个人要很聪明，要很会学习，然后愿意去尝试，不要怕犯错，就是这样子。从统计学的角度来说，经验越多，你获得的数据就越多，那你的模型的迭代速度就越快，效果就越好。所以其实就是这样一回事情。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：我看您对写作的理解就别具一格，好像写作对你来说不是简单的对学习过程的记录，甚至是您的一种思考方式了，对吧？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对，思考方式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：您的文章里提到过，有时候可能看起来很平庸的东西，通过写作，可以产生非常好的效果。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：写下来之后呢，你会有不一样的感觉。你脑子里面的东西，可能没有那么系统，甚至有自相矛盾的地方。当时没觉得什么，你写下来之后，才发现这个写下来不对，是不是要推倒重来呀，这个地方有问题呀？这就是迭代的过程。人的成长有时候得要抛弃成见，抛弃自己曾经认为十分正确的东西，再作总结，要有这个包容的意识，要知道自己可能全错。写作呢，就是提供了这样一种渠道。写作扩大了记忆力，你可以拿来思考的记忆就那么一点。你觉得你想到了所有的地方，思路很完美；但事实上是你拿了这个，把那个丢了，拿了那个，把这个丢了。只有全部写下来之后，才会发现有问题。才会去思考。我写博文的时候，第一遍不会直接发到网上的，会反复读几遍，看一看有什么问题。我自己觉得满意了，才会发。很多时候，我会觉得这个地方不通。这个是这个意思，下一段是别的意思，这两段没有连起来。你就会觉得语句有问题，语句有问题，你会自己去调整。在写作上会有这样一个洁癖嘛，你觉得这个文章写得不好，你不愿意发出来。然后这样的话，你可能对你的研究过程有思考，你把这个写下来，会发现这里做得不好，会有这个感觉，会反省，下次会想着要改进。那时间长了以后，自然会有一个比较系统性的方法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那您发的那么多的论文，背后是不是有特别大量的学习笔记？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有很多，其实我之前有写日记。反正不时就会写一点东西嘛。但说实在的，大部分论文都没有学习笔记，那样太花时间了，很多文章看两眼就过去了。毕竟文章太多，把时间花在刀刃上才是最重要的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：这些东西虽然不是特别的系统，或者有一些东西可能还有一些缺陷，您会跟人分享吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个还是不会分享，所以你看到的是冰山一角。能给大家分享的，都是写得比较好的，我比较满意的。你看到我写得特别系统，可能是个幻觉，因为还有大量不系统的堆着，要整理出来太费力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：对一些想进入学习机器学领域的年轻人，有没有什么建议？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：动作快，然后多学习，多交流，多尝试。不要怕犯错，计算机这一块犯错没有什么问题嘛，犯错就出 bug 嘛，计算机也不会爆炸。出了 bug 也没有关系，就反复调试，对吧。我觉得我们 CS（编者注：计算机科学）这个领域其实非常好，实验重复性很高。犯错了，也没有任何问题，整个周期非常短。所以我觉得特别适合年轻人学习，我觉得只要你有能动性，你只要抓住机会，多跟别人交流的话，我相信大家都能做得挺好的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：最后一个问题，推荐几本您觉得特别好的，技术性强的，或者是科普性强的书给我们的读者。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：说实在的，现在看书没有什么大用了。很多时候就是看论文，多了解一下，多跟别人交流，因为现在变化非常大。很多东西都不一样了，所以你看这些书能够知道以前的一些知识。其实你看论文也有同样的目标，比如说看论文第一段，这段里面其实就概括了以前的一些工作。然后你看多了，你自然而然就会对这个领域会有了解。看书当然也会看，比如说你特别想提高一下自己的数学能力的话，就要看一些经典的教材。最近我无聊去看群论，在看为什么一元五次方程得不到根式解。无聊嘛，你可以看看一些有趣的东西，并且深入思考。通过看和思考，你相当于磨炼自己的分析能力，长期不看的话感觉会变钝的，就可能人云亦云了，别人说好，你也觉得好，你作为研究员的价值就没有了。数学这些东西，经典的方法都是十年、百年的积累，不会过时的。所以好多都可以看。但是你要去追人工智能比较好的一些发展的话，其实看论文比较快。多看几遍论文的话，也基本上能够掌握这些方向的一些进展。然后多跟别人交流，我个人建议就是多交朋友。交流是很重要的，别人一句话就顶你看很多书了。你现在不可能看完所有的文章的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：那导论性的教材需要看吗？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：我觉得像算法这些的，还是可以看一些。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机器之心：就是看一些比较经典的教材，像贝尔萨写的书也是要看的，是吧？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：要看，但也取决于你有没有兴趣。你可能没有时间把所有推导都推一遍，这不可能的。但你你可以把整个方法和想法看一遍，把逻辑梳理出来。看论文的时候，往往跳跃和选择性的看。因为每篇文章的目标是把这个文章卖出去，他会说自己的方法特别好，别人的方法特别差。但是其实不是这样的，对吧。文章往往是有偏向性的，所以要选择性的看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：有没有哪些书是你觉得比较值得看的？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：这个没有定规啊，每个人的需求都不一样。另外，你可能当时看一下，到用的时候你再去翻，这样可能会好一点，看一本书会花很多时间。没有一个准则，到最后可能就是东看一点，西看一点，关键是把你的知识体系建立起来。比如说这块你觉得不懂，你就看这块不懂的文章。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;机器之心：就是有针对性的。&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;田渊栋&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：有针对性的去学，可能会比较好。比如说这次做围棋，我之前也没有做过游戏。那怎么办呢？你就看，看 David Silver 的博士论文，看以前增强学习的文献。你如果要做游戏的话，你看他的博士论文就得要看得比较仔细了。有一些关键的点，一定要搞清楚。相当于你要有选择性的看某些章节，某些文字，某些公式。你如果觉得这个重要的话，你就花时间搞定。如果不重要的话，你可以略过地看。这个就看你的积累了，你的积累肯定会告诉你什么重要，什么不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以最少代价去获得到你的知识体系，没有一定的准则。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;搜索能力是很重要的，想看什么就去找。反正现在网上有的是资源，现在已经不是图书馆的时代了，对吧。基本上一搜都搜到，关键怎么样搜。然后你愿意去搜什么样的东西。我觉得搜索是现代人的一个必备技能，不是说去图书馆一本一本啃下来就可以成为专家了，不是这样子的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，科普只是领进门的。我以前是比较喜欢化学，后来喜欢物理。再后面慢慢到数学去，再到做计算机去。所以说这样一条轨迹，基本上化学的专业文献，只要是浅显的我都能看懂。你有基础之后，你再去看科普文，你可以猜出来科普文和专业文献之间，是怎么样的对应关系，为了让外行人看懂，作出了什么样的省略。但是如果一个没有经验的人，只看科普的话是入不了门的，容易被各种名词误导。你需要花时间在专业文献上，让知识构成体系。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 17 Oct 2016 11:57:00 +0800</pubDate>
    </item>
    <item>
      <title>资源 | 数十种TensorFlow实现案例汇集：代码+笔记</title>
      <link>http://www.iwgc.cn/link/3111769</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自 Github&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;这是使用 TensorFlow 实现流行的机器学习算法的教程汇集。本汇集的目标是让读者可以轻松通过案例深入 TensorFlow。&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些案例适合那些想要清晰简明的 TensorFlow 实现案例的初学者。本教程还包含了笔记和带有注解的代码。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;项目地址：https://github.com/aymericdamien/TensorFlow-Examples&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;教程索引&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;0 - 先决条件&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;机器学习入门：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/ml_introduction.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;MNIST 数据集入门&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1 - 入门&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hello World：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/helloworld.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/helloworld.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基本操作：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/basic_operations.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/basic_operations.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2 - 基本模型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近邻：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/nearest_neighbor.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/nearest_neighbor.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;线性回归：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Logistic 回归：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3 - 神经网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多层感知器：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;卷积神经网络：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环神经网络（LSTM）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;双向循环神经网络（LSTM）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;动态循环神经网络（LSTM）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自编码器&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/autoencoder.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;4 - 实用技术&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;保存和恢复模型&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/save_restore_model.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/save_restore_model.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;图和损失可视化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/tensorboard_basic.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/tensorboard_basic.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Tensorboard——高级可视化&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/tensorboard_advanced.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;5 - 多 GPU&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多 GPU 上的基本操作&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_MultiGPU/multigpu_basics.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_MultiGPU/multigpu_basics.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据集&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一些案例需要 MNIST 数据集进行训练和测试。不要担心，运行这些案例时，该数据集会被自动下载下来（使用 input_data.py）。MNIST 是一个手写数字的数据库，查看这个笔记了解关于该数据集的描述：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;官方网站：http://yann.lecun.com/exdb/mnist/&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更多案例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来的示例来自 TFLearn（https://github.com/tflearn/tflearn），这是一个为 TensorFlow 提供了简化的接口的库。你可以看看，这里有很多示例和预构建的运算和层。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;示例：https://github.com/tflearn/tflearn/tree/master/examples&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;预构建的运算和层：http://tflearn.org/doc_index/#api&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;教程&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;TFLearn 快速入门。通过一个具体的机器学习任务学习 TFLearn 基础。开发和训练一个深度神经网络分类器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;笔记：https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;基础&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;线性回归，使用 TFLearn 实现线性回归&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/basics/linear_regression.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;逻辑运算符。使用 TFLearn 实现逻辑运算符：https://github.com/tflearn/tflearn/blob/master/examples/basics/logical.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;权重保持。保存和还原一个模型：https://github.com/tflearn/tflearn/blob/master/examples/basics/weights_persistence.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;微调。在一个新任务上微调一个预训练的模型：https://github.com/tflearn/tflearn/blob/master/examples/basics/finetuning.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 HDF5。使用 HDF5 处理大型数据集：https://github.com/tflearn/tflearn/blob/master/examples/basics/use_hdf5.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用 DASK。使用 DASK 处理大型数据集：https://github.com/tflearn/tflearn/blob/master/examples/basics/use_dask.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;计算机视觉&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;多层感知器。一种用于 MNIST 分类任务的多层感知实现：https://github.com/tflearn/tflearn/blob/master/examples/images/dnn.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积网络（MNIST）。用于分类 MNIST 数据集的一种卷积神经网络实现：https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;卷积网络（CIFAR-10）。用于分类 CIFAR-10 数据集的一种卷积神经网络实现：https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;网络中的网络。用于分类 CIFAR-10 数据集的 Network in Network 实现：https://github.com/tflearn/tflearn/blob/master/examples/images/network_in_network.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Alexnet。将 Alexnet 应用于 Oxford Flowers 17 分类任务：https://github.com/tflearn/tflearn/blob/master/examples/images/alexnet.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;VGGNet。将 VGGNet 应用于 Oxford Flowers 17 分类任务：https://github.com/tflearn/tflearn/blob/master/examples/images/vgg_network.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;VGGNet Finetuning (Fast Training)。使用一个预训练的 VGG 网络并将其约束到你自己的数据上，以便实现快速训练：https://github.com/tflearn/tflearn/blob/master/examples/images/vgg_network_finetuning.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;RNN Pixels。使用 RNN（在像素的序列上）分类图像：https://github.com/tflearn/tflearn/blob/master/examples/images/rnn_pixels.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Highway Network。用于分类 MNIST 数据集的 Highway Network 实现：https://github.com/tflearn/tflearn/blob/master/examples/images/highway_dnn.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Highway Convolutional Network。用于分类 MNIST 数据集的 Highway Convolutional Network 实现：https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_highway_mnist.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Residual Network (MNIST) (https://github.com/tflearn/tflearn/blob/master/examples/images/residual_network_mnist.py).。应用于 MNIST 分类任务的一种瓶颈残差网络（bottleneck residual network）：https://github.com/tflearn/tflearn/blob/master/examples/images/residual_network_mnist.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Residual Network (CIFAR-10)。应用于 CIFAR-10 分类任务的一种残差网络：https://github.com/tflearn/tflearn/blob/master/examples/images/residual_network_cifar10.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Google Inception（v3）。应用于 Oxford Flowers 17 分类任务的谷歌 Inception v3 网络：https://github.com/tflearn/tflearn/blob/master/examples/images/googlenet.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;自编码器。用于 MNIST 手写数字的自编码器：https://github.com/tflearn/tflearn/blob/master/examples/images/autoencoder.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;自然语言处理&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;循环神经网络（LSTM），应用 LSTM 到 IMDB 情感数据集分类任务&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;双向 RNN（LSTM），将一个双向 LSTM 应用到 IMDB 情感数据集分类任务&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/nlp/bidirectional_lstm.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;动态 RNN（LSTM），利用动态 LSTM 从 IMDB 数据集分类可变长度文本&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/nlp/dynamic_lstm.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;城市名称生成，使用 LSTM 网络生成新的美国城市名：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm_generator_cityname.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;莎士比亚手稿生成，使用 LSTM 网络生成新的莎士比亚手稿：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm_generator_shakespeare.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Seq2seq，seq2seq 循环网络的教学示例&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/nlp/seq2seq_example.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;CNN Seq，应用一个 1-D 卷积网络从 IMDB 情感数据集中分类词序列：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/nlp/cnn_sentence_classification.py&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Atari Pacman 1-step Q-Learning，使用 1-step Q-learning 教一台机器玩 Atari 游戏：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/reinforcement_learning/atari_1step_qlearning.py&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;其他&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Recommender-Wide&amp;amp;Deep Network，推荐系统中 wide &amp;amp; deep 网络的教学示例：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/others/recommender_wide_and_deep.py&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Notebooks&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Spiral Classification Problem，对斯坦福 CS231n spiral 分类难题的 TFLearn 实现&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/notebooks/spiral.ipynb&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;可延展的 TensorFlow&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;层，与 TensorFlow 一起使用 &amp;nbsp;TFLearn 层：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/layers.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;训练器，使用 TFLearn 训练器类训练任何 TensorFlow 图：&lt;/span&gt;&lt;span&gt;https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/layers.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Bulit-in Ops，连同 TensorFlow 使用 TFLearn built-in 操作&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/builtin_ops.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Summaries，连同 TensorFlow 使用 TFLearn summarizers&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/summaries.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Variables，连同 TensorFlow 使用 TFLearn Variables&lt;/span&gt;&lt;span&gt;：https://github.com/tflearn/tflearn/blob/master/examples/extending_tensorflow/variables.py&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 17 Oct 2016 11:57:00 +0800</pubDate>
    </item>
    <item>
      <title>业界 | King+Woman-Man=Queen:用基于Spark的机器学习来捕捉词意</title>
      <link>http://www.iwgc.cn/link/3111770</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ibmbigdatahub&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Word2Vec 是一种将词转换成向量的方法。其中文本是非结构化的数据，无论过去还是现在，其在数学中的研究远远少于向量。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;历史上的数学与文本数据&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;物理学家和数学家艾萨克牛顿是第一个在力学情境中研究向量的人呢。向量的概念已经存在了三个世纪，其科学性已非常成熟。而文本数据的数学探索这个概念只有几十年的历史。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现在文本数据的数学思考应用尤其重要。数据的价值已被理解但是还未兑现。大部分商业相关信息最初都是非结构化形式，主要是文本。数据只有读取之后才可见，才能用于商业、教育、政府管理和医疗之中。文本数据的数学探索能够产生洞见提供给医生、企业家、营销人员和教师用来做决策。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我目前正在做数据可读的工作，我使用了 Word2Vec 来生成向量捕捉词意并启用与单词相关联的算术运算。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如：向量（king）+向量（woman）-向量（man）=一个接近向量（queen）的向量&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibSRMYoPOcvDfyajbARlTgWvIAkwq4icDDoMxhGAJtPaI5NicFG1g6mMx1WhicibwtFibStGLp8tKc80dw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;词计算&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;谷歌的 Thomas Mikolos 等人在 2013 提出了 Word2Vec。这个算法是基于网络的，并将一个文本预料库映射到一个矩阵中，在这个矩阵中，每一行都关联到输入文本数据的中一个词上，例如，tweets、产品评价、播放列表等。这些合成向量空间能被用于多种用途，比如测量两个词之间的距离。这样一来，给定一个相关的词，前述的向量空间就能用来计算最接近的前几个词。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;例如，我用了 30 天的 Twitter 数据建立了一个模型，生成了最接近 #deeplearning 的 5 个 #（词）&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;#machinelearning&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;#ml&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;#smartdata&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;#predictiveanalytics&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;#datascience&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本文中用到的 Word2Vec 部署来源于 Apache Spark ML，一个机器学习包，属于 Apache Spark 的一部分，如果你有兴趣自己建立一个 Word2Vec 模型，可以借鉴一下这里 https://apsportal.ibm.com/analytics/notebooks/43120a9f-afa7-4715-9441-0388cb4f2d49/view?access_token=573d250f944bc0d912a8bb4dac997090dac2074fea07cbb2380bc802683aa60c。&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 17 Oct 2016 11:57:00 +0800</pubDate>
    </item>
    <item>
      <title>学界 | Yoshua Bengio论文：使用线性分类器探头理解中间层</title>
      <link>http://www.iwgc.cn/link/3111771</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自arXiv&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;日前，Yoshua Bengio 对其论文 Understanding intermediate layers using linear classifier probes 进行了修改，这是最新版本的，点击阅读原文下载。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：使用线性分类器探头理解中间层（Understanding intermediate layers using linear classifier probes）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibSRMYoPOcvDfyajbARlTgWxA0WFIo2ShXia9ur0ynf1pTJLYDEn6K6etbbDwd3ezTt0R08BCrtUHQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：神经网络模型在黑箱方面的问题可谓是尽人皆知。我们提出了一种用于更好地理解中间层的作用和动态的新方法。这对这样的模型的设计有直接的后果，而且它让专家可以证明特定的启发式方法（比如 Inception 模型中的辅助头（auxiliary heads））。我们的方法使用了被称为「探头（probe）」的线性分类器，其中一个探头只能使用一个给定的中间层的隐藏单元作为判别特征。此外，这些探头不能影响模型的训练阶段，而且它们一般是在训练之后才被加进去的。它们允许用户可视化模型在多个训练步骤的状态。我们演示了这种方法可以被如何被用于开发关于已知模型的更好的直觉知识（intuition）和诊断潜在的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 17 Oct 2016 11:57:00 +0800</pubDate>
    </item>
    <item>
      <title>深度 | 机器之心对话谷歌高级研究科学家Greg S Corrado：人工智能并不会让大公司形成垄断</title>
      <link>http://www.iwgc.cn/link/3091069</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;机器之心&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;作者：老红&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;10 月 14 日，来自谷歌的高级研究科学家 Greg S Corrado 在北京和部分中国媒体进行了短暂的交流，机器之心也受邀对 Greg 进行了采访。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvl8ULpHlibbR4GSoh27oSkGRx8cT4NfJvG4PHSYU0axdNDX6mQsMBqCg/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;早在今年 5 月的时候，&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect" target="_blank" data_ue_src="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650715740&amp;amp;idx=2&amp;amp;sn=51dd0ce59b25385a0d3c0d0d09aaba8c&amp;amp;scene=21#wechat_redirect"&gt;机器之心就曾作为唯一一家国内媒体出席了于瑞士洛桑举行的神经科学峰会 Brain Forum&lt;/a&gt;。我们也在现场的报道中详述了 Greg 关于深度学习和深度神经网络的演讲。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在实际的研究领域，Greg 的研究方向包括了生物神经科学、人工智能和可扩展的机器学习，并在行为经济学、系统神经科学和深度学习等领域发表了诸多的论文。在此前很长一段时间，Greg 在谷歌都从事着大脑计算领域的研究；近期，他也成为了谷歌大规模深度神经网络项目的联合技术主管。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于人工智能、机器学习以及无监督学习的现在和未来，Greg 和我们分享了许多有趣的观点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;以下为采访摘录：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：你曾在瑞士的 Brain Forum 上提到过，机器学习并不是什么黑魔法，而是一种工具。你觉得现在机器学习发展的最大瓶颈是什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这项技术其实在不断变化，机器学习需要数据样本、资源和工具，还有计算机运算能力等多方面的支持。回顾机器学习发展的历史会发现，由于计算机运算速度缓慢、成本过高等技术原因滞后，影响了程序运行的效率，无法满足需求，于是机器学习的发展进程也比较缓慢，也没有实际的产品和服务被推出来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;直到最近几年计算机运算能力有了大幅提升，速度提升、成本下降并且应用越来越广，这改变了整个局面。所以如今机器学习的瓶颈变成了与人相关的因素，在于人的创造力与创新能力，在于在擅长并懂得如何运用这项技术的人才。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以我们的重心也发生了变化，在其余所有因素和条件，诸如充足的数据、免费的工具、资源、足够强大的计算机运算能力等等都满足的情况下，我们需要教会和培养更多的人如何运用机器学习这个技术来将实现他们创新的构思。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：你觉得有哪些可以与深度学习相竞争的机器学习方法？另外，在你眼中，分布式计算对于机器学习来说具体有些什么益处？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;目前，Deep Learning 是关于机器学习非常热门的一项技术。市面上很多新产品和服务也都在使用 Deep Learning，但是这只是一项现有的、能满足当下需求的技术。但我认为更重要的是更多的研究人员和科学家能在更广的维度和视野下继续深入研究，这样才能发现与时俱进，发现更新的技术来满足这一领域下一阶段的更多需求。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于分布式计算的问题。因为机器学习本身是一个特别缓慢的过程，所以需要调用和运行大量的计算资源。分布式计算的重要性在于能够为我们实现足够快运行速度，来满足我们在创新实验方面的需求，测试我们的新点子，用结果告诉我们哪些想法可行，哪些不行。所以分步计算在我看来就是一个促进机器学习的助力，让运算能力更快成本更低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：去年你们的团队推出了 Smart Reply 功能，请问目前这项功能运转得怎样？在哪些情况下邮件可以代替人工进行自行处理？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;目前我们在智能回复方面发表了很多学术论文和研究报告。它的运作方式是根据收取的邮件，提取相关信息识别其中的逻辑、语境，来组织语言进行回复。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;目前能够实现的功能仅限于一些不那么正式的简短的即时回复，比如「到时候见！」「抱歉我可能办不到」，或者「我会尽快回复你」和「稍后联系」这样的简单地短句，这是目前机器可以比较有效地处理的范围，暂时还不能处理一些较为复杂和带有意图性的答复。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：我们知道目前人工智能和机器学习还仅限于处理一些较为局限、具体的专门领域，你认为什么时候会出现更为强大的通用型人工智能？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这是一个有趣的问题，我认为未来的发展趋势，还是专业的领域用专用的技术和模型解决特定的问题和任务，这样的应用对于一个系统和技术更为高效且更有实际意义。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于出现那种通用的技术我不是特别有信心，即便是有这样的技术，我认为也不会比专用针对性地解决特定问题的解决方案更快更有效，而只会更慢效率更低。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Question：你如何评价量子计算？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这是一个目前还停留在研究阶段的技术，并没有应用的实际，如果有也可能也是非常遥远的未来才会实现。我认目前它还只是一个惊艳的物理学科研课题，即便是应用到工程设备上都还需要很长时间。如果有人能在有生之年研发并制造出量子计算机，这将大大提高计算效率，但是我现在只能说，祝在这一领域的研究者们好运。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question： 能否谈论下你对 Google 开源 TensorFlow 的理解？在未来人工智能和机器学习的发展上，Google 有哪些计划？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;对于人工智能，我想强调的是它不是一个具体的可以包装销售的产品。它实际上是一个工具，软件工程师以及其他有创造力的人们可以使用这个工具来制造和开发新的产品和服务。而 TensorFlow 把这些 Google 正在使用的基本的工具开放给公众使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;未来这个领域相关的产品，Google 打算把自己开发的平台也通过云服务共享给公众使用，通过这种云机器学习，其他开发者可以开发和实现自己的机器学习构想，就像我们在 Google 中研发一样。他们可以通过 TensorFlow 使用我们提供的免费软件和工具，也可以用云服务运行他们自己构建的机器学习系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们也会通过 API 向开发者提供一些预置好的机器学习的子系统，这样开发者只需要再添加几行简单的代码就可以实现比如翻译、图片识别等技术。这样开发者并不需要成为机器学习的专家，就能开发自己的机器学习应用的产品。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：Google 是否有一些准则来确保人工智能技术会朝着你提到的这个方向发展？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这就是为什么 Google 会主导去建立了一个名为 Partnership on AI to Benefit People &amp;amp; Society 的组织的原因，Partnership on AI 是一个独立的非盈利组织，还有很多公司都参与其中促成一个关于人工智能技术如何与人类、社会、经济等互动的话题开放式的讨论平台，促进人们对人工智能技术的理解，讨论甚至是公开辩论。更多地把关于这些话题面临的挑战放到桌面上来公开讨论，要好过于由各家公司私底下研究。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：如今很多公司在推出自己的产品和服务的时候都会标榜人工智能，但是市场营销中提到的人工智能和深度学习是否真实可信还需要甄别和考量，如何辨别人工智能真伪?&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这么看来目前市面上确实存在一些公司把人工智能和机器学习用于品牌营销的策略，但是最终消费者应该在意和关注的并不在于技术是如何研发的，而在于这些技术是否真正在发挥作用。如果通过使用某些产品你确实感觉到它的智能，觉得它有用，何必在意技术是如何实现的。所以我的建议就是消费者还是要从产品自身的功能这些切实能考量的标准来识别，而不要轻易被市场营销左右，因为它根本不重要。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question：目前存在一些对于大公司关于隐私和垄断方面的质疑和顾虑；同时，很多小规模的公司也认为自己在竞争中处于弱势地位，无法和大公司竞争，因为大公司拥有太多用户信息。你怎么看这个问题?&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;首先谈一下用户隐私的问题，隐私对于消费者乃至每个人都很重要，所以对于公司来说，很重要的一点就是必须非常清晰明确地和用户沟通公司的隐私政策和标准是怎样的，用户则可以根据这些信息来决定是否认可并继续使用这些产品和服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我刚才讲到的 Partnership on AI，这个组织一个重要的功能，就是帮助制定一些原则和标准，来规范各个公司的具体操作。但最终决定权还是交回用户和消费者自己的手中。同时一家公司如果想要用户持续地使用自家的平台，他们就必须想办法赢得并维护用户的信任。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;接下来讲一下小公司如何成长壮大的过程，一家从事某些小研究专注小范围的产品的公司逐渐壮大起来是一件很寻常的事情，就像当年 IBM 并没有想方设法阻止微软的成长，微软也没有阻止 Google 的成长，Google 也没有阻止 Facebook 做大做强，这种情况会永远持续地发生下去。从好地方面看，目前我们的行业总是能以某种方式保持更迭并不断前行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，数据量是否足够这个问题的重要性目前其实正在削弱，数据是否足够取决于你想要达到的目标。比如物体识别功能对于当年的还在上学的我来说就是科幻小说，但当今任何一个计算机科学专业的研究生都能做到，收集到相关数据来运行某个程序也是很容易的事。同理，现在任何一个科学技术类的竞赛，所有参赛者能获取的数据都是一样的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;同样的数据量在 1997 年或许不够，但在 2010 年之后，数据量的差异已经不能起到多重要的作用了，更多的数据也不见得能有多大的益处。语音识别也是同样的道理，不需要特别多的数据就能实现某个新的尝试。及时是众所周知奇迹般的成就 AlphaGo，它其实也是使用的公开的围棋比赛资料和数据实现的机器学习。所以机器学习的秘诀并不在于录入了秘密的数据，而在于人的创意和计算资源本身的创造性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;Question:与多阶学习相比，使用基于多图表征的异构数据的联合学习的好处是什么？&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;目前这一领域的研究还尚有缺口，我们希望看到更多关于机器多任务学习的研究出现，还有比如 multi-renpresentation learning（多表征学习），trasfer learning（迁移学习）其实也是机器学习研究人员研究了很长一段时间的课题。但直到最近，我们才看到一些实际的应用成果出现，这些研究领域确实让人感到很兴奋。每年我能看到一些新的观点涌现出来，今年我也特别关注和期待这一领域的新进展（比如 ICML、ICLR 这些学术年会）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;em&gt;&lt;span&gt;Question： 你认为 Google 在人工智能领域最大的竞争对手是谁？&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：对于这个行业的所有公司来说，好消息每个公司都人才济济，同时全球很多大学以及公司都有自己的人工智能实验室在不断尝试创新和研发，所以整个行业都从中受益。如果这个行业只有一家公司独大，那么这个领域的发展将会更慢更低效。因此人工智能行业竞争越大越开放是一件好事，并且我们希望这个大环境能继续这么开放且持续地充满竞争。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Question：你分别对监督学习、无监督学习和半监督学习各有什么看法？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;人工监督下的机器学习已经很好的投入实际应用，无人工监督的机器学期据我所知还没有投入实际应用需要更多的科研努力，半人工刚好介于两者之间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Question： 你觉得目前的深度学习有哪些急需突破的点？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个机器学习不应当是一种我们所说的黑魔法。最重要的是需要让人们更容易地探索，不同的配置调配（tuning）和变量（因需而异作出差异化的模型调整），他们不需要去猜想这是什么黑魔法以及背后的工作原理，这将会是接下来 Deeplearning 的发展方向，无论是理论研究还是工程应用升级方面，更好地探索学术理论研究中的猜想指标构思和建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;Question：你觉得目前 multi-graph presentation（多图表达）面临的最大挑战是什么？Google 又是如何解决的？&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Greg：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;关于多图表达，我们现有的技术手段还停留在创新研发阶段，还有很多空白领域有待研究。可以说是刚刚起步吧，所以我们也很鼓励更多的研究人员能在这个领域积极探索。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心原创，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 16 Oct 2016 12:29:26 +0800</pubDate>
    </item>
    <item>
      <title>重磅 | DARPA首例人机交互实验成功，用脑机接口帮助瘫痪病人恢复触觉</title>
      <link>http://www.iwgc.cn/link/3091071</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自DARPA&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：Rick、杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;之前 DARPA 展示了通过脑机接口用神经控制机械臂，如今又有了新的突破，它允许个人通过一个连接到机器人手臂的神经接口系统来直接在大脑中体验触觉。DARPA 的史无前例的这项演示打开了人机交互的新可能。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;iframe class="video_iframe" data-vidtype="1" style="   z-index:1; " height="375" width="500" frameborder="0" data-src="https://v.qq.com/iframe/preview.html?vid=y0337zkjeyj&amp;amp;width=500&amp;amp;height=375&amp;amp;auto=0" allowfullscreen=""&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 资助的一个研究团队在人类历史上首次演示了一项技术，它允许个人通过一个连接到机器人手臂的神经接口系统来直接在大脑中体验触觉。通过实现大脑和机器之间的双向交流——运动的输出信号和知觉的输入信号——该技术最终可能使人与人、人与世界之间以一种全新的方式建立关系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项工作是由 DARPA 的变革假肢修复（Revolutionizing Prosthetics）计划所支持，并由匹兹堡大学和匹兹堡大学医学中心执行。该研究结果在《科学转化医学（Science Translational Medicine）》期刊于今天在线出版的一份研究中有详细描述，该技术在匹兹堡的一个白宫创新活动中向奥巴马总统展示。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvXfZlQIbANWNuDWicyphrUgibzgUvaJjqibV2bwhKjRrS1tKGiaZmXxhPHQ/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「DARPA 先前已经展示了大脑对机器人手臂的直接神经控制，而现在我们已经完成了回路，从机械手臂向大脑发送信息，」DARPA 生物技术办公室主任兼变革假肢修复计划的项目经理 Justin Sanchez 说。「这个新技术从根本上改变了人机关系。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该研究的一位志愿者 Nathan Copeland 在 2004 年的一起车祸中摔断了脖子并弄伤了脊髓，导致他的身体从胸部以下都瘫痪了。距离事故发生将近 10 年后，Nathan 同意参加临床试验并接受了手术，让四个微电极阵列——每个大约衬衫纽扣的一半大小——放在他的大脑中，两个在运动皮层，两个在对应于手指和手掌感觉区域的感觉皮层。研究人员将这些微电极阵列引出的电线布到由约翰霍普金斯大学的应用物理实验室（Applied Physics Laboratory/APL)）开发的一支机器人手臂上。这支 APL 手臂包含了先进的扭矩传感器，可以检测到施加在他的任何一根手指上的压力，还可以将那些身体的「感觉」转换成电信号，由电线传输回 Nathan 大脑的微电极阵列中，来向他的感觉神经元提供刺激的精确模式。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在最初的一组测试中，研究人员轻触了机器人的每根手指，在 Nathan 被蒙着眼睛的情况下指出被触碰手指的准确度将近 100%。他说这种感觉就像是自己的手被碰了似的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「有一次，该团队决定按两根手指而不是一根，他们没有告诉他」，Sanchez 说。「他打趣地询问是否有人想捉弄他。在那一刻我们知道他通过机器人手所感受到的感觉近乎自然。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这些最新研究结果建立在直接连接大脑与机器人手臂的一系列 DARPA 成就之上。与志愿者 Tim Hemmes 和 Jan Scheuermann 进行的早期研究展示了使用一个脑机接口来实现 APL 手臂的运动控制。「基于 DARPA 这些早期测试所取得的成功，我们想『我们能否反过来做这个实验？能否在感觉系统中做我们在运动系统中做的实验？』」Sanchez 说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DARPA 在 2015 年的「等等，什么？一个未来的技术论坛（Wait, What? A Future Technology Forum）」活动中概述了其在触觉复原方面取得的成功，该活动汇集了思想领袖、专家科学家和工程师来产生新的想法并加速全新功能的开发进程。该研究的同行评审详细信息于今天发表的一篇杂志文章中被首次完整公布。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;该接口系统是今天在白宫前沿会议（White House Frontiers Conference）中所展示的 24 项技术突破之一，Nathan 和匹兹堡大学的研究带头人们在这次会以上谈论了这项技术——它对于脊髓损伤的人会意味着什么，以及它能够为社会打开什么新的可能性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为奥巴马总统的脑计划（Brain Initiative，全称「推进创新神经技术脑研究计划」）的一部分，DARPA 的变革假肢修复计划正在资助一些研究工作以改善刺激模式，并加入了压力之外的新类型的知觉，来将近乎自然的运动控制和直觉传递给假肢修复用户。终有一天，这些改善以及相关的神经技术会实现大脑的认知功能和机器计算过程的无缝合作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;变革假肢修复计划不是 DARPA 实现截肢者恢复触觉的唯一项目。该机构的手部本体感觉与触摸接口（Hand Proprioception and Touch Interfaces/HAPTIX）计划正在寻求一种替代方法，使用外周神经系统在大脑和假肢之间交流运动指令与感觉反馈。该项目计划在 2019 年之前发起家庭实验，一个由 FDA 认证的 完整 HAPTIX 假体系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 16 Oct 2016 12:29:26 +0800</pubDate>
    </item>
    <item>
      <title>开源| 微软开源GB框架LightGBM，表现超越已有boosting工具</title>
      <link>http://www.iwgc.cn/link/3091072</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Git Hub&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em style="color: rgb(136, 136, 136); line-height: 1.6;"&gt;&lt;span&gt;LightGBM（Light Gradient Boosting Machine）是一个基于决策树算法的快速的、分布式的、高性能 gradient boosting（GBDT、GBRT、GBM 或 MART）框架，可被用于排行、分类以及其他许多机器学习任务中。&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开源项目地址：https://github.com/Microsoft/LightGBM&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LightGBM 是一个梯度 boosting 框架，使用基于学习算法的决策树。它可以说是分布式的，高效的，它有以下优势：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更快的训练效率&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;低内存使用&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;更好的准确率&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;支持并行学习&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可处理大规模数据&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在公开数据上的实验表明 LightGBM 能在学习效率和准确率上都表现出比其他已有 boosting 工具更好的表现，而且有着更低的内存消耗。此外，实验也表明 LightGBM 通过使用多台机器进行特定设定的训练，它能取得线性加速。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;优化速度和内存使用&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多 boosting 工具使用 pre-sorted based 算法进行决策树学习。这是一种简单的解决方案，但不易于优化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LightGBM 使用 histogram based 算法，将连续特征（属性）值装进离散的容器中，从而加速训练流程并减少内存使用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稀疏优化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;只需要 O(#non_zero_data) 来构建进行稀疏特征的 histogram。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;准确率优化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;大部分决策树学习算法通过 level（depth）-wise 成长决策树，如下图所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvrkJwRic077QtRv5LenJjGfJ8dACRmDnlZm69TN8dX1qaRPS84P3Ficng/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过 leaf-wise 成长决策树，它将选择带有 max delta loss 的 leaf 进行成长。在长到同样的 #leaf 时，Leaf-wise 算法要比 level-wise 算法能减少更多的损失。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvfU8HNJHPchBy1YXTfj6A5ZZzvyFR6zERMibSO0KXz2oQHrnXLPIfwLQ/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;网络通信优化&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 LightGBM 并行学习中，只需要使用一些 collective 通信算法，像是「All reduce」、 「All gather」和「Reduce scatter」。这些 collective 通信算法要比点到点通信能提供更好的表现。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;并行学习中的优化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LightGBM 能提供以下并行学习算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;特征并行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统算法的特征并行意图在决策树种并行「Find Best Split」，有着以下两个缺点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;因为不能加速「split」，所以有着计算费用，其时间复杂性是 O(#data)，因此在 #data 很大时，特征并行不能加速。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;需要 split 结果的通信，这大约花费 O(#data/8) (one bit for one data)。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在数据很大时，因为特征并行不能加速，因此在 LightGBM 中进行了小的改变：不再是垂直的分割数据，每个 worker 持有全部数据。因此由于每个 worker 都知道如何 split 数据，LightGBM 就不需要通信数据的 split 结果。而且由于 #data 不会更大，所以在每个机器中持有全部数据也是合理的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而，在 #data 较大时，该特征并行算法仍遭受进行"split"的计算费用，所以当 #data 较大时使用数据并行更好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;数据并行&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统算法的数据并行意在并行全部决策学习，其缺点是高通信成本。而在 LightGBM 数据并行中减少了通信成本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LightGBM 还有其他的许多特征，想了解更多可查看该网址：https://github.com/Microsoft/LightGBM/wiki/Features。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 16 Oct 2016 12:29:26 +0800</pubDate>
    </item>
    <item>
      <title>业界 | IBM、谷歌、英伟达、AMD等八巨头建立服务器新标准OpenCAPI，加速机器学习硬件发展（附新标准）</title>
      <link>http://www.iwgc.cn/link/3091073</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自TCC&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;日前，AMD、Dell EMC、谷歌、惠普、IBM、MellanoxTechnology、英伟达、Xilinx 等一群科技行业巨头宣布了一项新的服务器标准，OpenCAPI。该新规范会加速机器学习、分析、大数据及其他新兴的工作负载（点击阅读原文，下载新的服务器标准）。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvbcuLVtyc9Cn6mhYpyAcgcMLSkTYTlqJklVHQcAs73Ye0ndav6Kkdlw/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然关于到底什么是「开放（open）」的争论有很多，技术界的每个人都会同意开放标准是产业增长和繁荣的关键驱动力之一。开放技术能让公司通过分享共有的规范，避免重复无用工作，来实现盈利增长。从物联网到智能手机，到互联网到数据中心标准带动技术产业方方面面的增长。日前，AMD、Dell EMC、谷歌、惠普、IBM、MellanoxTechnology、英伟达、Xilinx 等一群科技行业巨头宣布了一项新的服务器标准，OpenCAPI。这是一个真正的大项目。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicN1YVWXic1PPsnaQH2j7dNvmO8fHCromAYryK05mH3XpYyMGSdB3wb4chss2u9xpAyOmiaI8vR1MWw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;OpenCAPI 的全新标准能让像 FPGA，图形、网络和存储加速器那样的所有加速器都能获&lt;/span&gt;&lt;span&gt;得非常高的性能提升，执行一些数据中心服务器的通用 CPU 无法优化的功能。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;加速器需要满足人工智能&lt;/span&gt;&lt;span&gt;、机器学习、大数据、分析、安全以及高性能计算上的新计算需求。除非你生活地底下，否则你一定会了解这些都是当今最热门的驱动技术。加速器需要一个性能非常高、低延迟、高速缓存一致的总线连接，而 OpenCAPI 多年来的设计都是为了实现这一技术。OpenCAPI 具有非常快的数据传输速率，25Mbit/秒，相比之下，PCIe 的速率只有 16Mbit/秒。缓存一致意味着加速器访问相同的主内存，而在传统的计算中，缓存是为通用 CPU 保留的。PCIe 不是缓存一致性的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果你是技术界的人，一定对 OpenCAPI 中的「CAPI」不陌生， CAPI（Coherent Accelerator Processor Interface）是由 IBM 开发的一个加速器标准，今天的 OpenPower 都在使用它。市场上的很多公司加速器都支持 CAPI，例如 DRC, Alpha Data, BittWare, RedisLabs, Nallatech, Edico Genome 和 Semptian。IBM 及其合作伙伴为建立 CAPI 生态系统做了很多工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然 CAPI 有 IBM 管控并由 OpenPOWER Consortium 计量，但是 OpenCAPI 本身是完全开放的。OpenCAPI consortium 称他们计划今年内让 OpenCAPI 规范对公共完全免费开放。Mellanox Technologies, Micron, 和 Xilinx 都是 CAPI 的支持者。OpenPOWER 成员现在也是 OpenCAPI 的一部分。英伟达和谷歌是 OpenPOWER 的成员，现在也成了 OpenCAPI 的新成员。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;CAPI 的新成员&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以 CAPI 的新成员 AMD、Dell EMC、惠普（HPE）给 OpenCAPI 带来了什么呢？很多东西。AMD 带来了 Radeon 系列的 GPU 加速器，加上英伟达，你会拥有 GPU 市场上的所有产品。. Dell EMC 和 HPE 是最大的服务器和存储器供应商，再加上 IBM 的服务器技术高端 Power8 和 POWER9，它会变得更加强大。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那是不是意味着 Dell EMC 和 HPE 未来会使用基于 OpenPOWER 的处理器？理论是可以，但是实际不可能。这两家公司加入 OpenCAPI 的理由相同。作为系统供应商，对于 Dell EMC、HPE 和 IBM 以及他们的消费者来说，重要的是需要具有活力的好创新。它们会根据自身的需要来决定自己的未来。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;英特尔要怎么办？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;英特尔会怎么样，它为什么没有加入 OpenCAPI 呢？这个问题有点复杂。这家公司总会为加速器找到新的高速、低延迟、缓存一致总线。英特尔支持加速证明通过 Altera 和 Nervana 收购以他们已经在 Xeons 实现的芯片和封装加速。英特尔也有带有 QuickPath（QPI）和 UltraPath Interconnect (UPI) 的高速、低延迟、缓存一致总线。此外，英特尔今天在其服务器平台上支持的业界标准 PCIe 3，也是目前业内大多数加速器使用的总线。鉴于英特尔 95% 服务器单元市场占有率，未来它很有可能加入 OpenCAPI。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;致力于 OpenCAPI 产品&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;支持标准是一回事，而在标准发布之后就应用于产品是另一回事了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;ul class=" list-paddingleft-2" style="list-style-type: disc;"&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;AMD 称他们的 Radeon GPUs 将支持 OpenCAPI。这并不是在说他们会在他们的 x86-based 服务器平台支持该标准。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;IBM 说他们将在 2017 年的下半年使用 OpenCAPI 引进基于 POWER9 的服务器。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;谷歌和 Rackspace 的新 Zaius 服务器将使用 POWER9 处理器技术并计划使用 OpenCAPI。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Xilinx 说他们计划将 OpenCAPI 加入他们 FPGA 中。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;未来会怎么样？同很多其他产业标准团体相比，OpenCAPI 会最先出成果。它已经有了自己的规范 、委员会 、管理、分层，用不了一年产品就能问世。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sun, 16 Oct 2016 12:29:26 +0800</pubDate>
    </item>
    <item>
      <title>前沿 | Sandia国家实验室联手哈佛大学打造世界上第一架量子计算机桥</title>
      <link>http://www.iwgc.cn/link/3081637</link>
      <description>&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自Sandia National Labs&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、李亚洲&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;通过在金刚石基体中强力嵌入两个硅原子，Sandia 实验室的研究人员首次展示了用于建立一架连接量子计算机的一个单一芯片所需要的所有部件。相关论文发表在最新的《科学》杂志上。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「有人已经建立了小型的量子计算机，」Sandia 的研究员 Ryan Camacho 说，「但是第一台实用性的量子计算机或许不是大型的那种，而是由许多小型机连在一起的量子计算集群。」&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW90Ab4NFDScs66h9RbrrnFRWsj8PYdOo9ZG6G50wcqjuKTFtKfftGn0bM6wiaXcXWRYScibcC8fsvGg/0?wx_fmt=jpeg"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;这张量子桥图展示了金刚石中的一系列孔，孔与孔之间嵌有两个硅原子。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在一架桥或一个网络上分布量子信息也可以形成全新的量子传感形式，因为量子关联允许网络中所有的原子表现出貌似只有单个原子的状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这项与哈佛大学合作的成果使用了一个聚焦离子束注入机（focused ion beam implanter），它由 Sandia实验室 下的离子束实验室设计，用于保证金刚石基板上的单离子爆破在精确位置。Sandia 的研究员 Ed Bielejec、 Jose Pacheco 与 Daniel Perry 使用注入的方式将金刚石的一个碳原子替换成较大的硅原子，挤压任意一边上两个碳原子，让它们逃离。这使得硅原子占有大部分空间，通过邻近的非导电空缺来缓冲杂散电流。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;即便这些硅原子被嵌入金刚石，它们的行为还是像在空气中漂浮一般，因此它们的电子对量子刺激的反应就不会被其他不需要的物质所笼罩。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「我们完成的是将硅原子精准放进我们想要的位置点，」Camacho 说。「我们可以创造出数千个注入点，都能产生出能够工作的量子设备，因为我们将原子很好 注入了金刚石基板表面之下，并将它们就地退火。在此之前，研究员们必须在几微米的金刚石基板上从大约 1000 个随机出现的缺陷中搜到发射原子（emitter atoms）——非碳原子——哪怕是仅仅一个发射足够强的，能在单光子水平上是有用。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一旦这些硅原子被放置到金刚石基板中，激光产生的光子就会碰撞出硅电子，进入下一个更高的原子能状态；当电子返回到较低的能量状态时，因为所有的东西都寻求尽可能低的能量水平，他们喷出携带信息的并按照它们的频率、密度和波的偏振来量化的光子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;「哈佛大学研究员做了这个实验，同时也做了光学和量子的测量，」Camacho 说。「我们造出了这个全新的设备，并用聪明的办法精确计算出有多少离子注入了金刚石基板。」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Sandia 的研究员 John Abraham 及其同事开发出的特殊的探测器——金刚石基板上的金属薄膜，展示了通过测量单离子产生的电离信号成功实现了离子束注入。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;论文：一个集成的金刚石纳米量子光学网络平台（&amp;nbsp;An integrated diamond nanophotonics platform for quantum optical networks）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW90Ab4NFDScs66h9RbrrnFRibXsBbNTRibsmoLoK729XgUt4qOyU31gponESRBKxn0obv9T520ZS9yw/0?wx_fmt=png"/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：光子和量子发射器（quantum emitters）之间的有效接口构成了量子网络的基础，也使得在单光子水平上的光学非线性成为可能。&lt;/span&gt;&lt;span&gt;我们展示了一个集成平台，基于硅空位 （silicon-vacancy，SiV） 色心的可扩展量子纳米光子学耦合金刚石纳米元件。通过取代金刚石光子晶体洞中的 SiV 中心，我们实现了通过单色心控制量子光学开关。我们使用 SiV 亚稳态（metastable state）控制该开关，并在单光子水平上观察光学开关。拉曼跃迁（Raman transitions ）被用来实现在金刚石波导中的可调频率和带宽的单光子源。通过测量难辨别的拉曼光子排放到单波导的强度相关性，我们观测超辐射发射两个纠缠 SiV 色心带来的量子干涉效果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心经授权编译，机器之心系今日头条签约作者，本文首发于头条号，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 15 Oct 2016 12:59:57 +0800</pubDate>
    </item>
    <item>
      <title>学界 | ECCV 2016公布最佳论文奖、最佳荣誉提名和最佳学生论文奖（附论文）</title>
      <link>http://www.iwgc.cn/link/3081638</link>
      <description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;选自ECCV&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;机器之心编译&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;参与：杜夏德、吴攀&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;本周机器视觉顶级大会计算机视觉欧洲大会（European Conference on Computer Vision（ECCV））在阿姆斯特丹召开，大会最佳论文，最佳荣誉提名论文以及最佳学生论文现已公开，机器之心第一时间编译并与大家分享，点击「&lt;em style="text-align: justify; white-space: pre-wrap;"&gt;&lt;span&gt;阅读原文&lt;/span&gt;&lt;/em&gt;」下载论文。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最佳论文：使用单个事件摄像机实现实时3D重建和6-DoF追踪（Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW90Ab4NFDScs66h9RbrrnFR2KNmINlqrAslVJsXicQ9AiaHkLw6dRjsPBmk6G6TM4g1WVPHPWicezpHA/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们提出了一种能够对不带传感的单个手持事件摄像机（event camera）拍摄的图像进行实时三维重建的方法，可以在某个陌生的非结构情境中使用。它以三个被解耦的概率过滤器为基础，每个都可以估算六个自由度（6-DoF）摄像机运动、场景对数（log）的亮度梯度（intensity gradient ），以及相对于一个关键帧的场景反转深度（scene inverse depth），并对这些场景建立了一个实时图来跟踪和对扩展的本地工作区建模。我们也升级了每一个进入灰度图像（intensity image）的关键帧的梯度估算，能从以低比特率输入的事件流（event stream）中恢复一个实时的类似视频的带有时空超分辨率的灰度序列。据我们所知，这很可能是第一个能够随着包含其灰度的任意结构的重建以及只依赖于事件摄像机数据的灰度视频的重建来跟踪一个通用的6D运动的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最佳荣誉提名论文：快速双边求解器（The Fast Bilateral Solver）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW90Ab4NFDScs66h9RbrrnFRofTNxicibuREda1n3U4iaesE5RKrz5kQKKpxabhwstPUclVliaYxsld2Tg/0?wx_fmt=png"/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要：我们提出双边求解器，一个用于平滑边缘感知（ edge-aware smoothing）的算法，结合了简单过滤方法的灵活性和速度，还带有特定领域优化算法的精确度。我们的技术在几种不同的视觉任务上（立体化、深度分辨率，彩色化，语义分割）能够比肩甚至对当下最好的结果进行改进。与基准技术相比，在同样精确的情况下，要快上10-1000倍，而在同样的运行时间里输出错误率更低。该双边求解器快速、强劲、便捷，能够推广到新的领域，可以容易地整合进深度学习通道中。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;最佳学生论文奖：使用散焦和差分运动测量距离和速度（Measuring Distance and Velocity with Defocus and Differential Motion）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;摘要 ：我们提出了焦流传感器（focal flow sensor）。这是一种无致动能力的单目相机，它可以同时利用散焦的和差分的运动来测量深度图和 3D 场景的速度场。它使用光流状的（optical-flow-like）、按像素的线性约束来实现这一点——它们可以将图像导数（image derivatives）与深度和速度关联起来。我们推导出了这种约束，证明了其相对场景纹理的不变性，还证明了只有当该传感器的模糊内核（blur kernel）是高斯的（Gaussian）的时候它才能完全满足。我们分析了理想的焦流传感器的固有灵敏度（inherent sensitivity），我们还构建了一个原型并对其进行了测试。实验得到了可用于更宽泛的孔径配置（aperture configurations）的深度和速度信息，其中包括带有一个小孔径（pillbox aperture）的简单透镜。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;©本文由机器之心编译，&lt;strong&gt;&lt;em style="max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;"&gt;&lt;span&gt;转载请联系本公众号获得授权&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;。&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;✄------------------------------------------------&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;加入机器之心（全职记者/实习生）：hr@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;投稿或寻求报道：editor@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;广告&amp;amp;商务合作：bd@almosthuman.cn&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://images.weserv.nl/?url=mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibWyk1o4CFoo380DegN6BOw7z4cMruG9ibPwWpE5avEExG18UOLQO0BNE7oQDvBAdnUllldEstaIHw/640?wx_fmt=png"/&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 15 Oct 2016 12:59:57 +0800</pubDate>
    </item>
  </channel>
</rss>
