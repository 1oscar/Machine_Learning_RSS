<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>

机器学习
 - 知乎收藏夹</title><link>https://www.zhihu.com/collection/113183484</link><description>每天整理和机器学习有关的优质回答</description><lastBuildDate>Sun, 21 Aug 2016 12:25:07 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>如何高效的学习TensorFlow代码?</title><link>https://m.zhihu.com/question/41667903/answer/99268024</link><description>&lt;div class="zm-editable-content"&gt;如题，或者如何掌握TensorFlow，应用到任何领域？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
也在学习中。&lt;br&gt;个人感觉先把TensorFlow的白皮书：&lt;a href="//link.zhihu.com/?target=http%3A//download.tensorflow.org/paper/whitepaper2015.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;download.tensorflow.org&lt;/span&gt;&lt;span class="invisible"&gt;/paper/whitepaper2015.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;这个论文整明白了，大致模型就明白了。然后学习demo。最后再深入整个代码。&lt;br&gt;白皮书有个快翻译完的中文版：&lt;a href="//link.zhihu.com/?target=http%3A//www.jianshu.com/p/65dc64e4c81f" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;[译] TensorFlow 白皮书&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;------------------------------------------------------------------------------------------------------------&lt;br&gt;最近陆续更新了一些学习笔记，与初学者共享：&lt;br&gt;&lt;a href="//link.zhihu.com/?target=http%3A//blog.csdn.net/snsn1984" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;SHINING的博客&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>小乖他爹</author><pubDate>2016-05-1</pubDate></item><item><title>深度学习有哪些好玩的且易于实现的论文？</title><link>https://m.zhihu.com/question/41231774/answer/117586812</link><description>&lt;div class="zm-editable-content"&gt;擅长python，theano，keras框架，求大神介绍一些新鲜的好玩的论文，注：画画的已经实现了。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
1. Variational Auto Encoder (&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6114" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1312.6114&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)&lt;br&gt;   paper里面的数学很高深，然而实现起来crazy easy，就是encoder + decoder + kld loss + gaussian loss, 而且很好玩，可以生成人脸和MNIST。&lt;br&gt;&lt;ul&gt;&lt;li&gt;Torch版本（&lt;a href="//link.zhihu.com/?target=https%3A//github.com/y0ast/VAE-Torch" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;GitHub - y0ast/VAE-Torch: Implementation of Variational Auto-Encoder in Torch7&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）&lt;br&gt;&lt;/li&gt;&lt;li&gt;Tensorflow版本（&lt;a href="//link.zhihu.com/?target=https%3A//github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;GitHub - ikostrikov/TensorFlow-VAE-GAN-DRAW: A collection of generative methods implemented with TensorFlow (Deep Convolutional Generative Adversarial Networks (DCGAN), Variational Autoencoder (VAE) and DRAW: A Recurrent Neural Network For Image Generation).&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）&lt;br&gt;&lt;/li&gt;&lt;li&gt;Caffe版本（&lt;a href="//link.zhihu.com/?target=https%3A//github.com/cdoersch/vae_tutorial" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;github.com/cdoersch/vae&lt;/span&gt;&lt;span class="invisible"&gt;_tutorial&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;2. Generative Adversarial Networks（&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.2661" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1406.2661&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）&lt;br&gt;这个模型真的十分intuitive和elegant, 两个网络，一个“造假者”，一个“判别真假的警察”，互相博弈训练，最后使得“造假者”可以以假乱真。&lt;br&gt;&lt;ul&gt;&lt;li&gt;Tensorflow版本（&lt;a href="//link.zhihu.com/?target=https%3A//github.com/carpedm20/DCGAN-tensorflow" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;GitHub - carpedm20/DCGAN-tensorflow: A tensorflow implementation of Deep Convolutional Generative Adversarial Networks&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;以上两个模型应该是generative model里面的basic了，他们的各种变体也都很好玩，比如：&lt;br&gt;&lt;ul&gt;&lt;li&gt;Draw(&lt;a href="//link.zhihu.com/?target=https%3A//github.com/ericjang/draw" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;GitHub - ericjang/draw: TensorFlow Implementation of "DRAW: A Recurrent Neural Network For Image Generation"&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)。 &lt;/li&gt;&lt;li&gt;前两天还见到一个生成神奇宝贝的（&lt;a href="//link.zhihu.com/?target=http%3A//bohemia.hatenablog.com/entry/2016/08/13/132314" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;ディープラーニングで新しいポケモン作ろうとしたら妖怪が生まれた&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;），不知道为啥是日文，我也很懵逼，然而生成的图还是很萌的：&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e58b089da19aaaa35470ee3644cf616_b.png" data-rawwidth="1024" data-rawheight="1024" class="origin_image zh-lightbox-thumb" width="1024" data-original="https://pic3.zhimg.com/7e58b089da19aaaa35470ee3644cf616_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1024" data-rawheight="1024" class="origin_image zh-lightbox-thumb lazy" width="1024" data-original="https://pic3.zhimg.com/7e58b089da19aaaa35470ee3644cf616_r.png" data-actualsrc="https://pic3.zhimg.com/7e58b089da19aaaa35470ee3644cf616_b.png"&gt;
&lt;/div&gt;</description><author>午后阳光</author><pubDate>2016-08-1</pubDate></item><item><title>梯度下降or拟牛顿法？</title><link>https://m.zhihu.com/question/46441403/answer/117594441</link><description>&lt;div class="zm-editable-content"&gt;为什么在神经网络的训练中，很多人都采用类似与sgd，rmsprop，adgrad等优化方法。而在训练逻辑回归等模型的时候采用各种拟牛顿的方法？为什么他们不在神经网络中也使用拟牛顿法？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
神经网络优化问题有三个特点：大数据（样本多），高维问题（参数极多），非凸&lt;br&gt;牛顿和拟牛顿法在凸优化情形下，如果迭代点离全局最优很近时，收敛速率快于gd。&lt;br&gt;然而：&lt;br&gt;&lt;ol&gt;&lt;li&gt;大数据带来的问题：因为数据量大，从计算上来说不可能每一次迭代都使用全部样本计算优化算法所需的统计量（梯度，Hessian等等），因此只能基于batch来计算，从而引入了噪声（sgd）。梯度的估计本身已经带了噪声，利用有噪声的梯度和历史梯度用近似公式逼近Hessian（L-BFGS），则噪声很可能更大，而且微分本身时放大噪声的。所以即使多花费了计算量，拟牛顿的效果未必会更好。&lt;br&gt;&lt;/li&gt;&lt;li&gt;高维带来的问题：因为参数极多，所以Hessian阵也会非常巨大，无法显式计算和存储，因此牛顿法几乎不可行，只能尝试用L-BFGS。在文本应用中，第一层embedding的参数量占整体参数量的绝大多数，因此第一层参数的计算速度与空间消耗几乎决定了整体消耗。由于文本输入是高维&lt;b&gt;稀疏&lt;/b&gt;的，因此在每一个batch下第一层的梯度也是稀疏的（只有该样本出现的字（词）所连接的那些权值的梯度是非零的），因此sgd每一步迭代不需要把第一层所有的参数的梯度进行通信，而只需要通信极小部分的非零的梯度，这样&lt;b&gt;稀疏更新&lt;/b&gt;会大大加快计算速度（通信量大大减小，因此通信速度增加）。而牛顿法或者拟牛顿法&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=p%3D-H%5E%7B-1%7Dg" alt="p=-H^{-1}g" eeimg="1"&gt;，本质上是使用几乎稠密的矩阵&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=H%5E%7B-1%7D" alt="H^{-1}" eeimg="1"&gt;（Hessian或其近似），对梯度的各维度信息进行混合、整定，因此实际的更新&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=p" alt="p" eeimg="1"&gt;几乎不稀疏，无法使用稀疏更新，会大大减慢计算速度。从直观上讲，sgd相当于样本里出现的每个字或词，其信息只利用在与该字或词直接相关的网络参数的更新。因此如果一个字或词始终没出现，那么其对应的网络就完全没有学习任何东西，其权值在最后还是等于初始化后的值。牛顿法或拟牛顿法的思路则是不同字词之间是存在关联的，一个字或词的信息，应该同时分享并贡献到其他字词的参数的更新。这个在小数据低维问题时能更好的利用数据，但是在大数据的情形下，基本每个字词都会有足够的样本来贡献信息，而过数据的速度则可能成了影响效果的瓶颈。例如如果sgd比牛顿快十倍，那么sgd能在同样的时间内见到十倍于牛顿的数据，那么这些多出来的新鲜的信息往往就能带来更大的收益。&lt;br&gt;&lt;/li&gt;&lt;li&gt;非凸带来的问题：非凸情形下，牛顿或拟牛顿法会被鞍点吸引。或者更直接的说，由于非凸，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=H" alt="H" eeimg="1"&gt;非正定，导致&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=-H%5E%7B-1%7Dg" alt="-H^{-1}g" eeimg="1"&gt;不再是下降方向（&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=-g%5ETH%5E%7B-1%7Dg%5Cnot%3C0" alt="-g^TH^{-1}g\not&amp;lt;0" eeimg="1"&gt;），每一步迭代反倒可能不降反升。当然，我们可以通过对一些维度做修正（Hessian modification），但是在高维情形，鞍点数量极多，而且&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=H" alt="H" eeimg="1"&gt;本身是带噪声的估计，因此修正未必是可行之路。&lt;/li&gt;&lt;/ol&gt;但是sgd本身也不是完美的。sgd的更新公式是&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=p%3D-%5Calpha+g" alt="p=-\alpha g" eeimg="1"&gt;，步长是按同一scale统一作用在各维度上的。但是在实际问题中，各维度本身的scale是不一样的（想象一下一个很扁的椭圆等高面），因此需要分维度作整定，这就导致了adagrad,adadelta,rmsprop,adam等一系列adaptive learning rate方法，其本质都是&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=p%3D-D%5E%7B-1%7Dg" alt="p=-D^{-1}g" eeimg="1"&gt;，其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=D" alt="D" eeimg="1"&gt;是一个对角矩阵，并且&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=D_%7Bii%7D" alt="D_{ii}" eeimg="1"&gt;尝试逼近&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Csqrt%7B%5Cpartial%5E2f%2F%5Cpartial+x_i%5E2%7D" alt="\sqrt{\partial^2f/\partial x_i^2}" eeimg="1"&gt;（或其类似物），即根号下第i个方向的曲率。这几种方法差别基本在于利用历史梯度估计曲率的公式设计。那么对问题（1）来说，更好的估计使得这种二阶导信息带的噪声更少，而且某一方向即使估计差了，其影响也只限在那一维，影响较小。对问题（2），由于只多估计了一个对角线的参数，所以相对sgd只是放大了一倍，实际中完全可以承受。对问题（3），由于保证了对角元都为正，所以是下降方向。所以实践中往往即不使用LBFGS，也不使用sgd，而是使用adaptive learning rate系列方法。据我的实验经验，adam和adadelta效果最好。&lt;br&gt;神经网络的优化方法还有很大的探索空间，尤其现在seq2seq的一系列复杂模型（NTM, attention等），其计算复杂度越来越高，对优化方法的要求也越来越高，还有待更具突破性的优化方法的出现。
&lt;/div&gt;</description><author>过拟合</author><pubDate>2016-08-1</pubDate></item><item><title>支持向量机(SVM)是什么意思？</title><link>https://m.zhihu.com/question/21094489/answer/117246987</link><description>&lt;div class="zm-editable-content"&gt;支持向量机/support vector machine (SVM)。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
这篇答案贡献给想捋一捋SVM思路的看官。我的初衷是想直观地捋顺SVM的原理和求最优解，尽可能只用到必需的数学表达，但仿佛所有的数学推导都了然于胸。&lt;br&gt;&lt;br&gt;先看思维导图：&lt;br&gt;&lt;ul&gt;&lt;li&gt;左边是求解基本的SVM问题&lt;br&gt;&lt;/li&gt;&lt;li&gt;右边是相关扩展&lt;/li&gt;&lt;/ul&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb lazy" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg" data-actualsrc="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg"&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;什么是SVM？&lt;/b&gt;&lt;br&gt;Support Vector Machine, 一个普通的SVM就是一条直线罢了，用来完美划分linearly separable的两类。但这又不是一条普通的直线，这是无数条可以分类的直线当中最完美的，因为它恰好在两个类的中间，距离两个类的点都一样远。而所谓的Support vector就是这些离分界线最近的『点』。如果去掉这些点，直线多半是要改变位置的。可以说是这些vectors（主，点点）support（谓，定义）了machine（宾，分类器）...&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_r.jpg" data-actualsrc="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_b.jpg"&gt;&lt;br&gt;&lt;br&gt;所以谜底就在谜面上啊朋友们，只要找到了这些最靠近的点不就找到了SVM了嘛。&lt;br&gt;如果是高维的点，SVM的分界线就是平面或者超平面。其实没有差，都是一刀切两块，我就统统叫直线了。&lt;br&gt;&lt;br&gt;&lt;b&gt;怎么求解SVM？&lt;/b&gt;&lt;br&gt;关于这条直线，我们知道(1)它在离两边一样远，(2)最近距离就是到support vector，其他距离只能更远。&lt;br&gt;于是自然而然可以得到重要表达 &lt;b&gt;I. direct representation&lt;/b&gt;:&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bboundary%7D+margin" alt="\arg\max_{boundary} margin" eeimg="1"&gt;, &lt;br&gt;subject to 所有&lt;u&gt;正确归类的&lt;/u&gt;苹果和香蕉到boundary的距离都&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cge+margin" alt="\ge margin" eeimg="1"&gt;&lt;/blockquote&gt;（求的是boundary，并且是使得margin最大化的boundary，而margin通过苹果和香蕉到boundary的最小距离）&lt;br&gt;其中距离，说白了就是点到直线的距离；只要定义带正负号的距离，是{苹果+1}面为正{香蕉-1}面为负的距离，互相乘上各自的label &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cin+%5Cleft%5C%7B+%2B1%2C-1+%5Cright%5C%7D+" alt="\in \left\{ +1,-1 \right\} " eeimg="1"&gt;，就和谐统一民主富强了。&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 定义直线为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=y%28x%29+%3D+w%5ETx%2Bb" alt="y(x) = w^Tx+b" eeimg="1"&gt;&lt;br&gt;# 任意点x到该直线的距离为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%7C%7Cw%7C%7C%7D%28w%5ETx%2Bb%29" alt="\frac{1}{||w||}(w^Tx+b)" eeimg="1"&gt;&lt;br&gt;# 对于N个训练点的信息(点的坐标，苹果还是香蕉)记为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="(x_i, y_i)" eeimg="1"&gt;&lt;br&gt;# 上述表达也就是[1]：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bw%2Cb%7D%0A%5Cleft%5C%7B+%5Cfrac%7B1%7D%7B%7C%7Cw%7C%7C%7D%5Cmin_%7Bn%7D%5Cleft%5B+y_i%28w%5ETx_i%2Bb%29+%5Cright%5D++%5Cright%5C%7D+" alt="\arg\max_{w,b}
\left\{ \frac{1}{||w||}\min_{n}\left[ y_i(w^Tx_i+b) \right]  \right\} " eeimg="1"&gt;&lt;br&gt;# 不知为何这是我见过的最喜欢的写法（比心）&lt;br&gt;# 也可以写成[2]：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bw%2Cb%2C%7C%7Cw%7C%7C%3D1%7D+margin" alt="\arg\max_{w,b,||w||=1} margin" eeimg="1"&gt;&lt;br&gt;    subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=y_i%28w%5ETx_i%2Bb%29%5Cge+margin%2C++%5Cforall+i" alt="y_i(w^Tx_i+b)\ge margin,  \forall i" eeimg="1"&gt;&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%7C%7Cw%7C%7C%3D1" alt="||w||=1" eeimg="1"&gt;就是为了表达方便[3]，后面会取消这个限制&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;到这里为止已经说完了所有关于SVM的直观了解，如果不想看求解，可以跳过下面一大段直接到objective function。&lt;br&gt;&lt;br&gt;直接表达虽然清楚但是求解无从下手。做一些简单地等价变换（分母倒上来）可以得到 &lt;b&gt;II. carnonical representation &lt;/b&gt;（敲黑板&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmin_%7Bboundary%7D%7C%7Cw%7C%7C" alt="\arg\min_{boundary}||w||" eeimg="1"&gt;&lt;br&gt;subject to 所有苹果和香蕉到boundary的距离&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cge+margin" alt="\ge margin" eeimg="1"&gt;&lt;/blockquote&gt;w不过是个定义直线的参数，知道这一步是等价变换出来的表达就可以了。&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 为了以后推导方便，一般写成：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmin_%7Bw%2Cb%7D%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2" alt="\arg\min_{w,b}\frac{1}{2}||w||^2" eeimg="1"&gt;&lt;br&gt;&lt;br&gt;   subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=y_i%28w%5ETx_i%2Bb%29%5Cge+1%2C++%5Cforall+i" alt="y_i(w^Tx_i+b)\ge 1,  \forall i" eeimg="1"&gt;&lt;br&gt;# 这个『1』就是一个常数，这样设置是为了以后的方便&lt;br&gt;# 这个选择的自由来自于直线&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=y%28x%29+%3D+w%5ETx%2Bb" alt="y(x) = w^Tx+b" eeimg="1"&gt;的参数如果rescale成&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=kw" alt="kw" eeimg="1"&gt;和&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=kb" alt="kb" eeimg="1"&gt;不改变距离。&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;要得到&lt;b&gt;III. dual representation&lt;/b&gt;之前需要大概知道一下拉格朗日乘子法 (method of lagrange multiplier)，它是用在有各种约束条件(各种"subject to")下的目标函数，也就是直接可以求导可以引出dual representation（怎么还没完摔）&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 稍微解释一下使用拉格朗日乘子法的直观理解，不作深入讨论&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2-%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_n%2A%5Cleft%5C%7B+y_n%5Cleft%28+w%5ETx_n%2Bb+%5Cright%29+-1+%5Cright%5C%7D+%7D+" alt="L=\frac{1}{2}||w||^2-\sum_{n=1}^{N}{a_n*\left\{ y_n\left( w^Tx_n+b \right) -1 \right\} } " eeimg="1"&gt;, 其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=a_n%3E0" alt="a_n&amp;gt;0" eeimg="1"&gt;是橙子（划去）乘子[1]&lt;br&gt;# 可以这样想：(1) 我们的两个任务：①对参数最小化L（解SVM要求），②对乘子又要最大化（拉格朗日乘子法要求）， (2) 如果上面的约束条件成立，整个求和都是非负的，很好L是可以求最小值的；(3) 约束条件不成立，又要对乘子最大化，全身非负的L直接原地爆炸&lt;br&gt;# 好棒棒，所以解题一定要遵守基本法&lt;br&gt;# ① 先搞定第一个任务对w,b最小化L&lt;br&gt;# 凸优化直接取导 =&amp;gt; 志玲（划去）置零，得到：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=w%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_nx_n%7D+" alt="w=\sum_{n=1}^{N}{a_ny_nx_n} " eeimg="1"&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=0%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_n%7D+" alt="0=\sum_{n=1}^{N}{a_ny_n} " eeimg="1"&gt;&lt;br&gt;# ② 第二个任务对a最大化L，就是dual representation了&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;稍微借用刚刚数学表达里面的内容看个有趣的东西：&lt;br&gt;还记得我们怎么预测一个新的水果是苹果还是香蕉吗？我们代入到分界的直线里，然后通过符号来判断。&lt;br&gt;刚刚w已经被表达出来了也就是说这个直线现在变成了：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=y%28x_0%29+%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_nx_n%5ETx_0%7D+%2Bb" alt="y(x_0) =\sum_{n=1}^{N}{a_ny_nx_n^Tx_0} +b" eeimg="1"&gt;&lt;br&gt;看似仿佛用到了所有的训练水果，但是其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=a_n%3D0" alt="a_n=0" eeimg="1"&gt;的水果都没有起到作用，剩下的就是小部分靠边边的Support vectors呀。&lt;br&gt;&lt;br&gt;&lt;b&gt;III. dual representation&lt;/b&gt;&lt;br&gt;把①的结果代回去就可以得到[1]：&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cmax_%7Ball%5C+a_n%7D+L%28a%29%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_n%7D+%0A-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Csum_%7Bm%3D1%7D%5E%7BN%7D%7Ba_na_my_ny_mx_n%5ETx_m%7D+%7D+" alt="\max_{all\ a_n} L(a)=\sum_{n=1}^{N}{a_n} 
-\frac{1}{2}\sum_{n=1}^{N}{\sum_{m=1}^{N}{a_na_my_ny_mx_n^Tx_m} } " eeimg="1"&gt;&lt;br&gt;subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=a_n+%5Cge0%2C%5Cforall+n" alt="a_n \ge0,\forall n" eeimg="1"&gt;, &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_n%7D%3D0+" alt="\sum_{n=1}^{N}{a_ny_n}=0 " eeimg="1"&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;b&gt;如果香蕉和苹果不能用直线分割呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/242109e537a220855b33184a6f8de554_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_r.jpg" data-actualsrc="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_b.jpg"&gt;&lt;br&gt;Kernel trick. &lt;br&gt;其实用直线分割的时候我们已经使用了kernel，那就是线性kernel, &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=k%28x_1%2Cx_2%29+%3D+x_1%5ETx_2" alt="k(x_1,x_2) = x_1^Tx_2" eeimg="1"&gt;&lt;br&gt;如果要替换kernel那么把目标函数里面的内积全部替换成新的kernel function就好了，就是这么简单。&lt;br&gt;高票答案武侠大师的比喻已经说得很直观了，低维非线性的分界线其实在高维是可以线性分割的，可以理解为——『你们是虫子！』分得开个p...（大雾）&lt;br&gt;&lt;br&gt;&lt;b&gt;如果香蕉和苹果有交集呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ca45458396bf807868674316793205b7_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_r.jpg"&gt;松弛变量 (slack variable &lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_r.jpg" data-actualsrc="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_b.jpg"&gt;松弛变量 (slack variable &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cxi%5Cge0" alt="\xi\ge0" eeimg="1"&gt;)&lt;br&gt;松弛变量允许错误的分类，但是要付出代价。图中以苹果为例，错误分类的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cxi%3E1" alt="\xi&amp;gt;1" eeimg="1"&gt;；在margin当中但是正确分类的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=0%3C%5Cxi%5Cle+1" alt="0&amp;lt;\xi\le 1" eeimg="1"&gt;；正确分类并且在margin外面的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cxi%3D0" alt="\xi=0" eeimg="1"&gt;。可以看出每一个数据都有一一对应的惩罚。&lt;br&gt;对于这一次整体的惩罚力度，要另外使用一个超参数 (&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"&gt;) 来衡量这一次分类的penalty程度。&lt;br&gt;从新的目标函数里可见一斑[1]：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cmin+%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%2B+%5Cfrac%7B%5Cgamma%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Cxi_n%5E2%7D+" alt="\min \frac{1}{2}||w||^2 + \frac{\gamma}{2}\sum_{n=1}^{N}{\xi_n^2} " eeimg="1"&gt;&lt;br&gt;（约束条件略）&lt;br&gt;&lt;br&gt;&lt;b&gt;如果还有梨呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_r.jpg"&gt;可以每个类别做一次SVM：是苹果还是不是苹果？是香蕉还是不是香蕉？是梨子还是不是梨子？从中选出可能性最大的。这是one-versus-the-rest approach。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_r.jpg" data-actualsrc="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_b.jpg"&gt;可以每个类别做一次SVM：是苹果还是不是苹果？是香蕉还是不是香蕉？是梨子还是不是梨子？从中选出可能性最大的。这是one-versus-the-rest approach。&lt;br&gt;也可以两两做一次SVM：是苹果还是香蕉？是香蕉还是梨子？是梨子还是苹果？最后三个分类器投票决定。这是one-versus-one approace。&lt;br&gt;但这其实都多多少少有问题，比如苹果特别多，香蕉特别少，我就无脑判断为苹果也不会错太多；多个分类器要放到一个台面上，万一他们的scale没有在一个台面上也未可知。&lt;br&gt;&lt;br&gt;这时候我们再回过头看一下思维导图划一下重点：&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg"&gt;课后习题：&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb lazy" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg" data-actualsrc="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg"&gt;课后习题：&lt;br&gt;1. vector不愿意support怎么办？&lt;br&gt;2. 苹果好吃还是香蕉好吃？&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;最后送一张图我好爱哈哈哈 (Credit: Burr Settles)&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_b.png" data-rawwidth="824" data-rawheight="734" class="origin_image zh-lightbox-thumb" width="824" data-original="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="824" data-rawheight="734" class="origin_image zh-lightbox-thumb lazy" width="824" data-original="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_r.png" data-actualsrc="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_b.png"&gt;&lt;br&gt;&lt;p&gt;[1] Bishop C M. Pattern recognition[J]. Machine Learning, 2006, 128.&lt;/p&gt;&lt;p&gt;[2] Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. Springer, Berlin: Springer series in statistics, 2001.&lt;/p&gt;&lt;p&gt;[3] James G, Witten D, Hastie T, et al. An introduction to statistical learning[M]. New York: springer, 2013.&lt;/p&gt;
&lt;/div&gt;</description><author>靠靠靠谱</author><pubDate>2016-08-1</pubDate></item><item><title>如何评价Word2Vec作者提出的fastText算法？深度学习是否在文本分类等简单任务上没有优势？</title><link>https://m.zhihu.com/question/48345431/answer/111513229</link><description>&lt;div class="zm-editable-content"&gt;Word2Vec作者Mikolov在预印本（Bag of Tricks for Efficient Text Classification，&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.01759v2.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/pdf/1607.0175&lt;/span&gt;&lt;span class="invisible"&gt;9v2.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）中提出了fastText文本分类方法，可以在普通CPU上快速训练，结果与深度学习训练出来的模型类似。深度学习是否在文本分类等简单任务上没有优势？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
fastText简而言之，就是把文档中所有词通过lookup table变成向量，取平均后直接用线性分类器得到分类结果。fastText和ACL-15上的deep averaging network [1] (DAN，如下图)非常相似，区别就是去掉了中间的隐层。两篇文章的结论也比较类似，也是指出对一些简单的分类任务，没有必要使用太复杂的网络结构就可以取得差不多的结果。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/439379a6670286603bbd51f573da0560_b.png" data-rawwidth="412" data-rawheight="301" class="content_image" width="412"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="412" data-rawheight="301" class="content_image lazy" width="412" data-actualsrc="https://pic1.zhimg.com/439379a6670286603bbd51f573da0560_b.png"&gt;&lt;br&gt;文中实验选取的都是对句子词序不是很敏感的数据集，所以得到文中的实验结果完全不奇怪。但是比如对下面的三个例子来说：&lt;br&gt;&lt;ul&gt;&lt;li&gt;The movie is not very good , but i still like it . [2]&lt;br&gt;&lt;/li&gt;&lt;li&gt;The movie is very good , but i still do not like it .&lt;br&gt;&lt;/li&gt;&lt;li&gt;I do not like it , but the movie is still very good .&lt;/li&gt;&lt;/ul&gt;其中第1、3句整体极性是positive，但第2句整体极性就是negative。如果只是通过简单的取平均来作为sentence representation进行分类的话，可能就会很难学出词序对句子语义的影响。&lt;br&gt;&lt;br&gt;从另一个角度来说，fastText可以看作是用window-size=1 + average pooling的CNN [3]对句子进行建模。&lt;br&gt;&lt;br&gt;总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。&lt;br&gt;&lt;br&gt;另外，fastText文中还提到的两个tricks分别是：&lt;br&gt;&lt;ul&gt;&lt;li&gt;hierarchical softmax&lt;br&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;类别数较多时，通过构建一个霍夫曼编码树来加速softmax layer的计算，和之前word2vec中的trick相同&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;N-gram features&lt;br&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;只用unigram的话会丢掉word order信息，所以通过加入N-gram features进行补充&lt;/li&gt;&lt;li&gt;用hashing来减少N-gram的存储&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br&gt;[1] Deep Unordered Composition Rivals Syntactic Methods for Text Classification&lt;br&gt;[2] A Statistical Parsing Framework for Sentiment Classification&lt;br&gt;[3] Natural Language Processing (Almost) from Scratch
&lt;/div&gt;</description><author>董力</author><pubDate>2016-07-1</pubDate></item><item><title>如何直观的解释back propagation算法？</title><link>https://m.zhihu.com/question/27239198/answer/89853077</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
BackPropagation算法是多层神经网络的训练中举足轻重的算法。&lt;br&gt;简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。&lt;br&gt;要回答题主这个问题“如何直观的解释back propagation算法？”  需要先直观理解多层神经网络的训练。&lt;br&gt;&lt;br&gt;机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系。&lt;br&gt;&lt;br&gt;深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/808254232cd4983cac374c5cc2a1fc87_b.png" data-rawwidth="400" data-rawheight="282" class="content_image" width="400"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="400" data-rawheight="282" class="content_image lazy" width="400" data-actualsrc="https://pic4.zhimg.com/808254232cd4983cac374c5cc2a1fc87_b.png"&gt;&lt;p&gt;其对应的表达式如下：&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_b.png" data-rawwidth="474" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="474" data-original="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_r.png"&gt;上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="474" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="474" data-original="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_r.png" data-actualsrc="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_b.png"&gt;上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。&lt;br&gt;&lt;br&gt;和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。&lt;br&gt;&lt;br&gt;梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？&lt;br&gt;&lt;br&gt;假设我们把cost函数表示为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=H%28W_%7B11%7D%2C+W_%7B12%7D%2C+%5Ccdots+%2C+W_%7Bij%7D%2C+%5Ccdots%2C+W_%7Bmn%7D%29" alt="H(W_{11}, W_{12}, \cdots , W_{ij}, \cdots, W_{mn})" eeimg="1"&gt;, 那么它的梯度向量[2]就等于&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cnabla+H++%3D+%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+W_%7B11%7D+%7D%5Cmathbf%7Be%7D_%7B11%7D+%2B+%5Ccdots+%2B+%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+W_%7Bmn%7D+%7D%5Cmathbf%7Be%7D_%7Bmn%7D" alt="\nabla H  = \frac{\partial H}{\partial W_{11} }\mathbf{e}_{11} + \cdots + \frac{\partial H}{\partial W_{mn} }\mathbf{e}_{mn}" eeimg="1"&gt;, 其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Be%7D_%7Bij%7D" alt="\mathbf{e}_{ij}" eeimg="1"&gt;表示正交单位向量。为此，我们需求出cost函数H对每一个权值Wij的偏导数。而&lt;b&gt;BP算法正是用来求解这种多层复合函数的所有变量的偏导数的利器&lt;/b&gt;。&lt;br&gt;&lt;br&gt;我们以求e=(a+b)*(b+1)的偏导[3]为例。&lt;br&gt;它的复合关系画出图可以表示如下：&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_b.png" data-rawwidth="1383" data-rawheight="800" class="origin_image zh-lightbox-thumb" width="1383" data-original="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_r.png"&gt;在图中，引入了中间变量c,d。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1383" data-rawheight="800" class="origin_image zh-lightbox-thumb lazy" width="1383" data-original="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_r.png" data-actualsrc="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_b.png"&gt;在图中，引入了中间变量c,d。&lt;br&gt;&lt;br&gt;为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_b.png" data-rawwidth="1405" data-rawheight="793" class="origin_image zh-lightbox-thumb" width="1405" data-original="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_r.png"&gt;利用链式法则我们知道：&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1405" data-rawheight="793" class="origin_image zh-lightbox-thumb lazy" width="1405" data-original="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_r.png" data-actualsrc="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_b.png"&gt;利用链式法则我们知道：&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+a%7D" alt="\frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}" eeimg="1"&gt;以及&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+b%7D%2B%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+d%7D%5Ccdot+%5Cfrac%7B%5Cpartial+d%7D%7B%5Cpartial+b%7D" alt="\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}" eeimg="1"&gt;。&lt;br&gt;&lt;br&gt;链式法则在上图中的意义是什么呢？其实不难发现，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D" alt="\frac{\partial e}{\partial a}" eeimg="1"&gt;的值等于从a到e的路径上的偏导值的乘积，而&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D" alt="\frac{\partial e}{\partial b}" eeimg="1"&gt;的值等于从b到e的路径1(b-c-e)上的偏导值的乘积加上路径2(b-d-e)上的偏导值的乘积。也就是说，对于上层节点p和下层节点q，要求得&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D" alt="\frac{\partial p}{\partial q}" eeimg="1"&gt;，需要找到从q节点到p节点的所有路径，并且对每条路径，求得该路径上的所有偏导数之乘积，然后将所有路径的 “乘积” 累加起来才能得到&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D" alt="\frac{\partial p}{\partial q}" eeimg="1"&gt;的值。&lt;br&gt;&lt;br&gt;大家也许已经注意到，这样做是十分冗余的，因为很多&lt;b&gt;路径被重复访问了&lt;/b&gt;。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。&lt;br&gt;&lt;br&gt;&lt;b&gt;同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。&lt;/b&gt;&lt;br&gt;正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。&lt;br&gt;&lt;br&gt;从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。&lt;br&gt;&lt;br&gt;以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.&lt;br&gt;&lt;br&gt;举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。&lt;br&gt;&lt;br&gt;而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。&lt;br&gt;------------------------------------------------------------------&lt;br&gt;【参考文献】&lt;br&gt;[1] &lt;a href="//link.zhihu.com/?target=http%3A//www.cnblogs.com/nsnow/p/4562308.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;技术向：一文读懂卷积神经网络CNN&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;[2] &lt;a href="//link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gradient" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Gradient&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;[3] &lt;a href="//link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Backprop/" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;colah.github.io/posts/2&lt;/span&gt;&lt;span class="invisible"&gt;015-08-Backprop/&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;其他推荐网页：&lt;br&gt;1. &lt;a href="//link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;tensorflow.org 的页面 &lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;2. &lt;a href="//link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Neural networks and deep learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>Evan Hoo</author><pubDate>2016-04-1</pubDate></item><item><title>如何配置一台适用于深度学习的工作站？</title><link>https://m.zhihu.com/question/33996159/answer/58821678</link><description>&lt;div class="zm-editable-content"&gt;刚买两块Titan Z GPU准备搞搞深度学习，结果原来的工作站功率不够，带不动，所以准备组装一台新工作站。求大神们给点意见，最好给个完整的list，我好照着买，谢谢。（本人新手，也不怎么会组装，最好是半成品机器，然后我组装一下就好的那种）&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
TitanX比TitanZ好用。&lt;br&gt;显卡数决定了CPU数，&amp;lt;=4个时没必要双CPU。单CPU &lt;b&gt;5930K&lt;/b&gt;足矣&lt;br&gt;如果是单CPU，内存32GB就够，不用ECC&lt;br&gt;硬盘比较容易成为瓶颈。根据剩余PCIE槽数可以考虑Intel 750 (PCIE) 或者Samsung SM951 (M2)&lt;br&gt;4TitanX用CXXNET跑满实测电源输出功率1000W左右，保险点上个1500W的电源吧。&lt;br&gt;这样配的话，4路TitanX加税也不到7000刀（不带显卡裸机2000多）&lt;br&gt;&lt;br&gt;&lt;br&gt;具体配置如下，仅供参考。没包含PCIE SSD因为插不下了：&lt;br&gt;&lt;br&gt;CPU i7 5820K&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Intel-i7-5930K-Haswell-E-Processor-BX80648I75930K/dp/B00MMLXMM8/" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/Intel-i7-5930K-Haswell-E-Processor-BX80648I75930K/dp/B00MMLXMM8/&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;主板 Gigabyte&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Gigabyte-CrossFireX-Motherboards-GA-X99-GAMING-WIFI/dp/B00MPIDZCK/ref%3Dsr_1_9%3Fs%3Dpc%26ie%3DUTF8%26qid%3D1426535776%26sr%3D1-9" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/Gigabyte-CrossFireX-Motherboards-GA-X99-GAMING-WIFI/dp/B00MPIDZCK/ref=sr_1_9?s=pc&amp;amp;ie=UTF8&amp;amp;qid=1426535776&amp;amp;sr=1-9&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;内存 32GB DDR4 &lt;a class=" external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Crucial-Ballistix-PC4-19200-Unbuffered-BLS2K8G4D240FSA/dp/B00MTSWFMM/ref%3Dsr_1_1%3Fs%3Dpc%26ie%3DUTF8%26qid%3D1426537593%26sr%3D1-1" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Crucial-Ball&lt;/span&gt;&lt;span class="invisible"&gt;istix-PC4-19200-Unbuffered-BLS2K8G4D240FSA/dp/B00MTSWFMM/ref=sr_1_1?s=pc&amp;amp;ie=UTF8&amp;amp;qid=1426537593&amp;amp;sr=1-1&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;硬盘 Samsung 850 Pro 1TB &lt;a class=" external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Samsung-2-5-Inch-SATA-Internal-MZ-7KE1T0BW/dp/B00LF10KTE/ref%3Dsr_1_2%3Fie%3DUTF8%26qid%3D1426572602%26sr%3D8-2%26keywords%3Dsamsung%2B1tb" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Samsung-2-5-&lt;/span&gt;&lt;span class="invisible"&gt;Inch-SATA-Internal-MZ-7KE1T0BW/dp/B00LF10KTE/ref=sr_1_2?ie=UTF8&amp;amp;qid=1426572602&amp;amp;sr=8-2&amp;amp;keywords=samsung+1tb&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;（除了 PCIE 以外，还可以考虑SSD RAID 0。关于IO的benchmark做的不多就不乱说了。欢迎其他童鞋补充）&lt;br&gt;&lt;br&gt;显卡Titan X*4 &lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.geforce.com/geforce-gtx-titan-x/buy-gpu" target="_blank" rel="nofollow noreferrer"&gt;NVIDIA Store&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;机箱 Corsair Carbide Air 540 &lt;a href="//link.zhihu.com/?target=http%3A//www.amazon.com/Corsair-Carbide-High-Airflow-CC-9011030-WW/dp/B00D6GINF4" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Corsair-Carb&lt;/span&gt;&lt;span class="invisible"&gt;ide-High-Airflow-CC-9011030-WW/dp/B00D6GINF4&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;（NVidia DevBox 同款）&lt;br&gt;&lt;br&gt;电源 EVGA 1600W&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/EVGA-Supernova-80Plus-Modular-120-G2-1600-X1/dp/B00MMLUIE8" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/EVGA-Supernova-80Plus-Modular-120-G2-1600-X1/dp/B00MMLUIE8&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;======9月10日更新======&lt;br&gt;原答案中CPU推荐的是5820K，经 &lt;a data-hash="4bfbc1bc2ff914bec204302c82022b98" href="//www.zhihu.com/people/4bfbc1bc2ff914bec204302c82022b98" class="member_mention" data-hovercard="p$b$4bfbc1bc2ff914bec204302c82022b98"&gt;@郝佳男&lt;/a&gt;  提醒发现，5820K的PCIE-lanes要比5930少许多。开4卡的话，5930K的额外两百块钱可能还是省不了。
&lt;/div&gt;</description><author>Filestorm</author><pubDate>2015-09-1</pubDate></item><item><title>NIPS 2016有什么值得关注的呢？</title><link>https://m.zhihu.com/question/49567256/answer/116858150</link><description>&lt;div class="zm-editable-content"&gt;Wikipedia: &lt;a href="//link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Conference on Neural Information Processing Systems&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; (NIPS),homepage:&lt;a href="//link.zhihu.com/?target=https%3A//nips.cc/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;2016 Conference&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
来强答一发，推荐关注新的优化(Optimization)算法。&lt;br&gt;&lt;br&gt;前段几天NIPS出结果，朋友圈里几位小伙伴晒了自己的NIPS战绩。之前审NIPS2016的时候，发现优化发面的投稿有不少我感兴趣和我所做的方向相关的论文，借这个问题，简单讲讲这方面的进展，以及今年NIPS可能会出现的新方法。&lt;br&gt;&lt;br&gt;随着数据规模和参数规模的增加，设计更快速的优化算法显得非常必要。通常加速现有的优化算法有两个思路&lt;br&gt;&lt;ul&gt;&lt;li&gt;一方面是从老算法中设计新算法，通过证明和实验说明其在收敛速度上更快&lt;/li&gt;&lt;li&gt;另一方面是设计异步分布式的算法通过多机来加速，并证明其正确性和通过实验验证其有效性。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;优化算法一直也是ML各大会议非常重视的方向，每年NIPS、ICML都会有一大票优化相关的paper。近几年，随机优化方向的一些新算法都发表在机器学习会议上，比如SAG、SVRG、SAGA这一类可以线性收敛的SGD变种。&lt;br&gt;&lt;br&gt;近段时间，个人比较关注上面提到的第二种思路，也就是在多机多线程环境下设计优化算法，并通过实验和证明来验证算法的正确性。这里回顾一下异步分布式梯度下降算法：&lt;br&gt;&lt;br&gt;假设目标函数是 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5EN+f_i%28%5Cmathbf%7Bx%7D%29+%2B+h%28%5Cmathbf%7Bx%7D%29" alt="\frac{1}{n} \sum_{i=1}^N f_i(\mathbf{x}) + h(\mathbf{x})" eeimg="1"&gt;，其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=f_i%28%5Cmathbf%7Bx%7D%29" alt="f_i(\mathbf{x})" eeimg="1"&gt;是第&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=i" alt="i" eeimg="1"&gt;个样本的损失函数，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=h%28%5Cmathbf%7Bx%7D%29" alt="h(\mathbf{x})" eeimg="1"&gt;是一个非平滑的正则化项，这个目标函数常常使用Proximal Stochastic Gradient Descent (P-SGD)求解，更新式子是这样。&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bk%2B1%7D+%3D+%5Ctext%7BProx%7D%28%5Cmathbf%7Bx%7D_k+-+%5Ceta_k+%5Ctriangledown+f_i%28%5Cmathbf%7Bx%7D_k%29%29" alt="\mathbf{x}_{k+1} = \text{Prox}(\mathbf{x}_k - \eta_k \triangledown f_i(\mathbf{x}_k))" eeimg="1"&gt;，这里的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;叫做Proximal operator，目的是为了处理非平滑的正则化项&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=h%28%5Cmathbf%7Bx%7D%29" alt="h(\mathbf{x})" eeimg="1"&gt;。&lt;br&gt;&lt;br&gt;在采用异步加速的算法中，早期做异步SGD的算法比较出名的有Hogwild!，参考：Hogwild: A lock-free approach to parallelizing stochastic gradient descent，只是Hogwild!并不处理非平滑正则化。此外，还有[Alekh Agarwal et al.] Distributed delayed stochastic optimization、[Xiangru Lian et al.]Asynchronous parallel stochastic gradient for nonconvex optimization等。&lt;br&gt;&lt;br&gt;把算法换到多机或者多线程的情况下，采用parameter server结构，让master存储参数并负责更新，而worker负责计算梯度并交给master，master来完成&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;操作，那么更新式子会变成。&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bk%2B1%7D+%3D+%5Ctext%7BProx%7D%28%5Cmathbf%7Bx%7D_k+-+%5Ceta_k+%5Ctriangledown+f_i%28%5Cmathbf%7Bx%7D_%7Bd%28k%29%7D%29%29" alt="\mathbf{x}_{k+1} = \text{Prox}(\mathbf{x}_k - \eta_k \triangledown f_i(\mathbf{x}_{d(k)}))" eeimg="1"&gt;，注意这里的梯度中所使用的参数是&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=x_%7Bd%28k%29%7D" alt="x_{d(k)}" eeimg="1"&gt;，它是一个有延迟的参数（&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=d%28k%29%5Cleq+k" alt="d(k)\leq k" eeimg="1"&gt;），也就是说这个参数是『过期』的，造成这个『过期』的原因是当worker从master上拉取参数计算完梯度再提交给master的时候，master的参数可能已经被其他worker更新了好几次，具体可以参考 &lt;a data-hash="13fd0fce2affd948bfd821a8f7ed10f3" href="//www.zhihu.com/people/13fd0fce2affd948bfd821a8f7ed10f3" class="member_mention" data-editable="true" data-title="@李沐" data-hovercard="p$b$13fd0fce2affd948bfd821a8f7ed10f3"&gt;@李沐&lt;/a&gt; 的论文Communication Efficient Distributed Machine
Learning with the Parameter Server。一般可以证明到次线性收敛速度。&lt;br&gt;&lt;br&gt;如果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;不是特别复杂（比如L1 norm的Proximal operator），把它交给master来计算是比较合适的，但是，有的时候由于求解的问题带有更加复杂的Nuclear norm、Group Lasso、Fused Lasso这些norm，他们的Proximal operator如果交给master做，会加大master的负担，如果有多个master的话，还会造成master之间的通信。这个时候，我们就有必要设计一种异步的SGD，使得它能够把Proximal operator的计算交给worker，并且不会让worker和worker之间，master和master之间产生额外的通信。&lt;br&gt;&lt;br&gt;为了解决这个问题，我们设计了一种新的异步优化算法，也往今年NIPS投了，目前paper在Arxiv上：&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1605.06619" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1605.0661&lt;/span&gt;&lt;span class="invisible"&gt;9&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;。很遗憾我们的没有中今年NIPS。简单介绍一下，我们这篇paper的思路，我们把更新式子设计成两步&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=x_%7Bd%28t%29%7D%27+%3D+%5Ctext%7BProx%7D+%28%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D+-+%5Ceta_%7Bd%28t%29%7D+%5Ctriangledown+f_%7Bi_%7Bd%28t%29%7D%7D+%28%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D%29%29" alt="x_{d(t)}' = \text{Prox} (\mathbf{x}_{d(t)} - \eta_{d(t)} \triangledown f_{i_{d(t)}} (\mathbf{x}_{d(t)}))" eeimg="1"&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bx%7D_%7Bt%7D+%2B+%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D%27+-+%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D" alt="\mathbf{x}_{t+1} = \mathbf{x}_{t} + \mathbf{x}_{d(t)}' - \mathbf{x}_{d(t)}" eeimg="1"&gt;&lt;br&gt;&lt;br&gt;把第二步中的加法更新交给master计算，剩余部分都可以交给worker来计算，此时，无论&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;的计算过程多么复杂，都可以由每个worker单独完成，而master仅仅需要做加法计算。&lt;br&gt;&lt;br&gt;目前我们能证明这个算法是次线性收敛的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=O%28%5Cfrac%7B%5Clog+T%7D%7BT%7D%29" alt="O(\frac{\log T}{T})" eeimg="1"&gt;，试验中性能也很好。&lt;br&gt;&lt;br&gt;此前，异步的SGD采用variance reduction后已经能做到线性收敛，参考On Variance Reduction in Stochastic Gradient
Descent and its Asynchronous Variants。但是异步的Proximal SGD目前还没看到有线性收敛的方法。&lt;br&gt;&lt;br&gt;在NIPS2016审稿bid阶段，看到几篇用了variance reduction trick的异步Proximal SGD，比较期待今年会不会出现能够线性收敛的异步Proximal SGD。&lt;br&gt;&lt;br&gt;SGD算法只是优化算法之一，还有很多值得关注的算法。等NIPS的paper列表放出来了，再来补充一些具体的。
&lt;/div&gt;</description><author>li Eta</author><pubDate>2016-08-1</pubDate></item><item><title>为什么 Deep Learning 最先在语音识别和图像处理领域取得突破？</title><link>https://m.zhihu.com/question/21815490/answer/26331835</link><description>&lt;div class="zm-editable-content"&gt;为什么在其他领域则进展没有那么大？是由于这种技术的什么特征导致的？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
以下观点纯粹为感受, 而非科学:&lt;br&gt;&lt;ul&gt;&lt;li&gt;DL 适合处理&lt;b&gt;感知&lt;/b&gt;, 而非&lt;b&gt;逻辑&lt;/b&gt;&lt;/li&gt;&lt;li&gt;感知与逻辑的重要区别在于输入数据在输入空间中做&lt;b&gt;连续变化还是离散变化 &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;图片/语音这类感知问题中, 输入的都是裸的信号, 这一点的好处在于, 输入数据具有连续性:&lt;br&gt;一个苹果, 它稍微胖一点瘦一点红一点, 点几个噪点上去, 对于人类来说仍然是苹果.&lt;br&gt;一句话, 稍微大声一点尖锐一点卡顿一点加点噪声变点音色, 对于人类来说仍然是这句话.&lt;br&gt;也即: 输入数据可以在它的&lt;b&gt;小邻域内做连续变化而不改变自身意义, &lt;/b&gt;或者说输入点可以做小的扰动而不改变自身意义&lt;br&gt;&lt;br&gt;然而对于其他问题, 如NLP, 推荐系统, 乱七八糟的DM问题, 输入数据不再是裸的&lt;b&gt;信号&lt;/b&gt;了, 人类还没有找到很好对这些问题的输入数据的描述方式, 也即feature, 使得这种描述的&lt;b&gt;信息损失很小&lt;/b&gt;, 且具有&lt;b&gt;连续变化&lt;/b&gt;, 或者说&lt;b&gt;抗扰动&lt;/b&gt;的能力, 同时这种描述最好别在输入空间中太sparse..&lt;br&gt;比如说, NLP 里如果要给document分类, 或者识别'情绪'什么的, 还是有解决的比较好的..因为这个问题抗扰动: document里多几个词少几个词不影响分类. 同时对于人类来说, 它比很多问题更像一个感知问题: 扫一眼文章就可以大致知道它的类别. 最近比较火的image description, 也比较类似于这种.&lt;br&gt;反之, 如果给一句话, 标POS, 指代消解等等...这些逻辑问题实在是太不抗扰动了, 几乎没人在用ML方法做...&lt;br&gt;推荐问题的输入算是抗扰动了吧..但是感觉又太sparse了. 你看一张图片的输入才几千个dimension..&lt;br&gt;&lt;br&gt;毕竟神经网络啊..还有其他不少ML算法..其实就是在输入空间中找一个很可能很扭曲的manifold把人标好的那些数据点强行连到一起. 当然manifold一定是&lt;b&gt;连续的&lt;/b&gt;, 所以如果数据点和它邻域的点就已经不在一类里了那这个manifold得多扭曲? 如果维度太高数据点太sparse这个manifold轻易就拟合上了那得多废柴...&lt;br&gt;&lt;br&gt;话说前段时间刚看到一个paper已经说了, 即使做图像, 神经网络搞出来的manifold其实已经很扭曲了....  &lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6199" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1312.6199&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;  做一做perturbation就可以分错类..
&lt;/div&gt;</description><author>吴育昕</author><pubDate>2016-03-0</pubDate></item><item><title>深度学习如何入门？</title><link>https://m.zhihu.com/question/26006703/answer/63572833</link><description>&lt;div class="zm-editable-content"&gt;本人研究生，刚开始研究深度学习，课题组无任何前期基础。数学基础较差，现在发现网上各种资料真是太多了。不知道从何下手。现在在看《Python基础教程》、Peter harrington《机器学习实战》和李航的《统计学习方法》，计划下一步看LISA的Deep Learning Tutorial，这看很多人说还要看神经网络方面的内容，但感觉如果再看神经网络，这看啥资料比较好，这样一下上手就比较慢了。还是直接就看Deep Learning Tutorial，到时候缺啥补啥？谢谢！&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
先看你有没有相关的应用需求，如果有的话就太好了。借助情景学习起来非常快。&lt;br&gt;&lt;br&gt;Tutorial&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//ufldl.stanford.edu/tutorial/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning Tutorial from Stanford&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - Stanford计算机系的官方tutorial，Andrew Ng执笔。要想了解DL的原理，这个最好用了。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;论文&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www.nature.com/news/computer-science-the-learning-machines-1.14481" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;The Learning Machines&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; -  一个导论性质的文章，让你大致了解深度学习是什么，用来干什么的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www.nature.com/articles/nature14539.epdf%3Freferrer_access_token%3DK4awZz78b5Yn2_AoPV_4Y9RgN0jAjWel9jnR3ZoTv0PU8PImtLRceRBJ32CtadUBVOwHuxbf2QgphMCsA6eTOw64kccq9ihWSKdxZpGPn2fn3B_8bxaYh0svGFqgRLgaiyW6CBFAb3Fpm6GbL8a_TtQQDWKuhD1XKh_wxLReRpGbR_NdccoaiKP5xvzbV-x7b_7Y64ZSpqG6kmfwS6Q1rw%253D%253D%26tracking_referrer%3Dwww.nature.com" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - (Review Article in Nature, May 2015) 三大神 Yann LeCun, Yoshua Bengio, and Geoffrey Hinton的文章，不解释。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//cacm.acm.org/news/188737-growing-pains-for-deep-learning/fulltext" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Growing Pains in Deep Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/pdf/1404.7828.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning in Neural Networks&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;  - This technical report provides an overview of deep learning and related techniques with a special focus on developments in recent years. 主要看点是深度学习近两年（2012-2014）的进展情况。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;课程&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=https%3A//class.coursera.org/neuralnets-2012-001/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Neural Networks for Machine Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - Geoffrey Hinton在Coursera开设的MOOC。现在没有重新开课，但里面的资料都有，论坛也开放。 &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www.computervisiontalks.com/tag/deep-learning-course/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Oxford Deep Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; -  Nando de Freitas 在 Oxford 开设的深度学习课程，有全套视频。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;教材&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www-labs.iro.umontreal.ca/%7Ebengioy/dlbook/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; -  Yoshua Bengio, Ian Goodfellow and Aaron Courville，目前最权威的DL教材了&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;p&gt;几个常用的深度学习代码库：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//0xdata.com/product/deep-learning/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;H2O&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - 一个开源的可扩展的库，支持Java, Python, Scala, and R&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning4j.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deeplearning4j&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - Java库，整合了Hadoop和Spark&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//caffe.berkeleyvision.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Caffe&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - Yangqing Jia读研究生的时候开发的，现在还是由Berkeley维护。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/software/theano/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Theano&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - 最流行的Python库&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;News&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//news.startup.ml/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning News&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; - 紧跟深度学习的新闻、研究进展和相关的创业项目。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;So，各位加油咯！！！&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;--------------------------------------------------------补    充--------------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;另外建议看看大神Yoshua Bengio的推荐（左边的链接是论文，右边的是代码），有理论有应用（主要应用于CV和NLP）&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/fastnc.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Page on Toronto&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/MatlabForSciencePaper.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Home Page of Geoffrey Hinton&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/dbm.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Page on Toronto&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=http%3A//www.utstat.toronto.edu/%7Ersalakhu/DBM.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Home Page of Ruslan R Salakhutdinov&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Page on Wustl&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=https%3A//github.com/ynd/cae.py" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;ynd/cae.py · GitHub&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//icml.cc/2012/papers/718.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Page on Icml&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=https%3A//github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/s3c.py" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;https://github.com/lisa-lab/pyle...&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v28/goodfellow13.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Page on Jmlr&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=https%3A//github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/papers/maxout" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;pylearn2&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v28/pascanu13.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;On the difficulty of training recurrent neural networks&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=https%3A//github.com/pascanur/trainingRNNs" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;trainingRNNs&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=http%3A//code.google.com/p/cuda-convnet/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;cuda-convnet - High-performance C++/CUDA implementation of convolutional neural networks - Google Project Hosting&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//research.microsoft.com/pubs/189726/rvecs.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Linguistic Regularities in Continuous Space Word Representations&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href="//link.zhihu.com/?target=https%3A//code.google.com/p/word2vec/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;word2vec - Tool for computing continuous distributed representations of words. - Google Project Hosting&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;</description><author>郭小贤</author><pubDate>2016-06-2</pubDate></item></channel></rss>