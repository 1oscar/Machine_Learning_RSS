<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>

机器学习
 - 知乎收藏夹</title><link>https://www.zhihu.com/collection/113183484</link><description>每天整理和机器学习有关的优质回答</description><lastBuildDate>Sat, 10 Sep 2016 00:02:11 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>如何评价余凯创立的horizon robotics？</title><link>https://www.zhihu.com/question/31943421/answer/120759325</link><description>&lt;div class="zm-editable-content"&gt;定位，业务，技术，市场以及需要的人才均可讨论。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
不看好。&lt;br&gt;&lt;br&gt;简单说就是：懂AI的人很多都不是合格的business man，business man里大部分人又不懂AI。即使是懂AI的business man，也需要几次失错机会。所以现在的AI创业，不管大V、小V还是草根团队，不管是技术切入、数据切入，离大的商业成熟还有很长的路要走。&lt;br&gt;&lt;br&gt;简单分析一下人工智能领域的创业机会：&lt;br&gt;&lt;br&gt;AI领域的创业机会可以分三个维度来分析：&lt;b&gt;人工部分、智能部分、场景部分&lt;/b&gt;。&lt;br&gt;&lt;br&gt;&lt;b&gt;人工的部分就是数据&lt;/b&gt;：数据是有壁垒的，在大家鼓吹大数据时代的时候，数据就是金钥匙；但是人工智能时代的数据，很多都是online的场景数据，也就是说，数据是伴随产品的使用而产生的。比如无人车，不是说到哪去买一堆训练数据，然后跑一下算法优化一下参数，就可以上路测试的。无人车的数据，都是在路上跑出来的。数据从input变为output（当然output要再次转化为input去提升性能），身份的变化，使得数据不再是人工智能时代的主要创业门槛。&lt;br&gt;&lt;br&gt;&lt;b&gt;智能的部分就是算法&lt;/b&gt;：目前AI的算法有很高的技术门槛，但是都没有很强的技术壁垒，微软研究院人工智能首席科学家邓力就曾提到，他们用AI做了个现场实时翻译的demo，然后告诉记者，这里面的技术，都是没有知识产权保护的。虽然技术的进入门槛挺高，但对于已经进入的团队来说，基本上是在同一起跑线上的竞争。&lt;br&gt;&lt;br&gt;&lt;b&gt;场景部分就是产品/市场痛点&lt;/b&gt;：这一块乍一看也没什么壁垒，场景可以复制，甚至复制难度还非常低。但实际上在AI领域创业，场景就是壁垒。因为场景带来第一批数据，数据带来性能提升，进而形成壁垒。复制的场景至少在数据层面是重复发明轮子，效率要比领跑的产品低，赶超难度也逐渐增大。所以，吃场景更容易吃到AI那块蛋糕。&lt;br&gt;&lt;br&gt;总结：&lt;b&gt;在AI领域创业，应该拼命往前端跑，往离钱近，离市场痛点近的地方跑，占领场景阵地。&lt;/b&gt;走一步就有充沛的现金流，有源源不断的数据积累，就已经是default alive的企业。然后再下面几步，逐渐形成自己的数据壁垒、行业地位。最后，再回到那个技术的菜市场去看看谁家的技术好，买回来用就是。&lt;br&gt;&lt;br&gt;所以，回头看余总的地平线机器人，在拼命的往后端跑，往离钱远，离市场痛点远的地方跑。但问题是大家GPU用的好好的啊，行业还远远没成熟到需要一个集成度更高的解决方案啊，那这个时候做芯片，要研发多久？卖给谁？能卖多少？这些都是变数很大的变量，一个企业面临那么多大变量，就算是明星企业，也不看好。&lt;br&gt;&lt;br&gt;后记：大多数时候，先进的技术，都是被整合的资源，是客体，而不能成为整合别的资源的主体。当一个站在技术前沿的人去讲商业故事的时候，通常都值得警惕，商业故事应该由合格的生意人去讲。&lt;br&gt;&lt;br&gt;ps：创业不易，虽然本人经常骂各种创业项目不靠谱，但本意是想共同进步，绝无恶意哈！
&lt;/div&gt;</description><author>邱彼特</author><pubDate>2016-09-10</pubDate></item><item><title>当前人工智能特别是深度学习最前沿的研究方向是什么？</title><link>https://www.zhihu.com/question/46485555/answer/119428123</link><description>&lt;div class="zm-editable-content"&gt;深度学习显然是人工智能中最热门的研究方向，不过这里我想问的是更具体一点的研究方向。比如OpenAI的研究方向：&lt;br&gt;1）Deep Generative Models 深度生成模型&lt;br&gt;2）Neural Turing Machine 神经图灵机&lt;br&gt;3）Deep Reinforcement Learning 深度增强学习&lt;br&gt;但这也让我产生疑问：这三个方向是不是人工智能领域最前沿最重要的研究方向，还有什么研究方向的重要性和这三个方向相当？为什么DeepMind和OpenAI重点研究这三个方向呢？&lt;br&gt;恳请知乎大牛们分享一下你们的看法。这个问题顺便可以回答 到底研究什么人工智能问题才是最有价值的，最值得去做的？&lt;br&gt;另外，无论是计算机视觉，语音识别，机器人控制，都只能算是人工智能算法的应用出口。上面提及的方向都能够应用到不同垂直领域。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;p&gt;当前深度学习技术主要是data driven的，即对一个特定任务来说，只要增加训练数据的规模，深度学习模型的表现就可以得到提高。但是发展到今天，这种思路面临很多挑战。主要面临下面几个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;很多领域（如医疗，教育），很难获取大量的监督数据或者数据的标注成本过高。&lt;/li&gt;&lt;li&gt;训练数据规模再大，也有难以覆盖的情况。例如聊天机器人，你不可能穷尽所有可能的答案。而且很多答案，也是随时间变化的（例如明星年龄，配偶）。因此仅仅依靠大规模的训练语料，并不能解决这些问题。&lt;/li&gt;&lt;li&gt;通用深度学习模型，直接应用到具体问题，表现（效果，性能，占用资源等）可能不尽如人意。这就要求根据特定的问题和数据，来定制和优化深度学习网络结构。这个是当前研究最多最热的地方。&lt;/li&gt;&lt;li&gt;训练的问题。包括网络层数增加带来的梯度衰减，如何更有效的进行大规模并行训练等等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了解决上面的问题，当前的研究前沿主要包括以下几个方向：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;引入外部知识(如知识图谱,WordNet) &lt;br&gt;Knowledge-Based Semantic Embedding for Machine Translation &lt;br&gt;A Neural Knowledge Language Model&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;深度学习与传统方法的结合。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;人工规则与神经网络的结合 &lt;br&gt;Harnessing Deep Neural Networks with Logic Rules&lt;/li&gt;&lt;li&gt;贝叶斯与神经网络的结合 &lt;br&gt;Human-level concept learning through probabilistic program induction(论文讲的是用贝叶斯让机器模仿人写字的，但是对深度学习有非常大的启发价值）&lt;/li&gt;&lt;li&gt;迁移学习与神经网络的结合&lt;/li&gt;&lt;li&gt;强化学习与神经网络的结合 &lt;br&gt;Mastering the game of Go with deep neural networks and tree search&lt;/li&gt;&lt;li&gt;图模型与神经网络的结合 &lt;br&gt;Bidirectional LSTM-CRF Models for Sequence Tagging &lt;br&gt;A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;无监督的深度生成模型。 &lt;br&gt;Generative Adversarial Networks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;新的网络结构 &lt;br&gt;Highway Networks &lt;br&gt;Neural Turing Machines &lt;br&gt;End-To-End Memory Networks &lt;br&gt;Deep Residual Learning for Image Recognition &lt;br&gt;Mollifying Networks&lt;/li&gt;&lt;li&gt;新的训练方法 &lt;br&gt;Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/li&gt;&lt;/ul&gt;从具体研究方向上来说，我觉得深度学习在图像和语音上已经非常成熟，因为图像信号和语音信号，都是比较原始的信号，从原始信号中抽取特征对人比较困难，但对深度学习模型比较容易，因此深度学习技术率先在这两个领域取得巨大成功。而NLP领域，因为文字是一种high level的信息，而且从文字到语义，存在一个比较大的语义鸿沟，因此深度学习技术在NLP上存在很大的挑战，但是挑战也意味着机会，因此除了传统NLP领域的研究人大量开始发力深度学习，许多其他领域的人（如机器学习，统计），也开始向NLP进军（Bengio组的人开始搞机器翻译，语言模型，对话系统等等）。&lt;br&gt;&lt;br&gt;上面是我一些不太成熟的看法，欢迎大家指正交流。
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-09-10</pubDate></item><item><title>如何评价百度开源的深度学习框架 Paddle?</title><link>https://www.zhihu.com/question/50185775/answer/119784535</link><description>&lt;div class="zm-editable-content"&gt;百度开源了自家的深度学习框架，与其他家的对比有什么优势？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
今天刚看到的，简单说一些第一印象（以目前的github repo为准）。整体的设计感觉和Caffe心有灵犀，同时解决了Caffe早期设计当中的一些问题（比如说default stream）。&lt;br&gt;&lt;br&gt;1. 很高质量的GPU代码&lt;br&gt;&lt;br&gt;2. 非常好的RNN设计&lt;br&gt;&lt;br&gt;3. 设计很干净，没有太多的abstraction，这一点比TensorFlow好很多。&lt;br&gt;&lt;br&gt;4. 高速RDMA的部分貌似没有开源（可能是因为RDMA对于cluster design有一定要求）：&lt;a href="//link.zhihu.com/?target=https%3A//github.com/baidu/Paddle/blob/master/paddle/pserver/RDMANetwork.h%23L17-L19" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Paddle/RDMANetwork.h at master · baidu/Paddle · GitHub&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;5. 设计思路比较像第一代的DL框架，不过考虑到paddle已经有年头了，这样设计还是有历史原因的。&lt;br&gt;&lt;br&gt;  5.1 config是hard-code的protobuf message，这对扩展性可能会有影响。&lt;br&gt;&lt;br&gt;  5.2 可以看到很多有意思的类似历史遗留的设计：采用了STREAM_DEFAULT macro，然后通过TLS的方式定向到非default stream：&lt;a href="//link.zhihu.com/?target=https%3A//github.com/baidu/Paddle/blob/4fe7d833cf0dd952bfa8af8d5d7772bbcd552c58/paddle/cuda/include/hl_base.h%23L228-L229" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Paddle/hl_base.h at 4fe7d833cf0dd952bfa8af8d5d7772bbcd552c58 · baidu/Paddle · GitHub&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; （所以Paddle off-the-shelf不支持mac？）&lt;br&gt;&lt;br&gt;  5.3 在梯度计算上采用了传统的粗粒度forward/backward设计（类似Caffe）。可能有人会说“所以paddle没有auto gradient generation”，这是不对的，autograd的存在与否和op的粒度粗细无关。事实上，TensorFlow在意识到细粒度operator超级慢的速度以后，也在逐渐转回粗粒度的operator上。&lt;br&gt;&lt;br&gt;目前只看到这里。总之是一个非常solid的框架，百度的开发功底还是不错的。
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-09-10</pubDate></item><item><title>机器学习、深度学习等人工智能技术在工业界的应用状况是怎样的？人力供需状况如何？</title><link>https://www.zhihu.com/question/41012507/answer/106702987</link><description>&lt;div class="zm-editable-content"&gt;&lt;a href="//link.zhihu.com/?target=http%3A//tech.sina.cn/detail.d.html%3FdocID%3Dfxqaffy3552046%26wm%3D3049_0016%26vt%3D4" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;李开复:我在硅谷看到了什么？&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;在李开复的关于硅谷前沿科技的这篇文章里提到，深度学习的博士毕业生可以拿到200万美金以上的年薪，很是厉害啊。&lt;br&gt;----&lt;br&gt;本题已收录至&lt;b&gt;知乎圆桌 » &lt;/b&gt;&lt;b&gt;&lt;a href="https://www.zhihu.com/roundtable/alphago-vs-lee" class="internal"&gt;对弈人工智能&lt;/a&gt;&lt;/b&gt;，更多关于&lt;b&gt;李世石对战人工智能&lt;/b&gt;的解读欢迎关注讨论。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
国内的情况不了解，本人在东京从事人工智能的工作，日本这边人工智能技术应用于工业界的比较多，随便举几个例子吧：&lt;br&gt;&lt;br&gt;（1）代替肉眼检查作业，实现製造检查智能化和无人化&lt;br&gt;例如工程岩体的分类，目前主要是通过有经验的工程师通过仔细鑑别来判断，效率比较低，并且因人有不同的判断偏差。通过採用人工智能，把工程师的经验转化为深度学习算法，判断的淮确率和人工判断相当。得到对应的权值后开发出APP，这样现场工程人员在使用tablet拍照后，就可以通过APP自动得到工程岩体分类的结果，高效且淮确率高。&lt;br&gt;&lt;br&gt;还有汽车零部件厂商，目前检查生产出的零件磨损种类与等级情况时，多是有经验的人工。同样，通过採用深度学习算法，可以把人工的检测经验转化为算法，从而实现无人化检测。&lt;br&gt;&lt;br&gt;（2）大幅改善工业机器人的作业性能，提升製造流程的自动化和无人化&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2ee505389f7a3fdc89be4812c2b64b16_b.jpg" data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic3.zhimg.com/2ee505389f7a3fdc89be4812c2b64b16_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic3.zhimg.com/2ee505389f7a3fdc89be4812c2b64b16_r.jpg" data-actualsrc="https://pic3.zhimg.com/2ee505389f7a3fdc89be4812c2b64b16_b.jpg"&gt;&lt;br&gt;例如bin picking机器人，工业上有许多需要分捡的作业，如上圖所示的零件分捡，採用人工的话，速度缓慢且成本高，而且还需要提供适宜的工作温度环境（夏天的空调，冬天的暖气等），如果採用工业机器人的话，可以大幅减低成本，提高速度。但是，一般需要分捡的零件是没有整齐摆放的，机器人虽然有camera看到零件，但却不知道如何把零件成功的捡起来。这种情况下，使用机器学习，先让工业随机的进行一次分捡动作，然后告诉它这次动作是成功分捡到零件还是抓空了，经过多次训练之后，机器人会知道按照怎样的顺序来分捡，会有更高的成功率，如下图。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/346241d6c0f6b510bc77d42992d19a45_b.jpg" data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic2.zhimg.com/346241d6c0f6b510bc77d42992d19a45_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic2.zhimg.com/346241d6c0f6b510bc77d42992d19a45_r.jpg" data-actualsrc="https://pic2.zhimg.com/346241d6c0f6b510bc77d42992d19a45_b.jpg"&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/34e86b54797982d228372621652808bf_b.jpg" data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic4.zhimg.com/34e86b54797982d228372621652808bf_r.jpg"&gt;（上面的图片显示，经过机器学习后，机器人知道了分捡时夹圆柱的哪个位置会有更高的捡起成功率）&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="800" data-rawheight="533" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic4.zhimg.com/34e86b54797982d228372621652808bf_r.jpg" data-actualsrc="https://pic4.zhimg.com/34e86b54797982d228372621652808bf_b.jpg"&gt;（上面的图片显示，经过机器学习后，机器人知道了分捡时夹圆柱的哪个位置会有更高的捡起成功率）&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e51198d518aae1ffc2b1ba7271a7d3e8_b.png" data-rawwidth="919" data-rawheight="268" class="origin_image zh-lightbox-thumb" width="919" data-original="https://pic1.zhimg.com/e51198d518aae1ffc2b1ba7271a7d3e8_r.png"&gt;（上面的图片表明通过机器学习后，机器人知道按照按照怎样的顺序分捡，成功率会更高，图中数字是分捡的先后次序）&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="919" data-rawheight="268" class="origin_image zh-lightbox-thumb lazy" width="919" data-original="https://pic1.zhimg.com/e51198d518aae1ffc2b1ba7271a7d3e8_r.png" data-actualsrc="https://pic1.zhimg.com/e51198d518aae1ffc2b1ba7271a7d3e8_b.png"&gt;（上面的图片表明通过机器学习后，机器人知道按照按照怎样的顺序分捡，成功率会更高，图中数字是分捡的先后次序）&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/af709798568df18678cb7b78c53cf026_b.jpg" data-rawwidth="596" data-rawheight="658" class="origin_image zh-lightbox-thumb" width="596" data-original="https://pic3.zhimg.com/af709798568df18678cb7b78c53cf026_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="596" data-rawheight="658" class="origin_image zh-lightbox-thumb lazy" width="596" data-original="https://pic3.zhimg.com/af709798568df18678cb7b78c53cf026_r.jpg" data-actualsrc="https://pic3.zhimg.com/af709798568df18678cb7b78c53cf026_b.jpg"&gt;&lt;br&gt;（上面的图片显示，经过8个小时的学习后，机器人的分捡成功率可以达到90%，和熟练工人的水平相當）&lt;br&gt;&lt;br&gt;（3）工业机器人异常的提前检知，从而有效避免机器故障带来的损失和影响&lt;br&gt;这方面和IoT(Internet of Things)结合比较多。例如在製造流水线上，有大量的工业机器人。如果其中一个机器人出现了故障，当人感知到这个故障时，可能已经造成大量的不合格品，从而带来不小的损失。如果能在故障发生以前就检知的话，可以有效做出预防，减少损失。例如下图的工業机器人减速机和主轴，如果给它们配上sensor，并提前採取它们正常／不正常工作时的波形，电流等信息，用于训练机器学习系统，那么训练出来的模型就可以用来提前预警，实际的数据也表明人工智能会比人更早地预知到故障，从而降低损失。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2c77682f512774c1652fb22e2a685503_b.png" data-rawwidth="928" data-rawheight="628" class="origin_image zh-lightbox-thumb" width="928" data-original="https://pic4.zhimg.com/2c77682f512774c1652fb22e2a685503_r.png"&gt;（上图表明，经过机器学习后，模型通过观测到的波形，可以检知到人很难感知到的细微的变化，并在工业机器人彻底故障的之前的数星期，就提出有效预警）&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="928" data-rawheight="628" class="origin_image zh-lightbox-thumb lazy" width="928" data-original="https://pic4.zhimg.com/2c77682f512774c1652fb22e2a685503_r.png" data-actualsrc="https://pic4.zhimg.com/2c77682f512774c1652fb22e2a685503_b.png"&gt;（上图表明，经过机器学习后，模型通过观测到的波形，可以检知到人很难感知到的细微的变化，并在工业机器人彻底故障的之前的数星期，就提出有效预警）&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8b633371a1dc4ef8d8f6bb7301b740bb_b.png" data-rawwidth="940" data-rawheight="625" class="origin_image zh-lightbox-thumb" width="940" data-original="https://pic4.zhimg.com/8b633371a1dc4ef8d8f6bb7301b740bb_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="940" data-rawheight="625" class="origin_image zh-lightbox-thumb lazy" width="940" data-original="https://pic4.zhimg.com/8b633371a1dc4ef8d8f6bb7301b740bb_r.png" data-actualsrc="https://pic4.zhimg.com/8b633371a1dc4ef8d8f6bb7301b740bb_b.png"&gt;&lt;br&gt;（上图是利用机器学习来提前预警主轴的故障，一般人都是主轴出现问题后才知道）&lt;br&gt;&lt;br&gt;（4）例如工业上的3D模型设计完成后，需要根据3D模型中参数，寻找可对应的现实中的零件，用于製造实际的产品。使用机器学习来完成这个任务的话，可以快速，高匹配率的找出符合3D模型参数的那些现实零件。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5f0c58aaac13b70a376df84d26c5c917_b.jpg" data-rawwidth="980" data-rawheight="229" class="origin_image zh-lightbox-thumb" width="980" data-original="https://pic4.zhimg.com/5f0c58aaac13b70a376df84d26c5c917_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="980" data-rawheight="229" class="origin_image zh-lightbox-thumb lazy" width="980" data-original="https://pic4.zhimg.com/5f0c58aaac13b70a376df84d26c5c917_r.jpg" data-actualsrc="https://pic4.zhimg.com/5f0c58aaac13b70a376df84d26c5c917_b.jpg"&gt;&lt;br&gt;（上图是根据3D模型设计的参数，机器学习模型计算各个现实零件与这些参数的类似度，从而筛选出匹配的现实零件。没有使用机器学习时，筛选的匹配率大概是68%，也就是说，找出的现实零件中有1/3不能满足3D模型设计的参数，而使用机器学习后，匹配率达到了96%）&lt;br&gt;&lt;br&gt;（5）PCB电路板的辅助设计&lt;br&gt;任何一块印製板，都存在著与其他结构件配合装配的问题，所以，印製板的外形和尺寸，必须以产品整机结构为依据，另外还需要考虑到生产工艺。层数方面，也需要根据电路性能要求，板尺寸和线路的密集程度而定。如果不是经验丰富的技术人员，很难设计出合适的多层板。通过机器学习，可以将技术人员的经验转化为模型，从而提升PCB设计的效率与成功率。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6e1ebd7ee08c659c42addf929f348122_b.jpg" data-rawwidth="980" data-rawheight="316" class="origin_image zh-lightbox-thumb" width="980" data-original="https://pic3.zhimg.com/6e1ebd7ee08c659c42addf929f348122_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="980" data-rawheight="316" class="origin_image zh-lightbox-thumb lazy" width="980" data-original="https://pic3.zhimg.com/6e1ebd7ee08c659c42addf929f348122_r.jpg" data-actualsrc="https://pic3.zhimg.com/6e1ebd7ee08c659c42addf929f348122_b.jpg"&gt;&lt;br&gt;&lt;br&gt;除了以上的例子，机器学习在日本还有各种各样的应用，如下图中利用机器学习来进行糖尿病的诊断等，准确率很高。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/44c50282157231e94dc85b5368f63ffd_b.png" data-rawwidth="800" data-rawheight="471" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic2.zhimg.com/44c50282157231e94dc85b5368f63ffd_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="800" data-rawheight="471" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic2.zhimg.com/44c50282157231e94dc85b5368f63ffd_r.png" data-actualsrc="https://pic2.zhimg.com/44c50282157231e94dc85b5368f63ffd_b.png"&gt;&lt;br&gt;总结一下，国内的话，人工智能应用于互联网的情况比较多，日本这边的人工智能技术更多是用来服务于製造业的。许多日本製造业公司正在通过人工智能实现製造智能化、最大程度减少人力、提升製造品质。
&lt;/div&gt;</description><author>Tomi</author><pubDate>2016-09-10</pubDate></item><item><title>你必读的 RSS 订阅源有哪些？</title><link>https://www.zhihu.com/question/19580096/answer/119486747</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
机器学习越来越火了，感觉不学一把机器学习简直赶不上时代了，下面公开一下本人的机器学习RSS订阅&lt;br&gt;&lt;br&gt;在机器学习的路上摸索前行，为了更快速的跟进行业最新的动态，我开始整理和收集机器学习相关的&lt;b&gt;优秀博客&lt;/b&gt;、&lt;b&gt;专栏&lt;/b&gt;和&lt;b&gt;活跃大V&lt;/b&gt;。&lt;br&gt;&lt;br&gt;我将最终整理到的学习资源整理为全文RSS订阅:&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d9e1c849e8ac017ebe22dae14428b7b9_b.jpg" data-rawwidth="1262" data-rawheight="815" class="origin_image zh-lightbox-thumb" width="1262" data-original="https://pic2.zhimg.com/d9e1c849e8ac017ebe22dae14428b7b9_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="1262" data-rawheight="815" class="origin_image zh-lightbox-thumb lazy" width="1262" data-original="https://pic2.zhimg.com/d9e1c849e8ac017ebe22dae14428b7b9_r.jpg" data-actualsrc="https://pic2.zhimg.com/d9e1c849e8ac017ebe22dae14428b7b9_b.jpg"&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;知乎专栏&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/9ef96b9eee6611b73aecb72131718d6e_b.jpg" data-rawwidth="893" data-rawheight="466" class="origin_image zh-lightbox-thumb" width="893" data-original="https://pic3.zhimg.com/9ef96b9eee6611b73aecb72131718d6e_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="893" data-rawheight="466" class="origin_image zh-lightbox-thumb lazy" width="893" data-original="https://pic3.zhimg.com/9ef96b9eee6611b73aecb72131718d6e_r.jpg" data-actualsrc="https://pic3.zhimg.com/9ef96b9eee6611b73aecb72131718d6e_b.jpg"&gt;&lt;br&gt;&lt;b&gt;微信订阅号&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/346e14c8fbd1aaaefe707625117afb6d_b.jpg" data-rawwidth="855" data-rawheight="269" class="origin_image zh-lightbox-thumb" width="855" data-original="https://pic2.zhimg.com/346e14c8fbd1aaaefe707625117afb6d_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="855" data-rawheight="269" class="origin_image zh-lightbox-thumb lazy" width="855" data-original="https://pic2.zhimg.com/346e14c8fbd1aaaefe707625117afb6d_r.jpg" data-actualsrc="https://pic2.zhimg.com/346e14c8fbd1aaaefe707625117afb6d_b.jpg"&gt;&lt;br&gt;&lt;b&gt;博客&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ad2ea2251d2f6971f3e26e4da8eb961b_b.jpg" data-rawwidth="896" data-rawheight="340" class="origin_image zh-lightbox-thumb" width="896" data-original="https://pic4.zhimg.com/ad2ea2251d2f6971f3e26e4da8eb961b_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="896" data-rawheight="340" class="origin_image zh-lightbox-thumb lazy" width="896" data-original="https://pic4.zhimg.com/ad2ea2251d2f6971f3e26e4da8eb961b_r.jpg" data-actualsrc="https://pic4.zhimg.com/ad2ea2251d2f6971f3e26e4da8eb961b_b.jpg"&gt;&lt;br&gt;&lt;b&gt;微博&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/56fd483613dd8058d986edf932b5abbb_b.jpg" data-rawwidth="877" data-rawheight="86" class="origin_image zh-lightbox-thumb" width="877" data-original="https://pic4.zhimg.com/56fd483613dd8058d986edf932b5abbb_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="877" data-rawheight="86" class="origin_image zh-lightbox-thumb lazy" width="877" data-original="https://pic4.zhimg.com/56fd483613dd8058d986edf932b5abbb_r.jpg" data-actualsrc="https://pic4.zhimg.com/56fd483613dd8058d986edf932b5abbb_b.jpg"&gt;&lt;br&gt;&lt;b&gt;OPML&lt;/b&gt;&lt;br&gt;最终整理的 OPML 链接为:&lt;br&gt;&lt;a href="//link.zhihu.com/?target=https%3A//raw.githubusercontent.com/RickyWong33/Machine_Learning_RSS/master/OPML.xml" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;raw.githubusercontent.com&lt;/span&gt;&lt;span class="invisible"&gt;/RickyWong33/Machine_Learning_RSS/master/OPML.xml&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;使用方法 1(推荐) - inoreader&lt;/b&gt;&lt;br&gt;在设置的 OPML 订阅源中填入本 repo 的 OPML RAW 文件地址:&lt;br&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;pre&gt;&lt;code class="language-text"&gt;https://raw.githubusercontent.com/RickyWong33/Machine_Learning_RSS/master/OPML.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/div&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/efd1db237f284203514302363e524d1b_b.jpg" data-rawwidth="1206" data-rawheight="757" class="origin_image zh-lightbox-thumb" width="1206" data-original="https://pic4.zhimg.com/efd1db237f284203514302363e524d1b_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="1206" data-rawheight="757" class="origin_image zh-lightbox-thumb lazy" width="1206" data-original="https://pic4.zhimg.com/efd1db237f284203514302363e524d1b_r.jpg" data-actualsrc="https://pic4.zhimg.com/efd1db237f284203514302363e524d1b_b.jpg"&gt;&lt;blockquote&gt;inoreader 能同步本 OPML 的更新，以后扩展的订阅源都会被 inoreader 自动同步，推荐使用&lt;/blockquote&gt;&lt;b&gt;&lt;br&gt;使用方法 2 - Feedly&lt;/b&gt;&lt;br&gt;下载 &lt;a href="//link.zhihu.com/?target=https%3A//raw.githubusercontent.com/RickyWong33/Machine_Learning_RSS/master/OPML.xml" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;OPML LINK&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 后保存为 OPML.xml，在  Feedly  源管理页面导入。&lt;br&gt;&lt;blockquote&gt;由于本 RSS可能会频繁更新订阅源（或者修订 RSS 输出的样式），如果需要更新的话，请重新下载后再导入 Feedly 。&lt;br&gt;&lt;/blockquote&gt;&lt;b&gt;&lt;br&gt;致谢&lt;/b&gt;&lt;br&gt;感谢以上提到的博主积极的知识输出&lt;br&gt;欢迎推荐更多的优质机器学习阅读资源 &lt;br&gt;Github 项目地址，欢迎加 star 加 watch 保持关注:  &lt;a href="//link.zhihu.com/?target=https%3A//github.com/RickyWong33/Machine_Learning_RSS" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Machine_Learning_RSS&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>Ricky</author><pubDate>2016-09-10</pubDate></item><item><title>ICML 2016上哪些论文值得关注？</title><link>https://www.zhihu.com/question/45716405/answer/105619854</link><description>&lt;div class="zm-editable-content"&gt;ICML 2016的录用论文列表已经发布，&lt;a href="//link.zhihu.com/?target=http%3A//icml.cc/2016/%3Fpage_id%3D1649" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Accepted Papers&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，我们可以一起交流下这个会议上哪些论文特别值得关注，代表未来的研究热点和重点？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;a data-hash="cc0767a011397b2adac4e1ae6ac00b83" href="//www.zhihu.com/people/cc0767a011397b2adac4e1ae6ac00b83" class="member_mention" data-editable="true" data-title="@刘知远" data-tip="p$b$cc0767a011397b2adac4e1ae6ac00b83" data-hovercard="p$b$cc0767a011397b2adac4e1ae6ac00b83"&gt;@刘知远&lt;/a&gt; 老师此题好比crowdsourcing：ICML涉及领域众多，欲精通所有不太现实，但博采众长即可去伪存真、沙里淘金。在此仅抛砖引玉，提几篇个人比较感兴趣的paper。&lt;br&gt;&lt;br&gt;近期一直关注深度学习的研究和发展，因此下面八篇文章均涉及DL：&lt;br&gt;&lt;ol&gt;&lt;li&gt;Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/shang16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/shang16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：这篇文章出自U. Michigan，以及著名的NEC和Oculus VR。作者受到自己对CNN可视化发现的启发，设计了一种新的激活函数，即concatenated ReLU (CReLU)，在ImageNet等数据上不仅能取得更好结果，同时所需参数也可大幅缩减。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Noisy Activation Functions (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/gulcehre16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/gulcehre16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：Bengio作品。为解决传统激活函数的「饱和效应」（也称「梯度弥散」现象），作者提出在梯度饱和部分加入合适的噪声以解决先前的优化难题。如此“反人类”的做法却取得了较好的泛化效果。&lt;/li&gt;&lt;li&gt;Learning End-to-end Video Classification with Rank-Pooling (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/fernando16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/fernando16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：先前在CNN中对video特征进行pooling的方法无异于max和average，该文提出了一种新的rank-pooling方法作为内嵌优化来学习如何更有效的将时序信息encode到最终特征中去。&lt;/li&gt;&lt;li&gt;Large-Margin Softmax Loss for Convolutional Neural Networks (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/liud16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/liud16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)提出了一种新型基于「大间隔」的softmax loss。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/zhangc16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/zhangc16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：通过构建CNN网络的“镜像网络”来「无监督」的重建图像，进而增强「监督」网络的判别能力。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Unsupervised Deep Embedding for Clustering Analysis (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/xieb16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/xieb16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：RBG作品。DL版本的聚类。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Learning Convolutional Neural Networks for Graphs (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/niepert16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/niepert16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：图结构在机器学习中举足轻重，本文提出可以利用CNN来学习任意图结构。（不知会不会在graph based learning或mining领域引起轩然大波。）&lt;br&gt;&lt;/li&gt;&lt;li&gt;Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/amodei16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/amodei16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：来自Baidu的一篇系统论文。工业界朋友可以重点参考，其中介绍了很多语音系统实现方法和技巧。&lt;br&gt;&lt;/li&gt;&lt;/ol&gt;&lt;u&gt;可以很容易发现，ICML'16中涉及DL/NN的文章，要么解决了DL中非常fundamental的问题，如提出新的activation function或pooling layer；要么就是用DL解决了很fundamental的任务，如clustering，graph learning和unsupervised learning；要么就是很具实践指导性的Deep Speech 2。鲜有非常CV口味的应用文章，也许这就是不同background对DL不同的喜好吧。&lt;/u&gt;
&lt;/div&gt;</description><author>魏秀参</author><pubDate>2016-09-10</pubDate></item><item><title>推荐系统有哪些比较好的论文？</title><link>https://www.zhihu.com/question/25566638/answer/37455091</link><description>&lt;div class="zm-editable-content"&gt;最好是像kdd的论文和presentation那样，从特定问题出发，特征工程、模型选择等整个过程都提及的&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
（昨天正好回答过一个相关问题，拿过来给题主参考以下）&lt;br&gt;推荐几篇对工业界比较有影响的论文吧：&lt;br&gt;1. The Wisdom of The Few 豆瓣阿稳在介绍豆瓣猜的时候极力推荐过这篇论文，豆瓣猜也充分应用了这篇论文中提出的算法；&lt;br&gt;2. Restricted Boltzmann Machines for Collaborative Filtering 目前Netflix使用的主要推荐算法之一；&lt;br&gt;3. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model 这个无需强调重要性，LFM几乎应用到了每一个商业推荐系统中；&lt;br&gt;4. Collaborative Filtering with Temporal Dynamics 加入时间因素的SVD++模型，曾在Netflix Prize中大放溢彩的算法模型；&lt;br&gt;5. Context-Aware Recommender Systems 基于上下文的推荐模型，现在不论是工业界还是学术界都非常火的一个topic；&lt;br&gt;6. Toward the next generation of recommender systems 对下一代推荐系统的一个综述；&lt;br&gt;7. Item-Based Collaborative Filtering Recommendation Algorithms 基于物品的协同过滤，Amazon等电商网站的主力模型算法之一；&lt;br&gt;8. Information Seeking-Convergence of Search, Recommendations and Advertising 搜索、推荐和广告的大融合也是未来推荐系统的发展趋势之一；&lt;br&gt;9. Ad Click Prediction: a View from the Trenches 可以对推荐结果做CTR预测排序；&lt;br&gt;10. Performance of Recommender Algorithm on top-n Recommendation Task TopN预测的一个综合评测，TopN现在是推荐系统的主流话题，可以全部实现这篇文章中提到的算法大概对TopN有个体会；&lt;br&gt;11. &lt;a href="//link.zhihu.com/?target=http%3A//dsec.pku.edu.cn/%7Ejinlong/publication/wjlthesis.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;dsec.pku.edu.cn/~jinlon&lt;/span&gt;&lt;span class="invisible"&gt;g/publication/wjlthesis.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 北大一博士对Netflix Prize算法的研究做的毕业论文，这篇论文本身对业界影响不大，但是Netflix Prize中运用到的算法极大地推动了推荐系统的发展；&lt;br&gt;通过这些论文可以对推荐系统有个总体上的全面认识，并且能够了解一些推荐系统的发展趋势。剩下的就是多实践了。Good luck！&lt;br&gt;&lt;br&gt;------------------------------------------------------------羊年大年初一补充-----------------------------------------------------&lt;br&gt;Quora上有一个相关的回答，质量非常不错：&lt;a href="//link.zhihu.com/?target=http%3A//www.quora.com/What-are-the-seminal-papers-on-recommender-systems" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;quora.com/What-are-the-&lt;/span&gt;&lt;span class="invisible"&gt;seminal-papers-on-recommender-systems&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 回答了关于推荐系统起源问题的相关论文，同样值得阅读。
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-09-10</pubDate></item><item><title>深度机器学习中的batch的大小对学习效果有何影响？</title><link>https://www.zhihu.com/question/32673260/answer/71137399</link><description>&lt;div class="zm-editable-content"&gt;如题，在深度学习中，刚入门的小弟一直听闻一个batch中同时训练多个数据可以得到较好的效果，于是小弟在caffe上跑deepID的网络时对如何选取batchsize颇具困惑。恳求万能的知友给予指点~~&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;b&gt;谈谈深度学习中的 Batch_Size&lt;/b&gt;&lt;br&gt;Batch_Size（批尺寸）是机器学习中一个重要参数，涉及诸多矛盾，下面逐一展开。&lt;br&gt;&lt;br&gt;&lt;b&gt;首先，为什么需要有 Batch_Size 这个参数？&lt;/b&gt;&lt;br&gt;Batch 的选择，&lt;b&gt;首先决定的是下降的方向。&lt;/b&gt;如果数据集比较小，完全可以采用&lt;b&gt;全数据集&lt;/b&gt; （ &lt;b&gt;Full Batch Learning&lt;/b&gt; ）的形式，这样做&lt;u&gt;至少&lt;/u&gt;有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而&lt;a href="http://www.zhihu.com/question/37129350/answer/70964527#" class="internal"&gt;更准确地朝向极值所在的方向&lt;/a&gt;。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用&lt;b&gt;Rprop&lt;/b&gt; 只基于梯度符号并且针对性单独更新各权值。&lt;br&gt;&lt;br&gt;对于更大的数据集，以上 2 个好处又变成了 2 个坏处：其一，随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。其二，以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 &lt;b&gt;RMSProp&lt;/b&gt; 的妥协方案。&lt;br&gt;&lt;br&gt;&lt;b&gt;既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？&lt;/b&gt;&lt;br&gt;所谓另一个极端，就是每次只训练一个样本，即 Batch_Size = 1。这就是&lt;b&gt;在线学习&lt;/b&gt;&lt;b&gt;（Online Learning）&lt;/b&gt;。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，&lt;b&gt;难以达到收敛&lt;/b&gt;。&lt;br&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png" data-rawwidth="952" data-rawheight="662" class="origin_image zh-lightbox-thumb" width="952" data-original="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="952" data-rawheight="662" class="origin_image zh-lightbox-thumb lazy" width="952" data-original="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_r.png" data-actualsrc="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png"&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;可不可以选择一个适中的 Batch_Size 值呢？&lt;/b&gt;&lt;br&gt;当然可以，这就是&lt;b&gt;批梯度下降法（Mini-batches Learning）&lt;/b&gt;。因为如果数据集足够充分，那么用一半（&lt;u&gt;甚至少得多&lt;/u&gt;）的数据训练算出来的梯度与用全部数据训练出来的梯度是&lt;u&gt;几乎一样&lt;/u&gt;的。&lt;br&gt;&lt;br&gt;&lt;b&gt;在合理范围内，增大 Batch_Size 有何&lt;u&gt;好处&lt;/u&gt;？&lt;/b&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;内存利用率提高了，大矩阵乘法的并行化效率提高。&lt;/li&gt;&lt;li&gt;跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。&lt;/li&gt;&lt;li&gt;在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;b&gt;盲目增大 Batch_Size 有何&lt;u&gt;坏处&lt;/u&gt;？&lt;/b&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;内存利用率提高了，但是内存容量可能撑不住了。&lt;/li&gt;&lt;li&gt;跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。&lt;/li&gt;&lt;li&gt;Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;b&gt;调节 Batch_Size 对训练效果影响到底如何？&lt;/b&gt;&lt;br&gt;这里跑一个 LeNet 在 MNIST 数据集上的效果。MNIST 是一个手写体标准库，我使用的是 &lt;b&gt;Theano &lt;/b&gt;框架。这是一个 Python 的深度学习库。&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/software/theano/install.html%23install" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;安装方便&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;（几行命令而已），调试简单（自带 Profile），GPU / CPU 通吃，&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/tutorial/contents.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;官方教程相当完备&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，支持模块十分丰富（除了 CNNs，更是支持 RBM / DBN / LSTM / RBM-RNN / SdA / MLPs）。在其上层有 &lt;a href="//link.zhihu.com/?target=http%3A//keras.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Keras&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 封装，支持 GRU / JZS1, JZS2, JZS3 等较新结构，支持 Adagrad / Adadelta / RMSprop / Adam 等优化算法。&lt;br&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_b.png" data-rawwidth="753" data-rawheight="176" class="origin_image zh-lightbox-thumb" width="753" data-original="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="753" data-rawheight="176" class="origin_image zh-lightbox-thumb lazy" width="753" data-original="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_r.png" data-actualsrc="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_b.png"&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_b.png" data-rawwidth="1558" data-rawheight="344" class="origin_image zh-lightbox-thumb" width="1558" data-original="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="1558" data-rawheight="344" class="origin_image zh-lightbox-thumb lazy" width="1558" data-original="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_r.png" data-actualsrc="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_b.png"&gt;&lt;br&gt;运行结果如上图所示，其中绝对时间做了标幺化处理。运行结果与上文分析相印证：&lt;br&gt;&lt;ul&gt;&lt;li&gt;Batch_Size 太小，算法在 200 epoches 内不收敛。&lt;br&gt;&lt;/li&gt;&lt;li&gt;随着 Batch_Size 增大，处理相同数据量的速度越快。&lt;/li&gt;&lt;li&gt;随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。&lt;br&gt;&lt;/li&gt;&lt;li&gt;由于上述两种因素的矛盾， Batch_Size 增大到&lt;u&gt;某个&lt;/u&gt;时候，达到&lt;b&gt;时间上&lt;/b&gt;的最优。&lt;/li&gt;&lt;li&gt;由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到&lt;u&gt;某些&lt;/u&gt;时候，达到最终收敛&lt;b&gt;精度上&lt;/b&gt;的最优。&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;欢迎一起讨论。
&lt;/div&gt;</description><author>程引</author><pubDate>2016-09-10</pubDate></item><item><title>深度学习（机器学习）的下一步如何发展？</title><link>https://www.zhihu.com/question/47602063/answer/106988408</link><description>&lt;div class="zm-editable-content"&gt;目前机器学习很火，但我对机器学习方向的前景有些困惑。&lt;br&gt;&lt;br&gt;机器学习、计算机视觉下一步的创新点在哪里？随着硬件的迭代，神经网络的隐藏层可以越做越多，CNN、DNN参数可以调得越来越准确，人脸识别、图像跟踪等算法准确率提高...但就像周志华教授调侃的，目前机器学习好像就是调调参数而已。那么机器学习的下一步该走向哪里呢？比方说目前很多公司在做计算机视觉，那么这一领域还有什么可以继续挖掘的呢？此外，机器学习依赖数据，而目前大公司牢牢掌握着数据，对于未来想从事这一领域的人而言，是否还有机会？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
提几个可能的发展方向，同时也就是尚待解决的问题：&lt;br&gt;&lt;br&gt;1.&lt;b&gt;让深度学习自动调超参。&lt;/b&gt;最近看到有人在一个AI群里推广自己的一篇论文《Deep Q-Networks for Accelerating the Training of Deep Neural Networks》&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.01467" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1606.0146&lt;/span&gt;&lt;span class="invisible"&gt;7&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，大致是用强化学习的方法训练一个控制器来自动控制学习率以及在一个batch中各个类的样本占比。虽然它那篇论文问题很大，训练出来的控制器极其不通用，只能用在它原本的任务上，但是感觉很容易解决掉，这个另说。想象一下，如果能够训练出一个通用的控制器，对于各类任务都能够自动调整超参（或者只在某个子领域比如图像分类做到通用也好），那我们就再也不用自称调参狗了，同时也可以解放出更多的时间用于设计模型、验证架构，想必深度学习的发展步伐会得到极大加速。&lt;br&gt;&lt;br&gt;2.&lt;b&gt;自动学习网络架构。&lt;/b&gt;其实说起来这个问题也可以归入自动调超参，但是感觉应该还是有很大的不同。说起来无非就是两个方面，一是加法二是减法。加法方面可以参考《Net2Net: Accelerating Learning via Knowledge Transfer》&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05641" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1511.0564&lt;/span&gt;&lt;span class="invisible"&gt;1&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，这篇是让CNN自动根据需要自动拓展架构，包括横向的增加filter和纵向的增加layer。减法方面可以参考各类Network Compression（网络压缩）的论文中的所谓Network Pruning（网络剪枝），比如《Deep Compression - Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding》&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1510.00149" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1510.0014&lt;/span&gt;&lt;span class="invisible"&gt;9&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，虽然这些论文出发点不在于自动学习网络架构而在于压缩网络规模，而且它们往往是在训练收敛之后才对网络进行裁剪而非边训练边裁剪，但是感觉只需要再跨一步就可以了。我个人觉得，自动学习网络架构需要解决的最根本问题就是“应该在什么时机进行架构变动”以及“应该怎么变”，第二个问题感觉上述论文算是回答得可以了，但是第一个问题似乎还有很多可以探索的地方。对于第一个问题，似乎强化学习就很适合解决，因为显然可以把它看成一个控制问题。&lt;br&gt;&lt;br&gt;3.&lt;b&gt;迁移学习。&lt;/b&gt;众所周知，深度学习的直接训练依赖大量数据，而transfer和finetune能够有效利用数据量大的外部任务训练出来特征来迁移到数据量小的目标任务上，使得目标任务对于数据量的要求大大减小。现在的问题在于，迁移学习的思想现在大家其实都在用，很多论文中都可以看到finetune的做法，但是对于两个任务之间需要“多像”才能够迁移这么一个问题还没有一个很好的回答。即使我们不奢求能够给出一个严格的数学理论，至少，如果有人能够做一个非常系统的对比实验，总结出一些规律，使得我们有信心说在如何如何这样一个边界内的任务都是基本上可以transfer的，那将会是一个很大的进步。这个问题也可以这么看，如今我们应该有信心说两个图像分类任务可以transfer，但是这个边界太过狭窄，我个人期待的就是能够有一套理论或者方法论使得这个边界大大拓展，然后在这个边界内我们可以像对两个图像分类任务一样自信满满地用迁移学习。&lt;br&gt;&lt;br&gt;4.&lt;b&gt;无监督／半监督学习。&lt;/b&gt;像LeCun等大佬其实一直在鼓吹这方面，但似乎还没有搞出像当年CNN（AlexNet）、最近强化学习（阿法狗）这样级别的大新闻来。我理解在这个问题上的努力方向应该是确定“何种representation最有用”。具体来说，就是找到一个指标，然后用深度网络优化这个指标，使得满足这个指标的data representation能够具有非常好的特性。再具体一些，下面举三个实际例子：&lt;br&gt;&lt;ul&gt;&lt;li&gt;autoencoder以重构损失作为指标来学习一个representation。&lt;/li&gt;&lt;li&gt;之前听一个讲座，演讲人介绍他的论文《Why Deep Learning Works: A Manifold Disentanglement Perspective》&lt;a href="//link.zhihu.com/?target=http%3A//ieeexplore.ieee.org/xpl/articleDetails.jsp%3Farnumber%3D7348689" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;IEEE Xplore Abstract&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，其中定义了三个指标来描述深度网络每一层中data representation的“蜷曲程度”，并发现，越高层的数据蜷曲度越低，换言之，越平展。那么无监督学习是否能够直接以这个蜷曲度作为损失函数来学习一个representation呢？&lt;/li&gt;&lt;li&gt;这篇论文《&lt;a href="//link.zhihu.com/?target=http%3A//people.eecs.berkeley.edu/%7Epathak/context_encoder/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Context Encoders: Feature Learning by Inpainting&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;》提出通过预测周边上下文像素来无监督学习视觉特征，感觉很像word2vec从一维变成二维。&lt;/li&gt;&lt;/ul&gt;除了上述的重构损失、蜷曲度、预测上下文精度，还有没有别的指标学习出来的representation更好呢？个人认为这些问题就是推动无监督／半监督学习进展的关键所在。&lt;br&gt;&lt;br&gt;5.&lt;b&gt;基于外部存储（external memory）的模型。&lt;/b&gt;如果说RNN、LSTM这样的模型属于&lt;u&gt;internal memory / long-term memory&lt;/u&gt;的话，那么以神经图灵机（Neural Turing Machine，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1410.5401" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1410.5401&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）、记忆网络（Memory Network，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1410.3916" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1410.3916&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）为代表的模型就应该称为&lt;u&gt;external memory / really long-term memory&lt;/u&gt;了。不过这两个模型刚出来的时候还太过naive，只能做一些很无聊的task，比如序列复制和排序以及非常简单的QA，但是现在已经开始看到它们被用到更加实际的问题上面，例如One-shot Learning：《One-shot Learning with Memory-Augmented Neural Networks》，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1605.06065" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1605.0606&lt;/span&gt;&lt;span class="invisible"&gt;5&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;。往大了说，如果未来要实现强AI，这种外部存储的机制肯定是必不可少的。现在的问题在于，神经图灵机和记忆网络用的外部存储虽然比LSTM那样简单的一个hidden state向量更进一步，但也其实就是很简单的一片矩阵，没有任何结构和层次可言，换言之，就是还不够复杂。所以我猜想接下来可能external memory会和知识图谱（Knowledge Graph）结合起来或至少是向知识图谱类似的做法靠拢，因为知识图谱更加结构化。
&lt;/div&gt;</description><author>郑华滨</author><pubDate>2016-09-10</pubDate></item><item><title>如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？</title><link>https://www.zhihu.com/question/35887527/answer/72876282</link><description>&lt;div class="zm-editable-content"&gt;或者相关的检测方法如OverFeat、SPPNet和最新的YOLO。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
泻药，终于看到符合自己胃口的问题啦！怒答一枚。RCNN和Fast-RCNN简直是引领了最近两年目标检测的潮流！&lt;br&gt;-------------------------------&lt;br&gt;提到这两个工作，不得不提到RBG大神&lt;a href="//link.zhihu.com/?target=http%3A//www.cs.berkeley.edu/%7Erbg/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;rbg's home page&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，该大神在读博士的时候就因为dpm获得过pascal voc 的终身成就奖。博士后期间更是不断发力，RCNN和Fast-RCNN就是他的典型作品。&lt;br&gt;&lt;br&gt;RCNN：RCNN可以看作是RegionProposal+CNN这一框架的开山之作，在imgenet/voc/mscoco上基本上所有top的方法都是这个框架，可见其影响之大。RCNN的主要缺点是重复计算，后来MSRA的kaiming组的SPPNET做了相应的加速。&lt;br&gt;&lt;br&gt;Fast-RCNN：RCNN的加速版本，在我看来，这不仅仅是一个加速版本，其优点还包括：&lt;br&gt;(a) 首先，它提供了在caffe的框架下，如何定义自己的层/参数/结构的范例，这个范例的一个重要的应用是python layer的应用，我在这里&lt;a href="http://www.zhihu.com/question/36847520/answer/72824645" class="internal"&gt;支持多label的caffe，有比较好的实现吗？ - 孔涛的回答&lt;/a&gt;也提到了。&lt;br&gt;(2) training and testing end-to-end 这一点很重要，为了达到这一点其定义了ROIPooling层，因为有了这个，使得训练效果提升不少。&lt;br&gt;(3) 速度上的提升，因为有了Fast-RCNN，这种基于CNN的 real-time 的目标检测方法看到了希望，在工程上的实践也有了可能，后续也出现了诸如Faster-RCNN/YOLO等相关工作。&lt;br&gt;&lt;br&gt;这个领域的脉络是：RCNN -&amp;gt; SPPNET -&amp;gt; Fast-RCNN -&amp;gt; Faster-RCNN。关于具体的细节，建议题主还是阅读相关文献吧。&lt;br&gt;&lt;br&gt;这使我看到了目标检测领域的希望。起码有这么一部分人，他们不仅仅是为了几个百分点的提升，而是切实踏实在做贡献，相信不久这个领域会有新的工作出来。&lt;br&gt;&lt;br&gt;以上纯属个人观点，欢迎批评指正。&lt;br&gt;&lt;br&gt;参考：&lt;br&gt;[1] R-CNN: Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C], CVPR, 2014.&lt;br&gt;[2] SPPNET: He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[C], ECCV, 2014.&lt;br&gt;[3] Fast-RCNN: Girshick R. Fast R-CNN[C]. ICCV, 2015.&lt;br&gt;[4] Fater-RCNN: Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]. NIPS, 2015.&lt;br&gt;[5] YOLO: Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[J]. arXiv preprint arXiv:1506.02640, 2015.
&lt;/div&gt;</description><author>孔巴巴</author><pubDate>2016-09-10</pubDate></item></channel></rss>