<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>

机器学习
 - 知乎收藏夹</title><link>https://www.zhihu.com/collection/113183484</link><description>每天整理和机器学习有关的优质回答</description><lastBuildDate>Mon, 22 Aug 2016 08:11:19 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>知乎上有哪些比较好的机器学习，数据挖掘相关的专栏？</title><link>https://m.zhihu.com/question/49428252/answer/115931823</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;a href="https://zhuanlan.zhihu.com/furthersight" class="internal"&gt;欲穷千里目 - 知乎专栏&lt;/a&gt; 作者是南京大学LAMDA组的Ph.D. Candidate，内容涉及计算机视觉、机器学习。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/easyml" class="internal"&gt;炼丹实验室 - 知乎专栏&lt;/a&gt; 深度学习相关。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/uqer2015" class="internal"&gt;量化哥 - 知乎专栏&lt;/a&gt; 量化交易相关，涉及机器学习。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/hsmyy" class="internal"&gt;无痛的机器学习 - 知乎专栏&lt;/a&gt; 机器学习原理及应用。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/dlclass" class="internal"&gt;深度学习大讲堂 - 知乎专栏&lt;/a&gt; 深度学习相关。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/prml-paper-reading" class="internal"&gt;PRML - 知乎专栏&lt;/a&gt; 机器学习与数据科学相关，作者是阿里巴巴算法工程师。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/paperweekly" class="internal"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; 介绍了很多机器学习在自然语言处理上的paper。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/gongwenjia" class="internal"&gt;Data Science - 知乎专栏&lt;/a&gt; 数据科学相关一些算法的介绍。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/pengtaox" class="internal"&gt;谢澎涛的知乎专栏 - 知乎专栏&lt;/a&gt;  作者为CMU机器学习方向的PhD学生。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="internal"&gt;智能单元 - 知乎专栏&lt;/a&gt; 翻译了CS231n的笔记，另有数篇介绍深度增强学习相关内容的文章。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/system" class="internal"&gt;【Machine Learning】 - 知乎专栏&lt;/a&gt;&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/dataman" class="internal"&gt;数据分析侠 - 知乎专栏&lt;/a&gt; 数据科学、机器学习相关的软硬文。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/cxwangyi" class="internal"&gt;Occam's Razor - 知乎专栏&lt;/a&gt; 机器学习相关。&lt;br&gt;&lt;a href="https://zhuanlan.zhihu.com/Stark" class="internal"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;zhuanlan.zhihu.com/Star&lt;/span&gt;&lt;span class="invisible"&gt;k&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;/a&gt; 作者是阿里巴巴算法工程师。
&lt;/div&gt;</description><author>Frankenstein</author><pubDate>2016-08-22</pubDate></item><item><title>支持向量机(SVM)是什么意思？</title><link>https://m.zhihu.com/question/21094489/answer/117246987</link><description>&lt;div class="zm-editable-content"&gt;支持向量机/support vector machine (SVM)。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
这篇答案贡献给想捋一捋SVM思路的看官。我的初衷是想直观地捋顺SVM的原理和求最优解，尽可能只用到必需的数学表达，但仿佛所有的数学推导都了然于胸。&lt;br&gt;&lt;br&gt;先看思维导图：&lt;br&gt;&lt;ul&gt;&lt;li&gt;左边是求解基本的SVM问题&lt;br&gt;&lt;/li&gt;&lt;li&gt;右边是相关扩展&lt;/li&gt;&lt;/ul&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb lazy" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg" data-actualsrc="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg"&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;什么是SVM？&lt;/b&gt;&lt;br&gt;Support Vector Machine, 一个普通的SVM就是一条直线罢了，用来完美划分linearly separable的两类。但这又不是一条普通的直线，这是无数条可以分类的直线当中最完美的，因为它恰好在两个类的中间，距离两个类的点都一样远。而所谓的Support vector就是这些离分界线最近的『点』。如果去掉这些点，直线多半是要改变位置的。可以说是这些vectors（主，点点）support（谓，定义）了machine（宾，分类器）...&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_r.jpg" data-actualsrc="https://pic3.zhimg.com/00becdd15361c8e5ceb65da02bcf7fda_b.jpg"&gt;&lt;br&gt;&lt;br&gt;所以谜底就在谜面上啊朋友们，只要找到了这些最靠近的点不就找到了SVM了嘛。&lt;br&gt;如果是高维的点，SVM的分界线就是平面或者超平面。其实没有差，都是一刀切两块，我就统统叫直线了。&lt;br&gt;&lt;br&gt;&lt;b&gt;怎么求解SVM？&lt;/b&gt;&lt;br&gt;关于这条直线，我们知道(1)它在离两边一样远，(2)最近距离就是到support vector，其他距离只能更远。&lt;br&gt;于是自然而然可以得到重要表达 &lt;b&gt;I. direct representation&lt;/b&gt;:&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bboundary%7D+margin" alt="\arg\max_{boundary} margin" eeimg="1"&gt;, &lt;br&gt;subject to 所有&lt;u&gt;正确归类的&lt;/u&gt;苹果和香蕉到boundary的距离都&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cge+margin" alt="\ge margin" eeimg="1"&gt;&lt;/blockquote&gt;（求的是boundary，并且是使得margin最大化的boundary，而margin通过苹果和香蕉到boundary的最小距离）&lt;br&gt;其中距离，说白了就是点到直线的距离；只要定义带正负号的距离，是{苹果+1}面为正{香蕉-1}面为负的距离，互相乘上各自的label &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cin+%5Cleft%5C%7B+%2B1%2C-1+%5Cright%5C%7D+" alt="\in \left\{ +1,-1 \right\} " eeimg="1"&gt;，就和谐统一民主富强了。&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 定义直线为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=y%28x%29+%3D+w%5ETx%2Bb" alt="y(x) = w^Tx+b" eeimg="1"&gt;&lt;br&gt;# 任意点x到该直线的距离为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%7C%7Cw%7C%7C%7D%28w%5ETx%2Bb%29" alt="\frac{1}{||w||}(w^Tx+b)" eeimg="1"&gt;&lt;br&gt;# 对于N个训练点的信息(点的坐标，苹果还是香蕉)记为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="(x_i, y_i)" eeimg="1"&gt;&lt;br&gt;# 上述表达也就是[1]：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bw%2Cb%7D%0A%5Cleft%5C%7B+%5Cfrac%7B1%7D%7B%7C%7Cw%7C%7C%7D%5Cmin_%7Bn%7D%5Cleft%5B+y_i%28w%5ETx_i%2Bb%29+%5Cright%5D++%5Cright%5C%7D+" alt="\arg\max_{w,b}
\left\{ \frac{1}{||w||}\min_{n}\left[ y_i(w^Tx_i+b) \right]  \right\} " eeimg="1"&gt;&lt;br&gt;# 不知为何这是我见过的最喜欢的写法（比心）&lt;br&gt;# 也可以写成[2]：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmax_%7Bw%2Cb%2C%7C%7Cw%7C%7C%3D1%7D+margin" alt="\arg\max_{w,b,||w||=1} margin" eeimg="1"&gt;&lt;br&gt;    subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=y_i%28w%5ETx_i%2Bb%29%5Cge+margin%2C++%5Cforall+i" alt="y_i(w^Tx_i+b)\ge margin,  \forall i" eeimg="1"&gt;&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%7C%7Cw%7C%7C%3D1" alt="||w||=1" eeimg="1"&gt;就是为了表达方便[3]，后面会取消这个限制&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;到这里为止已经说完了所有关于SVM的直观了解，如果不想看求解，可以跳过下面一大段直接到objective function。&lt;br&gt;&lt;br&gt;直接表达虽然清楚但是求解无从下手。做一些简单地等价变换（分母倒上来）可以得到 &lt;b&gt;II. carnonical representation &lt;/b&gt;（敲黑板&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmin_%7Bboundary%7D%7C%7Cw%7C%7C" alt="\arg\min_{boundary}||w||" eeimg="1"&gt;&lt;br&gt;subject to 所有苹果和香蕉到boundary的距离&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cge+margin" alt="\ge margin" eeimg="1"&gt;&lt;/blockquote&gt;w不过是个定义直线的参数，知道这一步是等价变换出来的表达就可以了。&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 为了以后推导方便，一般写成：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Carg%5Cmin_%7Bw%2Cb%7D%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2" alt="\arg\min_{w,b}\frac{1}{2}||w||^2" eeimg="1"&gt;&lt;br&gt;&lt;br&gt;   subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=y_i%28w%5ETx_i%2Bb%29%5Cge+1%2C++%5Cforall+i" alt="y_i(w^Tx_i+b)\ge 1,  \forall i" eeimg="1"&gt;&lt;br&gt;# 这个『1』就是一个常数，这样设置是为了以后的方便&lt;br&gt;# 这个选择的自由来自于直线&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=y%28x%29+%3D+w%5ETx%2Bb" alt="y(x) = w^Tx+b" eeimg="1"&gt;的参数如果rescale成&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=kw" alt="kw" eeimg="1"&gt;和&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=kb" alt="kb" eeimg="1"&gt;不改变距离。&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;要得到&lt;b&gt;III. dual representation&lt;/b&gt;之前需要大概知道一下拉格朗日乘子法 (method of lagrange multiplier)，它是用在有各种约束条件(各种"subject to")下的目标函数，也就是直接可以求导可以引出dual representation（怎么还没完摔）&lt;br&gt;&lt;br&gt;# ========== 数学表达 begin ========== #&lt;br&gt;# 稍微解释一下使用拉格朗日乘子法的直观理解，不作深入讨论&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2-%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_n%2A%5Cleft%5C%7B+y_n%5Cleft%28+w%5ETx_n%2Bb+%5Cright%29+-1+%5Cright%5C%7D+%7D+" alt="L=\frac{1}{2}||w||^2-\sum_{n=1}^{N}{a_n*\left\{ y_n\left( w^Tx_n+b \right) -1 \right\} } " eeimg="1"&gt;, 其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=a_n%3E0" alt="a_n&amp;gt;0" eeimg="1"&gt;是橙子（划去）乘子[1]&lt;br&gt;# 可以这样想：(1) 我们的两个任务：①对参数最小化L（解SVM要求），②对乘子又要最大化（拉格朗日乘子法要求）， (2) 如果上面的约束条件成立，整个求和都是非负的，很好L是可以求最小值的；(3) 约束条件不成立，又要对乘子最大化，全身非负的L直接原地爆炸&lt;br&gt;# 好棒棒，所以解题一定要遵守基本法&lt;br&gt;# ① 先搞定第一个任务对w,b最小化L&lt;br&gt;# 凸优化直接取导 =&amp;gt; 志玲（划去）置零，得到：&lt;br&gt;# &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=w%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_nx_n%7D+" alt="w=\sum_{n=1}^{N}{a_ny_nx_n} " eeimg="1"&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=0%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_n%7D+" alt="0=\sum_{n=1}^{N}{a_ny_n} " eeimg="1"&gt;&lt;br&gt;# ② 第二个任务对a最大化L，就是dual representation了&lt;br&gt;# ========== 数学表达 end ========== #&lt;br&gt;&lt;br&gt;稍微借用刚刚数学表达里面的内容看个有趣的东西：&lt;br&gt;还记得我们怎么预测一个新的水果是苹果还是香蕉吗？我们代入到分界的直线里，然后通过符号来判断。&lt;br&gt;刚刚w已经被表达出来了也就是说这个直线现在变成了：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=y%28x_0%29+%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_nx_n%5ETx_0%7D+%2Bb" alt="y(x_0) =\sum_{n=1}^{N}{a_ny_nx_n^Tx_0} +b" eeimg="1"&gt;&lt;br&gt;看似仿佛用到了所有的训练水果，但是其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=a_n%3D0" alt="a_n=0" eeimg="1"&gt;的水果都没有起到作用，剩下的就是小部分靠边边的Support vectors呀。&lt;br&gt;&lt;br&gt;&lt;b&gt;III. dual representation&lt;/b&gt;&lt;br&gt;把①的结果代回去就可以得到[1]：&lt;br&gt;&lt;blockquote&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cmax_%7Ball%5C+a_n%7D+L%28a%29%3D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_n%7D+%0A-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Csum_%7Bm%3D1%7D%5E%7BN%7D%7Ba_na_my_ny_mx_n%5ETx_m%7D+%7D+" alt="\max_{all\ a_n} L(a)=\sum_{n=1}^{N}{a_n} 
-\frac{1}{2}\sum_{n=1}^{N}{\sum_{m=1}^{N}{a_na_my_ny_mx_n^Tx_m} } " eeimg="1"&gt;&lt;br&gt;subject to &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=a_n+%5Cge0%2C%5Cforall+n" alt="a_n \ge0,\forall n" eeimg="1"&gt;, &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7Ba_ny_n%7D%3D0+" alt="\sum_{n=1}^{N}{a_ny_n}=0 " eeimg="1"&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;b&gt;如果香蕉和苹果不能用直线分割呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/242109e537a220855b33184a6f8de554_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_r.jpg"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_r.jpg" data-actualsrc="https://pic1.zhimg.com/242109e537a220855b33184a6f8de554_b.jpg"&gt;&lt;br&gt;Kernel trick. &lt;br&gt;其实用直线分割的时候我们已经使用了kernel，那就是线性kernel, &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=k%28x_1%2Cx_2%29+%3D+x_1%5ETx_2" alt="k(x_1,x_2) = x_1^Tx_2" eeimg="1"&gt;&lt;br&gt;如果要替换kernel那么把目标函数里面的内积全部替换成新的kernel function就好了，就是这么简单。&lt;br&gt;高票答案武侠大师的比喻已经说得很直观了，低维非线性的分界线其实在高维是可以线性分割的，可以理解为——『你们是虫子！』分得开个p...（大雾）&lt;br&gt;&lt;br&gt;&lt;b&gt;如果香蕉和苹果有交集呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ca45458396bf807868674316793205b7_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_r.jpg"&gt;松弛变量 (slack variable &lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_r.jpg" data-actualsrc="https://pic4.zhimg.com/ca45458396bf807868674316793205b7_b.jpg"&gt;松弛变量 (slack variable &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cxi%5Cge0" alt="\xi\ge0" eeimg="1"&gt;)&lt;br&gt;松弛变量允许错误的分类，但是要付出代价。图中以苹果为例，错误分类的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cxi%3E1" alt="\xi&amp;gt;1" eeimg="1"&gt;；在margin当中但是正确分类的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=0%3C%5Cxi%5Cle+1" alt="0&amp;lt;\xi\le 1" eeimg="1"&gt;；正确分类并且在margin外面的苹果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cxi%3D0" alt="\xi=0" eeimg="1"&gt;。可以看出每一个数据都有一一对应的惩罚。&lt;br&gt;对于这一次整体的惩罚力度，要另外使用一个超参数 (&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"&gt;) 来衡量这一次分类的penalty程度。&lt;br&gt;从新的目标函数里可见一斑[1]：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cmin+%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%2B+%5Cfrac%7B%5Cgamma%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Cxi_n%5E2%7D+" alt="\min \frac{1}{2}||w||^2 + \frac{\gamma}{2}\sum_{n=1}^{N}{\xi_n^2} " eeimg="1"&gt;&lt;br&gt;（约束条件略）&lt;br&gt;&lt;br&gt;&lt;b&gt;如果还有梨呢？&lt;/b&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_b.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_r.jpg"&gt;可以每个类别做一次SVM：是苹果还是不是苹果？是香蕉还是不是香蕉？是梨子还是不是梨子？从中选出可能性最大的。这是one-versus-the-rest approach。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1280" data-rawheight="880" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_r.jpg" data-actualsrc="https://pic3.zhimg.com/b3dec8344863f8e993abdf86cef4c856_b.jpg"&gt;可以每个类别做一次SVM：是苹果还是不是苹果？是香蕉还是不是香蕉？是梨子还是不是梨子？从中选出可能性最大的。这是one-versus-the-rest approach。&lt;br&gt;也可以两两做一次SVM：是苹果还是香蕉？是香蕉还是梨子？是梨子还是苹果？最后三个分类器投票决定。这是one-versus-one approace。&lt;br&gt;但这其实都多多少少有问题，比如苹果特别多，香蕉特别少，我就无脑判断为苹果也不会错太多；多个分类器要放到一个台面上，万一他们的scale没有在一个台面上也未可知。&lt;br&gt;&lt;br&gt;这时候我们再回过头看一下思维导图划一下重点：&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg"&gt;课后习题：&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="3085" data-rawheight="1280" class="origin_image zh-lightbox-thumb lazy" width="3085" data-original="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_r.jpg" data-actualsrc="https://pic3.zhimg.com/7e49cf765eb3680c85185bc30a9db196_b.jpg"&gt;课后习题：&lt;br&gt;1. vector不愿意support怎么办？&lt;br&gt;2. 苹果好吃还是香蕉好吃？&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;最后送一张图我好爱哈哈哈 (Credit: Burr Settles)&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_b.png" data-rawwidth="824" data-rawheight="734" class="origin_image zh-lightbox-thumb" width="824" data-original="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="824" data-rawheight="734" class="origin_image zh-lightbox-thumb lazy" width="824" data-original="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_r.png" data-actualsrc="https://pic3.zhimg.com/a720d60ae40fd1612f4d458ca963ce66_b.png"&gt;&lt;br&gt;&lt;p&gt;[1] Bishop C M. Pattern recognition[J]. Machine Learning, 2006, 128.&lt;/p&gt;&lt;p&gt;[2] Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. Springer, Berlin: Springer series in statistics, 2001.&lt;/p&gt;&lt;p&gt;[3] James G, Witten D, Hastie T, et al. An introduction to statistical learning[M]. New York: springer, 2013.&lt;/p&gt;
&lt;/div&gt;</description><author>靠靠靠谱</author><pubDate>2016-08-22</pubDate></item><item><title>为什么 Deep Learning 最先在语音识别和图像处理领域取得突破？</title><link>https://m.zhihu.com/question/21815490/answer/26331835</link><description>&lt;div class="zm-editable-content"&gt;为什么在其他领域则进展没有那么大？是由于这种技术的什么特征导致的？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
以下观点纯粹为感受, 而非科学:&lt;br&gt;&lt;ul&gt;&lt;li&gt;DL 适合处理&lt;b&gt;感知&lt;/b&gt;, 而非&lt;b&gt;逻辑&lt;/b&gt;&lt;/li&gt;&lt;li&gt;感知与逻辑的重要区别在于输入数据在输入空间中做&lt;b&gt;连续变化还是离散变化 &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;图片/语音这类感知问题中, 输入的都是裸的信号, 这一点的好处在于, 输入数据具有连续性:&lt;br&gt;一个苹果, 它稍微胖一点瘦一点红一点, 点几个噪点上去, 对于人类来说仍然是苹果.&lt;br&gt;一句话, 稍微大声一点尖锐一点卡顿一点加点噪声变点音色, 对于人类来说仍然是这句话.&lt;br&gt;也即: 输入数据可以在它的&lt;b&gt;小邻域内做连续变化而不改变自身意义, &lt;/b&gt;或者说输入点可以做小的扰动而不改变自身意义&lt;br&gt;&lt;br&gt;然而对于其他问题, 如NLP, 推荐系统, 乱七八糟的DM问题, 输入数据不再是裸的&lt;b&gt;信号&lt;/b&gt;了, 人类还没有找到很好对这些问题的输入数据的描述方式, 也即feature, 使得这种描述的&lt;b&gt;信息损失很小&lt;/b&gt;, 且具有&lt;b&gt;连续变化&lt;/b&gt;, 或者说&lt;b&gt;抗扰动&lt;/b&gt;的能力, 同时这种描述最好别在输入空间中太sparse..&lt;br&gt;比如说, NLP 里如果要给document分类, 或者识别'情绪'什么的, 还是有解决的比较好的..因为这个问题抗扰动: document里多几个词少几个词不影响分类. 同时对于人类来说, 它比很多问题更像一个感知问题: 扫一眼文章就可以大致知道它的类别. 最近比较火的image description, 也比较类似于这种.&lt;br&gt;反之, 如果给一句话, 标POS, 指代消解等等...这些逻辑问题实在是太不抗扰动了, 几乎没人在用ML方法做...&lt;br&gt;推荐问题的输入算是抗扰动了吧..但是感觉又太sparse了. 你看一张图片的输入才几千个dimension..&lt;br&gt;&lt;br&gt;毕竟神经网络啊..还有其他不少ML算法..其实就是在输入空间中找一个很可能很扭曲的manifold把人标好的那些数据点强行连到一起. 当然manifold一定是&lt;b&gt;连续的&lt;/b&gt;, 所以如果数据点和它邻域的点就已经不在一类里了那这个manifold得多扭曲? 如果维度太高数据点太sparse这个manifold轻易就拟合上了那得多废柴...&lt;br&gt;&lt;br&gt;话说前段时间刚看到一个paper已经说了, 即使做图像, 神经网络搞出来的manifold其实已经很扭曲了....  &lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6199" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1312.6199&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;  做一做perturbation就可以分错类..
&lt;/div&gt;</description><author>吴育昕</author><pubDate>2016-08-22</pubDate></item><item><title>如何高效的学习TensorFlow代码?</title><link>https://m.zhihu.com/question/41667903/answer/99268024</link><description>&lt;div class="zm-editable-content"&gt;如题，或者如何掌握TensorFlow，应用到任何领域？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
也在学习中。&lt;br&gt;个人感觉先把TensorFlow的白皮书：&lt;a href="//link.zhihu.com/?target=http%3A//download.tensorflow.org/paper/whitepaper2015.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;download.tensorflow.org&lt;/span&gt;&lt;span class="invisible"&gt;/paper/whitepaper2015.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;这个论文整明白了，大致模型就明白了。然后学习demo。最后再深入整个代码。&lt;br&gt;白皮书有个快翻译完的中文版：&lt;a href="//link.zhihu.com/?target=http%3A//www.jianshu.com/p/65dc64e4c81f" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;[译] TensorFlow 白皮书&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;------------------------------------------------------------------------------------------------------------&lt;br&gt;最近陆续更新了一些学习笔记，与初学者共享：&lt;br&gt;&lt;a href="//link.zhihu.com/?target=http%3A//blog.csdn.net/snsn1984" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;SHINING的博客&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>小乖他爹</author><pubDate>2016-08-22</pubDate></item><item><title>CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？</title><link>https://m.zhihu.com/question/34681168/answer/84061846</link><description>&lt;div class="zm-editable-content"&gt;CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？以及他们的主要用途是什么？只知道CNN是局部感受和参数共享，比较适合用于图像这方面。刚入门的小白真心求助&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;p&gt;首先，我感觉不必像 @李Shawn 同学一样认为DNN、CNN、RNN完全不能相提并论。从广义上来说，NN（或是更美的DNN）确实可以认为包含了CNN、RNN这些具体的变种形式。在实际应用中，所谓的深度神经网络DNN，往往融合了多种已知的结构，包括卷积层或是LSTM单元。但是就题主的意思来看，这里的DNN应该特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。因此，题主一定要将DNN、CNN、RNN等进行对比，也未尝不可。&lt;/p&gt;&lt;p&gt;其实，如果我们顺着神经网络技术发展的脉络，就很容易弄清这几种网络结构发明的初衷，和他们之间本质的区别，希望对题主有所帮助。&lt;/p&gt;&lt;p&gt;=========================== 分 割 线 就 是 我
================================&lt;/p&gt;&lt;br&gt;&lt;p&gt;神经网络技术起源于上世纪五、六十年代，当时叫&lt;b&gt;感知机&lt;/b&gt;（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。&lt;i&gt;（扯一个不相关的：由于计算技术的落后，当时感知器传输函数是用线拉动变阻器改变电阻的方法机械实现的，脑补一下科学家们扯着密密麻麻的导线的样子…）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，即它对稍复杂一些的函数都无能为力（比如最为典型的“异或”操作）。连异或都不能拟合，你还能指望这货有什么实际用途么o(╯□╰)o&lt;/p&gt;&lt;br&gt;&lt;p&gt;随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人（反正就是一票大牛）发明的&lt;b&gt;多层感知机&lt;/b&gt;（multilayer
perceptron）克服。多层感知机，顾名思义，就是有多个隐含层的感知机（废话……）。好好，我们看一下多层感知机的结构：&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_b.png" data-rawwidth="866" data-rawheight="249" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="249" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_r.png" data-actualsrc="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图1&lt;/i&gt;&lt;/b&gt;&lt;i&gt;上下层神经元全部相连的神经网络——多层感知机&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这货就是我们现在所说的&lt;b&gt;神经网络&lt;/b&gt;&lt;b&gt;NN&lt;/b&gt;——神经网络听起来不知道比感知机高端到哪里去了！这再次告诉我们起一个好听的名字对于研（zhuang）究（bi）很重要！&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。相信年轻如Hinton当时一定是春风得意。&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机给我们带来的启示是，&lt;b&gt;神经网络的层数直接决定了它对现实的刻画能力&lt;/b&gt;——利用每层更少的神经元拟合更加复杂的函数[1]。&lt;/p&gt;&lt;p&gt;（Bengio如是说：functions that can be compactly
represented by a depth k architecture might require an exponential number of
computational elements to be represented by a depth k − 1 architecture.）&lt;/p&gt;&lt;br&gt;&lt;p&gt;即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，&lt;b&gt;优化函数越来越容易陷入局部最优解&lt;/b&gt;，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，&lt;b&gt;“梯度消失”现象更加严重&lt;/b&gt;。具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。&lt;/p&gt;&lt;br&gt;&lt;p&gt;2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层[2]，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。单从结构上来说，&lt;b&gt;全连接的&lt;/b&gt;&lt;b&gt;DNN&lt;/b&gt;&lt;b&gt;和图&lt;/b&gt;&lt;b&gt;1&lt;/b&gt;&lt;b&gt;的多层感知机是没有任何区别的&lt;/b&gt;。&lt;/p&gt;&lt;br&gt;&lt;p&gt;值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度消失，网络层数达到了前所未有的一百多层（深度残差学习：152层）[3,4]！具体结构题主可自行搜索了解。如果你之前在怀疑是不是有很多方法打上了“深度学习”的噱头，这个结果真是深得让人心服口服。&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_b.png" data-rawwidth="866" data-rawheight="1228" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="1228" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_r.png" data-actualsrc="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图2&lt;/i&gt;&lt;/b&gt;&lt;i&gt;缩减版的深度残差学习网络，仅有34&lt;/i&gt;&lt;i&gt;层，终极版有152&lt;/i&gt;&lt;i&gt;层，自行感受一下&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;如图1所示，我们看到&lt;b&gt;全连接&lt;/b&gt;&lt;b&gt;DNN&lt;/b&gt;&lt;b&gt;的结构里下层神经元和所有上层神经元都能够形成连接&lt;/b&gt;，带来的潜在问题是&lt;b&gt;参数数量的膨胀&lt;/b&gt;。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出题主所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是&lt;b&gt;通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。&lt;/b&gt;两层之间的卷积传输的示意图如下：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/440765dbaab356739fb855834f901e7d_b.png" data-rawwidth="866" data-rawheight="457" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="457" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_r.png" data-actualsrc="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图3&lt;/i&gt;&lt;/b&gt;&lt;i&gt;卷积神经网络隐含层（摘自Theano&lt;/i&gt;&lt;i&gt;教程）&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;通过一个例子简单说明卷积神经网络的结构。假设图3中m-1=1是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100*100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100*100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_b.png" data-rawwidth="866" data-rawheight="203" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="203" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_r.png" data-actualsrc="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图4&lt;/i&gt;&lt;/b&gt;&lt;i&gt;一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层（摘自Theano&lt;/i&gt;&lt;i&gt;教程）&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;在这个例子里，我们注意到&lt;b&gt;输入层到隐含层的参数瞬间降低到了&lt;/b&gt;&lt;b&gt;100*100*100=10^6&lt;/b&gt;&lt;b&gt;个&lt;/b&gt;！这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于&lt;b&gt;CNN&lt;/b&gt;&lt;b&gt;模型限制参数了个数并挖掘了局部结构的这个特点&lt;/b&gt;。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。&lt;/p&gt;&lt;br&gt;&lt;p&gt;全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，&lt;b&gt;样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要&lt;/b&gt;。对了适应这种需求，就出现了题主所说的另一种神经网络结构——循环神经网络RNN。&lt;/p&gt;&lt;br&gt;&lt;p&gt;在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。而在&lt;b&gt;RNN&lt;/b&gt;&lt;b&gt;中，神经元的输出可以在下一个时间戳直接作用到自身&lt;/b&gt;，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_b.png" data-rawwidth="866" data-rawheight="441" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="441" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_r.png" data-actualsrc="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图5&lt;/i&gt;&lt;/b&gt;&lt;i&gt; RNN&lt;/i&gt;&lt;i&gt;网络结构&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_b.png" data-rawwidth="866" data-rawheight="348" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="348" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_r.png" data-actualsrc="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图6&lt;/i&gt;&lt;/b&gt;&lt;i&gt; RNN&lt;/i&gt;&lt;i&gt;在时间上进行展开&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Cool，&lt;b&gt;（&lt;/b&gt;&lt;b&gt;t+1&lt;/b&gt;&lt;b&gt;）时刻网络的最终结果O(t+1)&lt;/b&gt;&lt;b&gt;是该时刻输入和所有历史共同作用的结果&lt;/b&gt;！这就达到了对时间序列建模的目的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，&lt;b&gt;“梯度消失”现象又要出现了，只不过这次发生在时间轴上&lt;/b&gt;。对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。&lt;/p&gt;&lt;br&gt;&lt;p&gt;为了解决时间上的梯度消失，机器学习领域发展出了&lt;b&gt;长短时记忆单元&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;，通过门的开关实现时间上记忆功能，并防止梯度消失&lt;/b&gt;，一个LSTM单元长这个样子：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_b.png" data-rawwidth="866" data-rawheight="555" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="555" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_r.png" data-actualsrc="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图7 &lt;/i&gt;&lt;/b&gt;&lt;i&gt;LSTM&lt;/i&gt;&lt;i&gt;的模样&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;除了题主疑惑的三种网络，和我之前提到的深度残差学习、LSTM外，深度学习还有许多其他的结构。举个例子，RNN既然能继承历史信息，是不是也能吸收点未来的信息呢？因为在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的。因此就有了&lt;b&gt;双向&lt;/b&gt;&lt;b&gt;RNN&lt;/b&gt;&lt;b&gt;、双向&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;，同时利用历史和未来的信息。&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_b.png" data-rawwidth="866" data-rawheight="365" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="866" data-rawheight="365" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_r.png" data-actualsrc="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图8&lt;/i&gt;&lt;/b&gt;&lt;i&gt;双向RNN&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;事实上，&lt;b&gt;不论是那种网络，他们在实际应用中常常都混合着使用，比如&lt;/b&gt;&lt;b&gt;CNN&lt;/b&gt;&lt;b&gt;和RNN&lt;/b&gt;&lt;b&gt;在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。&lt;/b&gt;不难想象随着深度学习热度的延续，更灵活的组合方式、更多的网络结构将被发展出来。尽管看起来千变万化，但研究者们的出发点肯定都是为了解决特定的问题。题主如果想进行这方面的研究，不妨仔细分析一下这些结构各自的特点以及它们达成目标的手段。入门的话可以参考：&lt;/p&gt;&lt;p&gt;Ng写的Ufldl：&lt;a href="//link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/UFLDL%25E6%2595%2599%25E7%25A8%258B" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;UFLDL教程 - Ufldl&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;也可以看Theano内自带的教程，例子非常具体：&lt;a href="//link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning Tutorials&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;欢迎大家继续推荐补充。&lt;/p&gt;&lt;p&gt;当然啦，如果题主只是想凑个热闹时髦一把，或者大概了解一下方便以后把妹使，这样看看也就罢了吧。&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;参考文献：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[1]  
Bengio Y. Learning Deep
Architectures for AI[J]. Foundations &amp;amp; Trends® in Machine Learning, 2009,
2(1):1-127.&lt;/p&gt;&lt;p&gt;[2]  
Hinton G E, Salakhutdinov R R.
Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006,
313(5786):504-507.&lt;/p&gt;&lt;p&gt;[3]  
He K, Zhang X, Ren S, Sun J. Deep
Residual Learning for Image Recognition. arXiv:1512.03385, 2015.&lt;/p&gt;&lt;p&gt;[4]  
Srivastava R K, Greff K,
Schmidhuber J. Highway networks. arXiv:1505.00387, 2015.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;【“科研君”公众号初衷始终是希望聚集各专业一线科研人员和工作者，在进行科学研究的同时也作为知识的传播者，利用自己的专业知识解释和普及生活中的 一些现象和原理，展现科学有趣生动的一面。该公众号由清华大学一群在校博士生发起，目前参与的作者人数有10人，但我们感觉这远远不能覆盖所以想科普的领域，并且由于空闲时间有限，导致我们只能每周发布一篇文章。我们期待更多的战友加入，认识更多志同道合的人，每个人都是科研君，每个人都是知识的传播者。我们期待大家的参与，想加入我们，进QQ群吧~：108141238】&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;【非常高兴看到大家喜欢并赞同我们的回答。应许多知友的建议，最近我们开通了同名公众号：&lt;b&gt;PhDer&lt;/b&gt;，也会定期更新我们的文章，如果您不想错过我们的每篇回答，欢迎扫码关注~ 】&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//weixin.qq.com/r/5zsuNoHEZdwarcVV9271" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;weixin.qq.com/r/5zsuNoH&lt;/span&gt;&lt;span class="invisible"&gt;EZdwarcVV9271&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; (二维码自动识别)&lt;/p&gt;
&lt;/div&gt;</description><author>科研君</author><pubDate>2016-08-22</pubDate></item><item><title>有哪些LSTM(Long Short Term Memory)和RNN(Recurrent)网络的教程？</title><link>https://m.zhihu.com/question/29411132/answer/51515231</link><description>&lt;div class="zm-editable-content"&gt;不需要面面俱到，只需把他们解决什么问题，训练的过程是怎样的讲清楚就好。最好看完后就能直接上手写代码。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;p&gt;刚好毕设相关，论文写完顺手就答了&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;先给出一个最快的了解+上手的教程：&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;    直接看theano官网的LSTM教程+代码：&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/tutorial/lstm.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;LSTM Networks for Sentiment Analysis&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;但是，前提是你有RNN的基础，因为LSTM本身不是一个完整的模型，LSTM是对RNN隐含层的改进。一般所称的LSTM网络全叫全了应该是使用LSTM单元的RNN网络。教程就给了个LSTM的图，它只是RNN框架中的一部分，如果你不知道RNN估计看不懂。&lt;/p&gt;&lt;p&gt;    比较好的是，你只需要了解前馈过程，你都不需要自己求导就能写代码使用了。&lt;/p&gt;&lt;p&gt;    补充，今天刚发现一个中文的博客：&lt;a href="//link.zhihu.com/?target=http%3A//blog.csdn.net/a635661820/article/details/45390671" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;LSTM简介以及数学推导(FULL BPTT)&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;    不过，稍微深入下去还是得老老实实的好好学，下面是我认为比较好的&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;完整&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;学习流程&lt;/b&gt;：&lt;/p&gt;&lt;br&gt;&lt;p&gt;    我一直都觉得了解一个模型的前世今生对模型理解有巨大的帮助。到LSTM这里（假设题主零基础）那比较好的路线是MLP-&amp;gt;RNN-&amp;gt;LSTM。还有LSTM本身的发展路线（97年最原始的LSTM到forget gate到peephole再到CTC ）&lt;/p&gt;&lt;p&gt;    按照这个路线学起来会比较顺，所以我优先推荐的两个教程都是按照这个路线来的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;多伦多大学的 Alex Graves 的RNN专著&lt;i&gt;《Supervised Sequence Labelling with Recurrent Neural
Networks》&lt;/i&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt; Felix Gers的博士论文&lt;i&gt;《Long short-term memory in recurrent neural networks》&lt;/i&gt;&lt;/li&gt;&lt;/ol&gt;这两个内容都挺多的，不过可以跳着看，反正我是没看完
┑(￣Д
￣)┍&lt;br&gt;&lt;p&gt;还有一个最新的（今年2015）的综述，&lt;i&gt;《A
Critical Review of Recurrent Neural Networks for Sequence Learning》&lt;/i&gt;不过很多内容都来自以上两个材料。&lt;/p&gt;&lt;p&gt;    其他可以当做教程的材料还有：&lt;/p&gt;&lt;p&gt;&lt;i&gt;《From
Recurrent Neural Network to Long Short Term Memory Architecture Application to
Handwriting Recognition Author》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Generating Sequences With Recurrent Neural Networks》&lt;/i&gt;（这个有对应源码，虽然实例用法是错的，自己用的时候还得改代码，主要是摘出一些来用，供参考）&lt;/p&gt;&lt;br&gt;&lt;p&gt;然后呢，可以开始编码了。除了前面提到的theano教程还有一些论文的开源代码，到github上搜就好了。&lt;/p&gt;&lt;br&gt;&lt;p&gt;顺便安利一下theano，theano的自动求导和GPU透明对新手以及学术界研究者来说非常方便，LSTM拓扑结构对于求导来说很复杂，上来就写LSTM反向求导还要GPU编程代码非常费时间的，而且搞学术不是实现一个现有模型完了，得尝试创新，改模型，每改一次对应求导代码的修改都挺麻烦的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;其实到这应该算是一个阶段了，如果你想继续深入可以具体看看几篇经典论文，比如LSTM以及各个改进对应的经典论文。&lt;/p&gt;&lt;br&gt;&lt;p&gt;还有楼上提到的&lt;i&gt;《LSTM: A Search Space Odyssey》&lt;/i&gt; 通过从新进行各种实验来对比考查LSTM的各种改进（组件）的效果。挺有意义的，尤其是在指导如何使用LSTM方面。&lt;/p&gt;&lt;p&gt;不过，玩LSTM，最好有相应的硬件支持。我之前用Titan 780，现在实验室买了Titan X，应该可以说是很好的配置了（TitanX可以算顶配了）。但是我任务数据量不大跑一次实验都要好几个小时（前提是我独占一个显卡），（当然和我模型复杂有关系，LSTM只是其中一个模块）。&lt;/p&gt;&lt;br&gt;&lt;p&gt;===========================================&lt;/p&gt;&lt;p&gt;如果想玩的深入一点可以看看LSTM最近的发展和应用。老的就不说了，就提一些比较新比较好玩的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;LSTM网络本质还是RNN网络，基于LSTM的RNN架构上的变化有最先的BRNN（双向），还有今年Socher他们提出的树状LSTM用于情感分析和句子相关度计算&lt;i&gt;《Improved Semantic Representations From Tree-Structured Long
Short-Term Memory Networks》&lt;/i&gt;（类似的还有一篇，不过看这个就够了）。他们的代码用Torch7实现，我为了整合到我系统里面自己实现了一个，但是发现效果并不好。我觉的这个跟用于建树的先验信息有关，看是不是和你任务相关。还有就是感觉树状LSTM对比BLSTM是有信息损失的，因为只能使用到子节点信息。要是感兴趣的话，这有一篇树状和线性RNN对比&lt;i&gt;《(treeRNN vs seqRNN )When Are Tree Structures Necessary for Deep
Learning of Representations?》&lt;/i&gt;。当然，关键在于树状这个概念重要，感觉现在的研究还没完全利用上树状的潜力。&lt;/p&gt;&lt;br&gt;&lt;p&gt;今年ACL（2015）上有一篇层次的LSTM&lt;i&gt;《A
Hierarchical Neural Autoencoder for Paragraphs and Documents》&lt;/i&gt;。使用不同的LSTM分别处理词、句子和段落级别输入，并使用自动编码器（autoencoder）来检测LSTM的文档特征抽取和重建能力。&lt;/p&gt;&lt;br&gt;还有一篇文章&lt;i&gt;《Chung J, Gulcehre C, Cho K, et al. Gated feedback recurrent neural networks[J]. arXiv preprint arXiv:1502.02367, 2015.&lt;/i&gt;&lt;i&gt;》&lt;/i&gt;，把gated的思想从记忆单元扩展到了网络架构上，提出多层RNN各个层的隐含层数据可以相互利用（之前的多层RNN多隐含层只是单向自底向上连接），不过需要设置门（gated）来调节。&lt;br&gt;&lt;br&gt;&lt;p&gt;记忆单元方面，Bahdanau
Dzmitry他们在构建RNN框架的机器翻译模型的时候使用了GRU单元（gated recurrent unit）替代LSTM，其实LSTM和GRU都可以说是gated hidden unit。两者效果相近，但是GRU相对LSTM来说参数更少，所以更加不容易过拟合。（大家堆模型堆到dropout也不管用的时候可以试试换上GRU这种参数少的模块）。这有篇比较的论文&lt;i&gt;《（GRU/LSTM对比）Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
Modeling》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;应用嘛，宽泛点来说就是挖掘序列数据信息，大家可以对照自己的任务有没有这个点。比如（直接把毕设研究现状搬上来(｡･∀･)ﾉﾞ）：&lt;/p&gt;&lt;br&gt;&lt;p&gt;先看比较好玩的，&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像处理（对，不用CNN用RNN）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Visin F, Kastner K,
Cho K, et al. ReNet: A Recurrent Neural Network Based Alternative to
Convolutional Networks[J]. arXiv preprint arXiv:1505.00393, 2015》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;4向RNN（使用LSTM单元）替代CNN。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;使用LSTM读懂python程序：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Zaremba W, Sutskever I.
Learning to execute[J]. arXiv preprint arXiv:1410.4615, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用基于LSTM的深度模型用于读懂python程序并且给出正确的程序输出。文章的输入是短小简单python程序，这些程序的输出大都是简单的数字，例如0-9之内加减法程序。模型一个字符一个字符的输入python程序，经过多层LSTM后输出数字结果，准确率达到99%&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;手写识别：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Liwicki M, Graves A,
Bunke H, et al. A novel approach to on-line handwriting recognition based on
bidirectional long short-term memory》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;机器翻译：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Sutskever I, Vinyals
O, Le Q V V. Sequence to sequence learning with neural networks[C]//Advances in
neural information processing systems. 2014: 3104-3112.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用多层LSTM构建了一个seq2seq框架（输入一个序列根据任务不同产生另外一个序列），用于机器翻译。先用一个多层LSTM从不定长的源语言输入中学到特征v。然后使用特征v和语言模型（另一个多层LSTM）生成目标语言句子。&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using rnn encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这篇文章第一次提出GRU和RNN encoder-decoder框架。使用RNN构建编码器-解码器（encoder-decoder）框架用于机器翻译。文章先用encoder从不定长的源语言输入中学到固定长度的特征V，然后decoder使用特征V和语言模型解码出目标语言句子&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上两篇文章提出的seq2seq和encoder-decoder这两个框架除了在机器翻译领域，在其他任务上也被广泛使用。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在上一篇的基础上引入了BRNN用于抽取特征和注意力信号机制（attention signal）用于源语言和目标语言的对齐。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;对话生成：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Shang L, Lu Z, Li H. Neural Responding Machine for Short-Text Conversation[J]. arXiv preprint arXiv:1503.02364, 2015.》&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;华为诺亚方舟实验室，李航老师他们的作品。基本思想是把对话看成是翻译过程。然后借鉴Bahdanau D他们的机器翻译方法（&lt;b&gt;encoder-decoder，GRU&lt;/b&gt;，attention signal）解决。训练使用微博评论数据。&lt;/p&gt;&lt;p&gt;&lt;i&gt;《VINYALS O, LE Q，.A Neural Conversational Model[J]. arXiv:1506.05869 [cs], 2015.》&lt;/i&gt;&lt;/p&gt;google前两天出的论文（2015-6-19）。看报道说结果让人觉得“creepy”：&lt;a href="//link.zhihu.com/?target=http%3A//motherboard.vice.com/read/googles-new-chatbot-taught-itself-to-be-creepy" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Google's New Chatbot Taught Itself to Be Creepy&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 。还以为有什么NB模型，结果看了论文发现就是一套用&lt;b&gt;seq2seq框架&lt;/b&gt;的实验报告。（对话可不是就是你一句我一句，一个序列对应产生另一序列么）。论文里倒是说的挺谨慎的，只是说纯数据驱动（没有任何规则）的模型能做到这样不错了，但还是有很多问题，需要大量修改（加规则呗？）。主要问题是缺乏上下文一致性。（模型只用对话的最后一句来产生下一句也挺奇怪的，为什么不用整个对话的历史信息？）&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;句法分析：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Vinyals O, Kaiser L,
Koo T, et al. Grammar as a foreign language[J]. arXiv preprint arXiv:1412.7449,
2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;把LSTM用于句法分析任务，文章把树状的句法结构进行了线性表示，从而把句法分析问题转成翻译问题，然后套用机器翻译的seq2seq框架使用LSTM解决。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;信息检索：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Palangi H, Deng L,
Shen Y, et al. Deep Sentence Embedding Using the Long Short Term Memory Network:
Analysis and Application to Information Retrieval[J]. arXiv preprint
arXiv:1502.06922, 2015.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用LSTM获得大段文本或者整个文章的特征向量，用点击反馈来进行弱监督，最大化query的特性向量与被点击文档的特性向量相似度的同时最小化与其他未被点击的文档特性相似度。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;图文转换：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;图文转换任务看做是特殊的图像到文本的翻译问题，还是使用encoder-decoder翻译框架。不同的是输入部分使用卷积神经网络（Convolutional Neural Networks，CNN）抽取图像的特征，输出部分使用LSTM生成文本。对应论文有：&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Karpathy A, Fei-Fei L. Deep
visual-semantic alignments for generating image descriptions[J]. arXiv preprint
arXiv:1412.2306, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Mao J, Xu W, Yang Y, et al. Deep
captioning with multimodal recurrent neural networks (m-rnn)[J]. arXiv preprint
arXiv:1412.6632, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Vinyals O, Toshev A, Bengio S, et al. Show and
tell: A neural image caption generator[J]. arXiv preprint arXiv:1411.4555,
2014.》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;就粘这么多吧，呼呼~复制粘贴好爽\(^o^)/~&lt;/p&gt;&lt;p&gt;其实，相关工作还有很多，各大会议以及arxiv上不断有新文章冒出来，实在是读不过来了。。。&lt;/p&gt;&lt;br&gt;&lt;p&gt;然而我有种预感，说了这么多，工作之后很有可能发现：&lt;/p&gt;&lt;p&gt; 这些东西对我工作并没有什么卵用
(＞﹏＜＝&lt;/p&gt;
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-08-22</pubDate></item><item><title>如何评价Word2Vec作者提出的fastText算法？深度学习是否在文本分类等简单任务上没有优势？</title><link>https://m.zhihu.com/question/48345431/answer/111513229</link><description>&lt;div class="zm-editable-content"&gt;Word2Vec作者Mikolov在预印本（Bag of Tricks for Efficient Text Classification，&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.01759v2.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/pdf/1607.0175&lt;/span&gt;&lt;span class="invisible"&gt;9v2.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）中提出了fastText文本分类方法，可以在普通CPU上快速训练，结果与深度学习训练出来的模型类似。深度学习是否在文本分类等简单任务上没有优势？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
fastText简而言之，就是把文档中所有词通过lookup table变成向量，取平均后直接用线性分类器得到分类结果。fastText和ACL-15上的deep averaging network [1] (DAN，如下图)非常相似，区别就是去掉了中间的隐层。两篇文章的结论也比较类似，也是指出对一些简单的分类任务，没有必要使用太复杂的网络结构就可以取得差不多的结果。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/439379a6670286603bbd51f573da0560_b.png" data-rawwidth="412" data-rawheight="301" class="content_image" width="412"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="412" data-rawheight="301" class="content_image lazy" width="412" data-actualsrc="https://pic1.zhimg.com/439379a6670286603bbd51f573da0560_b.png"&gt;&lt;br&gt;文中实验选取的都是对句子词序不是很敏感的数据集，所以得到文中的实验结果完全不奇怪。但是比如对下面的三个例子来说：&lt;br&gt;&lt;ul&gt;&lt;li&gt;The movie is not very good , but i still like it . [2]&lt;br&gt;&lt;/li&gt;&lt;li&gt;The movie is very good , but i still do not like it .&lt;br&gt;&lt;/li&gt;&lt;li&gt;I do not like it , but the movie is still very good .&lt;/li&gt;&lt;/ul&gt;其中第1、3句整体极性是positive，但第2句整体极性就是negative。如果只是通过简单的取平均来作为sentence representation进行分类的话，可能就会很难学出词序对句子语义的影响。&lt;br&gt;&lt;br&gt;从另一个角度来说，fastText可以看作是用window-size=1 + average pooling的CNN [3]对句子进行建模。&lt;br&gt;&lt;br&gt;总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。&lt;br&gt;&lt;br&gt;另外，fastText文中还提到的两个tricks分别是：&lt;br&gt;&lt;ul&gt;&lt;li&gt;hierarchical softmax&lt;br&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;类别数较多时，通过构建一个霍夫曼编码树来加速softmax layer的计算，和之前word2vec中的trick相同&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;N-gram features&lt;br&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;只用unigram的话会丢掉word order信息，所以通过加入N-gram features进行补充&lt;/li&gt;&lt;li&gt;用hashing来减少N-gram的存储&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br&gt;[1] Deep Unordered Composition Rivals Syntactic Methods for Text Classification&lt;br&gt;[2] A Statistical Parsing Framework for Sentiment Classification&lt;br&gt;[3] Natural Language Processing (Almost) from Scratch
&lt;/div&gt;</description><author>董力</author><pubDate>2016-08-22</pubDate></item><item><title>如何直观的解释back propagation算法？</title><link>https://m.zhihu.com/question/27239198/answer/89853077</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
BackPropagation算法是多层神经网络的训练中举足轻重的算法。&lt;br&gt;简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。&lt;br&gt;要回答题主这个问题“如何直观的解释back propagation算法？”  需要先直观理解多层神经网络的训练。&lt;br&gt;&lt;br&gt;机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系。&lt;br&gt;&lt;br&gt;深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/808254232cd4983cac374c5cc2a1fc87_b.png" data-rawwidth="400" data-rawheight="282" class="content_image" width="400"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="400" data-rawheight="282" class="content_image lazy" width="400" data-actualsrc="https://pic4.zhimg.com/808254232cd4983cac374c5cc2a1fc87_b.png"&gt;&lt;p&gt;其对应的表达式如下：&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_b.png" data-rawwidth="474" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="474" data-original="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_r.png"&gt;上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="474" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="474" data-original="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_r.png" data-actualsrc="https://pic4.zhimg.com/e62889afe359c859e9a6a1ad2a432ebb_b.png"&gt;上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。&lt;br&gt;&lt;br&gt;和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。&lt;br&gt;&lt;br&gt;梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？&lt;br&gt;&lt;br&gt;假设我们把cost函数表示为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=H%28W_%7B11%7D%2C+W_%7B12%7D%2C+%5Ccdots+%2C+W_%7Bij%7D%2C+%5Ccdots%2C+W_%7Bmn%7D%29" alt="H(W_{11}, W_{12}, \cdots , W_{ij}, \cdots, W_{mn})" eeimg="1"&gt;, 那么它的梯度向量[2]就等于&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cnabla+H++%3D+%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+W_%7B11%7D+%7D%5Cmathbf%7Be%7D_%7B11%7D+%2B+%5Ccdots+%2B+%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+W_%7Bmn%7D+%7D%5Cmathbf%7Be%7D_%7Bmn%7D" alt="\nabla H  = \frac{\partial H}{\partial W_{11} }\mathbf{e}_{11} + \cdots + \frac{\partial H}{\partial W_{mn} }\mathbf{e}_{mn}" eeimg="1"&gt;, 其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Be%7D_%7Bij%7D" alt="\mathbf{e}_{ij}" eeimg="1"&gt;表示正交单位向量。为此，我们需求出cost函数H对每一个权值Wij的偏导数。而&lt;b&gt;BP算法正是用来求解这种多层复合函数的所有变量的偏导数的利器&lt;/b&gt;。&lt;br&gt;&lt;br&gt;我们以求e=(a+b)*(b+1)的偏导[3]为例。&lt;br&gt;它的复合关系画出图可以表示如下：&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_b.png" data-rawwidth="1383" data-rawheight="800" class="origin_image zh-lightbox-thumb" width="1383" data-original="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_r.png"&gt;在图中，引入了中间变量c,d。&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1383" data-rawheight="800" class="origin_image zh-lightbox-thumb lazy" width="1383" data-original="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_r.png" data-actualsrc="https://pic1.zhimg.com/ee59254c9432b47cfcc3b11eab3e5984_b.png"&gt;在图中，引入了中间变量c,d。&lt;br&gt;&lt;br&gt;为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_b.png" data-rawwidth="1405" data-rawheight="793" class="origin_image zh-lightbox-thumb" width="1405" data-original="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_r.png"&gt;利用链式法则我们知道：&lt;/noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg" data-rawwidth="1405" data-rawheight="793" class="origin_image zh-lightbox-thumb lazy" width="1405" data-original="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_r.png" data-actualsrc="https://pic2.zhimg.com/986aacfebb87f4e9573fa2fe87f439d1_b.png"&gt;利用链式法则我们知道：&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+a%7D" alt="\frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}" eeimg="1"&gt;以及&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+b%7D%2B%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+d%7D%5Ccdot+%5Cfrac%7B%5Cpartial+d%7D%7B%5Cpartial+b%7D" alt="\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}" eeimg="1"&gt;。&lt;br&gt;&lt;br&gt;链式法则在上图中的意义是什么呢？其实不难发现，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D" alt="\frac{\partial e}{\partial a}" eeimg="1"&gt;的值等于从a到e的路径上的偏导值的乘积，而&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D" alt="\frac{\partial e}{\partial b}" eeimg="1"&gt;的值等于从b到e的路径1(b-c-e)上的偏导值的乘积加上路径2(b-d-e)上的偏导值的乘积。也就是说，对于上层节点p和下层节点q，要求得&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D" alt="\frac{\partial p}{\partial q}" eeimg="1"&gt;，需要找到从q节点到p节点的所有路径，并且对每条路径，求得该路径上的所有偏导数之乘积，然后将所有路径的 “乘积” 累加起来才能得到&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D" alt="\frac{\partial p}{\partial q}" eeimg="1"&gt;的值。&lt;br&gt;&lt;br&gt;大家也许已经注意到，这样做是十分冗余的，因为很多&lt;b&gt;路径被重复访问了&lt;/b&gt;。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。&lt;br&gt;&lt;br&gt;&lt;b&gt;同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。&lt;/b&gt;&lt;br&gt;正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。&lt;br&gt;&lt;br&gt;从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。&lt;br&gt;&lt;br&gt;以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.&lt;br&gt;&lt;br&gt;举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。&lt;br&gt;&lt;br&gt;而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。&lt;br&gt;------------------------------------------------------------------&lt;br&gt;【参考文献】&lt;br&gt;[1] &lt;a href="//link.zhihu.com/?target=http%3A//www.cnblogs.com/nsnow/p/4562308.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;技术向：一文读懂卷积神经网络CNN&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;[2] &lt;a href="//link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gradient" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Gradient&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;[3] &lt;a href="//link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Backprop/" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;colah.github.io/posts/2&lt;/span&gt;&lt;span class="invisible"&gt;015-08-Backprop/&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;其他推荐网页：&lt;br&gt;1. &lt;a href="//link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;tensorflow.org 的页面 &lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;2. &lt;a href="//link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Neural networks and deep learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>Evan Hoo</author><pubDate>2016-08-22</pubDate></item><item><title>如何配置一台适用于深度学习的工作站？</title><link>https://m.zhihu.com/question/33996159/answer/58821678</link><description>&lt;div class="zm-editable-content"&gt;刚买两块Titan Z GPU准备搞搞深度学习，结果原来的工作站功率不够，带不动，所以准备组装一台新工作站。求大神们给点意见，最好给个完整的list，我好照着买，谢谢。（本人新手，也不怎么会组装，最好是半成品机器，然后我组装一下就好的那种）&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
TitanX比TitanZ好用。&lt;br&gt;显卡数决定了CPU数，&amp;lt;=4个时没必要双CPU。单CPU &lt;b&gt;5930K&lt;/b&gt;足矣&lt;br&gt;如果是单CPU，内存32GB就够，不用ECC&lt;br&gt;硬盘比较容易成为瓶颈。根据剩余PCIE槽数可以考虑Intel 750 (PCIE) 或者Samsung SM951 (M2)&lt;br&gt;4TitanX用CXXNET跑满实测电源输出功率1000W左右，保险点上个1500W的电源吧。&lt;br&gt;这样配的话，4路TitanX加税也不到7000刀（不带显卡裸机2000多）&lt;br&gt;&lt;br&gt;&lt;br&gt;具体配置如下，仅供参考。没包含PCIE SSD因为插不下了：&lt;br&gt;&lt;br&gt;CPU i7 5820K&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Intel-i7-5930K-Haswell-E-Processor-BX80648I75930K/dp/B00MMLXMM8/" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/Intel-i7-5930K-Haswell-E-Processor-BX80648I75930K/dp/B00MMLXMM8/&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;主板 Gigabyte&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Gigabyte-CrossFireX-Motherboards-GA-X99-GAMING-WIFI/dp/B00MPIDZCK/ref%3Dsr_1_9%3Fs%3Dpc%26ie%3DUTF8%26qid%3D1426535776%26sr%3D1-9" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/Gigabyte-CrossFireX-Motherboards-GA-X99-GAMING-WIFI/dp/B00MPIDZCK/ref=sr_1_9?s=pc&amp;amp;ie=UTF8&amp;amp;qid=1426535776&amp;amp;sr=1-9&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;内存 32GB DDR4 &lt;a class=" external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Crucial-Ballistix-PC4-19200-Unbuffered-BLS2K8G4D240FSA/dp/B00MTSWFMM/ref%3Dsr_1_1%3Fs%3Dpc%26ie%3DUTF8%26qid%3D1426537593%26sr%3D1-1" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Crucial-Ball&lt;/span&gt;&lt;span class="invisible"&gt;istix-PC4-19200-Unbuffered-BLS2K8G4D240FSA/dp/B00MTSWFMM/ref=sr_1_1?s=pc&amp;amp;ie=UTF8&amp;amp;qid=1426537593&amp;amp;sr=1-1&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;硬盘 Samsung 850 Pro 1TB &lt;a class=" external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/Samsung-2-5-Inch-SATA-Internal-MZ-7KE1T0BW/dp/B00LF10KTE/ref%3Dsr_1_2%3Fie%3DUTF8%26qid%3D1426572602%26sr%3D8-2%26keywords%3Dsamsung%2B1tb" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Samsung-2-5-&lt;/span&gt;&lt;span class="invisible"&gt;Inch-SATA-Internal-MZ-7KE1T0BW/dp/B00LF10KTE/ref=sr_1_2?ie=UTF8&amp;amp;qid=1426572602&amp;amp;sr=8-2&amp;amp;keywords=samsung+1tb&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;（除了 PCIE 以外，还可以考虑SSD RAID 0。关于IO的benchmark做的不多就不乱说了。欢迎其他童鞋补充）&lt;br&gt;&lt;br&gt;显卡Titan X*4 &lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.geforce.com/geforce-gtx-titan-x/buy-gpu" target="_blank" rel="nofollow noreferrer"&gt;NVIDIA Store&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;机箱 Corsair Carbide Air 540 &lt;a href="//link.zhihu.com/?target=http%3A//www.amazon.com/Corsair-Carbide-High-Airflow-CC-9011030-WW/dp/B00D6GINF4" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;amazon.com/Corsair-Carb&lt;/span&gt;&lt;span class="invisible"&gt;ide-High-Airflow-CC-9011030-WW/dp/B00D6GINF4&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;（NVidia DevBox 同款）&lt;br&gt;&lt;br&gt;电源 EVGA 1600W&lt;a class=" wrap external" href="//link.zhihu.com/?target=http%3A//www.amazon.com/EVGA-Supernova-80Plus-Modular-120-G2-1600-X1/dp/B00MMLUIE8" target="_blank" rel="nofollow noreferrer"&gt; http://www.amazon.com/EVGA-Supernova-80Plus-Modular-120-G2-1600-X1/dp/B00MMLUIE8&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;======9月10日更新======&lt;br&gt;原答案中CPU推荐的是5820K，经 &lt;a data-hash="4bfbc1bc2ff914bec204302c82022b98" href="//www.zhihu.com/people/4bfbc1bc2ff914bec204302c82022b98" class="member_mention" data-hovercard="p$b$4bfbc1bc2ff914bec204302c82022b98"&gt;@郝佳男&lt;/a&gt;  提醒发现，5820K的PCIE-lanes要比5930少许多。开4卡的话，5930K的额外两百块钱可能还是省不了。
&lt;/div&gt;</description><author>Filestorm</author><pubDate>2016-08-22</pubDate></item><item><title>NIPS 2016有什么值得关注的呢？</title><link>https://m.zhihu.com/question/49567256/answer/116858150</link><description>&lt;div class="zm-editable-content"&gt;Wikipedia: &lt;a href="//link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Conference on Neural Information Processing Systems&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; (NIPS),homepage:&lt;a href="//link.zhihu.com/?target=https%3A//nips.cc/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;2016 Conference&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
来强答一发，推荐关注新的优化(Optimization)算法。&lt;br&gt;&lt;br&gt;前段几天NIPS出结果，朋友圈里几位小伙伴晒了自己的NIPS战绩。之前审NIPS2016的时候，发现优化发面的投稿有不少我感兴趣和我所做的方向相关的论文，借这个问题，简单讲讲这方面的进展，以及今年NIPS可能会出现的新方法。&lt;br&gt;&lt;br&gt;随着数据规模和参数规模的增加，设计更快速的优化算法显得非常必要。通常加速现有的优化算法有两个思路&lt;br&gt;&lt;ul&gt;&lt;li&gt;一方面是从老算法中设计新算法，通过证明和实验说明其在收敛速度上更快&lt;/li&gt;&lt;li&gt;另一方面是设计异步分布式的算法通过多机来加速，并证明其正确性和通过实验验证其有效性。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;优化算法一直也是ML各大会议非常重视的方向，每年NIPS、ICML都会有一大票优化相关的paper。近几年，随机优化方向的一些新算法都发表在机器学习会议上，比如SAG、SVRG、SAGA这一类可以线性收敛的SGD变种。&lt;br&gt;&lt;br&gt;近段时间，个人比较关注上面提到的第二种思路，也就是在多机多线程环境下设计优化算法，并通过实验和证明来验证算法的正确性。这里回顾一下异步分布式梯度下降算法：&lt;br&gt;&lt;br&gt;假设目标函数是 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5EN+f_i%28%5Cmathbf%7Bx%7D%29+%2B+h%28%5Cmathbf%7Bx%7D%29" alt="\frac{1}{n} \sum_{i=1}^N f_i(\mathbf{x}) + h(\mathbf{x})" eeimg="1"&gt;，其中&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=f_i%28%5Cmathbf%7Bx%7D%29" alt="f_i(\mathbf{x})" eeimg="1"&gt;是第&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=i" alt="i" eeimg="1"&gt;个样本的损失函数，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=h%28%5Cmathbf%7Bx%7D%29" alt="h(\mathbf{x})" eeimg="1"&gt;是一个非平滑的正则化项，这个目标函数常常使用Proximal Stochastic Gradient Descent (P-SGD)求解，更新式子是这样。&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bk%2B1%7D+%3D+%5Ctext%7BProx%7D%28%5Cmathbf%7Bx%7D_k+-+%5Ceta_k+%5Ctriangledown+f_i%28%5Cmathbf%7Bx%7D_k%29%29" alt="\mathbf{x}_{k+1} = \text{Prox}(\mathbf{x}_k - \eta_k \triangledown f_i(\mathbf{x}_k))" eeimg="1"&gt;，这里的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;叫做Proximal operator，目的是为了处理非平滑的正则化项&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=h%28%5Cmathbf%7Bx%7D%29" alt="h(\mathbf{x})" eeimg="1"&gt;。&lt;br&gt;&lt;br&gt;在采用异步加速的算法中，早期做异步SGD的算法比较出名的有Hogwild!，参考：Hogwild: A lock-free approach to parallelizing stochastic gradient descent，只是Hogwild!并不处理非平滑正则化。此外，还有[Alekh Agarwal et al.] Distributed delayed stochastic optimization、[Xiangru Lian et al.]Asynchronous parallel stochastic gradient for nonconvex optimization等。&lt;br&gt;&lt;br&gt;把算法换到多机或者多线程的情况下，采用parameter server结构，让master存储参数并负责更新，而worker负责计算梯度并交给master，master来完成&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;操作，那么更新式子会变成。&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bk%2B1%7D+%3D+%5Ctext%7BProx%7D%28%5Cmathbf%7Bx%7D_k+-+%5Ceta_k+%5Ctriangledown+f_i%28%5Cmathbf%7Bx%7D_%7Bd%28k%29%7D%29%29" alt="\mathbf{x}_{k+1} = \text{Prox}(\mathbf{x}_k - \eta_k \triangledown f_i(\mathbf{x}_{d(k)}))" eeimg="1"&gt;，注意这里的梯度中所使用的参数是&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=x_%7Bd%28k%29%7D" alt="x_{d(k)}" eeimg="1"&gt;，它是一个有延迟的参数（&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=d%28k%29%5Cleq+k" alt="d(k)\leq k" eeimg="1"&gt;），也就是说这个参数是『过期』的，造成这个『过期』的原因是当worker从master上拉取参数计算完梯度再提交给master的时候，master的参数可能已经被其他worker更新了好几次，具体可以参考 &lt;a data-hash="13fd0fce2affd948bfd821a8f7ed10f3" href="//www.zhihu.com/people/13fd0fce2affd948bfd821a8f7ed10f3" class="member_mention" data-editable="true" data-title="@李沐" data-hovercard="p$b$13fd0fce2affd948bfd821a8f7ed10f3"&gt;@李沐&lt;/a&gt; 的论文Communication Efficient Distributed Machine
Learning with the Parameter Server。一般可以证明到次线性收敛速度。&lt;br&gt;&lt;br&gt;如果&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;不是特别复杂（比如L1 norm的Proximal operator），把它交给master来计算是比较合适的，但是，有的时候由于求解的问题带有更加复杂的Nuclear norm、Group Lasso、Fused Lasso这些norm，他们的Proximal operator如果交给master做，会加大master的负担，如果有多个master的话，还会造成master之间的通信。这个时候，我们就有必要设计一种异步的SGD，使得它能够把Proximal operator的计算交给worker，并且不会让worker和worker之间，master和master之间产生额外的通信。&lt;br&gt;&lt;br&gt;为了解决这个问题，我们设计了一种新的异步优化算法，也往今年NIPS投了，目前paper在Arxiv上：&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1605.06619" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1605.0661&lt;/span&gt;&lt;span class="invisible"&gt;9&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;。很遗憾我们的没有中今年NIPS。简单介绍一下，我们这篇paper的思路，我们把更新式子设计成两步&lt;br&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com///zhihu.com/equation?tex=x_%7Bd%28t%29%7D%27+%3D+%5Ctext%7BProx%7D+%28%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D+-+%5Ceta_%7Bd%28t%29%7D+%5Ctriangledown+f_%7Bi_%7Bd%28t%29%7D%7D+%28%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D%29%29" alt="x_{d(t)}' = \text{Prox} (\mathbf{x}_{d(t)} - \eta_{d(t)} \triangledown f_{i_{d(t)}} (\mathbf{x}_{d(t)}))" eeimg="1"&gt;&lt;br&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com///zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bx%7D_%7Bt%7D+%2B+%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D%27+-+%5Cmathbf%7Bx%7D_%7Bd%28t%29%7D" alt="\mathbf{x}_{t+1} = \mathbf{x}_{t} + \mathbf{x}_{d(t)}' - \mathbf{x}_{d(t)}" eeimg="1"&gt;&lt;br&gt;&lt;br&gt;把第二步中的加法更新交给master计算，剩余部分都可以交给worker来计算，此时，无论&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com///zhihu.com/equation?tex=%5Ctext%7BProx%7D%28.%29" alt="\text{Prox}(.)" eeimg="1"&gt;的计算过程多么复杂，都可以由每个worker单独完成，而master仅仅需要做加法计算。&lt;br&gt;&lt;br&gt;目前我们能证明这个算法是次线性收敛的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com///zhihu.com/equation?tex=O%28%5Cfrac%7B%5Clog+T%7D%7BT%7D%29" alt="O(\frac{\log T}{T})" eeimg="1"&gt;，试验中性能也很好。&lt;br&gt;&lt;br&gt;此前，异步的SGD采用variance reduction后已经能做到线性收敛，参考On Variance Reduction in Stochastic Gradient
Descent and its Asynchronous Variants。但是异步的Proximal SGD目前还没看到有线性收敛的方法。&lt;br&gt;&lt;br&gt;在NIPS2016审稿bid阶段，看到几篇用了variance reduction trick的异步Proximal SGD，比较期待今年会不会出现能够线性收敛的异步Proximal SGD。&lt;br&gt;&lt;br&gt;SGD算法只是优化算法之一，还有很多值得关注的算法。等NIPS的paper列表放出来了，再来补充一些具体的。
&lt;/div&gt;</description><author>li Eta</author><pubDate>2016-08-22</pubDate></item></channel></rss>