<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>

机器学习
 - 知乎收藏夹</title><link>https://www.zhihu.com/collection/113183484</link><description>每天整理和机器学习有关的优质回答</description><lastBuildDate>Sat, 27 Aug 2016 18:02:08 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>如何评价rcnn、fast-rcnn和faster-rcnn这一系列方法？</title><link>https://www.zhihu.com/question/35887527/answer/72876282</link><description>&lt;div class="zm-editable-content"&gt;或者相关的检测方法如OverFeat、SPPNet和最新的YOLO。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
泻药，终于看到符合自己胃口的问题啦！怒答一枚。RCNN和Fast-RCNN简直是引领了最近两年目标检测的潮流！&lt;br&gt;-------------------------------&lt;br&gt;提到这两个工作，不得不提到RBG大神&lt;a href="//link.zhihu.com/?target=http%3A//www.cs.berkeley.edu/%7Erbg/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;rbg's home page&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，该大神在读博士的时候就因为dpm获得过pascal voc 的终身成就奖。博士后期间更是不断发力，RCNN和Fast-RCNN就是他的典型作品。&lt;br&gt;&lt;br&gt;RCNN：RCNN可以看作是RegionProposal+CNN这一框架的开山之作，在imgenet/voc/mscoco上基本上所有top的方法都是这个框架，可见其影响之大。RCNN的主要缺点是重复计算，后来MSRA的kaiming组的SPPNET做了相应的加速。&lt;br&gt;&lt;br&gt;Fast-RCNN：RCNN的加速版本，在我看来，这不仅仅是一个加速版本，其优点还包括：&lt;br&gt;(a) 首先，它提供了在caffe的框架下，如何定义自己的层/参数/结构的范例，这个范例的一个重要的应用是python layer的应用，我在这里&lt;a href="http://www.zhihu.com/question/36847520/answer/72824645" class="internal"&gt;支持多label的caffe，有比较好的实现吗？ - 孔涛的回答&lt;/a&gt;也提到了。&lt;br&gt;(2) training and testing end-to-end 这一点很重要，为了达到这一点其定义了ROIPooling层，因为有了这个，使得训练效果提升不少。&lt;br&gt;(3) 速度上的提升，因为有了Fast-RCNN，这种基于CNN的 real-time 的目标检测方法看到了希望，在工程上的实践也有了可能，后续也出现了诸如Faster-RCNN/YOLO等相关工作。&lt;br&gt;&lt;br&gt;这个领域的脉络是：RCNN -&amp;gt; SPPNET -&amp;gt; Fast-RCNN -&amp;gt; Faster-RCNN。关于具体的细节，建议题主还是阅读相关文献吧。&lt;br&gt;&lt;br&gt;这使我看到了目标检测领域的希望。起码有这么一部分人，他们不仅仅是为了几个百分点的提升，而是切实踏实在做贡献，相信不久这个领域会有新的工作出来。&lt;br&gt;&lt;br&gt;以上纯属个人观点，欢迎批评指正。&lt;br&gt;&lt;br&gt;参考：&lt;br&gt;[1] R-CNN: Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C], CVPR, 2014.&lt;br&gt;[2] SPPNET: He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[C], ECCV, 2014.&lt;br&gt;[3] Fast-RCNN: Girshick R. Fast R-CNN[C]. ICCV, 2015.&lt;br&gt;[4] Fater-RCNN: Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]. NIPS, 2015.&lt;br&gt;[5] YOLO: Redmon J, Divvala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection[J]. arXiv preprint arXiv:1506.02640, 2015.
&lt;/div&gt;</description><author>孔巴巴</author><pubDate>2016-08-27</pubDate></item><item><title>ICML 2016上哪些论文值得关注？</title><link>https://www.zhihu.com/question/45716405/answer/105619854</link><description>&lt;div class="zm-editable-content"&gt;ICML 2016的录用论文列表已经发布，&lt;a href="//link.zhihu.com/?target=http%3A//icml.cc/2016/%3Fpage_id%3D1649" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Accepted Papers&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，我们可以一起交流下这个会议上哪些论文特别值得关注，代表未来的研究热点和重点？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;a data-hash="cc0767a011397b2adac4e1ae6ac00b83" href="//www.zhihu.com/people/cc0767a011397b2adac4e1ae6ac00b83" class="member_mention" data-editable="true" data-title="@刘知远" data-tip="p$b$cc0767a011397b2adac4e1ae6ac00b83" data-hovercard="p$b$cc0767a011397b2adac4e1ae6ac00b83"&gt;@刘知远&lt;/a&gt; 老师此题好比crowdsourcing：ICML涉及领域众多，欲精通所有不太现实，但博采众长即可去伪存真、沙里淘金。在此仅抛砖引玉，提几篇个人比较感兴趣的paper。&lt;br&gt;&lt;br&gt;近期一直关注深度学习的研究和发展，因此下面八篇文章均涉及DL：&lt;br&gt;&lt;ol&gt;&lt;li&gt;Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/shang16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/shang16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：这篇文章出自U. Michigan，以及著名的NEC和Oculus VR。作者受到自己对CNN可视化发现的启发，设计了一种新的激活函数，即concatenated ReLU (CReLU)，在ImageNet等数据上不仅能取得更好结果，同时所需参数也可大幅缩减。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Noisy Activation Functions (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/gulcehre16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/gulcehre16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：Bengio作品。为解决传统激活函数的「饱和效应」（也称「梯度弥散」现象），作者提出在梯度饱和部分加入合适的噪声以解决先前的优化难题。如此“反人类”的做法却取得了较好的泛化效果。&lt;/li&gt;&lt;li&gt;Learning End-to-end Video Classification with Rank-Pooling (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/fernando16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/fernando16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：先前在CNN中对video特征进行pooling的方法无异于max和average，该文提出了一种新的rank-pooling方法作为内嵌优化来学习如何更有效的将时序信息encode到最终特征中去。&lt;/li&gt;&lt;li&gt;Large-Margin Softmax Loss for Convolutional Neural Networks (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/liud16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/liud16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)提出了一种新型基于「大间隔」的softmax loss。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/zhangc16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/zhangc16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：通过构建CNN网络的“镜像网络”来「无监督」的重建图像，进而增强「监督」网络的判别能力。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Unsupervised Deep Embedding for Clustering Analysis (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/xieb16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/xieb16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：RBG作品。DL版本的聚类。&lt;br&gt;&lt;/li&gt;&lt;li&gt;Learning Convolutional Neural Networks for Graphs (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/niepert16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/niepert16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：图结构在机器学习中举足轻重，本文提出可以利用CNN来学习任意图结构。（不知会不会在graph based learning或mining领域引起轩然大波。）&lt;br&gt;&lt;/li&gt;&lt;li&gt;Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin (&lt;a href="//link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v48/amodei16.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;jmlr.org/proceedings/pa&lt;/span&gt;&lt;span class="invisible"&gt;pers/v48/amodei16.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;)：来自Baidu的一篇系统论文。工业界朋友可以重点参考，其中介绍了很多语音系统实现方法和技巧。&lt;br&gt;&lt;/li&gt;&lt;/ol&gt;&lt;u&gt;可以很容易发现，ICML'16中涉及DL/NN的文章，要么解决了DL中非常fundamental的问题，如提出新的activation function或pooling layer；要么就是用DL解决了很fundamental的任务，如clustering，graph learning和unsupervised learning；要么就是很具实践指导性的Deep Speech 2。鲜有非常CV口味的应用文章，也许这就是不同background对DL不同的喜好吧。&lt;/u&gt;
&lt;/div&gt;</description><author>魏秀参</author><pubDate>2016-08-27</pubDate></item><item><title>知乎上有哪些比较好的机器学习，数据挖掘相关的专栏？</title><link>https://www.zhihu.com/question/49428252/answer/115931823</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
感谢大家的厚爱，这几天赞、感谢和关注的增长有点不讲道理。我严格意义上开始学习机器学习，主要是深度学习是从7月初CS231n开始的，现在差不多把这门课学完了，但很多地方还是囫囵吞枣过去的，半瓶水摇晃的程度。这个帖子基本是我把我看到的机器学习，特别是机器学习里深度学习相关的专栏全部陈列在这里。具体每个专栏的质量怎么样，应该是有什么样的受众我都不能保证。这个帖子我会继续更新，大家如果有一些关于这些专栏的反馈或者看到了其它好的专栏都欢迎在下面评论，我如果看到其它好的专栏也会在这里更新，谢谢大家。&lt;br&gt;&lt;br&gt;---------------------------------------------------------------------------------------------------------------&lt;br&gt;&lt;br&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/furthersight" class="internal"&gt;欲穷千里目 - 知乎专栏&lt;/a&gt; 作者是南京大学LAMDA组的Ph.D. Candidate，内容涉及计算机视觉、机器学习。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/easyml" class="internal"&gt;炼丹实验室 - 知乎专栏&lt;/a&gt; 深度学习相关。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/uqer2015" class="internal"&gt;量化哥 - 知乎专栏&lt;/a&gt; 量化交易相关，涉及机器学习。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/hsmyy" class="internal"&gt;无痛的机器学习 - 知乎专栏&lt;/a&gt; 机器学习原理及应用。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/dlclass" class="internal"&gt;深度学习大讲堂 - 知乎专栏&lt;/a&gt; 深度学习相关。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/prml-paper-reading" class="internal"&gt;PRML - 知乎专栏&lt;/a&gt; 机器学习与数据科学相关，作者是阿里巴巴算法工程师。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/paperweekly" class="internal"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; 介绍了很多机器学习在自然语言处理上的paper。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/gongwenjia" class="internal"&gt;Data Science - 知乎专栏&lt;/a&gt; 数据科学相关一些算法的介绍。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/pengtaox" class="internal"&gt;谢澎涛的知乎专栏 - 知乎专栏&lt;/a&gt;  作者为CMU机器学习方向的PhD学生。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="internal"&gt;智能单元 - 知乎专栏&lt;/a&gt; 翻译了CS231n的笔记，另有数篇介绍深度增强学习相关内容的文章。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/system" class="internal"&gt;【Machine Learning】 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/dataman" class="internal"&gt;数据分析侠 - 知乎专栏&lt;/a&gt; 数据科学、机器学习相关的软硬文。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/cxwangyi" class="internal"&gt;Occam's Razor - 知乎专栏&lt;/a&gt; 机器学习相关。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/Stark" class="internal"&gt;机器鼓励师手册 - 知乎专栏&lt;/a&gt;  作者是阿里巴巴算法工程师。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/mlearn" class="internal"&gt;机器学习笔记 - 知乎专栏&lt;/a&gt; Coursera上Andrew Ng课程的笔记，供英语不够好的朋友们参考用。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/ItsFuture" class="internal"&gt;机器之眼 - 知乎专栏&lt;/a&gt; 关于深度学习在计算机视觉方面的应用。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/chicken-life" class="internal"&gt;菜鸡的啄米日常 - 知乎专栏&lt;/a&gt; 介绍了深度学习里比较流行的Keras框架，另有二次元福利【大雾】。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/TsingJyuData" class="internal"&gt;清雨的 Data Science 笔记 - 知乎专栏&lt;/a&gt; 机器学习相关。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/NLPBlog-WuYu" class="internal"&gt;NLPer新手练习场 - 知乎专栏&lt;/a&gt; 作者北航CS博士在读，NLP方向。&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/HicRhodushicsalta" class="internal"&gt;Hic Rhodus, hic salta - 知乎专栏&lt;/a&gt; 作者为中科院、德国马普学会计算神经科学方向博士，专栏涉及深度学习和神经科学的一些交叉应用。&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description><author>Frankenstein</author><pubDate>2016-08-27</pubDate></item><item><title>你心中的deep learning（深度学习）领域世界十大名校是哪些？</title><link>https://www.zhihu.com/question/35764062/answer/118091832</link><description>&lt;div class="zm-editable-content"&gt;&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
补充几个（可能主要偏CV方面）&lt;br&gt;&lt;ul&gt;&lt;li&gt;UC Berkeley，&lt;a href="//link.zhihu.com/?target=http%3A//bvlc.eecs.berkeley.edu" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Berkeley Vision and Learning Center&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;， 这个Caffe太有名了，而且牛人辈出，不敢妄言。就说我感兴趣的吧，其中的Pieter Abbeel 做reinforcement learning，近些年比较厉害。&lt;/li&gt;&lt;li&gt;UCLA，&lt;a href="//link.zhihu.com/?target=http%3A//ccvl.stat.ucla.edu" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;CCVL | Center for Cognition, Vision, and Learning&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，Alan L. Yuille，老教授非常和蔼可亲，现在去了JHU。他学生Leo Zhu，依图的创始人&lt;/li&gt;&lt;li&gt;Oxford，&lt;a href="//link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/%7Evgg/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Visual Geometry Group Home Page&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，著名的VGG就出自这里。还有Nando de Freitas也非常年轻有为，网课很火&lt;/li&gt;&lt;li&gt;UCL，做认知神经科学很厉害，也是Google Deepmind发源地。虽然主要不是做deep learning的，但是把强化学习引进来，未来潜力无限的。&lt;/li&gt;&lt;li&gt;&lt;a href="//link.zhihu.com/?target=http%3A//people.idsia.ch/%7Ejuergen/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Juergen Schmidhuber's home page -
Universal Artificial Intelligence&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，单提这位大叔（因为我也不知道他是哪个大学的，LSTM，highway network。大叔很帅，而且很有脾气（吃瓜群众.jpg）&lt;br&gt;&lt;/li&gt;&lt;li&gt;CUHK，&lt;a href="//link.zhihu.com/?target=http%3A//mmlab.ie.cuhk.edu.hk" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Multimedia Laboratory&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，xo大神！！！cvpr这种会水一水就好（目瞪狗呆.jpg）&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;再有就是大型研究院了&lt;br&gt;&lt;ul&gt;&lt;li&gt;Google&lt;/li&gt;&lt;li&gt;FAIR&lt;/li&gt;&lt;li&gt;MSRA&lt;/li&gt;&lt;li&gt;还有最近的OpenAI。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;其实主要看人，牛人在哪里，哪里就是所谓“名校”。&lt;br&gt;&lt;br&gt;Yann LeCun 在quora下一个回答说的很好&lt;br&gt;&lt;blockquote&gt;Forget about the “ranking” of the school for now. Find a reputable professor who works on topics that you are interested in. Pick a person whose papers you like or admire&lt;br&gt;&lt;/blockquote&gt;答案连接&lt;a href="//link.zhihu.com/?target=https%3A//www.quora.com/What%25E2%2580%2599s-your-advice-for-undergraduate-student-who-aspires-to-be-a-research-scientist-in-deep-learning-or-related-field-one-day/answer/Yann-LeCun%3Fsrid%3Dz3bk" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;quora.com 的页面&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;欢迎讨论补充( ^_^ )&lt;br&gt;&lt;br&gt;======8.22补充=======&lt;br&gt;感谢大家，看来很多人对深度学习领域感兴趣，那我就再补充几条&lt;br&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;Université de Montréal，这个已经有答案说了，Bengio大神，无需多言。Bengio的学生也是牛人辈出。比如&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Aaron Courville，现在也在Montreal做Assistant Professor&lt;/li&gt;&lt;li&gt;Ian Goodfellow，提出著名GAN的（现在应该去了OpenAI？）&lt;br&gt;&lt;/li&gt;&lt;li&gt;Hugo Larochelle，人气博主，经常写paper reading note（养活多少国内公众号啊hhhh）&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;U of T，Hinton大神自不用说，其弟子更是桃李天下。最有名的比如&lt;a href="//link.zhihu.com/?target=http%3A//www.cs.cmu.edu/%7Ersalakhu/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Ruslan Salakhutdinov&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，09年博士毕业，博士期间就主要推动deep learning发展。15年更是发了篇Nature！！现在已经是CMU的Associate Professor了啊啊啊！！&lt;/li&gt;&lt;li&gt;University of Michigan，&lt;a href="//link.zhihu.com/?target=http%3A//web.eecs.umich.edu/%7Ehonglak/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Honglak Lee&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; ，是Andrew Ng的弟子，今年就已经发了3篇cvpr，4篇icml，2篇nips 。。。&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</description><author>Yulong Wang</author><pubDate>2016-08-27</pubDate></item><item><title>CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？</title><link>https://www.zhihu.com/question/34681168/answer/84061846</link><description>&lt;div class="zm-editable-content"&gt;CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？以及他们的主要用途是什么？只知道CNN是局部感受和参数共享，比较适合用于图像这方面。刚入门的小白真心求助&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;p&gt;首先，我感觉不必像 @李Shawn 同学一样认为DNN、CNN、RNN完全不能相提并论。从广义上来说，NN（或是更美的DNN）确实可以认为包含了CNN、RNN这些具体的变种形式。在实际应用中，所谓的深度神经网络DNN，往往融合了多种已知的结构，包括卷积层或是LSTM单元。但是就题主的意思来看，这里的DNN应该特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。因此，题主一定要将DNN、CNN、RNN等进行对比，也未尝不可。&lt;/p&gt;&lt;p&gt;其实，如果我们顺着神经网络技术发展的脉络，就很容易弄清这几种网络结构发明的初衷，和他们之间本质的区别，希望对题主有所帮助。&lt;/p&gt;&lt;p&gt;=========================== 分 割 线 就 是 我
================================&lt;/p&gt;&lt;br&gt;&lt;p&gt;神经网络技术起源于上世纪五、六十年代，当时叫&lt;b&gt;感知机&lt;/b&gt;（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。&lt;i&gt;（扯一个不相关的：由于计算技术的落后，当时感知器传输函数是用线拉动变阻器改变电阻的方法机械实现的，脑补一下科学家们扯着密密麻麻的导线的样子…）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，即它对稍复杂一些的函数都无能为力（比如最为典型的“异或”操作）。连异或都不能拟合，你还能指望这货有什么实际用途么o(╯□╰)o&lt;/p&gt;&lt;br&gt;&lt;p&gt;随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人（反正就是一票大牛）发明的&lt;b&gt;多层感知机&lt;/b&gt;（multilayer
perceptron）克服。多层感知机，顾名思义，就是有多个隐含层的感知机（废话……）。好好，我们看一下多层感知机的结构：&lt;/p&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_b.png" data-rawwidth="866" data-rawheight="249" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="249" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_r.png" data-actualsrc="https://pic4.zhimg.com/e186f18d73fdafa8d4a5e75ed55ed4a3_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图1&lt;/i&gt;&lt;/b&gt;&lt;i&gt;上下层神经元全部相连的神经网络——多层感知机&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这货就是我们现在所说的&lt;b&gt;神经网络&lt;/b&gt;&lt;b&gt;NN&lt;/b&gt;——神经网络听起来不知道比感知机高端到哪里去了！这再次告诉我们起一个好听的名字对于研（zhuang）究（bi）很重要！&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。相信年轻如Hinton当时一定是春风得意。&lt;/p&gt;&lt;br&gt;&lt;p&gt;多层感知机给我们带来的启示是，&lt;b&gt;神经网络的层数直接决定了它对现实的刻画能力&lt;/b&gt;——利用每层更少的神经元拟合更加复杂的函数[1]。&lt;/p&gt;&lt;p&gt;（Bengio如是说：functions that can be compactly
represented by a depth k architecture might require an exponential number of
computational elements to be represented by a depth k − 1 architecture.）&lt;/p&gt;&lt;br&gt;&lt;p&gt;即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，&lt;b&gt;优化函数越来越容易陷入局部最优解&lt;/b&gt;，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，&lt;b&gt;“梯度消失”现象更加严重&lt;/b&gt;。具体来说，我们常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。&lt;/p&gt;&lt;br&gt;&lt;p&gt;2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层[2]，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。单从结构上来说，&lt;b&gt;全连接的&lt;/b&gt;&lt;b&gt;DNN&lt;/b&gt;&lt;b&gt;和图&lt;/b&gt;&lt;b&gt;1&lt;/b&gt;&lt;b&gt;的多层感知机是没有任何区别的&lt;/b&gt;。&lt;/p&gt;&lt;br&gt;&lt;p&gt;值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度消失，网络层数达到了前所未有的一百多层（深度残差学习：152层）[3,4]！具体结构题主可自行搜索了解。如果你之前在怀疑是不是有很多方法打上了“深度学习”的噱头，这个结果真是深得让人心服口服。&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_b.png" data-rawwidth="866" data-rawheight="1228" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="1228" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_r.png" data-actualsrc="https://pic3.zhimg.com/7b3ee9e4f4a2e61acf35820a2768cc12_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图2&lt;/i&gt;&lt;/b&gt;&lt;i&gt;缩减版的深度残差学习网络，仅有34&lt;/i&gt;&lt;i&gt;层，终极版有152&lt;/i&gt;&lt;i&gt;层，自行感受一下&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;如图1所示，我们看到&lt;b&gt;全连接&lt;/b&gt;&lt;b&gt;DNN&lt;/b&gt;&lt;b&gt;的结构里下层神经元和所有上层神经元都能够形成连接&lt;/b&gt;，带来的潜在问题是&lt;b&gt;参数数量的膨胀&lt;/b&gt;。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出题主所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是&lt;b&gt;通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。&lt;/b&gt;两层之间的卷积传输的示意图如下：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/440765dbaab356739fb855834f901e7d_b.png" data-rawwidth="866" data-rawheight="457" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="457" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_r.png" data-actualsrc="https://pic2.zhimg.com/440765dbaab356739fb855834f901e7d_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图3&lt;/i&gt;&lt;/b&gt;&lt;i&gt;卷积神经网络隐含层（摘自Theano&lt;/i&gt;&lt;i&gt;教程）&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;通过一个例子简单说明卷积神经网络的结构。假设图3中m-1=1是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100*100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100*100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性。&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_b.png" data-rawwidth="866" data-rawheight="203" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="203" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_r.png" data-actualsrc="https://pic3.zhimg.com/c71cd39abe8b0dd29e229f37058404da_b.png"&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图4&lt;/i&gt;&lt;/b&gt;&lt;i&gt;一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层（摘自Theano&lt;/i&gt;&lt;i&gt;教程）&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;在这个例子里，我们注意到&lt;b&gt;输入层到隐含层的参数瞬间降低到了&lt;/b&gt;&lt;b&gt;100*100*100=10^6&lt;/b&gt;&lt;b&gt;个&lt;/b&gt;！这使得我们能够用已有的训练数据得到良好的模型。题主所说的适用于图像识别，正是由于&lt;b&gt;CNN&lt;/b&gt;&lt;b&gt;模型限制参数了个数并挖掘了局部结构的这个特点&lt;/b&gt;。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。&lt;/p&gt;&lt;br&gt;&lt;p&gt;全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，&lt;b&gt;样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要&lt;/b&gt;。对了适应这种需求，就出现了题主所说的另一种神经网络结构——循环神经网络RNN。&lt;/p&gt;&lt;br&gt;&lt;p&gt;在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。而在&lt;b&gt;RNN&lt;/b&gt;&lt;b&gt;中，神经元的输出可以在下一个时间戳直接作用到自身&lt;/b&gt;，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_b.png" data-rawwidth="866" data-rawheight="441" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="441" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_r.png" data-actualsrc="https://pic2.zhimg.com/bef6a6073d311e79cad53eb47757af9d_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图5&lt;/i&gt;&lt;/b&gt;&lt;i&gt; RNN&lt;/i&gt;&lt;i&gt;网络结构&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_b.png" data-rawwidth="866" data-rawheight="348" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="348" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_r.png" data-actualsrc="https://pic3.zhimg.com/c2eb9099048761fd25f0e90aa66d363a_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图6&lt;/i&gt;&lt;/b&gt;&lt;i&gt; RNN&lt;/i&gt;&lt;i&gt;在时间上进行展开&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Cool，&lt;b&gt;（&lt;/b&gt;&lt;b&gt;t+1&lt;/b&gt;&lt;b&gt;）时刻网络的最终结果O(t+1)&lt;/b&gt;&lt;b&gt;是该时刻输入和所有历史共同作用的结果&lt;/b&gt;！这就达到了对时间序列建模的目的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;不知题主是否发现，RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，&lt;b&gt;“梯度消失”现象又要出现了，只不过这次发生在时间轴上&lt;/b&gt;。对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。&lt;/p&gt;&lt;br&gt;&lt;p&gt;为了解决时间上的梯度消失，机器学习领域发展出了&lt;b&gt;长短时记忆单元&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;，通过门的开关实现时间上记忆功能，并防止梯度消失&lt;/b&gt;，一个LSTM单元长这个样子：&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_b.png" data-rawwidth="866" data-rawheight="555" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="555" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_r.png" data-actualsrc="https://pic4.zhimg.com/a8f4582707b70d41f250fdf0a43812fb_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图7 &lt;/i&gt;&lt;/b&gt;&lt;i&gt;LSTM&lt;/i&gt;&lt;i&gt;的模样&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;除了题主疑惑的三种网络，和我之前提到的深度残差学习、LSTM外，深度学习还有许多其他的结构。举个例子，RNN既然能继承历史信息，是不是也能吸收点未来的信息呢？因为在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的。因此就有了&lt;b&gt;双向&lt;/b&gt;&lt;b&gt;RNN&lt;/b&gt;&lt;b&gt;、双向&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;，同时利用历史和未来的信息。&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_b.png" data-rawwidth="866" data-rawheight="365" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="866" data-rawheight="365" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_r.png" data-actualsrc="https://pic1.zhimg.com/a3ab3ac82679db4f51ecdafda617db0c_b.png"&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;图8&lt;/i&gt;&lt;/b&gt;&lt;i&gt;双向RNN&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;事实上，&lt;b&gt;不论是那种网络，他们在实际应用中常常都混合着使用，比如&lt;/b&gt;&lt;b&gt;CNN&lt;/b&gt;&lt;b&gt;和RNN&lt;/b&gt;&lt;b&gt;在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。&lt;/b&gt;不难想象随着深度学习热度的延续，更灵活的组合方式、更多的网络结构将被发展出来。尽管看起来千变万化，但研究者们的出发点肯定都是为了解决特定的问题。题主如果想进行这方面的研究，不妨仔细分析一下这些结构各自的特点以及它们达成目标的手段。入门的话可以参考：&lt;/p&gt;&lt;p&gt;Ng写的Ufldl：&lt;a href="//link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/UFLDL%25E6%2595%2599%25E7%25A8%258B" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;UFLDL教程 - Ufldl&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;也可以看Theano内自带的教程，例子非常具体：&lt;a href="//link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Deep Learning Tutorials&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;欢迎大家继续推荐补充。&lt;/p&gt;&lt;p&gt;当然啦，如果题主只是想凑个热闹时髦一把，或者大概了解一下方便以后把妹使，这样看看也就罢了吧。&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;参考文献：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[1]  
Bengio Y. Learning Deep
Architectures for AI[J]. Foundations &amp;amp; Trends® in Machine Learning, 2009,
2(1):1-127.&lt;/p&gt;&lt;p&gt;[2]  
Hinton G E, Salakhutdinov R R.
Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006,
313(5786):504-507.&lt;/p&gt;&lt;p&gt;[3]  
He K, Zhang X, Ren S, Sun J. Deep
Residual Learning for Image Recognition. arXiv:1512.03385, 2015.&lt;/p&gt;&lt;p&gt;[4]  
Srivastava R K, Greff K,
Schmidhuber J. Highway networks. arXiv:1505.00387, 2015.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;【“科研君”公众号初衷始终是希望聚集各专业一线科研人员和工作者，在进行科学研究的同时也作为知识的传播者，利用自己的专业知识解释和普及生活中的 一些现象和原理，展现科学有趣生动的一面。该公众号由清华大学一群在校博士生发起，目前参与的作者人数有10人，但我们感觉这远远不能覆盖所以想科普的领域，并且由于空闲时间有限，导致我们只能每周发布一篇文章。我们期待更多的战友加入，认识更多志同道合的人，每个人都是科研君，每个人都是知识的传播者。我们期待大家的参与，想加入我们，进QQ群吧~：108141238】&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;【非常高兴看到大家喜欢并赞同我们的回答。应许多知友的建议，最近我们开通了同名公众号：&lt;b&gt;PhDer&lt;/b&gt;，也会定期更新我们的文章，如果您不想错过我们的每篇回答，欢迎扫码关注~ 】&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="//link.zhihu.com/?target=http%3A//weixin.qq.com/r/5zsuNoHEZdwarcVV9271" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;weixin.qq.com/r/5zsuNoH&lt;/span&gt;&lt;span class="invisible"&gt;EZdwarcVV9271&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; (二维码自动识别)&lt;/p&gt;
&lt;/div&gt;</description><author>[已重置]</author><pubDate>2016-08-27</pubDate></item><item><title>有哪些LSTM(Long Short Term Memory)和RNN(Recurrent)网络的教程？</title><link>https://www.zhihu.com/question/29411132/answer/51515231</link><description>&lt;div class="zm-editable-content"&gt;不需要面面俱到，只需把他们解决什么问题，训练的过程是怎样的讲清楚就好。最好看完后就能直接上手写代码。&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;p&gt;刚好毕设相关，论文写完顺手就答了&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;先给出一个最快的了解+上手的教程：&lt;/b&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;    直接看theano官网的LSTM教程+代码：&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/tutorial/lstm.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;LSTM Networks for Sentiment Analysis&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;但是，前提是你有RNN的基础，因为LSTM本身不是一个完整的模型，LSTM是对RNN隐含层的改进。一般所称的LSTM网络全叫全了应该是使用LSTM单元的RNN网络。教程就给了个LSTM的图，它只是RNN框架中的一部分，如果你不知道RNN估计看不懂。&lt;/p&gt;&lt;p&gt;    比较好的是，你只需要了解前馈过程，你都不需要自己求导就能写代码使用了。&lt;/p&gt;&lt;p&gt;    补充，今天刚发现一个中文的博客：&lt;a href="//link.zhihu.com/?target=http%3A//blog.csdn.net/a635661820/article/details/45390671" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;LSTM简介以及数学推导(FULL BPTT)&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;    不过，稍微深入下去还是得老老实实的好好学，下面是我认为比较好的&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;完整&lt;/b&gt;&lt;b&gt;LSTM&lt;/b&gt;&lt;b&gt;学习流程&lt;/b&gt;：&lt;/p&gt;&lt;br&gt;&lt;p&gt;    我一直都觉得了解一个模型的前世今生对模型理解有巨大的帮助。到LSTM这里（假设题主零基础）那比较好的路线是MLP-&amp;gt;RNN-&amp;gt;LSTM。还有LSTM本身的发展路线（97年最原始的LSTM到forget gate到peephole再到CTC ）&lt;/p&gt;&lt;p&gt;    按照这个路线学起来会比较顺，所以我优先推荐的两个教程都是按照这个路线来的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;多伦多大学的 Alex Graves 的RNN专著&lt;i&gt;《Supervised Sequence Labelling with Recurrent Neural
Networks》&lt;/i&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt; Felix Gers的博士论文&lt;i&gt;《Long short-term memory in recurrent neural networks》&lt;/i&gt;&lt;/li&gt;&lt;/ol&gt;这两个内容都挺多的，不过可以跳着看，反正我是没看完
┑(￣Д
￣)┍&lt;br&gt;&lt;p&gt;还有一个最新的（今年2015）的综述，&lt;i&gt;《A
Critical Review of Recurrent Neural Networks for Sequence Learning》&lt;/i&gt;不过很多内容都来自以上两个材料。&lt;/p&gt;&lt;p&gt;    其他可以当做教程的材料还有：&lt;/p&gt;&lt;p&gt;&lt;i&gt;《From
Recurrent Neural Network to Long Short Term Memory Architecture Application to
Handwriting Recognition Author》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Generating Sequences With Recurrent Neural Networks》&lt;/i&gt;（这个有对应源码，虽然实例用法是错的，自己用的时候还得改代码，主要是摘出一些来用，供参考）&lt;/p&gt;&lt;br&gt;&lt;p&gt;然后呢，可以开始编码了。除了前面提到的theano教程还有一些论文的开源代码，到github上搜就好了。&lt;/p&gt;&lt;br&gt;&lt;p&gt;顺便安利一下theano，theano的自动求导和GPU透明对新手以及学术界研究者来说非常方便，LSTM拓扑结构对于求导来说很复杂，上来就写LSTM反向求导还要GPU编程代码非常费时间的，而且搞学术不是实现一个现有模型完了，得尝试创新，改模型，每改一次对应求导代码的修改都挺麻烦的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;其实到这应该算是一个阶段了，如果你想继续深入可以具体看看几篇经典论文，比如LSTM以及各个改进对应的经典论文。&lt;/p&gt;&lt;br&gt;&lt;p&gt;还有楼上提到的&lt;i&gt;《LSTM: A Search Space Odyssey》&lt;/i&gt; 通过从新进行各种实验来对比考查LSTM的各种改进（组件）的效果。挺有意义的，尤其是在指导如何使用LSTM方面。&lt;/p&gt;&lt;p&gt;不过，玩LSTM，最好有相应的硬件支持。我之前用Titan 780，现在实验室买了Titan X，应该可以说是很好的配置了（TitanX可以算顶配了）。但是我任务数据量不大跑一次实验都要好几个小时（前提是我独占一个显卡），（当然和我模型复杂有关系，LSTM只是其中一个模块）。&lt;/p&gt;&lt;br&gt;&lt;p&gt;===========================================&lt;/p&gt;&lt;p&gt;如果想玩的深入一点可以看看LSTM最近的发展和应用。老的就不说了，就提一些比较新比较好玩的。&lt;/p&gt;&lt;br&gt;&lt;p&gt;LSTM网络本质还是RNN网络，基于LSTM的RNN架构上的变化有最先的BRNN（双向），还有今年Socher他们提出的树状LSTM用于情感分析和句子相关度计算&lt;i&gt;《Improved Semantic Representations From Tree-Structured Long
Short-Term Memory Networks》&lt;/i&gt;（类似的还有一篇，不过看这个就够了）。他们的代码用Torch7实现，我为了整合到我系统里面自己实现了一个，但是发现效果并不好。我觉的这个跟用于建树的先验信息有关，看是不是和你任务相关。还有就是感觉树状LSTM对比BLSTM是有信息损失的，因为只能使用到子节点信息。要是感兴趣的话，这有一篇树状和线性RNN对比&lt;i&gt;《(treeRNN vs seqRNN )When Are Tree Structures Necessary for Deep
Learning of Representations?》&lt;/i&gt;。当然，关键在于树状这个概念重要，感觉现在的研究还没完全利用上树状的潜力。&lt;/p&gt;&lt;br&gt;&lt;p&gt;今年ACL（2015）上有一篇层次的LSTM&lt;i&gt;《A
Hierarchical Neural Autoencoder for Paragraphs and Documents》&lt;/i&gt;。使用不同的LSTM分别处理词、句子和段落级别输入，并使用自动编码器（autoencoder）来检测LSTM的文档特征抽取和重建能力。&lt;/p&gt;&lt;br&gt;还有一篇文章&lt;i&gt;《Chung J, Gulcehre C, Cho K, et al. Gated feedback recurrent neural networks[J]. arXiv preprint arXiv:1502.02367, 2015.&lt;/i&gt;&lt;i&gt;》&lt;/i&gt;，把gated的思想从记忆单元扩展到了网络架构上，提出多层RNN各个层的隐含层数据可以相互利用（之前的多层RNN多隐含层只是单向自底向上连接），不过需要设置门（gated）来调节。&lt;br&gt;&lt;br&gt;&lt;p&gt;记忆单元方面，Bahdanau
Dzmitry他们在构建RNN框架的机器翻译模型的时候使用了GRU单元（gated recurrent unit）替代LSTM，其实LSTM和GRU都可以说是gated hidden unit。两者效果相近，但是GRU相对LSTM来说参数更少，所以更加不容易过拟合。（大家堆模型堆到dropout也不管用的时候可以试试换上GRU这种参数少的模块）。这有篇比较的论文&lt;i&gt;《（GRU/LSTM对比）Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
Modeling》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;应用嘛，宽泛点来说就是挖掘序列数据信息，大家可以对照自己的任务有没有这个点。比如（直接把毕设研究现状搬上来(｡･∀･)ﾉﾞ）：&lt;/p&gt;&lt;br&gt;&lt;p&gt;先看比较好玩的，&lt;/p&gt;&lt;p&gt;&lt;b&gt;图像处理（对，不用CNN用RNN）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Visin F, Kastner K,
Cho K, et al. ReNet: A Recurrent Neural Network Based Alternative to
Convolutional Networks[J]. arXiv preprint arXiv:1505.00393, 2015》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;4向RNN（使用LSTM单元）替代CNN。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;使用LSTM读懂python程序：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Zaremba W, Sutskever I.
Learning to execute[J]. arXiv preprint arXiv:1410.4615, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用基于LSTM的深度模型用于读懂python程序并且给出正确的程序输出。文章的输入是短小简单python程序，这些程序的输出大都是简单的数字，例如0-9之内加减法程序。模型一个字符一个字符的输入python程序，经过多层LSTM后输出数字结果，准确率达到99%&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;手写识别：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Liwicki M, Graves A,
Bunke H, et al. A novel approach to on-line handwriting recognition based on
bidirectional long short-term memory》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;机器翻译：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Sutskever I, Vinyals
O, Le Q V V. Sequence to sequence learning with neural networks[C]//Advances in
neural information processing systems. 2014: 3104-3112.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用多层LSTM构建了一个seq2seq框架（输入一个序列根据任务不同产生另外一个序列），用于机器翻译。先用一个多层LSTM从不定长的源语言输入中学到特征v。然后使用特征v和语言模型（另一个多层LSTM）生成目标语言句子。&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using rnn encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这篇文章第一次提出GRU和RNN encoder-decoder框架。使用RNN构建编码器-解码器（encoder-decoder）框架用于机器翻译。文章先用encoder从不定长的源语言输入中学到固定长度的特征V，然后decoder使用特征V和语言模型解码出目标语言句子&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上两篇文章提出的seq2seq和encoder-decoder这两个框架除了在机器翻译领域，在其他任务上也被广泛使用。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在上一篇的基础上引入了BRNN用于抽取特征和注意力信号机制（attention signal）用于源语言和目标语言的对齐。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;对话生成：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Shang L, Lu Z, Li H. Neural Responding Machine for Short-Text Conversation[J]. arXiv preprint arXiv:1503.02364, 2015.》&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;华为诺亚方舟实验室，李航老师他们的作品。基本思想是把对话看成是翻译过程。然后借鉴Bahdanau D他们的机器翻译方法（&lt;b&gt;encoder-decoder，GRU&lt;/b&gt;，attention signal）解决。训练使用微博评论数据。&lt;/p&gt;&lt;p&gt;&lt;i&gt;《VINYALS O, LE Q，.A Neural Conversational Model[J]. arXiv:1506.05869 [cs], 2015.》&lt;/i&gt;&lt;/p&gt;google前两天出的论文（2015-6-19）。看报道说结果让人觉得“creepy”：&lt;a href="//link.zhihu.com/?target=http%3A//motherboard.vice.com/read/googles-new-chatbot-taught-itself-to-be-creepy" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Google's New Chatbot Taught Itself to Be Creepy&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 。还以为有什么NB模型，结果看了论文发现就是一套用&lt;b&gt;seq2seq框架&lt;/b&gt;的实验报告。（对话可不是就是你一句我一句，一个序列对应产生另一序列么）。论文里倒是说的挺谨慎的，只是说纯数据驱动（没有任何规则）的模型能做到这样不错了，但还是有很多问题，需要大量修改（加规则呗？）。主要问题是缺乏上下文一致性。（模型只用对话的最后一句来产生下一句也挺奇怪的，为什么不用整个对话的历史信息？）&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;句法分析：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Vinyals O, Kaiser L,
Koo T, et al. Grammar as a foreign language[J]. arXiv preprint arXiv:1412.7449,
2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;把LSTM用于句法分析任务，文章把树状的句法结构进行了线性表示，从而把句法分析问题转成翻译问题，然后套用机器翻译的seq2seq框架使用LSTM解决。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;信息检索：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Palangi H, Deng L,
Shen Y, et al. Deep Sentence Embedding Using the Long Short Term Memory Network:
Analysis and Application to Information Retrieval[J]. arXiv preprint
arXiv:1502.06922, 2015.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;使用LSTM获得大段文本或者整个文章的特征向量，用点击反馈来进行弱监督，最大化query的特性向量与被点击文档的特性向量相似度的同时最小化与其他未被点击的文档特性相似度。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;图文转换：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;图文转换任务看做是特殊的图像到文本的翻译问题，还是使用encoder-decoder翻译框架。不同的是输入部分使用卷积神经网络（Convolutional Neural Networks，CNN）抽取图像的特征，输出部分使用LSTM生成文本。对应论文有：&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Karpathy A, Fei-Fei L. Deep
visual-semantic alignments for generating image descriptions[J]. arXiv preprint
arXiv:1412.2306, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Mao J, Xu W, Yang Y, et al. Deep
captioning with multimodal recurrent neural networks (m-rnn)[J]. arXiv preprint
arXiv:1412.6632, 2014.》&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;《Vinyals O, Toshev A, Bengio S, et al. Show and
tell: A neural image caption generator[J]. arXiv preprint arXiv:1411.4555,
2014.》&lt;/i&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;就粘这么多吧，呼呼~复制粘贴好爽\(^o^)/~&lt;/p&gt;&lt;p&gt;其实，相关工作还有很多，各大会议以及arxiv上不断有新文章冒出来，实在是读不过来了。。。&lt;/p&gt;&lt;br&gt;&lt;p&gt;然而我有种预感，说了这么多，工作之后很有可能发现：&lt;/p&gt;&lt;p&gt; 这些东西对我工作并没有什么卵用
(＞﹏＜＝&lt;/p&gt;
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-08-27</pubDate></item><item><title>如何高效的学习TensorFlow代码?</title><link>https://www.zhihu.com/question/41667903/answer/99268024</link><description>&lt;div class="zm-editable-content"&gt;如题，或者如何掌握TensorFlow，应用到任何领域？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
也在学习中。&lt;br&gt;个人感觉先把TensorFlow的白皮书：&lt;a href="//link.zhihu.com/?target=http%3A//download.tensorflow.org/paper/whitepaper2015.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;download.tensorflow.org&lt;/span&gt;&lt;span class="invisible"&gt;/paper/whitepaper2015.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;这个论文整明白了，大致模型就明白了。然后学习demo。最后再深入整个代码。&lt;br&gt;白皮书有个快翻译完的中文版：&lt;a href="//link.zhihu.com/?target=http%3A//www.jianshu.com/p/65dc64e4c81f" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;[译] TensorFlow 白皮书&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;------------------------------------------------------------------------------------------------------------&lt;br&gt;最近陆续更新了一些学习笔记，与初学者共享：&lt;br&gt;&lt;a href="//link.zhihu.com/?target=http%3A//blog.csdn.net/snsn1984" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;SHINING的博客&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;</description><author>小乖他爹</author><pubDate>2016-08-27</pubDate></item><item><title>推荐系统有哪些比较好的论文？</title><link>https://www.zhihu.com/question/25566638/answer/37455091</link><description>&lt;div class="zm-editable-content"&gt;最好是像kdd的论文和presentation那样，从特定问题出发，特征工程、模型选择等整个过程都提及的&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
（昨天正好回答过一个相关问题，拿过来给题主参考以下）&lt;br&gt;推荐几篇对工业界比较有影响的论文吧：&lt;br&gt;1. The Wisdom of The Few 豆瓣阿稳在介绍豆瓣猜的时候极力推荐过这篇论文，豆瓣猜也充分应用了这篇论文中提出的算法；&lt;br&gt;2. Restricted Boltzmann Machines for Collaborative Filtering 目前Netflix使用的主要推荐算法之一；&lt;br&gt;3. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model 这个无需强调重要性，LFM几乎应用到了每一个商业推荐系统中；&lt;br&gt;4. Collaborative Filtering with Temporal Dynamics 加入时间因素的SVD++模型，曾在Netflix Prize中大放溢彩的算法模型；&lt;br&gt;5. Context-Aware Recommender Systems 基于上下文的推荐模型，现在不论是工业界还是学术界都非常火的一个topic；&lt;br&gt;6. Toward the next generation of recommender systems 对下一代推荐系统的一个综述；&lt;br&gt;7. Item-Based Collaborative Filtering Recommendation Algorithms 基于物品的协同过滤，Amazon等电商网站的主力模型算法之一；&lt;br&gt;8. Information Seeking-Convergence of Search, Recommendations and Advertising 搜索、推荐和广告的大融合也是未来推荐系统的发展趋势之一；&lt;br&gt;9. Ad Click Prediction: a View from the Trenches 可以对推荐结果做CTR预测排序；&lt;br&gt;10. Performance of Recommender Algorithm on top-n Recommendation Task TopN预测的一个综合评测，TopN现在是推荐系统的主流话题，可以全部实现这篇文章中提到的算法大概对TopN有个体会；&lt;br&gt;11. &lt;a href="//link.zhihu.com/?target=http%3A//dsec.pku.edu.cn/%7Ejinlong/publication/wjlthesis.pdf" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;dsec.pku.edu.cn/~jinlon&lt;/span&gt;&lt;span class="invisible"&gt;g/publication/wjlthesis.pdf&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 北大一博士对Netflix Prize算法的研究做的毕业论文，这篇论文本身对业界影响不大，但是Netflix Prize中运用到的算法极大地推动了推荐系统的发展；&lt;br&gt;通过这些论文可以对推荐系统有个总体上的全面认识，并且能够了解一些推荐系统的发展趋势。剩下的就是多实践了。Good luck！&lt;br&gt;&lt;br&gt;------------------------------------------------------------羊年大年初一补充-----------------------------------------------------&lt;br&gt;Quora上有一个相关的回答，质量非常不错：&lt;a href="//link.zhihu.com/?target=http%3A//www.quora.com/What-are-the-seminal-papers-on-recommender-systems" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://www.&lt;/span&gt;&lt;span class="visible"&gt;quora.com/What-are-the-&lt;/span&gt;&lt;span class="invisible"&gt;seminal-papers-on-recommender-systems&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 回答了关于推荐系统起源问题的相关论文，同样值得阅读。
&lt;/div&gt;</description><author>知乎用户</author><pubDate>2016-08-27</pubDate></item><item><title>深度学习（机器学习）的下一步如何发展？</title><link>https://www.zhihu.com/question/47602063/answer/106988408</link><description>&lt;div class="zm-editable-content"&gt;目前机器学习很火，但我对机器学习方向的前景有些困惑。&lt;br&gt;&lt;br&gt;机器学习、计算机视觉下一步的创新点在哪里？随着硬件的迭代，神经网络的隐藏层可以越做越多，CNN、DNN参数可以调得越来越准确，人脸识别、图像跟踪等算法准确率提高...但就像周志华教授调侃的，目前机器学习好像就是调调参数而已。那么机器学习的下一步该走向哪里呢？比方说目前很多公司在做计算机视觉，那么这一领域还有什么可以继续挖掘的呢？此外，机器学习依赖数据，而目前大公司牢牢掌握着数据，对于未来想从事这一领域的人而言，是否还有机会？&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
提几个可能的发展方向，同时也就是尚待解决的问题：&lt;br&gt;&lt;br&gt;1.&lt;b&gt;让深度学习自动调超参。&lt;/b&gt;最近看到有人在一个AI群里推广自己的一篇论文《Deep Q-Networks for Accelerating the Training of Deep Neural Networks》&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.01467" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1606.0146&lt;/span&gt;&lt;span class="invisible"&gt;7&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，大致是用强化学习的方法训练一个控制器来自动控制学习率以及在一个batch中各个类的样本占比。虽然它那篇论文问题很大，训练出来的控制器极其不通用，只能用在它原本的任务上，但是感觉很容易解决掉，这个另说。想象一下，如果能够训练出一个通用的控制器，对于各类任务都能够自动调整超参（或者只在某个子领域比如图像分类做到通用也好），那我们就再也不用自称调参狗了，同时也可以解放出更多的时间用于设计模型、验证架构，想必深度学习的发展步伐会得到极大加速。&lt;br&gt;&lt;br&gt;2.&lt;b&gt;自动学习网络架构。&lt;/b&gt;其实说起来这个问题也可以归入自动调超参，但是感觉应该还是有很大的不同。说起来无非就是两个方面，一是加法二是减法。加法方面可以参考《Net2Net: Accelerating Learning via Knowledge Transfer》&lt;a href="//link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05641" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;https://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1511.0564&lt;/span&gt;&lt;span class="invisible"&gt;1&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，这篇是让CNN自动根据需要自动拓展架构，包括横向的增加filter和纵向的增加layer。减法方面可以参考各类Network Compression（网络压缩）的论文中的所谓Network Pruning（网络剪枝），比如《Deep Compression - Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding》&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1510.00149" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1510.0014&lt;/span&gt;&lt;span class="invisible"&gt;9&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，虽然这些论文出发点不在于自动学习网络架构而在于压缩网络规模，而且它们往往是在训练收敛之后才对网络进行裁剪而非边训练边裁剪，但是感觉只需要再跨一步就可以了。我个人觉得，自动学习网络架构需要解决的最根本问题就是“应该在什么时机进行架构变动”以及“应该怎么变”，第二个问题感觉上述论文算是回答得可以了，但是第一个问题似乎还有很多可以探索的地方。对于第一个问题，似乎强化学习就很适合解决，因为显然可以把它看成一个控制问题。&lt;br&gt;&lt;br&gt;3.&lt;b&gt;迁移学习。&lt;/b&gt;众所周知，深度学习的直接训练依赖大量数据，而transfer和finetune能够有效利用数据量大的外部任务训练出来特征来迁移到数据量小的目标任务上，使得目标任务对于数据量的要求大大减小。现在的问题在于，迁移学习的思想现在大家其实都在用，很多论文中都可以看到finetune的做法，但是对于两个任务之间需要“多像”才能够迁移这么一个问题还没有一个很好的回答。即使我们不奢求能够给出一个严格的数学理论，至少，如果有人能够做一个非常系统的对比实验，总结出一些规律，使得我们有信心说在如何如何这样一个边界内的任务都是基本上可以transfer的，那将会是一个很大的进步。这个问题也可以这么看，如今我们应该有信心说两个图像分类任务可以transfer，但是这个边界太过狭窄，我个人期待的就是能够有一套理论或者方法论使得这个边界大大拓展，然后在这个边界内我们可以像对两个图像分类任务一样自信满满地用迁移学习。&lt;br&gt;&lt;br&gt;4.&lt;b&gt;无监督／半监督学习。&lt;/b&gt;像LeCun等大佬其实一直在鼓吹这方面，但似乎还没有搞出像当年CNN（AlexNet）、最近强化学习（阿法狗）这样级别的大新闻来。我理解在这个问题上的努力方向应该是确定“何种representation最有用”。具体来说，就是找到一个指标，然后用深度网络优化这个指标，使得满足这个指标的data representation能够具有非常好的特性。再具体一些，下面举三个实际例子：&lt;br&gt;&lt;ul&gt;&lt;li&gt;autoencoder以重构损失作为指标来学习一个representation。&lt;/li&gt;&lt;li&gt;之前听一个讲座，演讲人介绍他的论文《Why Deep Learning Works: A Manifold Disentanglement Perspective》&lt;a href="//link.zhihu.com/?target=http%3A//ieeexplore.ieee.org/xpl/articleDetails.jsp%3Farnumber%3D7348689" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;IEEE Xplore Abstract&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，其中定义了三个指标来描述深度网络每一层中data representation的“蜷曲程度”，并发现，越高层的数据蜷曲度越低，换言之，越平展。那么无监督学习是否能够直接以这个蜷曲度作为损失函数来学习一个representation呢？&lt;/li&gt;&lt;li&gt;这篇论文《&lt;a href="//link.zhihu.com/?target=http%3A//people.eecs.berkeley.edu/%7Epathak/context_encoder/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Context Encoders: Feature Learning by Inpainting&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;》提出通过预测周边上下文像素来无监督学习视觉特征，感觉很像word2vec从一维变成二维。&lt;/li&gt;&lt;/ul&gt;除了上述的重构损失、蜷曲度、预测上下文精度，还有没有别的指标学习出来的representation更好呢？个人认为这些问题就是推动无监督／半监督学习进展的关键所在。&lt;br&gt;&lt;br&gt;5.&lt;b&gt;基于外部存储（external memory）的模型。&lt;/b&gt;如果说RNN、LSTM这样的模型属于&lt;u&gt;internal memory / long-term memory&lt;/u&gt;的话，那么以神经图灵机（Neural Turing Machine，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1410.5401" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1410.5401&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）、记忆网络（Memory Network，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1410.3916" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1410.3916&lt;/span&gt;&lt;span class="invisible"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;）为代表的模型就应该称为&lt;u&gt;external memory / really long-term memory&lt;/u&gt;了。不过这两个模型刚出来的时候还太过naive，只能做一些很无聊的task，比如序列复制和排序以及非常简单的QA，但是现在已经开始看到它们被用到更加实际的问题上面，例如One-shot Learning：《One-shot Learning with Memory-Augmented Neural Networks》，&lt;a href="//link.zhihu.com/?target=http%3A//arxiv.org/abs/1605.06065" class=" external" target="_blank" rel="nofollow noreferrer"&gt;&lt;span class="invisible"&gt;http://&lt;/span&gt;&lt;span class="visible"&gt;arxiv.org/abs/1605.0606&lt;/span&gt;&lt;span class="invisible"&gt;5&lt;/span&gt;&lt;span class="ellipsis"&gt;&lt;/span&gt;&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;。往大了说，如果未来要实现强AI，这种外部存储的机制肯定是必不可少的。现在的问题在于，神经图灵机和记忆网络用的外部存储虽然比LSTM那样简单的一个hidden state向量更进一步，但也其实就是很简单的一片矩阵，没有任何结构和层次可言，换言之，就是还不够复杂。所以我猜想接下来可能external memory会和知识图谱（Knowledge Graph）结合起来或至少是向知识图谱类似的做法靠拢，因为知识图谱更加结构化。
&lt;/div&gt;</description><author>郑华滨</author><pubDate>2016-08-27</pubDate></item><item><title>深度机器学习中的batch的大小对学习效果有何影响？</title><link>https://www.zhihu.com/question/32673260/answer/71137399</link><description>&lt;div class="zm-editable-content"&gt;如题，在深度学习中，刚入门的小弟一直听闻一个batch中同时训练多个数据可以得到较好的效果，于是小弟在caffe上跑deepID的网络时对如何选取batchsize颇具困惑。恳求万能的知友给予指点~~&lt;/div&gt;&lt;div class="zm-editable-content clearfix"&gt;
&lt;b&gt;谈谈深度学习中的 Batch_Size&lt;/b&gt;&lt;br&gt;Batch_Size（批尺寸）是机器学习中一个重要参数，涉及诸多矛盾，下面逐一展开。&lt;br&gt;&lt;br&gt;&lt;b&gt;首先，为什么需要有 Batch_Size 这个参数？&lt;/b&gt;&lt;br&gt;Batch 的选择，&lt;b&gt;首先决定的是下降的方向。&lt;/b&gt;如果数据集比较小，完全可以采用&lt;b&gt;全数据集&lt;/b&gt; （ &lt;b&gt;Full Batch Learning&lt;/b&gt; ）的形式，这样做&lt;u&gt;至少&lt;/u&gt;有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而&lt;a href="http://www.zhihu.com/question/37129350/answer/70964527#" class="internal"&gt;更准确地朝向极值所在的方向&lt;/a&gt;。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用&lt;b&gt;Rprop&lt;/b&gt; 只基于梯度符号并且针对性单独更新各权值。&lt;br&gt;&lt;br&gt;对于更大的数据集，以上 2 个好处又变成了 2 个坏处：其一，随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。其二，以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 &lt;b&gt;RMSProp&lt;/b&gt; 的妥协方案。&lt;br&gt;&lt;br&gt;&lt;b&gt;既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？&lt;/b&gt;&lt;br&gt;所谓另一个极端，就是每次只训练一个样本，即 Batch_Size = 1。这就是&lt;b&gt;在线学习&lt;/b&gt;&lt;b&gt;（Online Learning）&lt;/b&gt;。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，&lt;b&gt;难以达到收敛&lt;/b&gt;。&lt;br&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png" data-rawwidth="952" data-rawheight="662" class="origin_image zh-lightbox-thumb" width="952" data-original="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="952" data-rawheight="662" class="origin_image zh-lightbox-thumb lazy" width="952" data-original="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_r.png" data-actualsrc="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png"&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;可不可以选择一个适中的 Batch_Size 值呢？&lt;/b&gt;&lt;br&gt;当然可以，这就是&lt;b&gt;批梯度下降法（Mini-batches Learning）&lt;/b&gt;。因为如果数据集足够充分，那么用一半（&lt;u&gt;甚至少得多&lt;/u&gt;）的数据训练算出来的梯度与用全部数据训练出来的梯度是&lt;u&gt;几乎一样&lt;/u&gt;的。&lt;br&gt;&lt;br&gt;&lt;b&gt;在合理范围内，增大 Batch_Size 有何&lt;u&gt;好处&lt;/u&gt;？&lt;/b&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;内存利用率提高了，大矩阵乘法的并行化效率提高。&lt;/li&gt;&lt;li&gt;跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。&lt;/li&gt;&lt;li&gt;在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;b&gt;盲目增大 Batch_Size 有何&lt;u&gt;坏处&lt;/u&gt;？&lt;/b&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;内存利用率提高了，但是内存容量可能撑不住了。&lt;/li&gt;&lt;li&gt;跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。&lt;/li&gt;&lt;li&gt;Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;b&gt;调节 Batch_Size 对训练效果影响到底如何？&lt;/b&gt;&lt;br&gt;这里跑一个 LeNet 在 MNIST 数据集上的效果。MNIST 是一个手写体标准库，我使用的是 &lt;b&gt;Theano &lt;/b&gt;框架。这是一个 Python 的深度学习库。&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/software/theano/install.html%23install" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;安装方便&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;（几行命令而已），调试简单（自带 Profile），GPU / CPU 通吃，&lt;a href="//link.zhihu.com/?target=http%3A//deeplearning.net/tutorial/contents.html" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;官方教程相当完备&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt;，支持模块十分丰富（除了 CNNs，更是支持 RBM / DBN / LSTM / RBM-RNN / SdA / MLPs）。在其上层有 &lt;a href="//link.zhihu.com/?target=http%3A//keras.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer"&gt;Keras&lt;i class="icon-external"&gt;&lt;/i&gt;&lt;/a&gt; 封装，支持 GRU / JZS1, JZS2, JZS3 等较新结构，支持 Adagrad / Adadelta / RMSprop / Adam 等优化算法。&lt;br&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_b.png" data-rawwidth="753" data-rawheight="176" class="origin_image zh-lightbox-thumb" width="753" data-original="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="753" data-rawheight="176" class="origin_image zh-lightbox-thumb lazy" width="753" data-original="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_r.png" data-actualsrc="https://pic1.zhimg.com/8182178facd79a8828e31966e0c4587c_b.png"&gt;&lt;br&gt;&lt;noscript&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_b.png" data-rawwidth="1558" data-rawheight="344" class="origin_image zh-lightbox-thumb" width="1558" data-original="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_r.png"&gt;&lt;/noscript&gt;&lt;img rel="noreferrer"  data-rawwidth="1558" data-rawheight="344" class="origin_image zh-lightbox-thumb lazy" width="1558" data-original="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_r.png" data-actualsrc="https://pic1.zhimg.com/d6fb7abbaeef80e739d824582a0fa384_b.png"&gt;&lt;br&gt;运行结果如上图所示，其中绝对时间做了标幺化处理。运行结果与上文分析相印证：&lt;br&gt;&lt;ul&gt;&lt;li&gt;Batch_Size 太小，算法在 200 epoches 内不收敛。&lt;br&gt;&lt;/li&gt;&lt;li&gt;随着 Batch_Size 增大，处理相同数据量的速度越快。&lt;/li&gt;&lt;li&gt;随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。&lt;br&gt;&lt;/li&gt;&lt;li&gt;由于上述两种因素的矛盾， Batch_Size 增大到&lt;u&gt;某个&lt;/u&gt;时候，达到&lt;b&gt;时间上&lt;/b&gt;的最优。&lt;/li&gt;&lt;li&gt;由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到&lt;u&gt;某些&lt;/u&gt;时候，达到最终收敛&lt;b&gt;精度上&lt;/b&gt;的最优。&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;欢迎一起讨论。
&lt;/div&gt;</description><author>程引</author><pubDate>2016-08-27</pubDate></item></channel></rss>