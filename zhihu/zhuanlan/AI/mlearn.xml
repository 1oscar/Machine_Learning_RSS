<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器学习笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/mlearn</link><description>机器学习的笔记。非常浅显。不得不说ng的课很适合入门，几乎不需要太多的数学基础。</description><lastBuildDate>Thu, 15 Sep 2016 01:16:20 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>小结及接下来的打算：</title><link>https://zhuanlan.zhihu.com/p/22440031</link><description>目前主干内容已经补充完了。还有大量内容没补充完，今后会继续补充。（比如正规方程怎么推倒啊不对推导的）&lt;p&gt;接下来我有个想法，个人学了几套不同的机器学习课程，感觉光是课堂布置的练习还是不够的，打算自己给自己布置个简单的结业项目。目前的想法是做这两个：&lt;/p&gt;&lt;p&gt;1、做一个简单的房价预测网站。&lt;/p&gt;&lt;p&gt;2、做一个知乎的答案推荐系统。&lt;/p&gt;&lt;p&gt;先做第一个，第一个的计划是这样的，分为3部分：&lt;/p&gt;&lt;p&gt;A、房地产数据获取及数据清洗：嘛，有现成的api最好，找不到的话，就自己爬虫爬，爬来了以后做一个协同过滤系统补完缺失数据。&lt;/p&gt;&lt;p&gt;B、通过数据预测房价。第一个项目就不用神经网络了，用线性回归就行了……做完再慢慢完善嘛。而且就算是个线性回归，不用别人的库，从头开始撸代码要写好了也不是一件容易的事……&lt;/p&gt;&lt;p&gt;C、做个简单的网站通过输入数据可以输出房价。就是做个网站而已。因为光是做了学习系统，以后万一跟别人吹牛逼也没好吹的，放到网站里，以后不管是找工作还是吹牛逼都可以拿出来说了……&lt;/p&gt;&lt;p&gt;目前的想法是，根据这3部分，建立一个学习小组，如果你有兴趣加入，至少得对其中一个部分能有帮助，能编写你的代码并教会小组内其他小伙伴。不接受其他方式加入，做完以后会征求组内意见把学习过程及内容放出来大家同意放出来的部分。&lt;/p&gt;&lt;p&gt;嘛，有人来一起做边做边学最好。没人来的话，反正我这水货的编程能力，用python爬虫也能编，很丑的学习系统也能编，用flask做个小网站也不是不会。只是我自己一个人的话边做边学就会很久咯。&lt;/p&gt;&lt;p&gt;有兴趣的可以联系我。两周内开始。&lt;/p&gt;&lt;p&gt;想要加入请私信我告诉我你能在3部分中，在&lt;u&gt;哪一部分帮助到其他同学&lt;/u&gt;（例：你可以发私信的时候告诉我说你爬虫爬的很不错，会爬数据并整理到sql里云云的都可以。总之门槛很低，但是你必须得能分享知识给别人。除此之外不接受任何加入方式。）。&lt;/p&gt;</description><author>子楠</author><pubDate>Wed, 14 Sep 2016 08:03:57 GMT</pubDate></item><item><title>第十周笔记：大量数据算法</title><link>https://zhuanlan.zhihu.com/p/22168288</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ad95d41a014fb95c79712a446925d4d0_r.jpg"&gt;&lt;/p&gt;（第九周笔记还有推荐系统没补充完，不过要补充好推荐系统还得查很多资料，所以暂时没有补充的欲望。）&lt;p&gt;首先一个问题，是否需要用大数据？&lt;/p&gt;&lt;p&gt;记得之前笔记（第六周笔记）中，我们分析过一种情况，就是当样本数量m达到一定的程度时，增加样本数量，对于交叉验证集和训练集之间得出的误差的差距，并没有太多减少的空间时，就不再需要增加样本量。&lt;/p&gt;&lt;p&gt;一句话，当分别计算J_cv和J_train在当前样本数量下的误差时，差距极大时，应当使用更多数据，差距极小时，就不应当使用大量数据了。&lt;/p&gt;&lt;p&gt;那么，大量数据时，用什么算法呢？&lt;/p&gt;&lt;p&gt;数据太多，如果还按照批量梯度下降来玩，估计一个因子都得算个三五天，整个公式出出来，改一改，一个月就过去了……&lt;/p&gt;&lt;p&gt;那就不用所有因子来玩嘛。一步一步局部下降（贪婪算法）也不错嘛！但是贪婪算法也有问题，那就是，如果你的数据凹凸不平的，有很多个局部最优，那就会发生如下情况：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d0b5aa7bdd8bcab2ff1bcaa1c31e5d7e.png" data-rawwidth="471" data-rawheight="324"&gt;&lt;p&gt;那咋办呢？是的，你可以多重复随机选初始点迭代几次，然后在交叉集上比较，但是万一这种坑坑洼洼的地方很多，那你得重复很多次，运气不好兴许一辈子就过去了~&lt;/p&gt;&lt;p&gt;那我们就不用局部最优，还是得用全部数据，那不就慢了么，怎么办呢？那就每次就一个数据就拿去改变参数，凑合凑合得了。由于很随便，所以就叫它随便，啊不对随机梯度下降……&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机梯度下降：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们先看看以前的批量梯度下降的公式：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha *\frac{d}{d\theta_{j} } J_{\theta} &lt;/equation&gt;=&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{m}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么，现在我们改成了每次随便挑一个数据，于是公式就成了：&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*{} (h(x_i) -y_{i}  )*x_{j}&lt;/equation&gt;&lt;p&gt;也就是说，other than 把所有的数据拿来加一遍再改变参数值，我们现在变成了急不可耐地看到一个数据就改变一次参数值……&lt;/p&gt;&lt;p&gt;啥，你说这不就是贪婪算法，并不随机么……？&lt;/p&gt;&lt;p&gt;那好，那就……执行之前，把数据打乱，python的话就randshaffle一下，于是就随机了呗~~~&lt;/p&gt;&lt;p&gt;那么最后还有一个问题，我们以前的批量梯度下降，是一遍又一遍地迭代，直到收敛到某一个范围内。现在，你每个数据就下降一次，需要重复多少次呢？&lt;/p&gt;&lt;p&gt;不重复啊。说了这是给大量数据准备的，我们就假吧意思大量数据来说下降一次就够了。于是我们就总共只下降一次……只是对于其中，每个数据迭代的时候，就直接改变参数值&lt;equation&gt;\theta&lt;/equation&gt;了。所以你也看到了……由于只下降一次，数据不够多时就别用了。误差会很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迷你批量梯度下降（略拗口）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们也可以换个思路，还是用批量梯度下降，只是这时候不用所有数据，而是和刚才哪个随机梯度下降的思路结合一下。批量一度下降是所有数据一起算一次，改变一次参数值对吧？随机梯度下降时每个观测数据都拿来改变一次参数值对吧？咱中国人讲究中庸嘛。就既不拿所有的，也不光拿一个，而是一小批一小批地拿来下降，比如每次10个数据下降一次啦，每次20个数据下降一次啦。之类的。&lt;/p&gt;&lt;p&gt;那么我们怎么选呢？&lt;/p&gt;&lt;p&gt;把数据随机分成x份，每份里面有10~100个数据。以每份b个数据举例，我们要迭代的参数公式就成了这样：&lt;/p&gt;&lt;p&gt;i=1,b=10(假设b为10)&lt;/p&gt;&lt;p&gt;while i &amp;lt; m&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{i+b-1}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;p&gt;i += b&lt;/p&gt;&lt;p&gt;差不多就是这样……意思就是从每个拿来改变一次参数&lt;equation&gt;\theta&lt;/equation&gt;，变成了每b个拿来改一次参数&lt;equation&gt;\theta&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查收敛：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那么现在又有一个问题来了。我们数据多了就变懒了，算一次凑合凑合过了，万一不收敛怎么办？所以还得检查收敛……&lt;/p&gt;&lt;p&gt;怎么检查收敛呢？&lt;/p&gt;&lt;p&gt;拿出我们的代价函数：&lt;equation&gt;cost=1/2*(h(x_{i} )-y_{i}  )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;每迭代k次，就计算一下。最后把图画出来，看看收敛不收敛。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e04f380c74c1942eaeca162e3d2009ff.png" data-rawwidth="720" data-rawheight="419"&gt;蓝色是1000次看一下，红色是5000次看一下。横坐标是看的次数，纵坐标是当前看的时候cost的值。&lt;/p&gt;&lt;p&gt;可以看到，第一幅图是在收敛的表现，到达最低点后开始震荡。第二幅图意思是你看的范围越大，看到的振幅越小（批量梯度下降看起来就很光滑了。相反，k=1的时候，看起来就心电图了……）第三附图表示的意思是，如果你看着震荡不确定到最小没有，增加k，就可以知道到底是应该优化算法还是只是杂音造成的震荡。第四附图当然就是发散了，这时候可以试试小一点的&lt;equation&gt;\alpha &lt;/equation&gt;值。&lt;/p&gt;&lt;p&gt;接下来又有一个问题来了。大量数据一般都不是一次就给完的，很多时候，是连续给你赛数据，算法也要与时俱进嘛。当然，这时候你就可以直接用上面说的两种算法，新数据直接分成几份，算了就马上拿去改变参数值。这里可以让客户端完成一部分简单的数据处理再交给服务器，比如服务器每天计算一次参数，客户端通过这些参数，把缺失数据补充完整，再返回给客户端。又例如如果网络恨不好，而数据量足够客户端依据数据计算出一个代价函数，返回这个代价函数而不是数据，也许能提高一定的效率。嘛……再多的就要学分布式处理了。这里就不多扯淡了、&lt;/p&gt;&lt;p&gt;第11周的笔记内容很少，大致就是讲一下如何涉及一个机器学习的解决计划，以及感谢收看云云。在这里一并写了吧。基本思路就是，把一个复杂的机器学习问题分解成n个小问题（当然你也可以让神经网络自己分解去……）例如你要让机器自己看书，你就可以这样分解问题，让机器识别出书，让机器通过书识别出段落，语句，让机器通过段落语句识别其中的文字关联，进而识别出其抽象化的意思。让机器通过抽象化的意思重新组合合成你需要的信息。这种，把一个复杂的问题拆分成n个细小的问题一个一个解决。&lt;/p&gt;&lt;p&gt;常见的，例如我们要自动分析一个地区菜价，拿上来的数据不可能100%完整，那么就可以分为两套来做，第一个系统用来补充不完整信息（天气，道路交通，政府政策等），第二个系统用第一个系统已经加工好的信息来分析出想要的数据，第三个系统根据这些数据来预测未来的菜价。&lt;/p&gt;&lt;p&gt;实际过程中，由于每次系统都有偏差，最后总准确率会不断下降。例如上面说的哪个分析菜价的，也许总准确率为80%，然后如果是给完善的信息，不考虑第1个系统的误差，准确率是90%，如果给完善的数据，直接用来预测，准确率是98%。那么就可以简单的理解为，系统1可改善上限为10%，系统2可改善上限为8%，系统3可改善上限为2%。假如时间有限，也许就应该把宝贵的时间主要用以改善系统1和系统2。&lt;/p&gt;</description><author>子楠</author><pubDate>Tue, 13 Sep 2016 08:46:36 GMT</pubDate></item><item><title>第九周笔记：密度估计</title><link>https://zhuanlan.zhihu.com/p/21898453</link><description>考虑一个产品，每个工厂生产线都有一定的概率产生次品。假设在用户退货之前，我们没法知道一样东西是否是次品。那么我们只能通过产品的各项指标估计（重量，硬度，曲率，发热量等，不同产品指标不一样）将合格的某项指标画在图上，也许会得到这样一个图（左）：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/606054110fbd2fbdcc26ea5e5aaf8356.png" data-rawwidth="500" data-rawheight="250"&gt;把图按照浓度梯度描红，得到右边的这个区域。按照聚类的思考方式，大概就可以理解为，越接近高浓度区域，其正品概率越高。而越不太可能产生落点的区域，其次品的概率越高。&lt;/p&gt;&lt;p&gt;用数学语言描述就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)&amp;lt;\xi &lt;/equation&gt;时，产品判断为次品。&lt;equation&gt;\xi &lt;/equation&gt;为异常判断参数，&lt;equation&gt;p(x)&lt;/equation&gt;为当前特征的产品产生概率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)=\prod_{j=1}^{n} p(x_j,\mu _j,\sigma _j)&lt;/equation&gt;，其中&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\sigma _j^2=\frac{1}{m} \sum_{1}^{m}{(x_j^{(i)}-\mu _j)^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;当然，你可以简单地把正常和异常记作y=1和y=0.&lt;/p&gt;&lt;p&gt;选&lt;equation&gt;\xi &lt;/equation&gt;的方法，就是之前（第六周笔记）里计算F值，取F值最大时的&lt;equation&gt;\xi &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;那么问题来了，既然可以用y=1和y=0来标记出正常和异常，为什么不用监督学习，而要用密度估计呢？&lt;/p&gt;&lt;p&gt;因为大多数情况下，生产线上的异常值相对正常值来说，是极其少量的。在这种情况下，如果用监督学习，那么就没有足够的负样本用于训练集。此时，为了准确，我们就可以只把负样本用于交叉验证集与测验集，而训练集不用负样本。既然不用负样本，那么训练的时候也只要用非监督学习了……&lt;/p&gt;&lt;p&gt;还有一种可能，负样本相对与数据来说，非常奇怪，每一个负样本都不一样，或者未来可能产生的负样本具有不确定性，那么这种情况下，负样本无法建模，没法判断，只能通过和正样本的差异来判断是否为负样本。这种情况下，显然训练时，只用正样本的非监督学习是优于监督学习的……&lt;/p&gt;&lt;p&gt;反之，如果样本特征明显，可预测，未来样本可确定，且正负样本均足够多的情况下，用监督学习更优。&lt;/p&gt;</description><author>子楠</author><pubDate>Wed, 10 Aug 2016 18:46:02 GMT</pubDate></item><item><title>第八周笔记：聚类（clustering）</title><link>https://zhuanlan.zhihu.com/p/21798972</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2e901f902f1351d6824c7d5e27b9d8aa_r.jpg"&gt;&lt;/p&gt;聚类很好理解。你左手在地上撒一把盐，右手在地上撒一把糖。假设你分不清盐和糖，但是你分别是用左右手撒的，所以两个东西位置不同，你就可以通过俩玩意的位置，判断出两个东西是两类（左手撒的，右手撒的）。然而能不能区别出是糖还是盐？不行。你只能分出这是两类而已。但是分成两类以后再去分析，就比撒地上一堆分析容易多了。&lt;p&gt;聚类是典型的非监督学习。上面例子中，如果把盐和糖改成白色盐和染成黄色的糖，你可以通过颜色分析，颜色就是标签，有标签就是监督学习。没标签就是非监督学习。&lt;/p&gt;&lt;p&gt;聚类算法常用K均值算法：&lt;/p&gt;&lt;p&gt;随机选择包含K个中心的cluster(&lt;equation&gt;\mu_1,\mu_2,\mu_3,\mu_4,...,\mu_k&lt;/equation&gt; ,&lt;equation&gt;K\in R&lt;/equation&gt;)&lt;/p&gt;&lt;p&gt;以K=3为例，假设特征量是啥我也不知道……反正有特征量，现在画3个圈圈：&lt;/p&gt;&lt;p&gt;随机找3个点（图中①，②，③的蓝色点）当然，你找的点不一定就马上在中心了。有可能3个点都跑一个cluster里面去，无所谓……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7f7dbc4a792c474dc10b8b5159b05770.png" data-rawwidth="359" data-rawheight="372"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c98d20495d645ca6b8c27c8c5d33e0be.png" data-rawwidth="385" data-rawheight="398"&gt;&lt;p&gt;然后开始迭代，每次迭代方法如下：&lt;/p&gt;&lt;p&gt;对于每个特征点&lt;equation&gt;x_i&lt;/equation&gt;，找到和特征点&lt;equation&gt;x_i&lt;/equation&gt;最近的那个&lt;equation&gt;\mu _i&lt;/equation&gt;（例如&lt;equation&gt;\mu _1&lt;/equation&gt;），把每个和&lt;equation&gt;\mu _1&lt;/equation&gt;最近的点放到一起。对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;都这个操作，例如你选了3个点，就是3个&lt;equation&gt;\mu _i&lt;/equation&gt;（&lt;equation&gt;\mu _1&lt;/equation&gt;，&lt;equation&gt;\mu _2&lt;/equation&gt;，&lt;equation&gt;\mu _3&lt;/equation&gt;）&lt;/p&gt;&lt;p&gt;然后对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;的最近点&lt;equation&gt;x_i&lt;/equation&gt;，取个平均值&lt;equation&gt;\bar{x_i} &lt;/equation&gt;，当作该&lt;equation&gt;\mu _i&lt;/equation&gt;的新值。然后你会神奇的发现，每个&lt;equation&gt;\mu _i&lt;/equation&gt;都朝着一个cluster的中心近了一步！（一点也不神奇好吗……）&lt;/p&gt;&lt;p&gt;重复n多次，直到收敛为止……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d22d2960559a5a5ac8503b94c8f1fb01.png" data-rawwidth="354" data-rawheight="382"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/788ce87a02bf4de1903c6e4470d52ce4.png" data-rawwidth="377" data-rawheight="387"&gt;&lt;p&gt;你就会神奇地发现这3个点把区域按照某种神奇的规则分出来了（这个规则就是K均值最小……）这3个不同的区域就是3个cluster。&lt;/p&gt;&lt;p&gt;具体的算式如下：&lt;/p&gt;&lt;equation&gt;J=\frac{1}{m} \sum_{m}^{i=1}{(x_i-\mu _c^{(i)})} &lt;/equation&gt;&lt;p&gt;由于这次咱不是监督算法了，所以没法去压导数去迭代。上文也说了怎么迭代，所以这次迭代方法是：&lt;equation&gt;minJ(c^{(1)}......c^{(m)};\mu ^{(1)}......\mu ^{(k)})&lt;/equation&gt;不和导数扯上关系了，自己根据每个点算最小值不断迭代到收敛就是了……&lt;/p&gt;&lt;p&gt;然后就是选每个初始K的点也有技巧，一旦选不好，有可能3个点选到一个簇，本来应该是分成3个圆形片区，结果把整张图片分成3个长方块的也不是不可能……&lt;/p&gt;所以要随机选，为了保险，可以多随机几次。&lt;p&gt;然后就是另一个问题了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;K元素选几个？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选少了，偏差很大。选多了，过拟合了。如果愿意画图，有个方法叫肘子方法（我饿了……）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4695f9282ddbcac507e6697fda29ac78.png" data-rawwidth="395" data-rawheight="287"&gt;诺，就是画这么一个图，那个大概是肘子的地方就是我们要选的K的个数……不过肘子方法缺点很明显，老画图人工去看，一点也不机器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;维数缩减（Rimensionality Reduction ）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;举个例子。你做房子的数据的爬虫。假设全是正方形房子，爬虫中有这3个特征量：面积，长度，宽度。那么因为面积和长度宽度相乘相等，就可以缩减了。再例如，你爬到了一个是摄氏度，一个是华氏度，由于数据本身是一样的，也可以缩减了。把实际内核为同一个特征的的缩减到同一个，就是维数缩减。估计你也看出来了。K的个数就是维度嘛……所以维数缩减一定程度上可以看作选择k的个数。&lt;/p&gt;&lt;p&gt;当然……既然数据缩减了，储存需要的空间也就小了。（对于我们这种屌丝来说，省一张硬盘就是几百块钱，四舍五入就是一个亿啊……多缩减几次数据就省出一个王思聪了。）&lt;/p&gt;&lt;p&gt;实际做法，将J维的数据投影到一个I维空间上。&lt;/p&gt;&lt;p&gt;比如这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/61b75d03e002a7a17b8b875ae1d39ea8.jpg" data-rawwidth="224" data-rawheight="225"&gt;二向箔攻击！（容我中二一下……）三维空间里的数据就被压缩成了二维的了……~&lt;p&gt;通常要可视化数据的话，由于我们人类很傻，只能看到3维，所以一般要把数据降到3维或者二维……举个例子……把一个国家杂七杂八的经济发展指标压缩成一个GDP，然后分别看人口和GDP关系，工人比率和GDP关系……这种。&lt;/p&gt;&lt;p&gt;&lt;b&gt;主成分分析算法（PCA）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上文说了一个可视化数据要压缩，那怎么知道压缩了以后不会产生极大偏差呢？要投影数据，那么投影到哪个屏幕啊不对平面上呢？&lt;/p&gt;&lt;p&gt;实际做法：找能使投影误差最小的那个维度（平面）&lt;/p&gt;&lt;p&gt;定义i个向量，将J个特征量投影到这i个向量组成的子空间上。&lt;/p&gt;&lt;p&gt;如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\mu _i &lt;/equation&gt;这玩意就是我们说的向量。&lt;/p&gt;&lt;p&gt;然后把原来的&lt;equation&gt;x_j^{(i)}&lt;/equation&gt;变成&lt;equation&gt;\frac{x_j{(i)}-\mu _i}{s_j} &lt;/equation&gt;。&lt;equation&gt;s_j&lt;/equation&gt;是数据归一化用的玩意（例如方差，最大值什么的。随便你选个你喜欢的）。&lt;/p&gt;&lt;p&gt;然后问题来了，咱算数据肯定是一列一列算，谁闲着没事一个一个算。那么如何一列一列算呢？&lt;/p&gt;&lt;p&gt;定义sigma，&lt;equation&gt;Sigma=\frac{1}{m}\sum_{a}^{b}{(x^{(i)}*x^{(i)T})}  &lt;/equation&gt;拿矩阵说的话就是&lt;equation&gt;Sigma=\frac{1}{m}*X'*X &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然后用一个Octave里面的算法svd，神奇的事情就发生了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[U,S,V]=svd(Sigma)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们就得到了包含n个（特征量个数）&lt;equation&gt;\mu _i &lt;/equation&gt;的一个矩阵U了。然后我们要缩减到k个纬度，就从这个矩阵U里面提取前k个就行了（误差好大的感觉……不过算了。压缩数据本来误差就大）。&lt;/p&gt;&lt;p&gt;用Octave的话就是这么个玩意,假设我们压缩到k个纬度。：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;U_reduce=U(:,1:k);
Z=U_reduce'*X
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就可以用这个U_reduce开开心心地投影出了我们要的z(压缩过的特征量)了。&lt;/p&gt;&lt;p&gt;那假如我后悔了。我不该压缩我亲爱的数据的，我一个臭屌丝又没钱买硬盘所以并没有数据的备份，那我想要回最初的数据咋办呢？&lt;/p&gt;&lt;p&gt;那就……那就算一算原数据吧……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;由于：Z=U_reduce'*X&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1所以我们可以反推X：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个X当然是有误差的。所以就别写作X了，写作X_approx好了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X_approx=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就是自动选K的问题。自动选K这事很有趣，如果X被压缩以后，解压缩回来误差很小，那大概可以认为此时选取的k是比较合适的。方法如下：&lt;/p&gt;&lt;p&gt;计算误差&lt;equation&gt;error=\frac{\sum_{i=1}^{m}{(x_i-x_{i,approx})}^2 }{\sum_{i=1}^{m}{x_i} ^2} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;如果error&amp;lt;0.01的话，那这时候的k大概就在肘子那里了……当然，你也可以写个循环，找出error最小时的k。&lt;/p&gt;&lt;p&gt;顺便说一句，PCA也可以用于监督学习中，以便我们这些穷B减少自己心爱的计算机的负荷（好心酸。我要买个显卡坞来增加运算能力，谁也别拦着我！）具体的做法就是把&lt;equation&gt;(x_i,y_i)&lt;/equation&gt;PCA成&lt;equation&gt;(z_i,y_i)&lt;/equation&gt;，然后代入假定函数中。当然，既然特征量已经从x变成z了。记得在用交叉函数J_cv和测试函数J_test的时候，也要把x变成z。因为特征量的数据并不同所以需要再变一次，这地方容易出错……&lt;/p&gt;&lt;p&gt;由于PCA整个方案都没用到y，所以过拟合问题并不能用PCA来降维攻击，还是老老实实的用正则化吧……正则化简单粗暴还不&lt;/p&gt;</description><author>子楠</author><pubDate>Sat, 30 Jul 2016 23:03:40 GMT</pubDate></item><item><title>第七周笔记：支持向量机SVM</title><link>https://zhuanlan.zhihu.com/p/21678475</link><description>以逻辑回归为例：&lt;equation&gt;\theta^T*x_i&lt;/equation&gt;&lt;p&gt;我们要找到（不断缩小）的代价函数（的导数）。代价函数长这样：&lt;/p&gt;&lt;equation&gt;J(\theta_j)=1/2m*(\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )&lt;/equation&gt;&lt;p&gt;他的导数就长这样：&lt;equation&gt;1/2*(\sum_{i=1}^{m}(-y_i*log(h(x_i))+(y_i-1)log(1-h(x_i)))+\lambda/2m* \sum_{j=1}^{n}{\theta_j} )&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;把函数换一种写法：&lt;/p&gt;&lt;equation&gt;1/2*(\sum_{i=1}^{m}-y_i*cost(\theta^T*x_i)+(y_i-1)cost(\theta^T*x))+\lambda/2m* \sum_{j=1}^{n}{\theta_j} )&lt;/equation&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/66f7ed7af4fbb60a76f8e30193fdd4a2.jpg" data-rawwidth="352" data-rawheight="143"&gt;&lt;p&gt;我们知道，我们预测有两个结果，&lt;equation&gt;h(x)&lt;/equation&gt;也就是样本的结果y为1或者为0所以很明显，分类时就是：y=1时&lt;equation&gt;\theta^T*x_i\geq  1&lt;/equation&gt;，y=0时就是&lt;equation&gt;\theta^T*x_i \leq -1&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;这样很开心，因为我们的决策边界从一条线，变成了一个区域。所以这玩意又称为大间距分类器。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/02c18abd0da8e95346fc5a931b065e27.png" data-rawwidth="800" data-rawheight="862"&gt;&lt;/p&gt;&lt;p&gt;大间距分类器有个好处，可以自动忽视异常值（区域内值）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;核函数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;好多人搞不明白为毛要用核函数。其实很好理解。我举个例子你就理解了。&lt;/p&gt;&lt;p&gt;假设现在我在地板上画了一条线，然后扔了两张纸在地上，让你判断纸在线的左边呢，还是在右边。&lt;/p&gt;&lt;p&gt;由于纸张压线了，或者两张纸举例不一样，你估计会一脸蒙蔽。于是要找个方法对吧，那就把纸张折叠一些，用我优雅的折纸技术把这张纸折成了一个球。再扔到线上。&lt;/p&gt;&lt;p&gt;这次由于纸折成了球，你相对于纸张整个压在线上，就更容易判断现在纸在左边还是右边了。核函数，就是类似于这个例子里折纸，当然，纸就是你的特征量咯……&lt;/p&gt;&lt;p&gt;实际操作中和这个例子还是有很大区别的，比如几张纸之类的……（手动滑稽）。&lt;/p&gt;&lt;p&gt;回到正题。&lt;/p&gt;&lt;p&gt;现在我们定义一个新的特征量l。l和x都是特征量，为了让他们共同预测结果，我们要对其建立一个新的函数关系式以形成cost函数。&lt;/p&gt;&lt;p&gt;当然……你（的折纸方法）写成 x-l 或者 x / l 也没人拦得住你，不过一般情况下，我们选择高斯核函数：&lt;/p&gt;&lt;equation&gt;f_j=similarity(x,l^{(j)})=exp(\frac{-(x-l^{(j)})^2}{2\sigma ^2} )&lt;/equation&gt;&lt;p&gt;这里，当x与l越接近，f越接近1.x与l差距越大，f越接近0.&lt;/p&gt;&lt;p&gt;嗯，猜对了，实际处理SVM问题，核函数的意义就在于替换那个x为f。&lt;/p&gt;&lt;p&gt;实际问题中，是否选择SVM可以如下考虑：&lt;/p&gt;&lt;p&gt;n多，甚至大于m时，用逻辑回归或者无核SVM。&lt;/p&gt;&lt;p&gt;n小时，用高斯核函数处理特征量后的SVM。&lt;/p&gt;&lt;p&gt;n非常小，而m非常大时，也许需要增加更多的特征量（n是特征量数，m是数据数量），而由于数据很多，出于计算速度考虑的话，可以用逻辑回归或者无核SVM。&lt;/p&gt;&lt;p&gt;如果电脑够好，不需要考虑计算速度，在此之上可以使用神经网络。（神经网络很早以前就有了，近几年火主要是因为计算机计算能力提升）&lt;/p&gt;</description><author>子楠</author><pubDate>Fri, 22 Jul 2016 10:40:32 GMT</pubDate></item><item><title>第六周笔记：评估假定函数</title><link>https://zhuanlan.zhihu.com/p/21551533</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/118716ddfa124a64b0b8ebe57b68ec02_r.jpg"&gt;&lt;/p&gt;机器学习算法可能给出多个假定函数，例如多维线性回归时多组最优&lt;equation&gt;\theta&lt;/equation&gt;，神经网络选取多少个隐藏层等，显然一直用人脑画图判断是不适合的。所以要了解如何评估假定函数，以选取一定情况下最适合的假定函数。&lt;p&gt;评估假定函数第一点，数据要分组，一组是训练组（training set），一组是测试组（test set）。通过训练组训练假定函数，通过测试组测试误差。&lt;/p&gt;&lt;p&gt;假设我们训练组得出的的假定函数为&lt;equation&gt;J(\theta)&lt;/equation&gt;,我们把测试组的特征值装进这个假定函数中，得到测试组预测结果&lt;equation&gt;J_{test}(\theta)&lt;/equation&gt;。那么，我们得到的误差就是测试组真实结果&lt;equation&gt;y_{test}&lt;/equation&gt;减去测试组预测结果（然后取绝对值。）因为取绝对值太麻烦，我们干脆给个平方就好，即：&lt;equation&gt;(y_{test}-J_{test}(\theta))^2&lt;/equation&gt;我们称这个玩意为测试误差&lt;equation&gt;err(h(x),y)&lt;/equation&gt;，所以咱现在就有公式了：&lt;equation&gt;err(h(x),y)=(y_{test}-J_{test}(\theta))^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;有了这个公式，我们现在就可以知道我们训练出来的假定函数的误差是多少了，但是现在又存在了另一个问题，如果我们只把数据分为俩组，那么我们一个拿来评估，一个拿来训练，然而我们知道，既然是训练，存在一个问题叫做过拟合，那么当然是函数越复杂训得出的结论越优秀，而如果我们用评估结果来确认选哪个，如何知道我们的选择没有“过拟合”呢？所以这样做，并没有一个合适的标准来确定该选哪个假定函数。&lt;/p&gt;&lt;p&gt;所以要把数组分为3组，一组用来训练出假定函数，一组用来选出最适合的假定函数（称之为交叉验证组），一组用来评估，知道我们没选错（一般3份按照0.6，0.2，0.2的方式分类，也有0.7，0.2，0.1的分组方式，不管怎么说，训练组占大头，具体看你数据多寡与实际预测情况）。&lt;/p&gt;&lt;p&gt;函数具体画出来是这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/17ec84ff3f63f77f6b368f0eb6ef1890.png" data-rawwidth="584" data-rawheight="466"&gt;&lt;p&gt;图中可以看出，对于已有数据（training set），假定函数越复杂，计算误差（training error）越小。而对于新数据（交叉验证组或其他测试组或预测具体数据时）假定函数越复杂，假定函数越复杂，计算误差（Prediction Error for New Data）就越大。这也解释了为什么过拟合时预测结果不准确。&lt;/p&gt;&lt;p&gt;理论上来说，我们就是要用交叉验证组，找到测试误差（Model Prediction Error ）最小时的那个假定函数。&lt;/p&gt;&lt;p&gt;然后，有了测试误差这个评估指标，我们就可以讨论很多先前说道的高偏差或高方差问题了。例如，我们现在讨论&lt;equation&gt;\lambda &lt;/equation&gt;（正则化）为什么能一定程度上解决过拟合问题。就可以画个图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/269759f605334469972aed75856cac80.png" data-rawwidth="243" data-rawheight="207"&gt;&lt;p&gt;这样就可以看出为什么选lambda过小过大都不适宜，而如何用交叉验证组选了。&lt;/p&gt;&lt;p&gt;然后还有一个问题，是否数据越多越好？我们知道，数据越多，意味着得花更多钱买数据，或者得用更多的时间去爬各个网站，总之数据越多成本越高。那么，测试误差在数据量下，对于交叉验证集和训练集的作图就如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4e3715996f5ea81beca097157d850f2c.png" data-rawwidth="405" data-rawheight="219"&gt;&lt;/p&gt;&lt;p&gt;可以看出，随着数据量上升，交叉验证集的误差虽然越来越小，但是会趋于平缓，而且由我们的预测方法可知，其他集合的测试误差理论上不可能比训练误差更小，而且预测准确率不可能是100%，所以J_train和J_cv的测试误差之间是一定会有空隙的，这个空隙如果已经比预测的可接受误差范围小，那就没必要再增加训练样本了。（这种思维方式在生活中是很有用的）&lt;/p&gt;&lt;p&gt;从另一个角度出发，如果获取一个新的数据能得到的对预测准确性的经济价值，已经低于获取一个新数据的成本，也是没必要再增加新的数据的。所以不管怎么说，数据量并不是越多越好。&lt;/p&gt;&lt;p&gt;同理，其他预测的小技巧，也能用这种方式，验证其是提高的偏差，还是降低了拟合有效性。预测结果的误差无非就是两种，一种是过拟合，一种是欠拟合。所以这个程度上是可以很简单地判断的……&lt;/p&gt;&lt;p&gt;简单分类，过拟合问题包含：训练集数据太多，训练集包含过度多项式，过于复杂。隐藏层过多，lambda太小等。而欠拟合问题就是相反……&lt;/p&gt;&lt;p&gt;由于你没法提前知道自己的机器学习系统结果误差会有多大，所以正确的做法是先糊弄一下，随便搞一个出来，然后不断迭代……（找到误差率，判断是欠拟合还是过拟合，然后改进）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;然后是精准率和召回率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个词对于大多数做过数据处理的人并不陌生……用大白话说：&lt;/p&gt;&lt;p&gt;精准率就是所有可能的事件中，我预测的要发生事件有多少件发生了。&lt;/p&gt;&lt;p&gt;而召回率就是，所有发生了的事件中，我预测到了几个。&lt;/p&gt;&lt;p&gt;显然，单纯地高精准率或者高召回率，得到的结果都是没有意义的。例如，假设有1万人，其中有1人得了高传染性的病，你预测说这1万人都没得病，预测精准率是99.99%，然而我觉得不用解释你也明白，这99.99%的精准率一点卵用也没有……&lt;/p&gt;&lt;p&gt;数学语言表达，假设事件被分为2种，一种是发生了的(True)，一种是没发生(Fake)的。而我们的预测也是两种，一种是预测到的(Predicted)一种是没预测到的(Unpredicted).&lt;/p&gt;&lt;p&gt;精准率就是：&lt;equation&gt;Pre=\frac{T.P}{(T+F).P} &lt;/equation&gt;（.p的表示是这玩意的概率，T.p+F.p+P.p+UN.p=1）&lt;/p&gt;&lt;p&gt;召回率就是：&lt;equation&gt;Recall=\frac{T.P}{(P+UN).P} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;理论上来说，预测结果应当是精准率与召回率双高才可。自动选择的话，参考方式是选择&lt;equation&gt;F=\frac{1}{\frac{1}{Pre} +\frac{1}{Recall} } &lt;/equation&gt;最高的值。&lt;/p&gt;&lt;p&gt;不过一般不这样写，一般写作&lt;equation&gt;F=\frac{2*Pre*Recall}{{Pre} +{Recall} } &lt;/equation&gt;（乘以一个2的意思是，理想最佳情况下，也就是两者均为1时，得到的F也为1.）&lt;/p&gt;</description><author>子楠</author><pubDate>Mon, 11 Jul 2016 14:40:30 GMT</pubDate></item><item><title>第五周笔记：神经网络分类问题应用</title><link>https://zhuanlan.zhihu.com/p/21464253</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b0a9f28d6b0a9d0b50e46b7ba0f4f82b_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;分类问题是什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;简单来说，假设nxn矩阵K，K的每列表示对一个特征的分类，一共n个特征。&lt;/p&gt;&lt;p&gt;当n为1时，K就是[0]或者[1]，意思就是这个特征有还是没有。这时候输出的单元（向量）是一个。&lt;/p&gt;&lt;p&gt;当n&amp;gt;=2时，K可以写成&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[
1,0...0,0
0,1...0,0
.       .
.       .
.       .
0,0...1,0
0,0...0,1
]
[
1,0...0,0
0,0...1,0
.       .
.       .
.       .
0,1...0,0
0,0...0,1
]
这类的矩阵形式&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样，这个矩阵是n个单元（向量）凑到一起的，每个向量当作一个特征的分类，单个向量里，1的位置在哪儿，表示该特征分到那一类。那么，神经网络分类问题，就是算出这样一个矩阵来。&lt;/p&gt;&lt;p&gt;怎么算呢？既然分类问题是一个单元推衍到多个单元，计算方式也可以理解为，计算单元分类问题，推衍到多元分类问题。&lt;/p&gt;&lt;p&gt;单元分类问题，我们的逻辑回归算法的代价函数长这样：&lt;/p&gt;&lt;equation&gt;J=1/2m*(\sum_{i=1}^{m}(-y_i*log(h(x_i))+(y_i-1)log(1-h(x_i)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )&lt;/equation&gt;&lt;p&gt;推衍到多元，就是简单地多加一个纬度，然后加起来而已。公式和上面那个长得很像，如下：&lt;/p&gt;&lt;equation&gt;J=1/2m*(\sum_{i=1}^{m}\sum_{k=1}^{K}(-y_{k,i}*log(h(x_{i}))_k+(y_{k,i})log(1-h(x_{i})))_k+\lambda \sum_{l=1}^{L-1}\sum_{i=1}^{sl}\sum_{j=1}^{sl+1}{\theta_{l,ji}} )&lt;/equation&gt;&lt;p&gt;之前笔记里有说过，机器学习问题，就是要计算&lt;equation&gt;J(\theta) &lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) &lt;/equation&gt;，这里也一样。只不过这里的&lt;equation&gt;\theta&lt;/equation&gt;变成了一堆&lt;equation&gt;\theta&lt;/equation&gt;凑在一起 ，我们把符号写为大写&lt;equation&gt;\Theta &lt;/equation&gt;表示这是一堆小&lt;equation&gt;\theta&lt;/equation&gt;向量凑在一起合体变成的矩阵，原公式就变成了要计算&lt;equation&gt;J(\Theta) &lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;所以，神经网络分类算法变成了两个具体问题。第一个，计算&lt;equation&gt;J(\Theta) &lt;/equation&gt;第二个，计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;J(\Theta) &lt;/equation&gt;的公式我们已经有了，就是上面那个，因为我们theta的符号由小写变大写了，稍微改一下就是了：&lt;/p&gt;&lt;equation&gt;J(\Theta)=1/2m*(\sum_{i=1}^{m}\sum_{k=1}^{K}(-y_{k,i}*log(h(x_{i}))_k+(y_{k,i})log(1-h(x_{i})))_k+\lambda \sum_{l=1}^{L-1}\sum_{i=1}^{sl}\sum_{j=1}^{sl+1}{\Theta_{l,ji}} )&lt;/equation&gt;&lt;p&gt;这样就OK了。那么问题来了，如何计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;呢？&lt;/p&gt;&lt;p&gt;用反向传播算法。&lt;/p&gt;&lt;p&gt;那什么是反向传播算法呢？&lt;/p&gt;&lt;p&gt;理解这个问题，首先要理解什么是正向的传播。&lt;/p&gt;&lt;p&gt;上一周笔记里面，我们记了，每一层的数据是上一层数据处理的结果，用数学语言表示出来如下：&lt;/p&gt;&lt;equation&gt;a^{(1)}=x&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;z^{(2)}=\Theta^{(1)}*a{(1)}&lt;/equation&gt;,&lt;equation&gt;a^{(2)}=g(z^{(2)})+a_0^{(2)}&lt;/equation&gt;(&lt;equation&gt;a_0^{(2)}&lt;/equation&gt;就是1啦。之所以写成这样是表示它插入位置)&lt;/p&gt;&lt;p&gt;&lt;equation&gt;z^{(3)}=\Theta^{(2)}*a{(2)}&lt;/equation&gt;,&lt;equation&gt;a^{(3)}=g(z^{(3)})+a_0^{(3)}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;z^{(4)}=\Theta^{(3)}*a{(3)}&lt;/equation&gt;,&lt;equation&gt;a^{(4)}=g(z^{(4)})+a_0^{(4)}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……依此类推.&lt;/p&gt;&lt;p&gt;可以看到，我们计算i层的玩意是基于i-1层的东西。&lt;/p&gt;&lt;p&gt;那么反向传播算法，简单理解的话，就是把这个过程“倒过来”，计算i层的东西，基于i+1层就可以了。&lt;/p&gt;&lt;p&gt;以前我们计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;用的是什么公式呢？（&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) =1/m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}&lt;/equation&gt;）&lt;/p&gt;&lt;p&gt;我们可以看到，其中有一个&lt;equation&gt;{(h(x_i)-y_i)}&lt;/equation&gt;对吧，这是什么玩意呢？这是计算我们的假定函数和真实函数之间的偏差。&lt;/p&gt;&lt;p&gt;那么，我们反向传播算法，定义一个这样的，计算第n层j列的元素偏差的值，假设这个符号是&lt;equation&gt;\delta _j^{(n)}&lt;/equation&gt;,称之为“代价函数偏差”&lt;/p&gt;&lt;p&gt;以总共4层，第四层就是最后一层为例，这个&lt;equation&gt;\delta _j^{(n)}&lt;/equation&gt;就变成了：&lt;equation&gt;\delta _j^{(4)}=a_j^{(4)}-y_j
&lt;/equation&gt;这最后一层没问题，就按照定义来而已。&lt;/p&gt;&lt;p&gt;那么倒数第二层（正数第3层）呢？&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\delta _j^{(3)}=((\Theta^{(3)})^T*\delta _j^{(4)}).*g'(z^{(3)})
&lt;/equation&gt;其实就是用我们正向传播的函数，拿&lt;equation&gt;\delta _j^{(4)}&lt;/equation&gt;作为参数，导过来而已……理解不了请往上看看正向传播的这一层的函数，然后自己用结果倒推参数，你会得到一样的公式。&lt;/p&gt;&lt;p&gt;同理，第二层就是：&lt;equation&gt;\delta _j^{(2)}=((\Theta^{(2)})^T*\delta _j^{(3)}).*g'(z^{(2)})
&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;第一层就是……第一次不需要计算偏差啦……因为这是我们输入的特征量的层。第一层就是输入层特征量x。&lt;/p&gt;&lt;p&gt;然后说如何应用这个偏差计算我们需要的&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;&lt;/p&gt;为了写起来方便，先定义一个偏差总量&lt;equation&gt;\Delta&lt;/equation&gt;，&lt;equation&gt;\Delta_{i,j}^{(l)}&lt;/equation&gt;=0。&lt;p&gt;然后，计算出这一层总偏差&lt;equation&gt;\Delta_{i,j}^{(l)}=\Delta_{i,j}^{(l)}+a_j^{(l)}*\delta ^{(l+1)}&lt;/equation&gt;（这里等号是赋值符号。）&lt;/p&gt;&lt;p&gt;然后用我们对于代价函数偏差的定义，照搬之前的公式，&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;就等于：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) =1/m*\Delta_{i,j}^{(l)}+\lambda *\Theta_{i,j}^{(l)}&lt;/equation&gt;（j！=0时，这里等号依然是赋值符号。）&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;=&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) =1/m*\Delta_{i,j}^{(l)}&lt;/equation&gt;（j=0时。原因和以前一样，第一层不需要正则化）&lt;/p&gt;&lt;p&gt;到这里，你就从数学上理解了神经网络算法的分类问题了。如果你实在搞不懂反向传播算法，无所谓。照猫画虎写出代码就可以了。如果你搞懂了，恭喜你，你理解了整套ng的机器学习课程里最难的一个玩意。means 其他玩意你都能很轻松的搞懂了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;编程注意事项：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;注意，这里由于每次处理都是一层一层的，所以这时候你要么得用很多循环，要么就用向量。当然，在octave，matlab之类的玩意里，推荐用向量而不是一堆循环。&lt;/p&gt;&lt;p&gt;用向量的话，一般而言，&lt;equation&gt;\theta
&lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;得到的值都是一个n+1阶的向量。而问题在于，&lt;equation&gt;\Theta&lt;/equation&gt;是一个矩阵，所以在应用时，要先把矩阵向量化。计算完之后，要矩阵化。&lt;/p&gt;&lt;p&gt;向量化和反向量化的方法，依然是可以写循环……这里说一下octave里做。以&lt;equation&gt;\theta
&lt;/equation&gt;举例：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;theta_vec=[Theta1(:);Theta2(:);Theta3(:)...]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由你所见，向量化就是很单纯的把一个矩阵一列一列堆到一起……&lt;/p&gt;&lt;p&gt;所以反向量化也就是很简单地拆分开然后排排坐……：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Theta1=reshape(theta_vec(1:100),10,11)
Theta1=reshape(theta_vec(111:220),10,11)
Theta1=reshape(theta_vec(221:330),10,11)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;依此类推……就把向量里每110行，拆成一堆11x10的矩阵了。很好理解。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;梯度检查&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这玩意对数据分析并没有实际用处，目的是为了让你确认自己的算法无误，（因为反向传播算法你可能搞不懂就编出来了，因为搞不懂，无法确定对不对）&lt;/p&gt;&lt;p&gt;理论上来说，&lt;equation&gt;\frac{d}{d\theta_{i}}J(\theta) \approx \frac{J(\theta+\xi )-J(\theta-\xi )}{2\xi }&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;所以我们就计算一下对于每个&lt;equation&gt;\theta&lt;/equation&gt;，都大约能符合上面那个公式就行了。&lt;/p&gt;&lt;p&gt;所以这里就是，写个循环，对于&lt;equation&gt;\theta_1&lt;/equation&gt;到&lt;equation&gt;\theta_n&lt;/equation&gt;，检查一下是否都符合当&lt;equation&gt;\theta&lt;/equation&gt;等于&lt;equation&gt;\theta_i&lt;/equation&gt;时：&lt;equation&gt;\frac{d}{d\theta_{i}}J(\theta) \approx \frac{J(\theta+\xi )-J(\theta-\xi )}{2\xi }&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;（由于是大约，所以如果正在使用的编程语言没有大约符号，可以设定一个可容忍误差判断，小于可容忍误差，记录，弹出具体误差多少。）&lt;/p&gt;当然……由于是挨个检查，又是写循环，所以梯度检查很占内存（土豪请无视）所以检查无误后须要关闭梯度检查。&lt;p&gt;（注意：在神经网络算法中，初始的&lt;equation&gt;\theta&lt;/equation&gt;选取不能选择0，因为会导致隐藏层相同，解决方案可以选个近似于0的随机值)&lt;/p&gt;&lt;p&gt;&lt;b&gt;第五周笔记结束。&lt;/b&gt;&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;总结一下神经网络算法应用：
&amp;gt;&amp;gt;  先随机选取初始值   
→   正向算出是什么   
→   算出代价函数是什么
→   反向传播算法算出是什么
→   梯度检查确认，无误后关闭梯度检查
→   用优选算法选出令J最小时的theta&lt;/code&gt;&lt;/pre&gt;</description><author>子楠</author><pubDate>Fri, 01 Jul 2016 09:13:24 GMT</pubDate></item><item><title>第四周笔记：神经网络是什么</title><link>https://zhuanlan.zhihu.com/p/21423252</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/fbddd3ecaab93e37f9363a5e1fc811cc_r.jpg"&gt;&lt;/p&gt;神经网络是什么？&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b82650c47eed7b4bc601360dd2ded2bd.jpg" data-rawwidth="640" data-rawheight="427"&gt;图中可以看出，就是一个细胞元，由突触，连接到另外几个细胞元，组成的三维网状结构。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cd20776a312dc0c38948f12b974d1bcc.jpg" data-rawwidth="756" data-rawheight="388"&gt;&lt;p&gt;细胞元之间传递信息的方式是兴奋或者抑制，可以按照上节课的逻辑回归算法的0和1来看待。同一时间，突触只能单向传递信号。&lt;/p&gt;&lt;p&gt;所以，把神经网络符号化，简化一下就是这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/808254232cd4983cac374c5cc2a1fc87.png" data-rawwidth="400" data-rawheight="282"&gt;我们把每一次，函数接收的所有数据，叫做一个层。简单地可以把数据接收作为层1，接受到层1的所有数据并处理的叫做层2，接收层2数据并处理后传递给下一次用的数据叫做层3……依此类推。&lt;/p&gt;&lt;p&gt;那么这样的话，多出来的那个+1是怎么回事呢？记得之前笔记里的正则化吗？这个+1就是这玩意……表示一个常数（处理数据的时候乘以向量&lt;equation&gt;\theta&lt;/equation&gt;和正则化常数&lt;equation&gt;\lambda &lt;/equation&gt;）也称之为偏置项（目的和正则化一样，解决过拟合问题用）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c11e267c2779931cc4d6047c72846ff5.png" data-rawwidth="827" data-rawheight="510"&gt;&lt;p&gt;按照我们之前笔记里的，把&lt;equation&gt;x_0&lt;/equation&gt;记作1，把&lt;equation&gt;a_0&lt;/equation&gt;记作1，那么，上面那个网络就变成了下面这样了。&lt;/p&gt;&lt;p&gt;我们把从外接接受数据的层，称之为输入曾（input layer），产生最终结果的层，称之为输出层（Output layer），中间的所有层我们一般不直接看到，所以称之为隐藏层（hiden layer）。&lt;/p&gt;&lt;p&gt;我们把隐藏层j里的第i个数据表示为&lt;equation&gt;a_i^{(j)}&lt;/equation&gt;，处理上一层的所有数据，得到这个数据的函数的参数&lt;equation&gt;\theta&lt;/equation&gt;表示为（向量）&lt;equation&gt;\theta_i&lt;/equation&gt;，（向量）&lt;equation&gt;\theta_i&lt;/equation&gt;里的第k个数据为&lt;equation&gt;\theta_{ik}&lt;/equation&gt;，处理它们的函数为&lt;equation&gt;g_\theta(x)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;例如，如果j等于2，也就是第2层，这个层里有3个数据，分别是&lt;equation&gt;a_1^{(2)}&lt;/equation&gt;,&lt;equation&gt;a_2^{(2)}&lt;/equation&gt;,&lt;equation&gt;a_3^{(2)}&lt;/equation&gt;,其中&lt;equation&gt;a_1^{(2)}=g(\theta_1^{(2)}.*X)&lt;/equation&gt;,也就是：&lt;equation&gt;a_1^{(2)}=g(\theta_{10}^{(2)}*x_0+\theta_{11}^{(2)}*x_1+\theta_{12}^{(2)}*x_2+\theta_{13}^{(2)}*x_3)&lt;/equation&gt;。同理，&lt;/p&gt;&lt;p&gt;&lt;equation&gt;a_2^{(2)}=g(\theta_2^{(2)}.*X)&lt;/equation&gt;,也就是：&lt;equation&gt;a_2^{(2)}=g(\theta_{20}^{(2)}*x_0+\theta_{21}^{(2)}*x_1+\theta_{22}^{(2)}*x_2+\theta_{23}^{(2)}*x_3)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;a_3^{(2)}=g(\theta_3^{(2)}.*X)&lt;/equation&gt;,也就是：&lt;equation&gt;a_3^{(2)}=g(\theta_{30}^{(2)}*x_0+\theta_{31}^{(2)}*x_1+\theta_{32}^{(2)}*x_2+\theta_{33}^{(2)}*x_3)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;而如果j大于2，那么里面的X就是&lt;equation&gt;[a_0^{(j-1)},a_1^{(j-1)},a_2^{(j-1)},a_3^{(j-1)}]&lt;/equation&gt;也就是x变成了a，也就是&lt;equation&gt;a_i^{(j)}=g(\theta_i^{(j-1)}.*A^{(j-1)})&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;同理，最后一个要得出的输出曾，也是&lt;equation&gt;h_\theta(x)=g(\theta_i^{(3)}.*A^{(3)})&lt;/equation&gt;(假设有3层。)&lt;/p&gt;&lt;p&gt;神经网络算法，就是模仿细胞的传递方式，每个“层”相当于包含很多细胞（函数），来处理上一层细胞传递过来的数据。每一层的数据依赖于上一层的数据，以及每一层的&lt;equation&gt;\theta&lt;/equation&gt;。常说的“我不知道是怎么实现的”说的就是隐藏层。&lt;/p&gt;&lt;p&gt;然后是理解神经网络为什么能分类……&lt;/p&gt;&lt;p&gt;还是拿逻辑回归当作里面的小函数举例：就是那个&lt;equation&gt;g(\theta*x)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那个函数图不是长这样吗？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8250fb4d3f943ecd1f9de4f756b3d890.png" data-rawwidth="492" data-rawheight="306"&gt;&lt;p&gt;那神经网络，就是兴奋和抑制，我们把兴奋当作1，抑制当作0。由于是以逻辑回归举例，所以特征量1或者0对吧。逻辑回归嘛。逻辑无非是，是，非，与，或，异或。其他的都是建立在这个之上的。&lt;/p&gt;&lt;p&gt;先说"是"：&lt;equation&gt;g(\theta*x)&lt;/equation&gt;中，&lt;equation&gt;\theta*x&lt;/equation&gt;大于10，那么&lt;equation&gt;g(\theta*x)&lt;/equation&gt;就无限趋近于1了对吧。所以系统学习时拟合个数字出来&lt;equation&gt;\theta&lt;/equation&gt;大于10的出来就可以了。例如&lt;equation&gt;g(20*x-10)&lt;/equation&gt;就可以表示逻辑是。这时候如果x为0，那就是否。（而如果你选择&lt;equation&gt;g(\theta*x)=g(10*x)&lt;/equation&gt;的话，当x为0，就是0.5……成薛定谔的猫了。）&lt;/p&gt;&lt;p&gt;然后"非"：同理，&lt;equation&gt;g(\theta*x)&lt;/equation&gt;中，&lt;equation&gt;\theta*x&lt;/equation&gt;小于10，就非常近似于0了。所以系统学习时拟合个数字出来&lt;equation&gt;\theta&lt;/equation&gt;出来，让&lt;equation&gt;\theta*x&lt;/equation&gt;小于10即可。例如&lt;equation&gt;g(10-20*x)&lt;/equation&gt;就可以表示逻辑非。&lt;/p&gt;&lt;p&gt;然后是与：同理，&lt;equation&gt;g(\theta_0+\theta_1*x_1+\theta_2*x_2)&lt;/equation&gt;中，x1和x2同为1时&lt;equation&gt;(\theta_0+\theta_1*x_1+\theta_2*x_2)&lt;/equation&gt;大于10，就非常近似于1了。也很简单，比如：&lt;equation&gt;g(-30+20*x_1+20*x_2)&lt;/equation&gt;就满足条件。&lt;/p&gt;&lt;p&gt;然后是或：&lt;equation&gt;g(\theta_0+\theta_1*x_1+\theta_2*x_2)&lt;/equation&gt;中，x1和x2有一个为1，&lt;equation&gt;(\theta_0+\theta_1*x_1+\theta_2*x_2)&lt;/equation&gt;大于10即可。也很简单，比如：&lt;equation&gt;g(-10+20*x_1+20*x_2)&lt;/equation&gt;就满足条件。&lt;/p&gt;&lt;p&gt;然后是异或，异或其实就是一个x1先是一下，另一个x1非一下的与组合……比如x1是，x2非时才成立，&lt;equation&gt;g(\theta_0+\theta_1*x_1+\theta_2*x_2)&lt;/equation&gt;中&lt;equation&gt;g(-10+20*x_1-20*x_2)&lt;/equation&gt;就可以了。&lt;/p&gt;&lt;p&gt;所以你看，也就是说（在我们假设的结果和特征量都只有0和1的情况下），每一层有&lt;equation&gt;\theta&lt;/equation&gt;和&lt;equation&gt;x&lt;/equation&gt;就可以得到下一层了。神经网络就是把很多层拉到一起，看起来复杂了些。&lt;/p&gt;&lt;p&gt;而多元分类问题里的话，就是把里面的参数变成向量（和矩阵）而已（和之前笔记里一样）。实际算法都差不多。&lt;/p&gt;&lt;p&gt;第四周笔记结束（这样看神经网络就好入门多了……先理解是什么，为什么有效，下周再理解怎么用。一上来就来让你算&lt;equation&gt;\theta&lt;/equation&gt;，搞不懂反向传播算法，是好多人神经网络算法学蒙了的重要原因……）。&lt;/p&gt;</description><author>子楠</author><pubDate>Mon, 27 Jun 2016 06:31:37 GMT</pubDate></item><item><title>第三周笔记：逻辑回归及正则化</title><link>https://zhuanlan.zhihu.com/p/21378251</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a71b27bd0b098833461dcfbd1a8631bf_r.jpg"&gt;&lt;/p&gt;这周学的是“逻辑回归算法”（分类算法）&lt;p&gt;在这个算法里，我们的假设函数&lt;equation&gt;h(x)&lt;/equation&gt;长这样：&lt;equation&gt;h(x)=\frac{1}{1+e^{-\theta^T*X} } &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;简化一下，把&lt;equation&gt;\theta^T*X&lt;/equation&gt;叫做Z，那么原函数就变成了&lt;equation&gt;g(z)=\frac{1}{1+e^{-z} } &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;图示如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/faa829b805c529c5dd039896034ac7dd.png" data-rawwidth="492" data-rawheight="306"&gt;是不是感觉长得很像累计分布函数啊……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a23df30a3d3d14306fd07d45ee490542.png" data-rawwidth="500" data-rawheight="354"&gt;（第二个图是累计分布函数的）&lt;/p&gt;&lt;p&gt;嗯。反正差不多，所以高数的知识可以直接拿过来用了。&lt;/p&gt;&lt;p&gt;逻辑回归算法，计算目的是为了分类。按照图中所示，当&lt;equation&gt;z&amp;gt;0&lt;/equation&gt;的时候，&lt;equation&gt;g(z)&lt;/equation&gt;大于0.5，那么假如你要分类，1为在某类，0为不在某类，那&lt;equation&gt;g(z)&lt;/equation&gt;四舍五入就是1个1……。同理，z&amp;lt;0时，g（z）&amp;lt;0.5，四舍五入到0……（当然实际不是这么做的，实际做法是用数学方法使z的绝对值超过一个界限，使得&lt;equation&gt;g(z)&lt;/equation&gt;和1或者0很近。）&lt;/p&gt;&lt;p&gt;我们可以把g(z)=0.5,也就是z=0的时候，称为&lt;b&gt;决策边界&lt;/b&gt;。假设y为结果，y的值是1或者0。&lt;/p&gt;&lt;p&gt;那么，z=0，也就是&lt;equation&gt;\theta^T*X&lt;/equation&gt;=0的时候为边界。&lt;equation&gt;\theta^T*X\geq 0&lt;/equation&gt;时y=1。&lt;equation&gt;\theta^T*X&amp;lt; 0&lt;/equation&gt;时y=0。分类就靠这个……&lt;/p&gt;&lt;p&gt;然后继续，机器学习嘛，根据前两周的经验，肯定有那个J，代价函数。逻辑回归算法的代价函数如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;J=1/m*\sum_{i=1}^{m}({h(x_i)-y_i} )^2&lt;/equation&gt;（m是y总数，嫌麻烦的话，和第二周一样，如果你把那个加和符号去掉，那么这里x和y就变成向量了也就是：&lt;equation&gt;J=1/m*({h(x)-y} ).^2&lt;/equation&gt;。）&lt;/p&gt;&lt;p&gt;然后因为y只有0和1两个取值，这里就可以分两种情况讨论，让y从函数中去掉：&lt;/p&gt;&lt;p&gt;y=0：&lt;equation&gt;J=1/m*\sum_{1}^{m}({h(x)-0} )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;y=1:&lt;equation&gt;J=1/m*\sum_{1}^{m}({h(x)-1} )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然而我们发现，这样拿出来的函数，因为y一会儿是1，一会儿是0，得到的图像如下（左边的）然而左边的凹凸不平（非凸），不方便梯度下降，我们想要的是右边那种滑溜溜（凸）的图像。：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/7ce98c3ddfb66f85017fc4af7a423176.jpg" data-rawwidth="403" data-rawheight="125"&gt;所以我们假设有个函数cost，&lt;equation&gt;cost=(h(x),y)&lt;/equation&gt;&lt;p&gt;原函数J变成了&lt;equation&gt;J=1/m*\sum_{1}^{m}(cost(h(x),y)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;反正我们要的是&lt;equation&gt;cost=(h(x),y)&lt;/equation&gt;符合决策边界的条件就可以了，反正都把结果四舍五入成1或者0了，也没必要非得是某个函数对吧？&lt;/p&gt;&lt;p&gt;所以它是什么函数无所谓。只要（它的函数处理了x以后，得到的y）符合决策边界的条件即可。而现在我们希望得到一个滑溜溜的凸函数。那么我们就假设（注意，这是假设出来的，符合条件的函数之一，没说非得这样。然而这是已有的函数中最好的。）：&lt;/p&gt;&lt;p&gt;y=1的时候，&lt;equation&gt;cost=(h(x),y)=-log(h(x))&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;y=0的时候，&lt;equation&gt;cost=(h(x),y)=-log(1-h(x))&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;它们的图像如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/49c392586f8b1280d37be6e0cc62e00e.png" data-rawwidth="800" data-rawheight="603"&gt;&lt;p&gt;符合条件。那么这时候还有一个问题，大家都很懒对吧，懒得分情况讨论，那么如何做呢？&lt;/p&gt;&lt;p&gt;你看，由于y只有1和0两个值，所以我们就直接把两种情况的cost函数相加，然后分别乘以y和y-1即可：&lt;/p&gt;&lt;equation&gt;cost=(h(x),y)=-y*log(h(x))+(y-1)log(1-h(x))&lt;/equation&gt;&lt;p&gt;看，是不是很方便呢？&lt;/p&gt;&lt;p&gt;于是原来的那个代价函数J就变成了：&lt;/p&gt;&lt;equation&gt;J=1/m*\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))&lt;/equation&gt;&lt;p&gt;然后照旧，我们目的是拿x和y训练样本得到&lt;equation&gt;\theta&lt;/equation&gt;嘛！所以对这玩意求导，然后取个α，然后不断重复梯度下降……以得到&lt;equation&gt;\theta&lt;/equation&gt;，就是下面这个玩意：&lt;/p&gt;&lt;p&gt;（repeat：）&lt;equation&gt;\theta_j=\theta_j-\alpha *\frac{d}{d\theta_j}J(\theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这坨玩意里，我们需要计算&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) &lt;/equation&gt;，直接照搬之前笔记里的，&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) =1/m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}&lt;/equation&gt;即可。只不过这里的&lt;equation&gt;h(x)=\frac{1}{1+e^{-\theta^T*X} } &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是我们最终得到的&lt;/p&gt;&lt;p&gt;（repeat：）&lt;equation&gt;\theta_j=\theta_j-\alpha /m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;逻辑回归函数就是这样了。&lt;/p&gt;&lt;p&gt;然后是拟合问题，下面是3种情况：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c278dcb3c42ff6b9c7d914b85030d77d.jpg" data-rawwidth="894" data-rawheight="316"&gt;&lt;/p&gt;&lt;p&gt;第一种是欠拟合，通常是因为特征量选少了。第二种是我们想要的，第三个是过拟合，通常是因为特征量选多了。&lt;/p&gt;&lt;p&gt;欠拟合的解决方法是增加特征量。&lt;/p&gt;&lt;p&gt;过拟合的解决方法是减少特征量或者正则化。&lt;/p&gt;&lt;p&gt;比如我们的逻辑回归函数，不选个自定义的函数，就用我们那个类似泰勒展开式的函数来做的画，这货长得凹凸不平的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e77581efac0120995f21720c424c0741.png" data-rawwidth="332" data-rawheight="242"&gt;，一点都不光滑。那么，按照这货拟合回来的函数，十有八九也是过拟合了。于是我们就会得到一个类似这样的决策边界（蓝色线）显然，这条决策边界很……不实用。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/b1d00724220a2791374343761ccf4b3f.jpg" data-rawwidth="399" data-rawheight="126"&gt;&lt;/p&gt;&lt;p&gt;我们想要的，是一根滑溜溜的凸函数，但是我们又不能确定哪些特征量该去掉，所以我们就选择正则化的方式解决过拟合。&lt;/p&gt;&lt;p&gt;正则化的方法，就是给代价函数后面加个“惩罚项”……来降低它对数据的拟合能力。&lt;/p&gt;&lt;p&gt;于是我们的&lt;equation&gt;J=1/2m*\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;就变成了：&lt;equation&gt;J=1/2m*(\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )&lt;/equation&gt;（这里n表示特征量的总数，意思就是让所有的n个正义的&lt;equation&gt;\theta&lt;/equation&gt;为了解决过拟合问题，大喊一声“合体！”然后一起来惩罚那个过度拟合了的函数……&lt;equation&gt;\lambda &lt;/equation&gt;是正规化参数，决定了你惩罚得有多狠。你要惩罚狠点，你就把&lt;equation&gt;\lambda &lt;/equation&gt;提高一点，&lt;equation&gt;\lambda &lt;/equation&gt;过高会变得欠拟合，&lt;equation&gt;\lambda &lt;/equation&gt;过小无法解决过拟合。）&lt;/p&gt;&lt;p&gt;那么我们的&lt;equation&gt;\theta_j=\theta_j-\alpha /m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}&lt;/equation&gt;就顺利变成了：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_j=\theta_j(1-\alpha *\lambda /m)-\alpha /m*\sum_{i=1}^{m}({(h(x_i)-y_i)} *x_{i,j})&lt;/equation&gt;    ~→&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_j=\theta_j-\alpha /m*(\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}+\lambda /m*\theta_j)&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;注意，当&lt;equation&gt;\theta_0&lt;/equation&gt;的时候，由于&lt;equation&gt;x_0&lt;/equation&gt;=1，所以这一项不会欠拟合也不会过拟合，所以不惩罚它。&lt;/p&gt;&lt;p&gt;然后，如果你用的不是线性回归，而是正规方程的话，同理，给&lt;/p&gt;&lt;equation&gt;\theta=(X^T*X)^{-1} *X^T*y&lt;/equation&gt;加个惩罚项就好了。这里加的惩罚项为&lt;equation&gt;\varsigma &lt;/equation&gt;,&lt;equation&gt;\varsigma &lt;/equation&gt;是一个n+1阶的单位矩阵把第一项变成0（因为第一项不惩罚），我们把&lt;equation&gt;\varsigma &lt;/equation&gt;写作varsigma，代码差不多就是下面这个：&lt;pre&gt;&lt;code lang="text"&gt;varsigma =ones(n+1,n+1); #没有ones方法的自己写个循环给列表赋值，非常简单。
varsigma [0,0]=0 #python，c，php等编程的话，等这里是0，0
varsigma [1,1]=0 #matlab等这里是1，1，因为一个从0开始计数，一个从1开始计数。真是……就不能统一一下么，老因为这个引起一群嘴强王者的唇战。
写出来差不多就是这样一个矩阵:
[
0,0,0,0,0,0,0,0,...,0,0
0,1,0,0,0,0,0,0,...,0,0
0,0,1,0,0,0,0,0,...,0,0
0,0,0,1,0,0,0,0,...,0,0
0,0,0,0,1,0,0,0,...,0,0
0,0,0,0,0,1,0,0,...,0,0
0,0,0,0,0,0,1,0,...,0,0
0,0,0,0,0,0,0,1,...,0,0
.                     .
.                     .
.                     .
0,0,0,0,0,0,0,0,...,0,1
]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;得到的正规化后的公式为&lt;equation&gt;\theta=(X^T*X+\lambda *\varsigma )^{-1} *X^T*y&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;第三周笔记结束。&lt;/p&gt;</description><author>子楠</author><pubDate>Sun, 19 Jun 2016 03:28:51 GMT</pubDate></item><item><title>第二周笔记：多元线性回归</title><link>https://zhuanlan.zhihu.com/p/21355099</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6d29e8b9b1d491690984d4d4b6cde315_r.jpg"&gt;&lt;/p&gt;第一周的笔记里，我们假设函数是这样的：&lt;equation&gt;h(x)=\theta_{0} *x^0 +\theta _{1}*x^1+\theta _{2}*x^2+...+\theta _{n}*x^n+...&lt;/equation&gt;&lt;p&gt;也就是，自变量只有一个，x。无论是x的1次方还是x的n次方，都只有一个x，也就是就一个特征量。然而人都是贪得无厌的，只有一个特征量，咋预测呢？&lt;/p&gt;&lt;p&gt;所以要学习&lt;b&gt;“多特征预测”&lt;/b&gt;（多元线性回归）&lt;/p&gt;&lt;p&gt;我们现在把x不看作字母x，而看作向量x。&lt;/p&gt;&lt;p&gt;那么，&lt;equation&gt;x=[x_1,x_2,x_3,...,x_i,...]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;现在来看&lt;equation&gt;\theta&lt;/equation&gt;，假设x没有2次方以上的情况，那么原假设函数（在x只有1个的情况下）：&lt;equation&gt;h(x)=\theta_{0} +\theta_{1} *x&lt;/equation&gt;就变成了&lt;equation&gt;h(x)=\theta_{0} +\theta_{1} *x_1+\theta_{2} *x_2+\theta_{3} *x_3+...+\theta_{i} *x_i+....&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;现在我们在&lt;equation&gt;x=[x_1,x_2,x_3,...,x_i,...]&lt;/equation&gt;增加一项&lt;equation&gt;x_0&lt;/equation&gt;,令&lt;equation&gt;x_0&lt;/equation&gt;=1，那么&lt;equation&gt;x=[x_0,x_1,x_2,x_3,...,x_i,...]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;现在把&lt;equation&gt;\theta&lt;/equation&gt;也看作向量，那么&lt;equation&gt;\theta=[\theta_0,\theta_1,\theta_2,\theta_3,...,\theta_i,...]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;h(x)=\theta_{0} +\theta_{1} *x_1+\theta_{2} *x_2+\theta_{3} *x_3+...+\theta_{i} *x_i+....&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;就可以简化为：&lt;equation&gt;h(x)=\theta^{T} *X&lt;/equation&gt;或者&lt;equation&gt;h(x)=\theta\cdot  *X&lt;/equation&gt;看你自己习惯。&lt;/p&gt;&lt;p&gt;原来计算&lt;equation&gt;\theta&lt;/equation&gt;用的公式也可以直接套用：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha *\frac{d}{d\theta_{j} } J_{\theta} &lt;/equation&gt; (这两个公式等号是赋值。)&lt;/p&gt;&lt;p&gt;~→ &lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{m}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;（据说有很多人搞混那个&lt;equation&gt;x_{i,j}&lt;/equation&gt;的,j是表示第几个特征量，i表示这个特征量对应的训练样本里（数组）里的第几个，第一周没那个j是因为我们假设的特征量只有一个……）&lt;/p&gt;&lt;p&gt;重复赋值，直到令函数得到的返回值最小。这一步你可以直接用matlab给的优选函数。无所谓。&lt;/p&gt;&lt;p&gt;那么这就是多特征预测的基本方式，就是仅仅把原来的一个特征量，变成了多个特征量组成的向量，相应的&lt;equation&gt;\theta&lt;/equation&gt;也是向量了而已。计算代价函数，计算最小化时的&lt;equation&gt;\theta&lt;/equation&gt;，都没啥大的区别。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;多特征预测时，要做归一化处理，也就是根据一定的比例，把每个特征值，缩放到一个特定区间内（一般是缩放到-1~1之间）。以免发生类似于“计算四肢数量和精子数量对泡妞影响”这种问题时，由于一个特征的范围是0~4之间，另一个是十亿级别的，结果梯度下降时很难下降到一个合理的预测范围……&lt;/p&gt;&lt;p&gt;那么，如何做归一化处理呢？&lt;/p&gt;&lt;p&gt;很简单，让计算机对x中的每个&lt;equation&gt;x_i&lt;/equation&gt;作如下处理：&lt;equation&gt;x_i=(x_i-u_i)/s_i&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;一般选择&lt;equation&gt;u_i&lt;/equation&gt;为均值，&lt;equation&gt;s_i&lt;/equation&gt;为方差，也有人令&lt;equation&gt;u_i&lt;/equation&gt;为最小值，&lt;equation&gt;s_i&lt;/equation&gt;为极差的。这个随便你。&lt;/p&gt;&lt;p&gt;然后是&lt;equation&gt;\alpha &lt;/equation&gt;的选取问题。&lt;equation&gt;\alpha &lt;/equation&gt;选大了，没一会儿就变成“梯度上升”了。&lt;equation&gt;\alpha &lt;/equation&gt;选小了。半天你都下降不到合适点。&lt;/p&gt;&lt;p&gt;如果x的特征值，有必要对某个x解高次，比如对于某个x，并不是&lt;equation&gt;\theta_i*x_i&lt;/equation&gt;,而是&lt;equation&gt;\theta_i*x_i+\theta_j*x_i^2+...\theta_k*x_i^n...&lt;/equation&gt;这种的话，把所有的高次的x看作一个新的特征量即可，，，&lt;/p&gt;&lt;p&gt;比如把&lt;equation&gt;x_i^2&lt;/equation&gt;就看作一个新的特征量&lt;equation&gt;x_j&lt;/equation&gt;就好。对了，这种高次的，有可能会过大或者过小，记得要缩放（归一化处理）……&lt;/p&gt;&lt;p&gt;然后，这周讲了一个玩意，叫做正规方程。（然而大部分时候用梯度下降就够了……）&lt;/p&gt;&lt;p&gt;正规方程的代价函数（J）长这样：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;J(\theta)=a*\theta^2+b*\theta+c&lt;/equation&gt;（1维情况下）&lt;/p&gt;&lt;p&gt;然而你如何换一个角度看的话，其实和梯度下降的区别不大：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;J({\theta_i})=1/2m*\sum_{i}^{m}{} ((h(x_i)-y_i  )^2&lt;/equation&gt;       ——→     &lt;equation&gt;J({\theta})=1/2m*\sum_{i}^{m}{} ((h(X)-y  )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;是不是很熟悉啦？区别在于，这里的&lt;equation&gt;x\in X&lt;/equation&gt;,&lt;equation&gt;X&lt;/equation&gt;是一个包含所有&lt;equation&gt;[1,x_1^T,x_2^T,x_3^T,...,x_i^T...]&lt;/equation&gt;的矩阵。为啥是矩阵呢？因为这里的1是一排1，是[1,1,1,1,1,1,1,1,1,1...],&lt;equation&gt;x_1&lt;/equation&gt;是一排，也就是&lt;equation&gt;[x_{1}^{\theta_1} ,x_{1}^{\theta_2},x_{1}^{\theta_3},...x_{1}^{\theta_n},...   ]&lt;/equation&gt;这种形式……啊你知道就行了这没必要记。&lt;/p&gt;要记的是&lt;equation&gt;\theta&lt;/equation&gt;的计算方式（&lt;equation&gt;\theta&lt;/equation&gt;是一堆&lt;equation&gt;\theta_i&lt;/equation&gt;组成的向量）&lt;equation&gt;\theta=(X^T*X)^{-1} *X^T*y&lt;/equation&gt;&lt;p&gt;用代码表示就是……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;theta=pinv(X'*X)*X'*y&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;嗯，这样就记住编程的时候，T是‘，pinv是-1了。记不住随时可以回来看。&lt;/p&gt;&lt;p&gt;正规方程有个好处，不用选取&lt;equation&gt;\alpha &lt;/equation&gt;，不用迭代，不用正规化特征量。然而正规方程的比起梯度下降有个严重的问题，当特征量很多时（例如你要通过喜欢过这个女孩的所有男孩的特征，来预测这个女孩时，如果你选的是个美女，你又把每个男孩都当作不同的特征量来训练样本……嗯，说不定特征量能上万？）正规方程会很卡。啊，土豪请无视。然而电脑很卡出数据很慢对于大多数人来说是不可改变的，选取&lt;equation&gt;\alpha &lt;/equation&gt;，正规化特征量，对很多人来说是可以通过写几个loop解决的。&lt;/p&gt;&lt;p&gt;顺便说一句，既然看到了-1,数学比较敏感的同学应该想到了，有些数字比如0，是没法取倒数的，这样的时候就因为0不能作分母，无法用正规方程了。那么也就是X中，&lt;equation&gt;x_i=x_j*k&lt;/equation&gt;(k是任意实数，当然也包括0)时，你就没法用正规方程了。&lt;/p&gt;&lt;p&gt;这种情况一般是把特征重复了……比如你要做个函数预测一个男生泡妞和他家房子大小的相关性时，你把一个特征量按平方米算，另一个特征量按亩算。那我们都知道一亩等于k个平方米（反正k是实数，我才懒得为了举个例子去查具体是多少呢）所以相当于一个特征两次使用了，导致你矩阵中有一行会变成0，你就用正规方程就得出错了。&lt;/p&gt;&lt;p&gt;（这周接下来的内容是讲如何在matlab或者0ctave上画图的，我就不做笔记了，如果有需要，请在Octave里打“help”即可……）&lt;/p&gt;</description><author>子楠</author><pubDate>Wed, 15 Jun 2016 07:55:53 GMT</pubDate></item></channel></rss>