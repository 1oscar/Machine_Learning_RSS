<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>机器学习笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/mlearn</link><description>机器学习的笔记。非常浅显。不得不说ng的课很适合入门，几乎不需要太多的数学基础。</description><lastBuildDate>Sun, 02 Oct 2016 20:16:16 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>杂谈：（菜鸟如我）用哪些轮子？</title><link>https://zhuanlan.zhihu.com/p/22697926</link><description>其实对于普通人来说，不用分析那么多模型，用各种机器学习库就可以了。再者，对于我们这些小白来说，经常写算法写的不6，一个模型写下来循环多得不得了，测一下时间和复杂度，用人家的库，10秒解决的问题，自己编写的模型，有可能俩小时还没跑出来（别问我怎么知道的……）。&lt;p&gt;那么问题来了，用什么库？&lt;/p&gt;&lt;p&gt;个人是用python，这里推荐几个常用的……估计大家都知道，不过不知道的可以试着去用一下。&lt;/p&gt;&lt;p&gt;首推numpy。原因除了点乘之类的很多东西用这玩意很方便以外，最主要的一个原因……是大量的机器学习库，你如果没安numpy……会没办法运行……（别问我怎么知道的……）&lt;/p&gt;&lt;p&gt;然后是pandas（熊猫），其实可以不用熊猫，比如我就经常用graphlab而不用熊猫。不过如果不是因为替代品的原因的话，类似的结构化数据的库你总得用一个。原因？&lt;/p&gt;&lt;p&gt;我给你举个例子……大量机器学习比赛，会给你一堆生数据，然后你得清洗数据对吧？比如三张表合为一张表，三张表长度不同，顺序是乱的，你要根据用户名来组合。现在你不用结构化数据的库，自己编循环来for和if组合。三张表，至少得三张表的长度相乘的复杂度。假设平均每张表100000行数据（长度10000），你3个for循环if一下，按照一个if语句算一个复杂度，就是10^15的复杂度，假设每个复杂度你要跑1纳秒……嗯……大概12天能跑完。我试了一下，10万行的3张表合到一起，我这渣电脑用熊猫的join函数大概2秒钟吧……当然你用别的什么结构化的库都行，你用SQL也行。&lt;/p&gt;&lt;p&gt;然后随便用个结构化的库处理完了以后，就到机器学习库了。主推sklearn。为啥？便宜啊……便宜到了免费的程度。像我们这些一张高级显卡都要纠结买不买的屌丝，用免费的库再正常不过啦……&lt;/p&gt;&lt;p&gt;不不不，这不是主要原因。其实最主要的原因是因为Sklearn好上手。sklearn的文档非常详细，而且几乎所有的方法都给了范例，稍微跑跑很容易理解，（这点熊猫就做得很不好……在熊猫里大量的方法没给范例……对新人很不友好。）而且因为sklearn是免费库，所以大量的数据分析比赛都允许使用……嗯……说不准哪个四线小城市的比赛人家都在用excel分析数据你就用sklearn去拿了个两百块的奖金呢。&lt;/p&gt;&lt;p&gt;graphlab。&amp;lt;delete&amp;gt;推荐graphlab主要是因为我在coursera上学的另一门课是用graphlab教的所以我这种菜鸡就习惯了用这个&amp;lt;/delete&amp;gt;……其实主要是因为两个原因。1、这玩意的结构化数据库有范例，查起来方便（虽然要翻墙）。2是因为……不知道为什么，这玩意很多时候跑得比sklearn快。不过这玩意你得用要申请（或者买）用户ID，所以简单来说，你如果指着参加点数据分析的比赛拿奖金糊口，那这个只能用来验证模型。具体还得自己编（不过一般验证了模型以后编起来难度已经小多了……）&lt;/p&gt;&lt;p&gt;不过！这玩意有个非常好的好处，倒不是这个库怎么有好处，而是这个GraphLab Create的安装包……直接把常用库一并打包了。对于如我这种电脑三天两头崩溃的来说，重装库简直是灾难。直接安一个GraphLab Create，连着anaconda和jupyter都给安好了，多方便……&lt;/p&gt;&lt;p&gt;最后就是推荐jupyter notebook了。你习惯用其他IDE可以不用这个。推荐这玩意主要是因为……非常方便……谁用谁知道。举个例子，我现在几乎不开计算器和atom算东西了，无论是简单的还是复杂的问题，都用jupyter notebook写。这玩意比计算器用着方便，比文本编辑器用着顺手，比ide加载得快……好处多得我跟你说，给我一瓶啤酒我能吹一晚上……&lt;/p&gt;&lt;p&gt;就这些吧。顺便福利一下GraphLab Create我留在网盘里的备份，懒得一个一个安装库的可以下这个：链接：&lt;a href="http://pan.baidu.com/s/1pLhHWkb" class="" data-editable="true" data-title="baidu.com 的页面"&gt;http://pan.baidu.com/s/1pLhHWkb&lt;/a&gt; 密码：fp0l （啥？你问为啥不发github？当然是因为很多人没法翻墙啦！才不是因为我太菜了git用得不熟练呢。）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697926&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Thu, 29 Sep 2016 17:07:13 GMT</pubDate></item><item><title>L0，L1，L2正则化</title><link>https://zhuanlan.zhihu.com/p/22505062</link><description>之前说起正则化，我们光说了加一个惩罚项&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;，用步长&lt;equation&gt;\lambda &lt;/equation&gt;来调节,但是为什么是这样，却没说。&lt;p&gt;那这篇文章就讨论一下为毛是这样，以及常用的别的惩罚项长啥样。&lt;/p&gt;&lt;p&gt;先说&lt;/p&gt;&lt;p&gt;L0正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1,\theta _j\ne 0}^{m}{\theta _j^0} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;0的0次方没有意义，在这里如果按照L1和L2看显然该定位0，这里讨论就是不参与惩罚项，不参与加权。即所有非零项算作1加起来，然后用步长&lt;equation&gt;\lambda &lt;/equation&gt;调节。意思很明显，每一个对预测产生了贡献的参数，我都惩罚一次，不多不少，大家都一样。就像一个法官判决，你偷了一毛钱，他杀了一个人，法官均以“价值观不正确”为由，把你们判一样的罪……只有一点都没参与的人，才不会被判刑。&lt;/p&gt;&lt;p&gt;很明显有问题，这玩意正则化后，岂不是无论什么样的函数，无论多么复杂，都往一根横着的直线上调节么？&lt;/p&gt;&lt;p&gt;还有一个问题，干嘛叫L0呢？直接调步数不就可以了么……&lt;/p&gt;&lt;p&gt;这个问题后面会说。&lt;/p&gt;&lt;p&gt;L1正则化：&lt;/p&gt;&lt;p&gt;这玩意长这样：&lt;equation&gt;\sum_{j=1}^{m}{|\theta _j|} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;拿法官举例子，就是，法官要按照你们的罪行量刑判罪，但是都得判，无论你影响最终是好是坏（比如你杀了个人，这个人也是个坏人，但是你还是犯了杀人罪得判刑）都按照罪行判罪。于是就都取个绝对值，表示都判，然后按照罪行大小判罪了……&lt;/p&gt;&lt;p&gt;这个地方估计大家可以理解了，惩罚项嘛，按照一个参数的影响来判才对嘛……&lt;/p&gt;&lt;p&gt;不过这还有个问题，x的绝对值，你给我求导一下看看，求出来就是 土1,大于0的时候是1，小于0的时候是-1，一个还好，一个向量X里的话，有神特么多个x，求个导出来能把人累死。&lt;/p&gt;&lt;p&gt;于是就引出L2了：&lt;/p&gt;&lt;equation&gt;\sum_{j=1}^{m}{\theta _j^2} &lt;/equation&gt;&lt;p&gt;就是我们看到的，笔记里记的了。平方了以后嘛，做包括求导在内的各种计算就方便了，啥，你说大了，无所谓啊，反正有个lamda来调节步长，谁在乎呢？&lt;/p&gt;&lt;p&gt;好吧……到这里我就在不用一堆专业术语的情况下，用这种糊弄的方式，勉强把L0，L1，L2解释出来了，啥欧氏距离马氏距离剃刀原则都糊弄过去没说，反正你编程也用不到这些玩意。不过还有一个问题，作为一个喜欢开脑洞的宅，万一某一天你好奇，为毛这要叫L0，L1，L2,为毛不叫L1，L2，L3，不叫个数，拉索和雷吉呢？我还想叫他山岭回归呢……&lt;/p&gt;&lt;p&gt;这个……你可以这样理解：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;L(\theta) = \sum_{j=1}^{m}{(\sqrt{\theta _j^2} )^p} &lt;/equation&gt;当p等于0的时候，就是L0，当P等于1的时候，就是L1，当p等于2的时候，就是L2.嗯……这下就师出有名了啊哈哈哈……&lt;/p&gt;&lt;p&gt;好了不扯淡了。实际上这玩意表示的是距离，比如x点和y点之间的距离：&lt;/p&gt;&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;&lt;p&gt;而正则化是表示到哪儿的距离呢？到0点，也就是和完全不改变原函数的区别（图我懒得找了，随便搜图搜个L2norm都能搜出来）。而由于是到0点，所以可以只保留一个参数，那么上面哪个&lt;equation&gt;L(x,y) = \sum_{j=1}^{m}{(\sqrt{(x_j-y_j)^2} )^p} &lt;/equation&gt;就变成了&lt;equation&gt;L(\theta,0) = \sum_{j=1}^{m}{(\sqrt{(\theta _j-0)^2} )^p} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;啊，差不多就糊弄完了……&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22505062&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 19 Sep 2016 12:24:44 GMT</pubDate></item><item><title>正规方程的推导过程</title><link>https://zhuanlan.zhihu.com/p/22474562</link><description>那啥，之前笔记里这部分是略过的。这里整理一下吧。有兴趣的可以对照看看和你推倒的过程一样不。&lt;p&gt;我们先回顾一下，我们定义观测结果y和预测结果y'之间的差别为Rss:&lt;/p&gt;&lt;equation&gt;Rss = \sum_{i=1}^{n}({y_i-y_i'} )^2= \sum_{i=1}^{n}({y_i-h(x_i)} )^2 = (y-h(X))^T*(y-h(X))&lt;/equation&gt;&lt;p&gt;设若参数的矩阵为&lt;equation&gt;\theta&lt;/equation&gt;,则&lt;equation&gt;h(X)=\theta*X&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么&lt;equation&gt;Rss  = (y-h(X))^T*(y-h(X)) =  (y-X*\theta)^T*(y-X*\theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;按照我们的定义，这个Rss的意思是y和y'之间的差，那么当Rss无限趋近于0的时候，则y≈y'，即我们求得的预测结果就等于实际结果。&lt;/p&gt;&lt;p&gt;于是，令Rss等于某一极小值&lt;equation&gt;\delta &lt;/equation&gt;，则&lt;equation&gt;(y-X*\theta)^T*(y-X*\theta) ==\delta &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;对参数&lt;equation&gt;\theta&lt;/equation&gt;求导，得：&lt;/p&gt;&lt;equation&gt;\frac{d}{d(\theta)}(y-X*\theta)^T*(y-X*\theta)== 2X^T*(y-X*\theta)==0&lt;/equation&gt;&lt;p&gt;展开，得&lt;equation&gt; 2X^T*y==2*X^T*X*\theta&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;进而就可以得到&lt;equation&gt;\theta ==(X^T*X)^{-1}*X^T*y&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是我们就得到正规方程了。&lt;/p&gt;&lt;p&gt;再讲一个推导方式：&lt;/p&gt;&lt;p&gt;我们可以用矩阵乘法：&lt;/p&gt;&lt;equation&gt;Y=X\theta &lt;/equation&gt;&lt;p&gt;两边同时乘以&lt;equation&gt;X^T&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;X^TY=X^TX\theta &lt;/equation&gt;&lt;p&gt;然后再乘以&lt;equation&gt;(X^TX)^{-1}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;(X^TX)^{-1}X^TY=(X^TX)^{-1}X^TX\theta &lt;/equation&gt;&lt;p&gt;就得到&lt;equation&gt;\theta = (X^TX)^{-1}X^TY&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……不过这第二种方法是在知道了正规方程是什么以后再推导的。虽然看起来很快，然而并没有告诉你为什么。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22474562&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 17 Sep 2016 08:44:47 GMT</pubDate></item><item><title>小结及接下来的打算：</title><link>https://zhuanlan.zhihu.com/p/22440031</link><description>目前主干内容已经补充完了。还有大量内容没补充完，今后会继续补充。（比如正规方程怎么推倒啊不对推导的）&lt;p&gt;接下来我有个想法，个人学了几套不同的机器学习课程，感觉光是课堂布置的练习还是不够的，打算自己给自己布置个简单的结业项目。目前的想法是做这两个：&lt;/p&gt;&lt;p&gt;1、做一个简单的房价预测网站。&lt;/p&gt;&lt;p&gt;2、做一个知乎的答案推荐系统。&lt;/p&gt;&lt;p&gt;先做第一个，第一个的计划是这样的，分为3部分：&lt;/p&gt;&lt;p&gt;A、房地产数据获取及数据清洗：嘛，有现成的api最好，找不到的话，就自己爬虫爬，爬来了以后做一个协同过滤系统补完缺失数据。&lt;/p&gt;&lt;p&gt;B、通过数据预测房价。第一个项目就不用神经网络了，用线性回归就行了……做完再慢慢完善嘛。而且就算是个线性回归，不用别人的库，从头开始撸代码要写好了也不是一件容易的事……&lt;/p&gt;&lt;p&gt;C、做个简单的网站通过输入数据可以输出房价。就是做个网站而已。因为光是做了学习系统，以后万一跟别人吹牛逼也没好吹的，放到网站里，以后不管是找工作还是吹牛逼都可以拿出来说了……&lt;/p&gt;&lt;p&gt;目前的想法是，根据这3部分，建立一个学习小组，如果你有兴趣加入，至少得对其中一个部分能有帮助，能编写你的代码并教会小组内其他小伙伴。不接受其他方式加入，做完以后会征求组内意见把学习过程及内容放出来大家同意放出来的部分。&lt;/p&gt;&lt;p&gt;嘛，有人来一起做边做边学最好。没人来的话，反正我这水货的编程能力，用python爬虫也能编，很丑的学习系统也能编，用flask做个小网站也不是不会。只是我自己一个人的话边做边学就会很久咯。&lt;/p&gt;&lt;p&gt;有兴趣的可以联系我。两周内开始。&lt;/p&gt;&lt;p&gt;想要加入请私信我告诉我你能在3部分中，在&lt;u&gt;哪一部分帮助到其他同学&lt;/u&gt;（例：你可以发私信的时候告诉我说你爬虫爬的很不错，会爬数据并整理到sql里云云的都可以。总之门槛很低，但是你必须得能分享知识给别人。除此之外不接受任何加入方式。）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22440031&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 14 Sep 2016 08:03:57 GMT</pubDate></item><item><title>第十周笔记：大量数据算法</title><link>https://zhuanlan.zhihu.com/p/22168288</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ad95d41a014fb95c79712a446925d4d0_r.jpg"&gt;&lt;/p&gt;（第九周笔记还有推荐系统没补充完，不过要补充好推荐系统还得查很多资料，所以暂时没有补充的欲望。）&lt;p&gt;首先一个问题，是否需要用大数据？&lt;/p&gt;&lt;p&gt;记得之前笔记（第六周笔记）中，我们分析过一种情况，就是当样本数量m达到一定的程度时，增加样本数量，对于交叉验证集和训练集之间得出的误差的差距，并没有太多减少的空间时，就不再需要增加样本量。&lt;/p&gt;&lt;p&gt;一句话，当分别计算J_cv和J_train在当前样本数量下的误差时，差距极大时，应当使用更多数据，差距极小时，就不应当使用大量数据了。&lt;/p&gt;&lt;p&gt;那么，大量数据时，用什么算法呢？&lt;/p&gt;&lt;p&gt;数据太多，如果还按照批量梯度下降来玩，估计一个因子都得算个三五天，整个公式出出来，改一改，一个月就过去了……&lt;/p&gt;&lt;p&gt;那就不用所有因子来玩嘛。一步一步局部下降（贪婪算法）也不错嘛！但是贪婪算法也有问题，那就是，如果你的数据凹凸不平的，有很多个局部最优，那就会发生如下情况：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d0b5aa7bdd8bcab2ff1bcaa1c31e5d7e.png" data-rawwidth="471" data-rawheight="324"&gt;&lt;p&gt;那咋办呢？是的，你可以多重复随机选初始点迭代几次，然后在交叉集上比较，但是万一这种坑坑洼洼的地方很多，那你得重复很多次，运气不好兴许一辈子就过去了~&lt;/p&gt;&lt;p&gt;那我们就不用局部最优，还是得用全部数据，那不就慢了么，怎么办呢？那就每次就一个数据就拿去改变参数，凑合凑合得了。由于很随便，所以就叫它随便，啊不对随机梯度下降……&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机梯度下降：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们先看看以前的批量梯度下降的公式：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha *\frac{d}{d\theta_{j} } J_{\theta} &lt;/equation&gt;=&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{m}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;那么，现在我们改成了每次随便挑一个数据，于是公式就成了：&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*{} (h(x_i) -y_{i}  )*x_{j}&lt;/equation&gt;&lt;p&gt;也就是说，other than 把所有的数据拿来加一遍再改变参数值，我们现在变成了急不可耐地看到一个数据就改变一次参数值……&lt;/p&gt;&lt;p&gt;啥，你说这不就是贪婪算法，并不随机么……？&lt;/p&gt;&lt;p&gt;那好，那就……执行之前，把数据打乱，python的话就randshaffle一下，于是就随机了呗~~~&lt;/p&gt;&lt;p&gt;那么最后还有一个问题，我们以前的批量梯度下降，是一遍又一遍地迭代，直到收敛到某一个范围内。现在，你每个数据就下降一次，需要重复多少次呢？&lt;/p&gt;&lt;p&gt;不重复啊。说了这是给大量数据准备的，我们就假吧意思大量数据来说下降一次就够了。于是我们就总共只下降一次……只是对于其中，每个数据迭代的时候，就直接改变参数值&lt;equation&gt;\theta&lt;/equation&gt;了。所以你也看到了……由于只下降一次，数据不够多时就别用了。误差会很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迷你批量梯度下降（略拗口）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们也可以换个思路，还是用批量梯度下降，只是这时候不用所有数据，而是和刚才哪个随机梯度下降的思路结合一下。批量一度下降是所有数据一起算一次，改变一次参数值对吧？随机梯度下降时每个观测数据都拿来改变一次参数值对吧？咱中国人讲究中庸嘛。就既不拿所有的，也不光拿一个，而是一小批一小批地拿来下降，比如每次10个数据下降一次啦，每次20个数据下降一次啦。之类的。&lt;/p&gt;&lt;p&gt;那么我们怎么选呢？&lt;/p&gt;&lt;p&gt;把数据随机分成x份，每份里面有10~100个数据。以每份b个数据举例，我们要迭代的参数公式就成了这样：&lt;/p&gt;&lt;p&gt;i=1,b=10(假设b为10)&lt;/p&gt;&lt;p&gt;while i &amp;lt; m&lt;/p&gt;&lt;equation&gt;\theta_{j}=\theta_{j}-\alpha /m*\sum_{i}^{i+b-1}{} (h(x_i) -y_{i}  )*x_{i,j}&lt;/equation&gt;&lt;p&gt;i += b&lt;/p&gt;&lt;p&gt;差不多就是这样……意思就是从每个拿来改变一次参数&lt;equation&gt;\theta&lt;/equation&gt;，变成了每b个拿来改一次参数&lt;equation&gt;\theta&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查收敛：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那么现在又有一个问题来了。我们数据多了就变懒了，算一次凑合凑合过了，万一不收敛怎么办？所以还得检查收敛……&lt;/p&gt;&lt;p&gt;怎么检查收敛呢？&lt;/p&gt;&lt;p&gt;拿出我们的代价函数：&lt;equation&gt;cost=1/2*(h(x_{i} )-y_{i}  )^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;每迭代k次，就计算一下。最后把图画出来，看看收敛不收敛。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e04f380c74c1942eaeca162e3d2009ff.png" data-rawwidth="720" data-rawheight="419"&gt;蓝色是1000次看一下，红色是5000次看一下。横坐标是看的次数，纵坐标是当前看的时候cost的值。&lt;/p&gt;&lt;p&gt;可以看到，第一幅图是在收敛的表现，到达最低点后开始震荡。第二幅图意思是你看的范围越大，看到的振幅越小（批量梯度下降看起来就很光滑了。相反，k=1的时候，看起来就心电图了……）第三附图表示的意思是，如果你看着震荡不确定到最小没有，增加k，就可以知道到底是应该优化算法还是只是杂音造成的震荡。第四附图当然就是发散了，这时候可以试试小一点的&lt;equation&gt;\alpha &lt;/equation&gt;值。&lt;/p&gt;&lt;p&gt;接下来又有一个问题来了。大量数据一般都不是一次就给完的，很多时候，是连续给你赛数据，算法也要与时俱进嘛。当然，这时候你就可以直接用上面说的两种算法，新数据直接分成几份，算了就马上拿去改变参数值。这里可以让客户端完成一部分简单的数据处理再交给服务器，比如服务器每天计算一次参数，客户端通过这些参数，把缺失数据补充完整，再返回给客户端。又例如如果网络恨不好，而数据量足够客户端依据数据计算出一个代价函数，返回这个代价函数而不是数据，也许能提高一定的效率。嘛……再多的就要学分布式处理了。这里就不多扯淡了、&lt;/p&gt;&lt;p&gt;第11周的笔记内容很少，大致就是讲一下如何涉及一个机器学习的解决计划，以及感谢收看云云。在这里一并写了吧。基本思路就是，把一个复杂的机器学习问题分解成n个小问题（当然你也可以让神经网络自己分解去……）例如你要让机器自己看书，你就可以这样分解问题，让机器识别出书，让机器通过书识别出段落，语句，让机器通过段落语句识别其中的文字关联，进而识别出其抽象化的意思。让机器通过抽象化的意思重新组合合成你需要的信息。这种，把一个复杂的问题拆分成n个细小的问题一个一个解决。&lt;/p&gt;&lt;p&gt;常见的，例如我们要自动分析一个地区菜价，拿上来的数据不可能100%完整，那么就可以分为两套来做，第一个系统用来补充不完整信息（天气，道路交通，政府政策等），第二个系统用第一个系统已经加工好的信息来分析出想要的数据，第三个系统根据这些数据来预测未来的菜价。&lt;/p&gt;&lt;p&gt;实际过程中，由于每次系统都有偏差，最后总准确率会不断下降。例如上面说的哪个分析菜价的，也许总准确率为80%，然后如果是给完善的信息，不考虑第1个系统的误差，准确率是90%，如果给完善的数据，直接用来预测，准确率是98%。那么就可以简单的理解为，系统1可改善上限为10%，系统2可改善上限为8%，系统3可改善上限为2%。假如时间有限，也许就应该把宝贵的时间主要用以改善系统1和系统2。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22168288&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Tue, 13 Sep 2016 08:46:36 GMT</pubDate></item><item><title>第九周笔记：密度估计</title><link>https://zhuanlan.zhihu.com/p/21898453</link><description>考虑一个产品，每个工厂生产线都有一定的概率产生次品。假设在用户退货之前，我们没法知道一样东西是否是次品。那么我们只能通过产品的各项指标估计（重量，硬度，曲率，发热量等，不同产品指标不一样）将合格的某项指标画在图上，也许会得到这样一个图（左）：&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/606054110fbd2fbdcc26ea5e5aaf8356.png" data-rawwidth="500" data-rawheight="250"&gt;把图按照浓度梯度描红，得到右边的这个区域。按照聚类的思考方式，大概就可以理解为，越接近高浓度区域，其正品概率越高。而越不太可能产生落点的区域，其次品的概率越高。&lt;/p&gt;&lt;p&gt;用数学语言描述就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)&amp;lt;\xi &lt;/equation&gt;时，产品判断为次品。&lt;equation&gt;\xi &lt;/equation&gt;为异常判断参数，&lt;equation&gt;p(x)&lt;/equation&gt;为当前特征的产品产生概率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p(x)=\prod_{j=1}^{n} p(x_j,\mu _j,\sigma _j)&lt;/equation&gt;，其中&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\sigma _j^2=\frac{1}{m} \sum_{1}^{m}{(x_j^{(i)}-\mu _j)^2}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;当然，你可以简单地把正常和异常记作y=1和y=0.&lt;/p&gt;&lt;p&gt;选&lt;equation&gt;\xi &lt;/equation&gt;的方法，就是之前（第六周笔记）里计算F值，取F值最大时的&lt;equation&gt;\xi &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;那么问题来了，既然可以用y=1和y=0来标记出正常和异常，为什么不用监督学习，而要用密度估计呢？&lt;/p&gt;&lt;p&gt;因为大多数情况下，生产线上的异常值相对正常值来说，是极其少量的。在这种情况下，如果用监督学习，那么就没有足够的负样本用于训练集。此时，为了准确，我们就可以只把负样本用于交叉验证集与测验集，而训练集不用负样本。既然不用负样本，那么训练的时候也只要用非监督学习了……&lt;/p&gt;&lt;p&gt;还有一种可能，负样本相对与数据来说，非常奇怪，每一个负样本都不一样，或者未来可能产生的负样本具有不确定性，那么这种情况下，负样本无法建模，没法判断，只能通过和正样本的差异来判断是否为负样本。这种情况下，显然训练时，只用正样本的非监督学习是优于监督学习的……&lt;/p&gt;&lt;p&gt;反之，如果样本特征明显，可预测，未来样本可确定，且正负样本均足够多的情况下，用监督学习更优。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21898453&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Wed, 10 Aug 2016 18:46:02 GMT</pubDate></item><item><title>第八周笔记：聚类（clustering）</title><link>https://zhuanlan.zhihu.com/p/21798972</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2e901f902f1351d6824c7d5e27b9d8aa_r.jpg"&gt;&lt;/p&gt;聚类很好理解。你左手在地上撒一把盐，右手在地上撒一把糖。假设你分不清盐和糖，但是你分别是用左右手撒的，所以两个东西位置不同，你就可以通过俩玩意的位置，判断出两个东西是两类（左手撒的，右手撒的）。然而能不能区别出是糖还是盐？不行。你只能分出这是两类而已。但是分成两类以后再去分析，就比撒地上一堆分析容易多了。&lt;p&gt;聚类是典型的非监督学习。上面例子中，如果把盐和糖改成白色盐和染成黄色的糖，你可以通过颜色分析，颜色就是标签，有标签就是监督学习。没标签就是非监督学习。&lt;/p&gt;&lt;p&gt;聚类算法常用K均值算法：&lt;/p&gt;&lt;p&gt;随机选择包含K个中心的cluster(&lt;equation&gt;\mu_1,\mu_2,\mu_3,\mu_4,...,\mu_k&lt;/equation&gt; ,&lt;equation&gt;K\in R&lt;/equation&gt;)&lt;/p&gt;&lt;p&gt;以K=3为例，假设特征量是啥我也不知道……反正有特征量，现在画3个圈圈：&lt;/p&gt;&lt;p&gt;随机找3个点（图中①，②，③的蓝色点）当然，你找的点不一定就马上在中心了。有可能3个点都跑一个cluster里面去，无所谓……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7f7dbc4a792c474dc10b8b5159b05770.png" data-rawwidth="359" data-rawheight="372"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c98d20495d645ca6b8c27c8c5d33e0be.png" data-rawwidth="385" data-rawheight="398"&gt;&lt;p&gt;然后开始迭代，每次迭代方法如下：&lt;/p&gt;&lt;p&gt;对于每个特征点&lt;equation&gt;x_i&lt;/equation&gt;，找到和特征点&lt;equation&gt;x_i&lt;/equation&gt;最近的那个&lt;equation&gt;\mu _i&lt;/equation&gt;（例如&lt;equation&gt;\mu _1&lt;/equation&gt;），把每个和&lt;equation&gt;\mu _1&lt;/equation&gt;最近的点放到一起。对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;都这个操作，例如你选了3个点，就是3个&lt;equation&gt;\mu _i&lt;/equation&gt;（&lt;equation&gt;\mu _1&lt;/equation&gt;，&lt;equation&gt;\mu _2&lt;/equation&gt;，&lt;equation&gt;\mu _3&lt;/equation&gt;）&lt;/p&gt;&lt;p&gt;然后对于每个&lt;equation&gt;\mu _i&lt;/equation&gt;的最近点&lt;equation&gt;x_i&lt;/equation&gt;，取个平均值&lt;equation&gt;\bar{x_i} &lt;/equation&gt;，当作该&lt;equation&gt;\mu _i&lt;/equation&gt;的新值。然后你会神奇的发现，每个&lt;equation&gt;\mu _i&lt;/equation&gt;都朝着一个cluster的中心近了一步！（一点也不神奇好吗……）&lt;/p&gt;&lt;p&gt;重复n多次，直到收敛为止……&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d22d2960559a5a5ac8503b94c8f1fb01.png" data-rawwidth="354" data-rawheight="382"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/788ce87a02bf4de1903c6e4470d52ce4.png" data-rawwidth="377" data-rawheight="387"&gt;&lt;p&gt;你就会神奇地发现这3个点把区域按照某种神奇的规则分出来了（这个规则就是K均值最小……）这3个不同的区域就是3个cluster。&lt;/p&gt;&lt;p&gt;具体的算式如下：&lt;/p&gt;&lt;equation&gt;J=\frac{1}{m} \sum_{m}^{i=1}{(x_i-\mu _c^{(i)})} &lt;/equation&gt;&lt;p&gt;由于这次咱不是监督算法了，所以没法去压导数去迭代。上文也说了怎么迭代，所以这次迭代方法是：&lt;equation&gt;minJ(c^{(1)}......c^{(m)};\mu ^{(1)}......\mu ^{(k)})&lt;/equation&gt;不和导数扯上关系了，自己根据每个点算最小值不断迭代到收敛就是了……&lt;/p&gt;&lt;p&gt;然后就是选每个初始K的点也有技巧，一旦选不好，有可能3个点选到一个簇，本来应该是分成3个圆形片区，结果把整张图片分成3个长方块的也不是不可能……&lt;/p&gt;所以要随机选，为了保险，可以多随机几次。&lt;p&gt;然后就是另一个问题了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;K元素选几个？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选少了，偏差很大。选多了，过拟合了。如果愿意画图，有个方法叫肘子方法（我饿了……）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4695f9282ddbcac507e6697fda29ac78.png" data-rawwidth="395" data-rawheight="287"&gt;诺，就是画这么一个图，那个大概是肘子的地方就是我们要选的K的个数……不过肘子方法缺点很明显，老画图人工去看，一点也不机器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;维数缩减（Rimensionality Reduction ）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;举个例子。你做房子的数据的爬虫。假设全是正方形房子，爬虫中有这3个特征量：面积，长度，宽度。那么因为面积和长度宽度相乘相等，就可以缩减了。再例如，你爬到了一个是摄氏度，一个是华氏度，由于数据本身是一样的，也可以缩减了。把实际内核为同一个特征的的缩减到同一个，就是维数缩减。估计你也看出来了。K的个数就是维度嘛……所以维数缩减一定程度上可以看作选择k的个数。&lt;/p&gt;&lt;p&gt;当然……既然数据缩减了，储存需要的空间也就小了。（对于我们这种屌丝来说，省一张硬盘就是几百块钱，四舍五入就是一个亿啊……多缩减几次数据就省出一个王思聪了。）&lt;/p&gt;&lt;p&gt;实际做法，将J维的数据投影到一个I维空间上。&lt;/p&gt;&lt;p&gt;比如这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/61b75d03e002a7a17b8b875ae1d39ea8.jpg" data-rawwidth="224" data-rawheight="225"&gt;二向箔攻击！（容我中二一下……）三维空间里的数据就被压缩成了二维的了……~&lt;p&gt;通常要可视化数据的话，由于我们人类很傻，只能看到3维，所以一般要把数据降到3维或者二维……举个例子……把一个国家杂七杂八的经济发展指标压缩成一个GDP，然后分别看人口和GDP关系，工人比率和GDP关系……这种。&lt;/p&gt;&lt;p&gt;&lt;b&gt;主成分分析算法（PCA）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上文说了一个可视化数据要压缩，那怎么知道压缩了以后不会产生极大偏差呢？要投影数据，那么投影到哪个屏幕啊不对平面上呢？&lt;/p&gt;&lt;p&gt;实际做法：找能使投影误差最小的那个维度（平面）&lt;/p&gt;&lt;p&gt;定义i个向量，将J个特征量投影到这i个向量组成的子空间上。&lt;/p&gt;&lt;p&gt;如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\mu _i = \frac{1}{m} \sum_{i=1}^{m}{x_j^{(i)}} &lt;/equation&gt;，&lt;equation&gt;\mu _i &lt;/equation&gt;这玩意就是我们说的向量。&lt;/p&gt;&lt;p&gt;然后把原来的&lt;equation&gt;x_j^{(i)}&lt;/equation&gt;变成&lt;equation&gt;\frac{x_j{(i)}-\mu _i}{s_j} &lt;/equation&gt;。&lt;equation&gt;s_j&lt;/equation&gt;是数据归一化用的玩意（例如方差，最大值什么的。随便你选个你喜欢的）。&lt;/p&gt;&lt;p&gt;然后问题来了，咱算数据肯定是一列一列算，谁闲着没事一个一个算。那么如何一列一列算呢？&lt;/p&gt;&lt;p&gt;定义sigma，&lt;equation&gt;Sigma=\frac{1}{m}\sum_{a}^{b}{(x^{(i)}*x^{(i)T})}  &lt;/equation&gt;拿矩阵说的话就是&lt;equation&gt;Sigma=\frac{1}{m}*X'*X &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然后用一个Octave里面的算法svd，神奇的事情就发生了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[U,S,V]=svd(Sigma)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们就得到了包含n个（特征量个数）&lt;equation&gt;\mu _i &lt;/equation&gt;的一个矩阵U了。然后我们要缩减到k个纬度，就从这个矩阵U里面提取前k个就行了（误差好大的感觉……不过算了。压缩数据本来误差就大）。&lt;/p&gt;&lt;p&gt;用Octave的话就是这么个玩意,假设我们压缩到k个纬度。：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;U_reduce=U(:,1:k);
Z=U_reduce'*X
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就可以用这个U_reduce开开心心地投影出了我们要的z(压缩过的特征量)了。&lt;/p&gt;&lt;p&gt;那假如我后悔了。我不该压缩我亲爱的数据的，我一个臭屌丝又没钱买硬盘所以并没有数据的备份，那我想要回最初的数据咋办呢？&lt;/p&gt;&lt;p&gt;那就……那就算一算原数据吧……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;由于：Z=U_reduce'*X&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;1所以我们可以反推X：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个X当然是有误差的。所以就别写作X了，写作X_approx好了……&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;X_approx=U_reduce*Z&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后就是自动选K的问题。自动选K这事很有趣，如果X被压缩以后，解压缩回来误差很小，那大概可以认为此时选取的k是比较合适的。方法如下：&lt;/p&gt;&lt;p&gt;计算误差&lt;equation&gt;error=\frac{\sum_{i=1}^{m}{(x_i-x_{i,approx})}^2 }{\sum_{i=1}^{m}{x_i} ^2} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;如果error&amp;lt;0.01的话，那这时候的k大概就在肘子那里了……当然，你也可以写个循环，找出error最小时的k。&lt;/p&gt;&lt;p&gt;顺便说一句，PCA也可以用于监督学习中，以便我们这些穷B减少自己心爱的计算机的负荷（好心酸。我要买个显卡坞来增加运算能力，谁也别拦着我！）具体的做法就是把&lt;equation&gt;(x_i,y_i)&lt;/equation&gt;PCA成&lt;equation&gt;(z_i,y_i)&lt;/equation&gt;，然后代入假定函数中。当然，既然特征量已经从x变成z了。记得在用交叉函数J_cv和测试函数J_test的时候，也要把x变成z。因为特征量的数据并不同所以需要再变一次，这地方容易出错……&lt;/p&gt;&lt;p&gt;由于PCA整个方案都没用到y，所以过拟合问题并不能用PCA来降维攻击，还是老老实实的用正则化吧……正则化简单粗暴还不&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21798972&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Sat, 30 Jul 2016 23:03:40 GMT</pubDate></item><item><title>第七周笔记：支持向量机SVM</title><link>https://zhuanlan.zhihu.com/p/21678475</link><description>以逻辑回归为例：&lt;equation&gt;\theta^T*x_i&lt;/equation&gt;&lt;p&gt;我们要找到（不断缩小）的代价函数（的导数）。代价函数长这样：&lt;/p&gt;&lt;equation&gt;J(\theta_j)=1/2m*(\sum_{1}^{m}(-y*log(h(x))+(y-1)log(1-h(x)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )&lt;/equation&gt;&lt;p&gt;他的导数就长这样：&lt;equation&gt;1/2*(\sum_{i=1}^{m}(-y_i*log(h(x_i))+(y_i-1)log(1-h(x_i)))+\lambda/2m* \sum_{j=1}^{n}{\theta_j} )&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;把函数换一种写法：&lt;/p&gt;&lt;equation&gt;1/2*(\sum_{i=1}^{m}-y_i*cost(\theta^T*x_i)+(y_i-1)cost(\theta^T*x))+\lambda/2m* \sum_{j=1}^{n}{\theta_j} )&lt;/equation&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/66f7ed7af4fbb60a76f8e30193fdd4a2.jpg" data-rawwidth="352" data-rawheight="143"&gt;&lt;p&gt;我们知道，我们预测有两个结果，&lt;equation&gt;h(x)&lt;/equation&gt;也就是样本的结果y为1或者为0所以很明显，分类时就是：y=1时&lt;equation&gt;\theta^T*x_i\geq  1&lt;/equation&gt;，y=0时就是&lt;equation&gt;\theta^T*x_i \leq -1&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;这样很开心，因为我们的决策边界从一条线，变成了一个区域。所以这玩意又称为大间距分类器。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/02c18abd0da8e95346fc5a931b065e27.png" data-rawwidth="800" data-rawheight="862"&gt;&lt;/p&gt;&lt;p&gt;大间距分类器有个好处，可以自动忽视异常值（区域内值）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;核函数：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;好多人搞不明白为毛要用核函数。其实很好理解。我举个例子你就理解了。&lt;/p&gt;&lt;p&gt;假设现在我在地板上画了一条线，然后扔了两张纸在地上，让你判断纸在线的左边呢，还是在右边。&lt;/p&gt;&lt;p&gt;由于纸张压线了，或者两张纸举例不一样，你估计会一脸蒙蔽。于是要找个方法对吧，那就把纸张折叠一些，用我优雅的折纸技术把这张纸折成了一个球。再扔到线上。&lt;/p&gt;&lt;p&gt;这次由于纸折成了球，你相对于纸张整个压在线上，就更容易判断现在纸在左边还是右边了。核函数，就是类似于这个例子里折纸，当然，纸就是你的特征量咯……&lt;/p&gt;&lt;p&gt;实际操作中和这个例子还是有很大区别的，比如几张纸之类的……（手动滑稽）。&lt;/p&gt;&lt;p&gt;回到正题。&lt;/p&gt;&lt;p&gt;现在我们定义一个新的特征量l。l和x都是特征量，为了让他们共同预测结果，我们要对其建立一个新的函数关系式以形成cost函数。&lt;/p&gt;&lt;p&gt;当然……你（的折纸方法）写成 x-l 或者 x / l 也没人拦得住你，不过一般情况下，我们选择高斯核函数：&lt;/p&gt;&lt;equation&gt;f_j=similarity(x,l^{(j)})=exp(\frac{-(x-l^{(j)})^2}{2\sigma ^2} )&lt;/equation&gt;&lt;p&gt;这里，当x与l越接近，f越接近1.x与l差距越大，f越接近0.&lt;/p&gt;&lt;p&gt;嗯，猜对了，实际处理SVM问题，核函数的意义就在于替换那个x为f。&lt;/p&gt;&lt;p&gt;实际问题中，是否选择SVM可以如下考虑：&lt;/p&gt;&lt;p&gt;n多，甚至大于m时，用逻辑回归或者无核SVM。&lt;/p&gt;&lt;p&gt;n小时，用高斯核函数处理特征量后的SVM。&lt;/p&gt;&lt;p&gt;n非常小，而m非常大时，也许需要增加更多的特征量（n是特征量数，m是数据数量），而由于数据很多，出于计算速度考虑的话，可以用逻辑回归或者无核SVM。&lt;/p&gt;&lt;p&gt;如果电脑够好，不需要考虑计算速度，在此之上可以使用神经网络。（神经网络很早以前就有了，近几年火主要是因为计算机计算能力提升）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21678475&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Fri, 22 Jul 2016 10:40:32 GMT</pubDate></item><item><title>第六周笔记：评估假定函数</title><link>https://zhuanlan.zhihu.com/p/21551533</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/118716ddfa124a64b0b8ebe57b68ec02_r.jpg"&gt;&lt;/p&gt;机器学习算法可能给出多个假定函数，例如多维线性回归时多组最优&lt;equation&gt;\theta&lt;/equation&gt;，神经网络选取多少个隐藏层等，显然一直用人脑画图判断是不适合的。所以要了解如何评估假定函数，以选取一定情况下最适合的假定函数。&lt;p&gt;评估假定函数第一点，数据要分组，一组是训练组（training set），一组是测试组（test set）。通过训练组训练假定函数，通过测试组测试误差。&lt;/p&gt;&lt;p&gt;假设我们训练组得出的的假定函数为&lt;equation&gt;J(\theta)&lt;/equation&gt;,我们把测试组的特征值装进这个假定函数中，得到测试组预测结果&lt;equation&gt;J_{test}(\theta)&lt;/equation&gt;。那么，我们得到的误差就是测试组真实结果&lt;equation&gt;y_{test}&lt;/equation&gt;减去测试组预测结果（然后取绝对值。）因为取绝对值太麻烦，我们干脆给个平方就好，即：&lt;equation&gt;(y_{test}-J_{test}(\theta))^2&lt;/equation&gt;我们称这个玩意为测试误差&lt;equation&gt;err(h(x),y)&lt;/equation&gt;，所以咱现在就有公式了：&lt;equation&gt;err(h(x),y)=(y_{test}-J_{test}(\theta))^2&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;有了这个公式，我们现在就可以知道我们训练出来的假定函数的误差是多少了，但是现在又存在了另一个问题，如果我们只把数据分为俩组，那么我们一个拿来评估，一个拿来训练，然而我们知道，既然是训练，存在一个问题叫做过拟合，那么当然是函数越复杂训得出的结论越优秀，而如果我们用评估结果来确认选哪个，如何知道我们的选择没有“过拟合”呢？所以这样做，并没有一个合适的标准来确定该选哪个假定函数。&lt;/p&gt;&lt;p&gt;所以要把数组分为3组，一组用来训练出假定函数，一组用来选出最适合的假定函数（称之为交叉验证组），一组用来评估，知道我们没选错（一般3份按照0.6，0.2，0.2的方式分类，也有0.7，0.2，0.1的分组方式，不管怎么说，训练组占大头，具体看你数据多寡与实际预测情况）。&lt;/p&gt;&lt;p&gt;函数具体画出来是这样：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/17ec84ff3f63f77f6b368f0eb6ef1890.png" data-rawwidth="584" data-rawheight="466"&gt;&lt;p&gt;图中可以看出，对于已有数据（training set），假定函数越复杂，计算误差（training error）越小。而对于新数据（交叉验证组或其他测试组或预测具体数据时）假定函数越复杂，计算误差（Prediction Error for New Data）就越大。这也解释了为什么过拟合时预测结果不准确。&lt;/p&gt;&lt;p&gt;理论上来说，我们就是要用交叉验证组，找到测试误差（Model Prediction Error ）最小时的那个假定函数。&lt;/p&gt;&lt;p&gt;然后，有了测试误差这个评估指标，我们就可以讨论很多先前说道的高偏差或高方差问题了。例如，我们现在讨论&lt;equation&gt;\lambda &lt;/equation&gt;（正则化）为什么能一定程度上解决过拟合问题。就可以画个图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/269759f605334469972aed75856cac80.png" data-rawwidth="243" data-rawheight="207"&gt;&lt;p&gt;这样就可以看出为什么选lambda过小过大都不适宜，而如何用交叉验证组选了。&lt;/p&gt;&lt;p&gt;然后还有一个问题，是否数据越多越好？我们知道，数据越多，意味着得花更多钱买数据，或者得用更多的时间去爬各个网站，总之数据越多成本越高。那么，测试误差在数据量下，对于交叉验证集和训练集的作图就如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4e3715996f5ea81beca097157d850f2c.png" data-rawwidth="405" data-rawheight="219"&gt;&lt;/p&gt;&lt;p&gt;可以看出，随着数据量上升，交叉验证集的误差虽然越来越小，但是会趋于平缓，而且由我们的预测方法可知，其他集合的测试误差理论上不可能比训练误差更小，而且预测准确率不可能是100%，所以J_train和J_cv的测试误差之间是一定会有空隙的，这个空隙如果已经比预测的可接受误差范围小，那就没必要再增加训练样本了。（这种思维方式在生活中是很有用的）&lt;/p&gt;&lt;p&gt;从另一个角度出发，如果获取一个新的数据能得到的对预测准确性的经济价值，已经低于获取一个新数据的成本，也是没必要再增加新的数据的。所以不管怎么说，数据量并不是越多越好。&lt;/p&gt;&lt;p&gt;同理，其他预测的小技巧，也能用这种方式，验证其是提高的偏差，还是降低了拟合有效性。预测结果的误差无非就是两种，一种是过拟合，一种是欠拟合。所以这个程度上是可以很简单地判断的……&lt;/p&gt;&lt;p&gt;简单分类，过拟合问题包含：训练集数据太多，训练集包含过度多项式，过于复杂。隐藏层过多，lambda太小等。而欠拟合问题就是相反……&lt;/p&gt;&lt;p&gt;由于你没法提前知道自己的机器学习系统结果误差会有多大，所以正确的做法是先糊弄一下，随便搞一个出来，然后不断迭代……（找到误差率，判断是欠拟合还是过拟合，然后改进）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;然后是精准率和召回率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个词对于大多数做过数据处理的人并不陌生……用大白话说：&lt;/p&gt;&lt;p&gt;精准率就是所有可能的事件中，我预测的要发生事件有多少件发生了。&lt;/p&gt;&lt;p&gt;而召回率就是，所有发生了的事件中，我预测到了几个。&lt;/p&gt;&lt;p&gt;显然，单纯地高精准率或者高召回率，得到的结果都是没有意义的。例如，假设有1万人，其中有1人得了高传染性的病，你预测说这1万人都没得病，预测精准率是99.99%，然而我觉得不用解释你也明白，这99.99%的精准率一点卵用也没有……&lt;/p&gt;&lt;p&gt;数学语言表达，假设事件被分为2种，一种是发生了的(True)，一种是没发生(Fake)的。而我们的预测也是两种，一种是预测到的(Predicted)一种是没预测到的(Unpredicted).&lt;/p&gt;&lt;p&gt;精准率就是：&lt;equation&gt;Pre=\frac{T.P}{(T+F).P} &lt;/equation&gt;（.p的表示是这玩意的概率，T.p+F.p+P.p+UN.p=1）&lt;/p&gt;&lt;p&gt;召回率就是：&lt;equation&gt;Recall=\frac{T.P}{(P+UN).P} &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;理论上来说，预测结果应当是精准率与召回率双高才可。自动选择的话，参考方式是选择&lt;equation&gt;F=\frac{1}{\frac{1}{Pre} +\frac{1}{Recall} } &lt;/equation&gt;最高的值。&lt;/p&gt;&lt;p&gt;不过一般不这样写，一般写作&lt;equation&gt;F=\frac{2*Pre*Recall}{{Pre} +{Recall} } &lt;/equation&gt;（乘以一个2的意思是，理想最佳情况下，也就是两者均为1时，得到的F也为1.）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21551533&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Mon, 11 Jul 2016 14:40:30 GMT</pubDate></item><item><title>第五周笔记：神经网络分类问题应用</title><link>https://zhuanlan.zhihu.com/p/21464253</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b0a9f28d6b0a9d0b50e46b7ba0f4f82b_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;分类问题是什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;简单来说，假设nxn矩阵K，K的每列表示对一个特征的分类，一共n个特征。&lt;/p&gt;&lt;p&gt;当n为1时，K就是[0]或者[1]，意思就是这个特征有还是没有。这时候输出的单元（向量）是一个。&lt;/p&gt;&lt;p&gt;当n&amp;gt;=2时，K可以写成&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[
1,0...0,0
0,1...0,0
.       .
.       .
.       .
0,0...1,0
0,0...0,1
]
[
1,0...0,0
0,0...1,0
.       .
.       .
.       .
0,1...0,0
0,0...0,1
]
这类的矩阵形式&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样，这个矩阵是n个单元（向量）凑到一起的，每个向量当作一个特征的分类，单个向量里，1的位置在哪儿，表示该特征分到那一类。那么，神经网络分类问题，就是算出这样一个矩阵来。&lt;/p&gt;&lt;p&gt;怎么算呢？既然分类问题是一个单元推衍到多个单元，计算方式也可以理解为，计算单元分类问题，推衍到多元分类问题。&lt;/p&gt;&lt;p&gt;单元分类问题，我们的逻辑回归算法的代价函数长这样：&lt;/p&gt;&lt;equation&gt;J=1/2m*(\sum_{i=1}^{m}(-y_i*log(h(x_i))+(y_i-1)log(1-h(x_i)))+\lambda \sum_{j=1}^{n}{\theta_j^2} )&lt;/equation&gt;&lt;p&gt;推衍到多元，就是简单地多加一个纬度，然后加起来而已。公式和上面那个长得很像，如下：&lt;/p&gt;&lt;equation&gt;J=1/2m*(\sum_{i=1}^{m}\sum_{k=1}^{K}(-y_{k,i}*log(h(x_{i}))_k+(y_{k,i})log(1-h(x_{i})))_k+\lambda \sum_{l=1}^{L-1}\sum_{i=1}^{sl}\sum_{j=1}^{sl+1}{\theta_{l,ji}} )&lt;/equation&gt;&lt;p&gt;之前笔记里有说过，机器学习问题，就是要计算&lt;equation&gt;J(\theta) &lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) &lt;/equation&gt;，这里也一样。只不过这里的&lt;equation&gt;\theta&lt;/equation&gt;变成了一堆&lt;equation&gt;\theta&lt;/equation&gt;凑在一起 ，我们把符号写为大写&lt;equation&gt;\Theta &lt;/equation&gt;表示这是一堆小&lt;equation&gt;\theta&lt;/equation&gt;向量凑在一起合体变成的矩阵，原公式就变成了要计算&lt;equation&gt;J(\Theta) &lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;所以，神经网络分类算法变成了两个具体问题。第一个，计算&lt;equation&gt;J(\Theta) &lt;/equation&gt;第二个，计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;J(\Theta) &lt;/equation&gt;的公式我们已经有了，就是上面那个，因为我们theta的符号由小写变大写了，稍微改一下就是了：&lt;/p&gt;&lt;equation&gt;J(\Theta)=1/2m*(\sum_{i=1}^{m}\sum_{k=1}^{K}(-y_{k,i}*log(h(x_{i}))_k+(y_{k,i})log(1-h(x_{i})))_k+\lambda \sum_{l=1}^{L-1}\sum_{i=1}^{sl}\sum_{j=1}^{sl+1}{\Theta_{l,ji}} )&lt;/equation&gt;&lt;p&gt;这样就OK了。那么问题来了，如何计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;呢？&lt;/p&gt;&lt;p&gt;用反向传播算法。&lt;/p&gt;&lt;p&gt;那什么是反向传播算法呢？&lt;/p&gt;&lt;p&gt;理解这个问题，首先要理解什么是正向的传播。&lt;/p&gt;&lt;p&gt;上一周笔记里面，我们记了，每一层的数据是上一层数据处理的结果，用数学语言表示出来如下：&lt;/p&gt;&lt;equation&gt;a^{(1)}=x&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;z^{(2)}=\Theta^{(1)}*a{(1)}&lt;/equation&gt;,&lt;equation&gt;a^{(2)}=g(z^{(2)})+a_0^{(2)}&lt;/equation&gt;(&lt;equation&gt;a_0^{(2)}&lt;/equation&gt;就是1啦。之所以写成这样是表示它插入位置)&lt;/p&gt;&lt;p&gt;&lt;equation&gt;z^{(3)}=\Theta^{(2)}*a{(2)}&lt;/equation&gt;,&lt;equation&gt;a^{(3)}=g(z^{(3)})+a_0^{(3)}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;equation&gt;z^{(4)}=\Theta^{(3)}*a{(3)}&lt;/equation&gt;,&lt;equation&gt;a^{(4)}=g(z^{(4)})+a_0^{(4)}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;……依此类推.&lt;/p&gt;&lt;p&gt;可以看到，我们计算i层的玩意是基于i-1层的东西。&lt;/p&gt;&lt;p&gt;那么反向传播算法，简单理解的话，就是把这个过程“倒过来”，计算i层的东西，基于i+1层就可以了。&lt;/p&gt;&lt;p&gt;以前我们计算&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;用的是什么公式呢？（&lt;equation&gt;\frac{d}{d\theta_j}J(\theta) =1/m*\sum_{i=1}^{m}{(h(x_i)-y_i)} *x_{i,j}&lt;/equation&gt;）&lt;/p&gt;&lt;p&gt;我们可以看到，其中有一个&lt;equation&gt;{(h(x_i)-y_i)}&lt;/equation&gt;对吧，这是什么玩意呢？这是计算我们的假定函数和真实函数之间的偏差。&lt;/p&gt;&lt;p&gt;那么，我们反向传播算法，定义一个这样的，计算第n层j列的元素偏差的值，假设这个符号是&lt;equation&gt;\delta _j^{(n)}&lt;/equation&gt;,称之为“代价函数偏差”&lt;/p&gt;&lt;p&gt;以总共4层，第四层就是最后一层为例，这个&lt;equation&gt;\delta _j^{(n)}&lt;/equation&gt;就变成了：&lt;equation&gt;\delta _j^{(4)}=a_j^{(4)}-y_j
&lt;/equation&gt;这最后一层没问题，就按照定义来而已。&lt;/p&gt;&lt;p&gt;那么倒数第二层（正数第3层）呢？&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\delta _j^{(3)}=((\Theta^{(3)})^T*\delta _j^{(4)}).*g'(z^{(3)})
&lt;/equation&gt;其实就是用我们正向传播的函数，拿&lt;equation&gt;\delta _j^{(4)}&lt;/equation&gt;作为参数，导过来而已……理解不了请往上看看正向传播的这一层的函数，然后自己用结果倒推参数，你会得到一样的公式。&lt;/p&gt;&lt;p&gt;同理，第二层就是：&lt;equation&gt;\delta _j^{(2)}=((\Theta^{(2)})^T*\delta _j^{(3)}).*g'(z^{(2)})
&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;第一层就是……第一次不需要计算偏差啦……因为这是我们输入的特征量的层。第一层就是输入层特征量x。&lt;/p&gt;&lt;p&gt;然后说如何应用这个偏差计算我们需要的&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;&lt;/p&gt;为了写起来方便，先定义一个偏差总量&lt;equation&gt;\Delta&lt;/equation&gt;，&lt;equation&gt;\Delta_{i,j}^{(l)}&lt;/equation&gt;=0。&lt;p&gt;然后，计算出这一层总偏差&lt;equation&gt;\Delta_{i,j}^{(l)}=\Delta_{i,j}^{(l)}+a_j^{(l)}*\delta ^{(l+1)}&lt;/equation&gt;（这里等号是赋值符号。）&lt;/p&gt;&lt;p&gt;然后用我们对于代价函数偏差的定义，照搬之前的公式，&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;就等于：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) =1/m*\Delta_{i,j}^{(l)}+\lambda *\Theta_{i,j}^{(l)}&lt;/equation&gt;（j！=0时，这里等号依然是赋值符号。）&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;=&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) =1/m*\Delta_{i,j}^{(l)}&lt;/equation&gt;（j=0时。原因和以前一样，第一层不需要正则化）&lt;/p&gt;&lt;p&gt;到这里，你就从数学上理解了神经网络算法的分类问题了。如果你实在搞不懂反向传播算法，无所谓。照猫画虎写出代码就可以了。如果你搞懂了，恭喜你，你理解了整套ng的机器学习课程里最难的一个玩意。means 其他玩意你都能很轻松的搞懂了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;编程注意事项：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;注意，这里由于每次处理都是一层一层的，所以这时候你要么得用很多循环，要么就用向量。当然，在octave，matlab之类的玩意里，推荐用向量而不是一堆循环。&lt;/p&gt;&lt;p&gt;用向量的话，一般而言，&lt;equation&gt;\theta
&lt;/equation&gt;和&lt;equation&gt;\frac{d}{d\Theta_{i,j}}J(\Theta) &lt;/equation&gt;得到的值都是一个n+1阶的向量。而问题在于，&lt;equation&gt;\Theta&lt;/equation&gt;是一个矩阵，所以在应用时，要先把矩阵向量化。计算完之后，要矩阵化。&lt;/p&gt;&lt;p&gt;向量化和反向量化的方法，依然是可以写循环……这里说一下octave里做。以&lt;equation&gt;\theta
&lt;/equation&gt;举例：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;theta_vec=[Theta1(:);Theta2(:);Theta3(:)...]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由你所见，向量化就是很单纯的把一个矩阵一列一列堆到一起……&lt;/p&gt;&lt;p&gt;所以反向量化也就是很简单地拆分开然后排排坐……：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Theta1=reshape(theta_vec(1:100),10,11)
Theta1=reshape(theta_vec(111:220),10,11)
Theta1=reshape(theta_vec(221:330),10,11)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;依此类推……就把向量里每110行，拆成一堆11x10的矩阵了。很好理解。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;梯度检查&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这玩意对数据分析并没有实际用处，目的是为了让你确认自己的算法无误，（因为反向传播算法你可能搞不懂就编出来了，因为搞不懂，无法确定对不对）&lt;/p&gt;&lt;p&gt;理论上来说，&lt;equation&gt;\frac{d}{d\theta_{i}}J(\theta) \approx \frac{J(\theta+\xi )-J(\theta-\xi )}{2\xi }&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;所以我们就计算一下对于每个&lt;equation&gt;\theta&lt;/equation&gt;，都大约能符合上面那个公式就行了。&lt;/p&gt;&lt;p&gt;所以这里就是，写个循环，对于&lt;equation&gt;\theta_1&lt;/equation&gt;到&lt;equation&gt;\theta_n&lt;/equation&gt;，检查一下是否都符合当&lt;equation&gt;\theta&lt;/equation&gt;等于&lt;equation&gt;\theta_i&lt;/equation&gt;时：&lt;equation&gt;\frac{d}{d\theta_{i}}J(\theta) \approx \frac{J(\theta+\xi )-J(\theta-\xi )}{2\xi }&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;（由于是大约，所以如果正在使用的编程语言没有大约符号，可以设定一个可容忍误差判断，小于可容忍误差，记录，弹出具体误差多少。）&lt;/p&gt;当然……由于是挨个检查，又是写循环，所以梯度检查很占内存（土豪请无视）所以检查无误后须要关闭梯度检查。&lt;p&gt;（注意：在神经网络算法中，初始的&lt;equation&gt;\theta&lt;/equation&gt;选取不能选择0，因为会导致隐藏层相同，解决方案可以选个近似于0的随机值)&lt;/p&gt;&lt;p&gt;&lt;b&gt;第五周笔记结束。&lt;/b&gt;&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;总结一下神经网络算法应用：
&amp;gt;&amp;gt;  先随机选取初始值   
→   正向算出是什么   
→   算出代价函数是什么
→   反向传播算法算出是什么
→   梯度检查确认，无误后关闭梯度检查
→   用优选算法选出令J最小时的theta&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21464253&amp;pixel&amp;useReferer"/&gt;</description><author>子楠</author><pubDate>Fri, 01 Jul 2016 09:13:24 GMT</pubDate></item></channel></rss>