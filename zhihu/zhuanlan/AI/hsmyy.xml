<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Sat, 01 Oct 2016 21:16:00 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>寻找CNN的弱点</title><link>https://zhuanlan.zhihu.com/p/22464575</link><description>CNN是现在十分火热的模型，在很多图像检索问题上，CNN模型的效果在以往的基础上有了很大的提高，但是CNN毕竟没有把这些问题完全解决，CNN还是有它自己的弱点的。这个弱点也不能算作是它独有的问题，但是由于它的效果实在太好了，很多人甚至对它产生了迷信，因此这盆冷水就泼到它身上了。&lt;p&gt;大神们看到了CNN模型的强大，但忍不住提出一个问题：CNN有没有什么搞不定的地方？比方说我们用CNN构建了一个人脸识别的模型，在训练数据集和测试数据集上表现良好，但是会不会有一些用例是它会误判的，而且我们可以找到规律生成这些用例？&lt;/p&gt;&lt;p&gt;我们可以想象，如果我们对之前识别正确的数据做轻微的改动，那么它还是有可能识别正确的。于是我们就有了一个方案，我们每将图像做一点改动，就把图像传入CNN做一下测试，然后看看CNN的预测结果有没有发生改变，如果没有发生改变，我们就保存这个图像，接着我们再进行下一轮的改动，经过若干轮的改动后，我们把生成的图像输出出来看看图像会变成什么样子。&lt;/p&gt;&lt;p&gt;这里我们将采用MNIST为例，以下的就是我们的改动方案：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用MNIST的训练集训练一个CNN的模型，我们的CNN模型结构是：conv32*3*3-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;conv64*3*6-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;fc256-&amp;gt;dropout0.5-&amp;gt;fc10。&lt;/li&gt;&lt;li&gt;找到一个训练数据，将其数据范围限定在0到1之间，我们对每一个像素点随机增减-0.1到0.1之间的一个数，这样得到64个随机的图像，然后经过CNN模型预测得到这64个图像的预测label，从中选择一个和原始label相同的图像。经过若干轮迭代后，我们就可以看看这个随机改变的数字变成了什么样子。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们选择了一个数字0：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1a666d86f42923d245036bec25270c59.png" data-rawwidth="421" data-rawheight="424"&gt;经过50轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c3725d6bbb1cde8c2f236cf24a63b5f4.png" data-rawwidth="423" data-rawheight="423"&gt;&lt;p&gt;经过100轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/38c4258464d5ff070a64f3a636a0864b.png" data-rawwidth="421" data-rawheight="421"&gt;&lt;p&gt;经过150轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/06d7faa4bd8245d98ec1ae265c98cd48.png" data-rawwidth="424" data-rawheight="426"&gt;&lt;p&gt;经过200轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/232da9eb664724791b4181a2a636edad.png" data-rawwidth="426" data-rawheight="425"&gt;到此为止，可以看出这个数字还是隐约可见，但是实际上图像已经变得模糊不清，大量的杂乱信息混入其中，已经和原始的数字完全不同。&lt;/p&gt;&lt;p&gt;这个套路被称作“fool CNN”，用东北话说就是忽悠。继续迭代下去，我们还能生成出更精彩的图像。当然这也只是忽悠CNN模型的一种办法，我们还有其他的办法来生成图像。其他的办法这里就不再介绍了。关于这种忽悠，大神们也给出了和机器学习有关的解释：&lt;/p&gt;&lt;p&gt;CNN的模型说到底还是个判别式模型，如果我们把图像设为X，label设为y，CNN的模型就相当于求p(y|X)的值。判别式模型相当于描述“什么样的图像是这个label的图像”，而满足了这些条件的图像有时并不是具有真实label的那个图像。而上面的忽悠套路就是利用了这个漏洞。&lt;/p&gt;&lt;p&gt;上面的例子中，我们用这种fool的方法让一张模糊不清的图像保持了原来的label，同时我们也可以让一张不算模糊的图像被CNN错认成另外一个label。&lt;/p&gt;&lt;p&gt;比方说下面这张经过40轮迭代的图像被认成了6：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2444cb2b26a5ee30177d5eb47f4b5abb.png" data-rawwidth="419" data-rawheight="422"&gt;这些套路的出现都让我们对CNN有了一些警惕，如果想让CNN对手写数字完全hold住，我们还需要其他的方法辅助，不然的话这种意外总会发生。&lt;/p&gt;&lt;p&gt;那么有没有什么方法能解决这样的问题呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464575&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 29 Sep 2016 22:48:26 GMT</pubDate></item><item><title>DCGAN的小尝试（2）</title><link>https://zhuanlan.zhihu.com/p/22389906</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-editable="true" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;上一回我们只是简单地展示了基于keras框架、MNIST数据集的DCGAN模型的结果，下面我们来详细地看一下这个代码的实现。&lt;h2&gt;生成模型的结构&lt;/h2&gt;&lt;pre&gt;&lt;code lang="text"&gt;def generator_model():
    model = Sequential()
    model.add(Dense(input_dim=100, output_dim=1024))
    model.add(Activation('tanh'))
    model.add(Dense(out_dim=128*7*7))
    model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=64, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=1, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;直接上代码了。keras的代码总体上比较直观，我在里面加了一些参数对应的描述，应该编译不过，但是会比较好理解。&lt;/p&gt;&lt;p&gt;这里需要说明的一点是，这个实现中的激活函数都是双曲正切，和论文中的描述不一样。当然，和论文中的模型架构也不一样，不过两者的数据集也不一样。&lt;/p&gt;&lt;h2&gt;判别模型的结构&lt;/h2&gt;&lt;p&gt;判别模型的结构如下所示，仔细地读一遍就可以理解，这里不需要赘述了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def discriminator_model():
    model = Sequential()
    model.add(Convolution2D(
                        64, 5, 5,
                        border_mode='same',
                        input_shape=(1, 28, 28)))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(128, 5, 5))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('tanh'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    return model&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;训练&lt;/h2&gt;&lt;p&gt;这里的训练的一轮迭代可以用下面的流程表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一批generated_image&lt;/li&gt;&lt;li&gt;将真实的数据和generated_image合并，并放入D中进行一轮训练，其中真实数据的label为1，generated_image的label为0。&lt;/li&gt;&lt;li&gt;利用G再生成一批generated_image&lt;/li&gt;&lt;li&gt;这一次将G和D连起来，并给第3步的gereated_image的label设为1，固定D的参数不变，进行一轮训练。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;可以看出第1，2步是为了优化D，第3，4步是为了优化G，而两者之间还是存在着紧密的联系。&lt;/p&gt;&lt;h2&gt;图像生成&lt;/h2&gt;&lt;p&gt;图像生成的过程可以用如下两步表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一大批generated_image&lt;/li&gt;&lt;li&gt;利用D计算这些generated_image的分类结果，把得分高的一批选出来&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;最终我们看到的就是从D的眼皮下逃出的优质的生成数据。&lt;/p&gt;&lt;p&gt;好了，前面对代码的几个核心部分做了介绍，下面我们来看看实验过程中的一些问题。&lt;/p&gt;&lt;h2&gt;图像的演变过程&lt;/h2&gt;&lt;p&gt;在优化刚开始时，从随机生成的100维向量生成的图像是这样子的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dee6ed0084ef2fbf17d76e2b0e947e9b.png" data-rawwidth="308" data-rawheight="336"&gt;其实就是噪声。&lt;/p&gt;&lt;p&gt;经过400轮的迭代，生成模型可以生成下面的图像了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/30549e49b67fb85af5acb25cd90042fa.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出数字的大体结构已经形成，但是能够表征数字细节的特征还没有出现。&lt;/p&gt;&lt;p&gt;经过10个Epoch后，生成模型的作品：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/93ba20c5958eee5a8db956743a580c40.png" data-rawwidth="308" data-rawheight="336"&gt;这时候有些数字已经成型，但是还有一些数字仍然存在欠缺。&lt;/p&gt;&lt;p&gt;然后是20轮Epoch的结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/10e43906800c936ea133757c98fa83be.png" data-rawwidth="308" data-rawheight="336"&gt;这个时候的数字已经具有很强的辨识度，但是与其同时，我们发现生成的数字中有大量的“1”。&lt;/p&gt;&lt;p&gt;当完成了所有的训练，我们拿出生成模型在最后一轮生成的图像，可以看到：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2b4d98d848f9d2da7a304f0fd0bc7f01.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出这里面的数字质量更高一些，但是里面的1也更多了。&lt;/p&gt;&lt;p&gt;从这个演化过程中，我们可以看出，从一开始的数字生成质量都很差但生成数字的多样性比较好，到后来的数字质量比较高但数字的多样性比较差，模型的特性在不断地发生变化。这也和两个模型的对抗有关系，而这个演变也和增强学习中的“探索-利用”困境有关系。&lt;/p&gt;&lt;p&gt;我们站在生成模型的角度去想，一开始生成模型会尽可能地生成各种各样形状的数字，而判别模型会识别出一些形状较差的模型，而放过一些形状较好的模型，随着学习的进程不断推进，判别模型的能力也在不断地加强，生成模型慢慢发现有一些固定的套路比较容易通过，而其他一些套路不那么容易通过，于是它就会尽可能地增大这些“套路”出现的概率，让自己的loss变小。这样，一个从探索为主的模型变成了一个以利用为主的模型，实际上它的数据分布已经不那么均匀了。&lt;/p&gt;&lt;p&gt;如果这个模型继续训练下去，生成模型有可能进一步地利用这个“套路”，这和我们传统意义上的过拟合也有很相近的地方。所以我们希望能够避免这样的过拟合。&lt;/p&gt;&lt;p&gt;这个小实验也到此结束了，下面我们将看看论文中关于DCGAN的介绍，以及关于模型的一些改进方案和其他框架的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22389906&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 26 Sep 2016 09:55:52 GMT</pubDate></item><item><title>DCGAN的小尝试（1）</title><link>https://zhuanlan.zhihu.com/p/22386494</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;话说当今的深度学习网络框架世界，除了Caffe，还有很多不错的框架。这一次为了省事，我们直接找一个开源的应用进行分析和尝试。而这次的框架主角是keras，一个拥有简洁API的框架。而我们今天的主角来自深度学习界的大神Yann LeCun（我比较喜欢叫他颜乐存，哈哈……）在Quora上的对这个问题的回答：&lt;p&gt;&lt;a class="" href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning" data-editable="true" data-title="What are some recent and potentially upcoming breakthroughs in deep learning?"&gt;What are some recent and potentially upcoming breakthroughs in deep learning?&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;There are many interesting recent development in deep learning, probably too many for me to describe them all here. But there are a few ideas that caught my attention enough for me to get personally involved in research projects.&lt;/p&gt;&lt;p&gt;The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks).&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;总结成一句话：深度学习的未来，就是干（GAN）！&lt;/p&gt;&lt;p&gt;听上去就让人热血沸腾啊～&lt;/p&gt;&lt;p&gt;抱着极大的好奇心，我们开始对GAN的探险之旅。&lt;/p&gt;&lt;h2&gt;DCGAN&lt;/h2&gt;&lt;p&gt;如果从GAN的起点开始聊起，那么等我们聊到正题，估计好几集都过去了。所以让我们忘掉前面的种种解法，直接来到我们的深度学习部分：DCGAN。全称是Deep Convolution GAN。也就是用深度卷积网络进行对抗生成网络的建模。&lt;/p&gt;&lt;p&gt;对抗神经网络（GAN）有两个主角——&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个是G（Generator）,也就是生成模型；它的输入是一个随机生成的向量，长度不定，输出是一个具有一定大小的图像（N*N*3）和（N*N*1）。&lt;/li&gt;&lt;li&gt;一个是D（Discriminator），也就是判别模型。在我们接下来介绍的模型中，它的输入维度和G的输出一样，输出是一个长度为1 的向量，数字的范围从0到1，表示图像像一个正常图片的程度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;G的输入和输出都比较好理解，D的输入也比较好理解，那么D的输出是什么含义呢？它表示了对给定输出是否像我们给定的标准的输入数据。这句话可能有点绕口，我们可以把判别模型理解成一个解决分类问题的模型，那么在这个问题中判别模型的结果就是区分一个输入属于下面两个类别中的哪个——“正常输入”和“非正常输入”。&lt;/p&gt;&lt;p&gt;举个更具体的例子。对于MNIST数据集来说，每一个手写的数字都可以认为是一个“正常输入”，而随便生成的一个不像手写数字的输入都可以认为是一个“非正常输入”。而我们的判别模型就是要判断这个问题，我们学习的目标也是学习出一个能够解决这个问题的模型。&lt;/p&gt;&lt;p&gt;那么，我们的生成模型的目标呢？就是我们能够从一个随机生成的向量生成一样“正常输入”的图像。听上去有点神奇吧，不过现实中这个效果是可以实现的。我们可以想象我们的输入空间是满足某种分布的一个空间，对于空间中的每一个点，我们都可以利用生成模型将其映射成为一个图像，现在我们限定了生成的图像必须是“正常输入”，那么输入和输出在某种程度上已经确定，我们就可以用监督学习的方式进行学习了。不过对于生成模型来说，我们的loss是生成图像的likelihood，这个和判别模型的loss不太一样。&lt;/p&gt;&lt;p&gt;好了，两个模型的输入输出已经说完了，下面还有两个问题需要解决：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;判别模型的训练数据该如何准备？正例可以用现有数据，那负例呢？&lt;/li&gt;&lt;li&gt;生成模型的loss该如何计算？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其实要想解决这两个问题，我们需要把两个模型连起来。因为生成模型和输出和判别模型的输入在维度和含义上都是相同的，这样连起来我们就可以解决上面的两个问题。我们利用判别模型去判断生成模型的likelihood，而用生成模型产生的结果去做判别模型的负例，这样就把上面的两个问题解决了。&lt;/p&gt;&lt;p&gt;当然，关于把生成模型的输出作负例这件事，听上去还是有点奇怪的。生成模型的目标是生成“正常输入”，那么生成了“正常输入”还被当成负例，也是够冤的。不过这种矛盾的关系在机器学习中经常存在，就像优化目标中的loss项和正则项一样，这两个目标往往也是一对矛盾体。所以这种矛盾的存在并不奇怪，这也是这个模型被称为“对抗”的原因。&lt;/p&gt;&lt;p&gt;大家都喜欢用警察和小偷的关系来比喻生成模型和判别模型之间的“对抗”关系，我觉得可以用“魔高一尺，道高一丈”，“道高一丈，魔高十丈”来解释两个模型随着对抗不断强化的关系。判别模型在进化中能够捕捉不像“正常输入”的所有细节，而生成模型则会尽全力地模仿判别模型心中“正常输入”的形象。&lt;/p&gt;&lt;p&gt;好了，说了这么多，我们来看看针对“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” 这篇论文的keras版“实现”：&lt;a href="https://github.com/jacobgil/keras-dcgan" data-editable="true" data-title="GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks" class=""&gt;GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks&lt;/a&gt;，说是“实现”是因为这个实现实际上和论文中期望的有点小不同。&lt;/p&gt;&lt;p&gt;这个代码使用的数据集是MNIST，经典的小数据集，手写数字。在我的实验结果中，生成模型生成的手写数字是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/65660a40ec7d70c5798737fd357a5aac.png" data-rawwidth="308" data-rawheight="336"&gt;除了个别数字之外，大多数的数字生成得还是有模有样的嘛！&lt;/p&gt;&lt;p&gt;另外我们看一下两个模型在训练过程中的Loss：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2ab472cb9adaa293b3544b6406530fd7.png" data-rawwidth="976" data-rawheight="309"&gt;其中蓝色是生成模型的loss，绿色是判别模型的loss，可以看出两个模型的Loss都存在一定程度的抖动，也可以算是对抗过程中的此消彼长吧。&lt;/p&gt;&lt;p&gt;最后套用乐存大师的话做结尾：&lt;/p&gt;&lt;blockquote&gt;It seems like a rather technical issue, but I really think it opens the door to an entire world of possibilities.&lt;/blockquote&gt;&lt;p&gt;既然乐存老师都这么说了，我们真得好好看看这个模型了。下一会我们来看看前面提到的论文和上面的提到的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22386494&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 20 Sep 2016 08:52:22 GMT</pubDate></item><item><title>Caffe源码阅读——DataLayer&amp;Data Transformer</title><link>https://zhuanlan.zhihu.com/p/22404295</link><description>又一次回到了Caffe的源码阅读的环节，这一次瞄准的目标是网络的输入，现在的CNN网络百花齐放，各种各样的网络结构搭配各种各样的输入让人眼花缭乱，所以我们也必要研究一下输入的代码结构。&lt;p&gt;Caffe的DataLayer基础版的主要目标是读入两种DB的训练数据作为输入，而两种DB内存储的格式默认是一种叫Datum的数据结构。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;message Datum {
  optional int32 channels = 1;
  optional int32 height = 2;
  optional int32 width = 3;
  // the actual image data, in bytes
  optional bytes data = 4;
  optional int32 label = 5;
  // Optionally, the datum could also hold float data.
  repeated float float_data = 6;
  // If true data contains an encoded image that need to be decoded
  optional bool encoded = 7 [default = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看出，这种Datum的结构主要的服务对象是经典的图像分类任务。我们同时输入两部分信息：图像信息data和类别信息label。对于其他的信息来说，使用这个结构进行存储就显得有些困难了。比方说Object Detection的任务，其中还涉及到许多BoundingBox的信息，存储的结构要比这个更复杂。比方说一个知名的图像物体检测的网络结构SSD的作者开源实现就用到了一种自定义的训练数据存储方式：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;// An extension of Datum which contains "rich" annotations.
message AnnotatedDatum {
  enum AnnotationType {
    BBOX = 0;
  }
  optional Datum datum = 1;
  // If there are "rich" annotations, specify the type of annotation.
  // Currently it only supports bounding box.
  // If there are no "rich" annotations, use label in datum instead.
  optional AnnotationType type = 2;
  // Each group contains annotation for a particular class.
  repeated AnnotationGroup annotation_group = 3;
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然，创建一个新的Datum类型只是开始，我们还需要围绕着这个新的Datum创造相关的读取数据的C++类。当然，在创建这些类之前，我们当然需要了解一下Caffe自身的数据层的机制。&lt;/p&gt;&lt;p&gt;为了更好地理解这部分有点绕弯的关系，我们先来上一张几个相关类的关系图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/a30df15ae709d83b0915fc23245f0959.jpg" data-rawwidth="1280" data-rawheight="960"&gt;这其中涉及到了两个线程和一个先向计算的过程，我们一一仔细看下。&lt;/p&gt;&lt;h2&gt;DataReader Thread&lt;/h2&gt;&lt;p&gt;DataReader是Caffe封装的读取两种DB的数据的类，这一步仅仅是把数据从DB中读取出来，也是上面图中右下角那个红框所展示的内容。这部分会给每一个读入的数据源创建一个独立的线程，专门负责这个数据源的读入工作。如果我们有多个Solver，比方说工作在多GPU下，而读入的数据源只有一份（比方说Train的DB只有一个），那么这一个读取数据的线程将会给这些Solver一并服务，这其中的原理可以详细看看DataReader这一部分。&lt;/p&gt;&lt;p&gt;最终每一个Solver里面的Net对象的DataLayer都会有一个自己的DataReader对象，其中会有一对变量：free和full。DataReader线程作为生产者将读入的数据放入full中，而下游的BasePrefetchingDataLayer的线程（后面会提到）将作为消费者将full中的内容取走。Caffe中继续使用BlockingQueue作为生产者和消费者之间同步的结构，并且设置两个队列的容量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每一次DataReader将free中已经被消费过的对象取出，填上新的数据，然后将其塞入full中；&lt;/li&gt;&lt;li&gt;每一次BasePrefetchingDataLayer将full中的对象取出并消费，然后将其塞入free中。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这样就保证了两边通信没有问题。&lt;/p&gt;&lt;h2&gt;BasePrefetchingDataLayer Thread&lt;/h2&gt;&lt;p&gt;BasePrefetchingDataLayer从名字上来看就是一个具有预先取出数据功能的数据层。每次前向计算时，我们并不需要在取数据这一步等待，我们完成可以把数据事先取好，等用的时候直接拿出来。这就是它们以线程的形式独立启动的原因。实际上DataReader的主要工作是把原始的数据从DB中取出，而BasePrefetchingDataLayer类要做的就是数据的加工了。这一部分主要完成两件事：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;确定数据层最终的输出（可以不输出label的）&lt;/li&gt;&lt;li&gt;完成数据层预处理（通常要做一些白化数据的简单工作，比如减均值，乘系数）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;前面已经提过，这一部分将消费DataReader的输入，同时这一部分将产生可以供上层计算网络直接使用的数据，这样在它的前向计算中，我们直接将BasePrefetchingDataLayer的输出拷贝到top_data就可以了，这样就节省了一定的时间。一般来说由GPU完成计算，由CPU完成数据准备，两者之间也不会出现严重的资源竞争。&lt;/p&gt;&lt;h2&gt;写个新结构？&lt;/h2&gt;&lt;p&gt;从上面的介绍中，我们看出：DataReader基本上不用变，我们只要根据不用的Datum类型创建不同的泛型类就好了，这部分的代码逻辑是固定的；而BasePrefetchingDataLayer的部分是有可能发生变化的。这也是一个新的Datum要面对的主要部分。而BasePrefetchingDataLayer也采用的Template的设计模式把不变的流程代码准备好了，一般来说只要实现两个函数即可：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DataLayerSetUp&lt;/li&gt;&lt;li&gt;load_batch&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一个是把类内的一些变量的维度进行初始化，一个是实现如何把一个DataReader返回的raw data转化为上层网络要的数据。&lt;/p&gt;&lt;p&gt;知道了这些，我们就可以看看SSD中的annotated_data_layer的实现了，它的label中需要存入8个信息：&lt;/p&gt;&lt;p&gt;[item_id, group_label, instance_id, xmin, ymin, xmax, ymax, diff]&lt;/p&gt;&lt;p&gt;所以相对应的load_batch部分也要做许多计算和准备。具体的计算内容我们可以以后再看，总之通过这两个部分的修改——prototxt中的数据结构定义和DataLayer部分相关位置的修改，我们就可以使得网络输入多种多样的数据，我们的Caffe也就可以完成更多有挑战的事情了。&lt;/p&gt;&lt;h2&gt;Data Transformer&lt;/h2&gt;&lt;p&gt;这一段新加入的，本来希望能单独写一篇，后来发现字数不够多，就把这部分和这篇合并起来了。我们来看一看Data Transformer的内容。这一部分的实现在C++和python上有所不同。一般来说，我们用C++的代码做训练，用python的代码做预测（当然现在用python做训练的也越来越多）。在C++中这是DataLayer中的一个小部分，而在python中这是一个独立的部分。&lt;/p&gt;&lt;h2&gt;C++中的DataTransformer&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Crop&lt;/b&gt;：这是在训练过程中经常用到的一种增强数据的方式。在train的过程中Caffe会进行随机crop，在test的过程中只会保留中间的部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mirror&lt;/b&gt;：做一个x轴的翻转&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mean&lt;/b&gt;: 给每个像素减去一个均值&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scale&lt;/b&gt;：给每个像素值乘以一个系数&lt;/p&gt;&lt;h2&gt;Python中的DataTransformer&lt;/h2&gt;&lt;p&gt;python的Transformer就有些复杂了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Resize&lt;/b&gt;：将输入数据缩放到指定的长宽比例&lt;/p&gt;&lt;p&gt;&lt;b&gt;Transpose&lt;/b&gt;：转换输入数据的维度。因为经过skimage读入后数据的维度是(Height * Width * Channel)，需要将数据的维度转换到(Channel*Height*Width)&lt;/p&gt;&lt;p&gt;&lt;b&gt;Channel&lt;/b&gt;&lt;b&gt;swap&lt;/b&gt;：这个主要针对彩色图的输入，不同的图像处理库对channel的处理顺序有所不同。像opencv，C++的主要合作伙伴，它的默认装载顺序是BGR——Blue,Green,Red。而skimage读入的是RGB，所以为了保证和C++训练的模型一致，所以这一步也是很必要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raw scale——Mean——Input scale&lt;/b&gt;：这里是将每个像素乘以一个Raw scale，减去一个Mean，再乘以一个Input scale。&lt;/p&gt;&lt;p&gt;python的版本中还包含一个&lt;b&gt;deprocess&lt;/b&gt;，用于做图像的反向处理。&lt;/p&gt;&lt;p&gt;python版本的crop和mirror功能在python/caffe/io.py中的oversample函数，不过他的逻辑和C++的逻辑不太一样了。实际使用中还可以针对自己的使用情况进行修改。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22404295&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Fri, 16 Sep 2016 08:43:04 GMT</pubDate></item><item><title>聊点轻松的3——什么是学习</title><link>https://zhuanlan.zhihu.com/p/22421787</link><description>之前给自己定下了一个目标，专栏的关注者每突破一千就写一篇和技术没有太大关系但是又比较有意义的文章。第一个1000人，我聊了点专栏未来的发展方向，第二个1000人，我放了一些“有趣”但实际上挺恶俗的图片，到了第三个1000人，我想了好久才想到了这个话题，于是让我们一起聊一聊什么是学习。&lt;p&gt;写在前面，专栏里正常的文章还是怀着科学的精神来写的，不过这个“聊点轻松的”支线剧情就不管那么多了，玄学为主哈。&lt;/p&gt;&lt;p&gt;我们聊了好多有关机器学习的事情，那么让我们回到自己身上，人又是怎么学习的？从人学习方式来看，我们有哪些优秀的模式可以利用到机器上呢？从机器学习的角度，人类的学习又该如何进行呢？（好玄学的问题啊）&lt;/p&gt;&lt;h2&gt;学习方法&lt;/h2&gt;&lt;p&gt;说到这个让我们首先来看看机器界的三种常见的学习方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;监督学习&lt;/li&gt;&lt;li&gt;非监督学习&lt;/li&gt;&lt;li&gt;增强学习&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;排名不分先后。这几种学习在我们的日常生活中会不会经常出现呢？当然会。下面我们来举几个例子：&lt;/p&gt;&lt;p&gt;监督学习：1+1等于几？要是答等于2就对了，答其他就错了（这里是严肃的1+1问题）。这里问题有，答案也有，我们的目标是让自己做桥梁，连通问题和答案；&lt;/p&gt;&lt;p&gt;非监督学习：多半来自脑中突然出现的一些宇宙终极问题，还有一些课外兴趣小组中偶然碰到的问题（当然还有现在工作研究中的一些问题），没人知道正确答案，能不能答对全看自己的思考分析了。（XX猜想……）当然一般这些问题最后慢慢都有了答案，不过当初解决问题时确实没有答案。&lt;/p&gt;&lt;p&gt;增强学习：生活小经验，生活小技能。人在江湖飘，哪能不挨刀，每一次成功与失败，老天都会给你一个“奖励”，并让这个世界稍微改变一点……当然这个奖励有时候不是物质的，比方说“心里乐开了花”这种……&lt;/p&gt;&lt;p&gt;从某种意义上看，机器学习的方式和人类学习的方式其实差不多，机器学习也可以算是门仿生学了。我们走进校园，在课堂上被老师进行监督学习，自己无聊时做些思考，进行一些非监督学习，其他的大部分时间，进行很多增强学习。于是学校里也经常出现各种各样的传奇人物：&lt;/p&gt;&lt;p&gt;学霸：典型的监督学习产物，一天刷一本练习册，这不是海量训练数据样本+高性能计算是什么？&lt;/p&gt;&lt;p&gt;学神：思考一些神秘的问题，经常有规律的进行非监督学习……当然学神一般是在完成了监督学习之后才进行这项高级活动的。&lt;/p&gt;&lt;p&gt;老（lao）江（si）湖（ji）：社会活动多，社交经验丰富，审时度势，察言观色，调节气氛，推动剧情的好帮手。&lt;/p&gt;&lt;p&gt;好吧，胡扯到此结束。不过似乎每个人都有自己擅长的学习方式，也有自己不擅长的学习方式，可能所谓的“因材施教”和这个也有关系吧。&lt;/p&gt;&lt;h2&gt;论语和机器学习&lt;/h2&gt;&lt;p&gt;玄学第二弹，论语！你没有看错，几千年前的孔老夫子，中华文化的形象代言人，已经在他的著作中告诉了我们很多机器学习的“套路”！已经忘了？没关系，让我们来一起复习下：&lt;/p&gt;&lt;p&gt;“学而时习之，不亦说乎？”&lt;/p&gt;&lt;p&gt;——训练数据要记得经常回测一下，以确保模型的泛化性，别捡了芝麻丢了西瓜。&lt;/p&gt;&lt;p&gt;“温故而知新，可以为师矣”&lt;/p&gt;&lt;p&gt;——要在旧数据上finetune，也要记得在新数据上继续提升哦～&lt;/p&gt;&lt;p&gt;——要是能通过研究旧数据就能产生新数据，那就逆天了！Deep Generative Model，我来了！（歪解）&lt;/p&gt;&lt;p&gt;“吾尝终日不食，终夜不寝，以思，无益，不如学也”&lt;/p&gt;&lt;p&gt;——跑了三天无监督训练精度还是上不去，还是去标点数据跑监督学习吧。&lt;/p&gt;&lt;p&gt;（镜像贴：《荀子  劝学》“吾尝终日而思，不如须臾之所学也”）&lt;/p&gt;&lt;h2&gt;有痛的学习&lt;/h2&gt;&lt;p&gt;专栏的名字叫做“无痛的机器学习”，可实际上谁都知道学习有痛。最近还被罗振宇罗胖打了脸。他的一期《罗辑思维》中《怎么样成为一个高手》&lt;a href="http://v.youku.com/v_show/id_XMTY4OTY3NjU3Ng==.html?from=y1.6-91.3.1.5bdbf57c947311e3b8b7" data-editable="true" data-title="罗辑思维"&gt;罗辑思维&lt;/a&gt;中提到了，学习是要专找自己不会的地方“刻意练习”。从机器学习的角度解释再简单不过了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;只有自己不会的地方才会有较大的loss梯度，才有优化目标函数的必要，总是找自己已经会的，都没有梯度可以下降，还优化个毛线啊……&lt;/li&gt;&lt;li&gt;刻意练习，反复练习。哪有一轮迭代就可以完美梯度下降的优化算法，兄弟一定要多跑几轮啊！&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实说到这里想想学习这个事情，还是听悲哀的。学习本来就是个苦差事，现在的中文中，从学习两个字不太能看出它的痛苦，主要是能看出它的手段——学和习，不过我中华上国，少年学习不谈苦和累，只谈使命和责任；不过我们的邻居就比较诚实了：&lt;/p&gt;&lt;p&gt;日语中的学习写作：“&lt;b&gt;勉強する&lt;/b&gt;”，我去太直白了，就是勉强嘛，凡事不能勉强啊……&lt;/p&gt;&lt;p&gt;韩语中的学习写作：“&lt;b&gt;공부하다&lt;/b&gt;”，后面两个字算是词缀，和日语的“する”类似，前面两个字念出来就是“恐怖”，恐怖……&lt;/p&gt;&lt;p&gt;所以多学点语言了解不同的国家人的内心还是很有必要的……一个是做勉强的事情，一个是做恐怖的事情，我们还在天真地认为那只是在知识的海洋遨游……&lt;/p&gt;&lt;p&gt;虽然学习是有痛的，但是“学海无涯苦作舟”的时代已经过去，现在的时代信息交流已经变得非常方便，我们可以寻找到很多方法让自己在学到一定知识的基础上减少痛苦，或者在一定痛苦的基础上多学知识。随着社会的发展，人类要学的会越来越多，不过人类学习的痛苦也一定会越来越轻的。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22421787&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 13 Sep 2016 00:00:31 GMT</pubDate></item><item><title>CNN-卷积反卷积（2）</title><link>https://zhuanlan.zhihu.com/p/22293817</link><description>在前面的斗图篇我们提过这篇文章《Visualizing and understanding convolutional networks》，这是一片介绍反卷积和可视化的文章，今天我们就来详细看看这篇文章的一个开源实现——来自&lt;a href="https://github.com/piergiaj/caffe-deconvnet" data-editable="true" data-title="GitHub - piergiaj/caffe-deconvnet: A deconvolutional network in caffe"&gt;GitHub - piergiaj/caffe-deconvnet: A deconvolutional network in caffe&lt;/a&gt;。&lt;p&gt;首先我们给出上面这篇论文的网络结构架构：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/06d0d9ad21e2f6d6039c3c6dec164652.jpg" data-rawwidth="649" data-rawheight="564"&gt;&lt;p&gt;从结构中可以看出，网络首先进行前向计算，在前向计算中收集一些数据，然后将这些数据塞入反向网络中进行反向计算，从而得到最终的反卷积结果。&lt;/p&gt;&lt;p&gt;这其中还包括一些传统网络中没有的层结构——当然不是网络中带有参数的核心层了。以下这张图是这个网络的结构展示以及在前向计算过程中每个层的维度大小：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/067af47f556e22c96654b4acd4e11fdc.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;从这张图可以看出它就是一个经典的AlexNet结构的网络，但是这个层中有一些特殊的层，它们是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PoolingSwitches&lt;/li&gt;&lt;li&gt;SliceHalf&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个层有什么作用呢？实际上它就是一个Pooling层，只不过会多输出一些信息。对于max_pooling来说，它会输出正常的max_pooling值，以及对应的max_pooling的index。这个index信息将会被用在反向计算中。因为反向计算时我们要将diff传输到pooling层前指定的index上，这里要做一个记录。&lt;/p&gt;&lt;p&gt;其中PoolingSwitches就是记录max pooling的值和max pooling选中的index，而SliceHalf则将这两部分分成两个输出，属于SliceLayer的一个特殊实现。&lt;/p&gt;&lt;p&gt;反向计算就是把之前的参数再传输回去，在前面我们已经介绍过这其中的思想了。返回的网络结构如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0819bac8ccd184778b1393007b509642.jpg" data-rawwidth="3024" data-rawheight="4032"&gt;可以看出除了去掉了relu层，反向的结构和前向的结构完全一样。反向中的InvPooling层做了Pooling的的反向计算，其中利用到了前向计算中保存的index值。&lt;/p&gt;&lt;p&gt;其中的一些效果可以参见前面的斗图篇，这里就不多介绍了。链接是：&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22112582" data-editable="true" data-title="聊点轻松的2——斗图篇 - 无痛的机器学习 - 知乎专栏"&gt;聊点轻松的2——斗图篇 - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;感兴趣的童鞋也可以自己尝试一下。&lt;/p&gt;&lt;p&gt;总体来说这个方法可以帮助我们观察出图像中的一些响应特点，在论文中作者还提出，他们通过可视化的方法发现了AlexNet中的一些不足，并提出了一些改进方法，也最终形成了在江湖上有一定影响力的ZF模型。&lt;/p&gt;&lt;p&gt;除了这种反卷积的可视化方法，其实还有其他的可视化方法。后面我们再慢慢介绍。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22293817&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 11 Sep 2016 20:23:47 GMT</pubDate></item><item><title>CNN-反卷积（1）</title><link>https://zhuanlan.zhihu.com/p/22245268</link><description>关于CNN的内容已经说了很多，虽然我们无法把这个长得像黑盒的东西完全摸清楚，但是我们多多少少也对它的外部结构有了一定的了解。下面我们来看看大神们对进一步探究CNN内部世界所做的工作。&lt;p&gt;这部分工作有一个响亮的Title，那就是，CNN的网络到底学到了什么？&lt;/p&gt;&lt;p&gt;对于浅层网络，尤其是一层网络，上面这个问题非常好回答。我们知道模型输入的特征和分布，我们也知道输出的特征和分布（这里特指监督学习），那么模型的目标就是把输入空间的数据映射到输出空间，而且映射的结果是正确的。如果我们把浅层网络替换成深层网络，还把它当作一个不可分割的整体，那么它的目标和浅层网络是完全一致的。&lt;/p&gt;&lt;p&gt;但问题是，我们并没有把深层网络当成浅层网络，因为深度学习涉及了浅层网络学习时代的两个部分——构建特征和从特征到结果。所以我们心里一定在想，这么多层，哪些层是在构建特征，哪些层是在把特征转换到结果？在CNN网络模型发展的初期，人们倾向于把这个分界点设立在卷积层和全连接层的交界处——卷积层负责收集特征信息，全连接层和早年的神经网络一样，只负责特征的处理。&lt;/p&gt;&lt;p&gt;如果你接受了这样的概念，那么我们下一个问题就来了。卷积层是如何把一张图片或者其他的东西转换成了特征，这些特征是如何表达我们的图片呢？这还是一个没有解决的问题。&lt;/p&gt;&lt;p&gt;于是很多大神开始了各种尝试，其中一种尝试就是反卷积操作。&lt;/p&gt;&lt;h2&gt;反卷积&lt;/h2&gt;&lt;p&gt;反卷积是什么？是一个新概念呢？其实不是。其实在CNN中Deconvolution的计算和图像的反卷积操作还是有一点不同，这种Deconvolution是将Convolution的方向反过来——没错，前向变后向，后向变前向。实际上关于它的名称还有很多，像back convolution, transpose convolution等。我个人比较喜欢back convolution。关于前向后向的计算可以参考我们前面对于卷积层的计算推导，这里就不再多说了。那么我们就从这个角度去看看反卷积究竟做了什么。&lt;/p&gt;&lt;p&gt;想知道反卷积的前向操作做了什么，我们可以去看卷积的后向操作做了什么。卷积的后向操作是为了计算卷积层的Loss对参数和输入的梯度，那么梯度是什么含义？负梯度表示了函数下降最快的方向，为了使函数值（也就是Loss）尽可能地小，我们希望梯度尽可能地小。但是如果某个参数的梯度非常大，能说明什么呢？说明当前函数参数的变动对函数造成的影响非常大。&lt;/p&gt;&lt;p&gt;举个例子，比方说有这样一个函数&lt;equation&gt;y=w_1*x_1+w_2*x_2&lt;/equation&gt;，y最终还要经过计算得到最终的loss，那么我们计算这个函数的梯度：&lt;/p&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x_1}=\frac{\partial Loss}{\partial y}*w_1&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x_2}=\frac{\partial Loss}{\partial y}*w_2&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial w_1}=\frac{\partial Loss}{\partial y}*x_1&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial w_2}=\frac{\partial Loss}{\partial y}*x_2&lt;/equation&gt;&lt;p&gt;如果w1非常大而w2非常小，那么x1的梯度就会远大于x2，如果x1做点小改动，它会对最终结果产生比较大的影响；而x2做点小改动，对应的影响就没有那么严重了；&lt;/p&gt;&lt;p&gt;如果x1非常大而x2非常小，那么w1的梯度就会远大雨w2，如果w1做点小改动，它会对最终结果产生比较大的影响；而w2做点小改动，对应的影响就没有那么严重了；&lt;/p&gt;&lt;p&gt;那么我们可以想想看，对结果产生较大影响以为着什么？对于一个人脸识别系统，上面公式中那些对结果影响比较大的因子往往对应着人脸中富有关键特征的地方，因为如果这些地方发生了变化，我们识别这个人的关键信息就变动了，换言之——我们就有可能认错人。&lt;/p&gt;&lt;p&gt;而对于那些对结果影响较小的因子，往往对应着一些不太重要的地方，比方说背景——不论你变成蓝色，红色，白色，我都能认出这个人。&lt;/p&gt;&lt;p&gt;所以我们可以得出一个推论，那就是梯度大的因子是当前这个这套模型和数据组成的整体的关键！对于w，它就是当前输入数据的关键参数，对于x，他就是当前模型参数的关键输入！&lt;/p&gt;&lt;p&gt;所以，如果我们能够找到对于某一个模型某一特定输入下梯度比较大的输入因子和参数因子，我们就能知道输入的哪些部分是模型最关心的，而模型的哪些部分是输入最关心的。&lt;/p&gt;&lt;p&gt;一般来说，由于输入的图像大多具备一定的可视性，所以我们观察“输入的哪些部分是模型最关心的”是相对容易的。&lt;/p&gt;&lt;p&gt;那么我们就可以开始我们的实验了，其实并不需要真的构建一个反卷积层，我们理论上只要把数据从前向传一遍，后向传一遍，然后分析Loss对输入的梯度，找出梯度最大的像素点，就是模型最关心的地方。这样是不是就可以完成任务？&lt;/p&gt;&lt;p&gt;理论上是这样的，那么下一回我们看看具体的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22245268&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Fri, 09 Sep 2016 21:14:47 GMT</pubDate></item><item><title>CNN--两个Loss层计算的数值问题</title><link>https://zhuanlan.zhihu.com/p/22260935</link><description>写在前面，这篇文章的原创性比较差，因为里面聊的已经是老生长谈的事情，但是为了保持对CNN问题的完整性，还是把它单独拿出来写一篇了。已经知道的童鞋可以忽略，没看过的童鞋可以来瞧瞧。&lt;p&gt;这次我们来聊一聊在计算Loss部分是可能出现的一些小问题以及现在的解决方法。其实也是仔细阅读下Caffe代码中有关Softmax loss和sigmoid cross entropy loss两个部分的真实计算方法。&lt;/p&gt;&lt;h2&gt;Softmax&lt;/h2&gt;&lt;p&gt;有关Softmax的起源以及深层含义这里不多说了，我们直接来看看从定义出发的计算方法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def naive_softmax(x):
    y = np.exp(x)
    return y / np.sum(y)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;随便生成一组数据，计算一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)
print a
print naive_softmax(a)

[ 0.67362493  0.20352691  0.02024274  0.29988184  0.2319521           
  0.43930833  0.98219225  0.54569955  0.00298489  0.83399241]
[ 0.12203807  0.07626659  0.06349434  0.08398094  0.07846559   
  0.09654569  0.16615155  0.10738362  0.06240797  0.14326563]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从结果来看比较正常，符合预期，但是如果我们的输入不那么正常呢？&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;b = np.random.rand(10) * 1000
print b
print naive_softmax(b)

[ 497.46732916  227.75385779  537.82669096  787.54950048  663.13861524
  224.69389572  958.39441314  139.09633232  381.35034548  604.08586655]
[  0.   0.   0.  nan   0.   0.  nan   0.   0.   0.]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们发现数值溢出了，因为指数函数是一个很容易让数值爆炸的函数，那么输入大概到多少会溢出呢？蛋疼的我还是做了一个实验：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;np.exp(709)
8.2184074615549724e+307
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这是在python能够正常输出的单一数字的极限了。实际上这接近double类型的数值极限了。&lt;/p&gt;&lt;p&gt;虽然我们前面讲过有一些方法可以控制住数字，使输出不会那么大，但是终究难免会有个别大数字使得计算溢出。而且实际场景中计算softmax的向量维度可能会比较大，大家累积起来的数字有时还是挺吓人的。&lt;/p&gt;&lt;p&gt;那么如何解决呢？我们只要给每个数字除以一个大数，保证它不溢出，问题不就解决了？老司机给出的方案是找出输入数据中最大的数，然后除以e的最大数次幂，相当于下面的代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def high_level_softmax(x):
    max_val = np.max(x)
    x -= max_val
    return naive_softmax(x)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样一来，之前的问题就解决了，数值不再溢出了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;b = np.random.rand(10) * 1000
print b
print high_level_softmax(b)

[ 903.27437996  260.68316085   22.31677464  544.80611744  506.26848644
  698.38019158  833.72024087  200.55675076  924.07740602  909.39841128]

[  9.23337324e-010   7.79004225e-289   0.00000000e+000   
   1.92562645e-165   3.53094986e-182   9.57072864e-099   
   5.73299537e-040   6.01134555e-315   9.99999577e-001   
   4.21690097e-007]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;虽然不溢出了，但是这个结果看着还是有点怪。上面的例子中最大的数字924.07740602的结果高达0.99999，而其他一众数字经过softmax之后都小的可怜，小到我们用肉眼无法从坐标轴上把它们区分出来，这说明softmax的最终结果和scale有很大的关系。&lt;/p&gt;&lt;p&gt;为了让这些小的可怜的数字不那么可怜，使用一点平滑的小技巧还是很有必要的，于是代码又变成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def practical_softmax(x):
    max_val = np.max(x)
    x -= max_val
    y = np.exp(x)
    y[y &amp;lt; 1e-20] = 1e-20
    return y / np.sum(y)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果变成了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[  9.23337325e-10   9.99999577e-21   9.99999577e-21   9.99999577e-21
   9.99999577e-21   9.99999577e-21   9.99999577e-21   9.99999577e-21
   9.99999577e-01   4.21690096e-07]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看上去比上面的还是要好一些，虽然不能扭转一家独大的局面。&lt;/p&gt;&lt;h2&gt;Sigmoid Cross Entropy Loss&lt;/h2&gt;&lt;p&gt;从上面的例子我们可以看出，exp这个函数实在是有毒。下面又轮到另外一个中毒专业户sigmoid出厂了。这里我们同样不解释算法原理，直接出代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def naive_sigmoid_loss(x, t):
    y = 1 / (1 + np.exp(-x))
    return -np.sum(t * np.log(y) + (1 - t) * np.log(1 - y)) / y.shape[0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们给出一个温和的例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)
b = a &amp;gt; 0.5
print a
print b
print naive_sigmoid_loss(a,b)

[ 0.39962673  0.04308825  0.18672843  0.05445796  0.82770513  
  0.16295996  0.18544111  0.57409273  0.63078192  0.62763516]
[False False False False  True False False  True  True  True]
0.63712381656
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下面自然是一个暴力的例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)* 1000
b = a &amp;gt; 500
print a
print b
print naive_sigmoid_loss(a,b)

[  63.20798359  958.94378279  250.75385942  895.49371345  965.62635077
   81.1217712   423.36466749  532.20604694  333.45425951  185.72621262]
[False  True False  True  True False False  True False False]
nan
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;果然不出所料，我们的程序又一次溢出了。&lt;/p&gt;&lt;p&gt;那怎么办呢？这里节省点笔墨，直接照搬老司机的推导过程：（侵删，我就自己推一遍了……）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/70203ed967a41892e424dfea23ffd2c6.jpg" data-rawwidth="1229" data-rawheight="1325"&gt;于是，代码变成了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def high_level_sigmoid_loss(x, t):
    first = (t - (x &amp;gt; 0)) * x
    second = np.log(1 + np.exp(x - 2 * x * (x &amp;gt; 0)))
    return -np.sum(first - second) / x.shape[0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;举一个例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)* 1000 - 500
b = a &amp;gt; 0
print a
print b
print high_level_sigmoid_loss(a,b)

[-173.48716596  462.06216262 -417.78666769    6.10480948  340.13986055
   23.64615392  256.33358957 -332.46689674  416.88593348 -246.51402684]
[False  True False  True  True  True  True False  True False]
0.000222961919658
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样一来数值的问题也就解决了！&lt;/p&gt;&lt;h2&gt;就剩一句话了&lt;/h2&gt;&lt;p&gt;计算中遇到Exp要小心溢出！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22260935&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Wed, 07 Sep 2016 09:04:13 GMT</pubDate></item><item><title>CNN--结构上的思考</title><link>https://zhuanlan.zhihu.com/p/22214112</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;前面我们通过几个数值展示了几个比较经典的网络的一些特性，下面我们就花一点时间来仔细观察CNN网络的变化，首先是VGG在网络结构上的一些思考，其次是Inception Module对于单层网络内部的扩展，最后我们再来看看ResidualNet对于网络计算的改变。当然，我们在介绍这些模型的同时还会聊一些同时代其他的模型。&lt;h2&gt;VGG模型&lt;/h2&gt;&lt;p&gt;介绍VGG模型的文章中自夸了VGG模型的几个特点，下面我们来仔细说说，&lt;/p&gt;&lt;p&gt;首先是卷积核变小。实际上在VGG之前已经有一些模型开始尝试小卷积核了，VGG模型只是成功案例之中的一个。&lt;/p&gt;&lt;p&gt;那么小卷积核有什么好处呢？文章中提出了两个好处，首先是参数数量变少，过去一个7*7的卷积核需要49个参数，而现在3个3*3的卷积核有27个参数，看上去参数数量降低了不少；第二是非线性层的增加，过去7*7的卷积层只有1层非线性层与其相配，现在有3个3*3的卷积层有3个非线性层。非线性层的增加会使模型变得更加复杂，因此模型的表现力也有了提高。&lt;/p&gt;&lt;p&gt;同时在文章还提出了VGG的模型收敛速度比之前的AlexNet还要快些，从后来人的角度来看，参数训练的速度和本层参数的数量相关。之前我们分析过CNN模型参数的方差，我们假设对于某一层，这层的输入维度为&lt;equation&gt;N_l&lt;/equation&gt;，输出维度为&lt;equation&gt;N_{l+1}&lt;/equation&gt;那么该层网络中每个参数的方差应该控制在&lt;equation&gt;\frac{2}{N_l+N_{l+1}}&lt;/equation&gt;。如果输入输出层的维度比较大，那么参数的理想方差就需要限定的更小，所以参数可以取值的范围就比较小，那么优化起来就比较费劲；如果输入输出维度比较小，那么每个参数的理想方差就会相对大一些，那么可以取值的范围就比较大，优化起来就相对容易些。从这个角度来看，减小每一层参数的数量对于优化来说是有意义的。&lt;/p&gt;&lt;p&gt;其次就是卷积层参数的规律。首先卷积层的操作不会改变输入数据的维度，这里的维度主要指feature map的长和宽。对于3*3的kernel，卷积层都会配一个大小为1的pad。同时stride被设为1。这样经过卷积层变换，长宽没有发生变化。这和之前的卷积层设计有些不同。而且每做一次pooling，feature map的长宽各缩小一倍，channel层就会增加一倍。这样的设计对于不同的feature map维度来说适配起来都比较容易。对于一些通过卷积减小维度的模型来说，对于不同的输入，卷积后的输出各不一样，所以适配起来有可能不太方便，而现在只有pooling层改变长宽维度，整体模型的维度计算就方便了许多。于是在论文中有输入为256和384等维度，模型不需要根据不同的输入维度设计不同的卷积结构，使用同样的结构或者直接加深网络深度就可以了。&lt;/p&gt;&lt;p&gt;此外，模型也提到了1*1的卷积核，这个卷积核我们在后面还会提到。这种卷积核也不会改变feature map的长宽，同时又可以进一步地增加模型的非线性层，也就增加了模型的表现能力。&lt;/p&gt;&lt;p&gt;上面就是VGGNet在架构上做的这些改变，这些改变也被后面一些的模型所接纳。&lt;/p&gt;&lt;h2&gt;丰富模型层的内部结构&lt;/h2&gt;&lt;p&gt;提到模型的内部结构，我们就来到了GoogLeNet模型（这个英文单词是在致敬LeNet？），模型中最核心的地方就是它的Inception Module。在此之前还有一个研究模型层内部结构的文章，叫做Network In Network，其中的道理也比较相似。&lt;/p&gt;&lt;p&gt;Network in Network和Inception Module这类结构主要看中的是模型在局部的拟合能力。有些模型在结构上是采用“一字长蛇阵”的方法，对于某一个特定的尺度，模型只采用一个特定尺度的卷积核进行处理，而上面两种模型却认为，采用一种尺度处理可能不太够，一张图象通常具有总体特征特征和细节特征这两类特征，我们用小卷积核能够更好地捕捉一些细节特征，而随着小卷积不断地卷下去，慢慢地一些总体特征也就被发现。&lt;/p&gt;&lt;p&gt;可是这里有一个问题，那就是我们在网络前段只有细节特征，后段才慢慢有一些总体特征，而有时候我们想让两方面的特征汇集在一起，同时出现发挥作用。那么采用单一的卷积核恐怕不太容易解决这样的问题。&lt;/p&gt;&lt;p&gt;于是上面两种模型开始考虑，与其把模型加深，不如把模型加厚（其实深度差不多），每一次feature map尺度的变化前后，我都尽可能地多做分析，把想得到的不同来源的信息都尽可能得到，这样的特征应该会更有价值吧！&lt;/p&gt;&lt;h2&gt;从乘法模型到加法模型&lt;/h2&gt;&lt;p&gt;ResNet的核心思路就是把曾经CNN模型中的乘法关系转变成加法关系,让模型有了点“Additive”的味道。关于这个问题，文章中采用一个极端的例子作说明。&lt;/p&gt;&lt;p&gt;假设我们已经有了一个较浅模型，我们的目标是去训练一个更深的模型。理论上如果我们能够找到一个靠谱的优化算法和足够的数据，那么这个更深的模型理论上应该比那个较浅的模型具有更好的表达能力。如果抛开优化和可能的过拟合问题不管，这个道理还是可以成立的。&lt;/p&gt;&lt;p&gt;就算较深的模型不能够超越较浅的模型，至少它是可以作到和具有较浅的模型同样的表达能力。如果我们把较深模型分成两部分——和较浅模型相同的部分，比较浅模型多出来的部分，那么我们保持和较浅模型相同的部分的参数完全相同，同时让多出来的模型部分“失效”，只原样传递数据而不做任何处理，那么较深模型就和较浅的模型完全一样了。在论文中，这些多出来的模型部分变成了“Identity Mapping”，也就是输入和输出完全一样。&lt;/p&gt;&lt;p&gt;好了，那么对于现在的架构来说，我们如何学习这些“Identity Mapping”呢？过去的学习方法就是按现在的乘法模式进行学习，我们一般的CNN模型都是一层套一层，层与层之间的关系是乘法，下一层的输出是上一层输入和卷积相乘得到的。学习这样的“Identity Mapping”还是有一点困难的，因为只要是想学到一个具体数值，它就具有一定的难度，不论是“Identity Mapping”还是其他。&lt;/p&gt;&lt;p&gt;于是，ResNet对上面的问题做了一些改变。既然是要学习“Identity Mapping”，那么我们能不能把过去的乘法转变为加法？我们假设多出来的层的函数形式是F(x)，那么乘法关系学习“Identity Mapping”就变成了&lt;equation&gt;F(x)=\sum{wx}=x&lt;/equation&gt;，由于学习的形式没有变，对于乘法我们学习起来同过去一样，但是对于加法就简单多了——&lt;equation&gt;F(x)=x+w&lt;/equation&gt;，只要将参数学习成0就可以了，0和其他数值相比具有很大的优势，这样训练难度就大大降低了。于是，我们也见到即使非常深的网络也可以训练，这也验证了将乘法关系改为加法关系后对模型训练带来的显著提升。&lt;/p&gt;&lt;p&gt;在ResNet之前，还有一些网络已经提出了类似的思想，比如Highway-Network。Highway-Network同样具有加法的特点，但是它并不是一个纯粹的加法，所以在优化过程总较ResNet弱一些。&lt;/p&gt;&lt;p&gt;这样我们就回顾完了上次我们提到的几个模型中的闪光点，如果想进一步地研究这些模型以及模型结构中的精妙之处，多多做实验多多分析数据才是王道。&lt;/p&gt;&lt;h2&gt;最后一点&lt;/h2&gt;&lt;p&gt;为什么GoogLeNet和ResNet的层数很深且参数很少？因为他们的全连接层比较少。为什么呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22214112&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 03 Sep 2016 08:54:49 GMT</pubDate></item><item><title>CNN——架构上的一些数字</title><link>https://zhuanlan.zhihu.com/p/22197188</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/08cd1d56513335b13d6a93725db969ad" data-hash="08cd1d56513335b13d6a93725db969ad" class="member_mention" data-hovercard="p$b$08cd1d56513335b13d6a93725db969ad"&gt;@夏龙&lt;/a&gt;对本文的审阅。&lt;/p&gt;&lt;p&gt;前面说了很多关于CNN的数值上的事，下面我们来看看网络架构。网络架构也是CNN的一个核心部分，由于CNN的特点是它的深度，所以深度模型的网络架构给了人们无数的想象，于是也有了无数的前辈创造了各种各样的模型。我们今天来看看那些经典的模型，不是从感性的角度上去观看，而是从理性的角度——去尝试计算一些具体的数字，让我们描绘出这些模型的一个简单轮廓。&lt;/p&gt;&lt;p&gt;我们的目标问题是ImageNet分类问题，那么我们主要关注哪些问题呢？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;模型的深度，模型的核心层（卷积层、全连接层）的数量，这代表了模型的某种“能力”，基本上大家都有一个共识，那忽略优化问题的情况下，就是越深的模型在函数拟合方面效果越好。这里直接利用Caffe计算其中的layers_.size()，由于其中还包括data layer和loss layer，所以统计数会比实际的层数要多。&lt;/li&gt;&lt;li&gt;每层模型的参数数量，参数的总量，这代表了模型的复杂度。从机器学习的理论上讲，参数越多，模型的表达能力理论上也会“越强”。这里通过Caffe计算所有learnable_params的count总和表示。&lt;/li&gt;&lt;li&gt;模型前向的所需的内存量。也就是Caffe中计算的memory_used_变量值。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;AlexNet&lt;/h2&gt;&lt;p&gt;本文不是负责介绍历史的，所以不会花什么篇幅去聊故事。模型的prototxt来自：&lt;a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt" data-editable="true" data-title="caffe/train_val.prototxt at master · BVLC/caffe · GitHub" class=""&gt;https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;VGGNet&lt;/h2&gt;&lt;p&gt;VGGNet也是一个比较有代表性的网络，关于这个网络的“哲学”我们后面再开新贴去聊。利用论文和各处得到的信息，我们可以详细给出VGG19层模型的具体结构，参考的prototxt来自：&lt;a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md" data-editable="true" data-title="github.com 的页面" class=""&gt;https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="CS231n Convolutional Neural Networks for Visual Recognition" class=""&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;对VGG模型的内存占用量和参数数量做过一个计算，仅作参考：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见在计算过程中偏置项并没有被计算在其中。我们也要做一个详细的计算。&lt;/p&gt;&lt;h2&gt;GoogleNet&lt;/h2&gt;&lt;p&gt;GoogleNet作为Inception module的代表，同样取得了不错的成绩，我们的参考prototxt来自：&lt;a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt" data-editable="true" data-title="caffe/train_val.prototxt at master · BVLC/caffe · GitHub" class=""&gt;https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;ResNet&lt;/h2&gt;&lt;p&gt;ResNet作为新一代的模型霸主，其对模型构建的思想可谓又上了一个台阶。这里的ResNet我们参考的prototxt是&lt;a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt" data-editable="true" data-title="deep-residual-networks/ResNet-152-deploy.prototxt at master · KaimingHe/deep-residual-networks · GitHub" class=""&gt;https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最终结果&lt;/p&gt;&lt;p&gt;下面揭晓最终的实验结果，并附上当年论文中或者网络上给出的单模型的精度。如果数字有错误欢迎指出。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/75c149e547912931687a1f4b241a2c60.png" data-rawwidth="602" data-rawheight="154"&gt;我们一列一列来看，从模型层数来看，几年间模型的层数已经得到了爆炸式的增长，虽然GoogleNet的Inception Module和ResNet的Residual Module的网络层数都存在水分（GoogleNet官方宣称22层，ResNet官方宣称152层），但是总体上的趋势还是很明显的，那就是网络结构向着复杂的方向演变，层数也向着变深的方向演变。&lt;/p&gt;&lt;p&gt;对于Memory来说，除了GoogleNet（GoogleNet一般也是几个模型ensemble一起用），其他的模型的体量都比较大，在前向计算时所花费的存储还是很大的。&lt;/p&gt;&lt;p&gt;模型参数也是比较有意思的地方，实际上VGGNet的参数主要集中在全连接层上，而GoogleNet和ResNet在参数数量上并不算多，因为他们的层数实际上已经比较深，从层数的角度来看，模型的参数密度实际上是在减少的。&lt;/p&gt;&lt;p&gt;关于精度……这里就不细说了。&lt;/p&gt;&lt;p&gt;最后补充一句关于VGG的数据，上面的Memory计算的是1个batch所花费的内存，batch_size=256，想要对比上面的公式推演和代码计算的数字，需要把Memory的值除以batch_size。&lt;/p&gt;&lt;p&gt;好了，展示了这么多参数，实际上也是要说明CNN网络发展的趋势，那就是从Shallow and wide的模型转型成deep but thin的模型。模型的复杂程度不断增加，模型的拟合能力不断增强，但参数总量控制得很好，152层的ResNet和5层conv+3层fc的模型参数数量相近，其实这里面也说明了很多问题。&lt;/p&gt;&lt;p&gt;那么这些模型究竟是如何演化过来的呢？VGG的“模型哲学”，Inception Module的思想，ResNet对模型结构的颠覆都是如何影响我们对模型结构的三观呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22197188&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 29 Aug 2016 21:43:10 GMT</pubDate></item></channel></rss>