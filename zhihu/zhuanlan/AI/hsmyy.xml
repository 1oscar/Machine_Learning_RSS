<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Wed, 09 Nov 2016 09:15:58 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>CenterLoss——实战&amp;源码</title><link>https://zhuanlan.zhihu.com/p/23444100</link><description>本来要继续连载CRF的，但是最近看到了知乎上各位大神的论文解读——《A Discriminative Feature Learning Approach for Deep Face Recognition》,其中介绍了他们设计的一个用户提高类别的区分度的损失函数——Center Loss，并且有一个基于MNist数据库的小实验，于是本人就简单实践了一下。&lt;p&gt;本着其他大神已经深度解析了这篇文章的内容，所以我觉得我再凑热闹把那些理论的东西讲一遍意义不大，倒不如抡起袖子干点实事——把代码写出来跑一跑。在我完成实验前，已经看到github上有人完成了MXNet的center loss代码，所以作为Caffe派的革命小斗士，我得抓紧时间了。&lt;/p&gt;&lt;p&gt;不说废话，先用一个8拍把问题说明白：&lt;/p&gt;&lt;h2&gt;一个8拍的center loss&lt;/h2&gt;&lt;p&gt;1：同样是分类问题，我们之前只关注了待识别的图像应该属于哪个类别，但是并没有关心一个同样重要的问题：最终分类器的分界面区域内的空间是不是都应该属于这个类别？空间内这些长得很像的图像，它们的特征会不会其实差距有点大？&lt;/p&gt;&lt;p&gt;2：于是乎作者设计了一个网络，在倒数第二层全连接层输出了一个2维的特征向量，并以此进行进一步的分类。我们把MNist的Test集合数据通过最终训练好的模型进行预测，倒数第二层的样子是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bdbd57903c0eac1e73626c340bcd487e.png" data-rawwidth="889" data-rawheight="572"&gt;嗯，貌似和论文上的图像差不多啊……&lt;/p&gt;&lt;p&gt;3：于是乎，作者就觉得，怎么每个类别的特征都是长长的一条啊？那么实际上同一个类内部的差距还很大呢……而且，同一类别下两个图像的距离可能比不同类的距离还大，这种现象如果一直存在，那么在作者关注的人脸识别领域，会不会出现一个人脸和别人的脸太相似而被误刷啊……&lt;/p&gt;&lt;p&gt;4：不能这样子，于是作者设计了一个新的loss叫center loss。我们给每个label的数据定义一个center，大家要向center靠近，离得远的要受惩罚，于是center loss就出现了：&lt;/p&gt;&lt;equation&gt;CenterLoss=\frac{1}{2N}\sum_{i=1}^N|x_i-c|^2_2&lt;/equation&gt;&lt;p&gt;5：众人纷纷表示这个思路很好，但是这个c怎么定义呢？首先拍脑袋想到的就是在batch训练的时候不断地计算更新它，每一轮我们计算一下当前数据和center的距离，然后把这个距离以某种形式——就是梯度叠加到center上：&lt;/p&gt;&lt;equation&gt;\frac{\partial CenterLoss}{\partial x}=\frac{1}{N}\sum_{i=1}^N(c-x_i)&lt;/equation&gt;&lt;p&gt;6：吃瓜群众立刻表示：每个batch的数据并不算多，这样更新会不会容易center产生抖动？数值上的不稳定在优化中是大忌啊！于是作者简单粗暴地讲：那我们加个scale，让它不要太大：&lt;/p&gt;&lt;equation&gt;\Delta c = \frac{\alpha}{N} \sum_{i=1}^N(c-x_i)&lt;/equation&gt;&lt;p&gt;这个scale肯定是小于1的。&lt;/p&gt;&lt;p&gt;7：吃瓜群众满意了，吃瓜子的群众有发话了：现在你有两个loss了，我们该怎么平衡这两个loss之间的权重呢？作者心想：你这不是明知故问么……于是又加了一个超参数&lt;equation&gt;\lambda &lt;/equation&gt;，用于控制两个loss之间的比例。&lt;/p&gt;&lt;p&gt;反正多来个超参数无所谓，你们慢慢调去吧～&lt;/p&gt;&lt;p&gt;8：该定义的终于都结束了，我们加入新的loss，训练之后得到了这个结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d4e7f06eb2628d4c115d3a089b4d8e69.png" data-rawwidth="859" data-rawheight="604"&gt;于是掌声雷动，这个效果看着确实不错啊～&lt;/p&gt;&lt;h2&gt;一些细节&lt;/h2&gt;&lt;p&gt;由于倒数第二层的特征维度被缩减成了2，所以识别的精度肯定会受到些影响，不过这只是为了可视化的效果，所以在真正的实验中我们可以把这个数字调大。在我的实验中最终的Test Accuracy是0.9888。比正常LeNet的0.99稍低一点。&lt;/p&gt;&lt;p&gt;直接修改LeNet倒数第二层的维度会造成无法训练，所以论文中的LeNet++使用了6层卷积。对于MNist这样的小问题使用6层卷积也是没谁了，所以训练起来还是要费点时间的。&lt;/p&gt;&lt;p&gt;在我的实验中加入center loss后Test Accuracy实际上是下降了一点的。不过这点下降并不能说明center loss对这个问题起了反作用，后面还是需要尝试当倒数第二层的维度大于2时的情况。&lt;/p&gt;&lt;h2&gt;干货来了&lt;/h2&gt;&lt;p&gt;说了这么多废话，it's time to show me the code。链接：&lt;a href="https://github.com/hsmyy/CenterLoss_Caffe_Mnist" data-editable="true" data-title="GitHub - hsmyy/CenterLoss_Caffe_Mnist: It's the script of Center loss on mnist dataset running on Caffe."&gt;GitHub - hsmyy/CenterLoss_Caffe_Mnist: It's the script of Center loss on mnist dataset running on Caffe.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;由于是快速尝试，只实现了cpu的版本，而且写得比较粗糙，望各位大神不吝赐教。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"6群已经准备发车：337537549，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;337537549！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23444100&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sat, 05 Nov 2016 18:39:21 GMT</pubDate></item><item><title>FCN(2)——CRF通俗非严谨的入门</title><link>https://zhuanlan.zhihu.com/p/22464581</link><description>&lt;p&gt;前面我们简单介绍了FCN——这个将High-Level任务转到Low-Level任务的模型。这里的High和Low并不是我们通常意义中的High和Low，两种任务并没有高低之分，但是两种任务实际上需要的技术还是有所不同的。CNN模型从High-Level任务起家，直接将它们放到Low-Level的任务中还是有些“水土不服”，于是乎，大神们想出了用概率图模型来补充这些细粒度的任务。&lt;/p&gt;&lt;p&gt;由于在这个专栏中我们还没有介绍概率图模型的基本内容，这一篇我们简单介绍下概率图模型和CRF的基本概念，为后面的内容做铺垫。&lt;/p&gt;&lt;h2&gt;无向图模型&lt;/h2&gt;&lt;p&gt;想了解无向图模型，先要了解无向图的特点。无向图和有向图有什么区别呢？不用说，就是方向嘛。那么有方向会有什么好处呢？当然就是整个概率图中概率或者信念（belief）的流动性。在有向图模型中，每一个小部分可以看作是一个CPD，也就是Conditional Probablistic Distribution。这样的局部条件概率是很有用的，但是对于无向图来说，没有了方向也就丧失了这样的优势。&lt;/p&gt;&lt;p&gt;没有方向的无向图也就没法拥有CPD了，但是无向图模型还是有自己的办法。无向图模型一个个小部分被称作Factor，像CPD一样，Factor也可以表示成tabular的形式。也就是对于几个随机变量，我们随机变量的某个赋值会对应一个实数。但是factor有一个特点，那就是一个factor内容没有和为1的约束。&lt;/p&gt;&lt;p&gt;没有和为1的约束？Are you kidding？当然不是kidding。如果我们想求解概率还是有方法的，那就是把所有的Factor像有向图模型的贝叶斯网络那样都乘起来，再做一个归一化。我们就得到了总体的联合概率。得到了联合概率，就不用担心得到那些marginal probabilities和conditional probabilities了。这样无向图模型和有向图模型又走到同一起跑线。&lt;/p&gt;&lt;p&gt;那么问题又来了？为什么无向图模型不像有向图模型学习，也用CPD表示一个个的子部分，而要使用一个新东西呢？实际上有向图模型并不能够表示所有的真实场景，有向图模型通常需要一个有顺序的推断过程，其中的一些依赖关系和独立关系是有限制的，而无向图模型就没那么多限制了。所以说无向图模型可以对更多的问题进行建模。但是放弃了方向，也就意味着放弃了条件依赖和一些条件独立的特性，于是我们只能用Factor的形式进行表示。&lt;/p&gt;&lt;p&gt;当然Factor也有自己的好处，因为没有和为1的限制，所以整体上它的数值要求不是那么严格。但是它也有自己的坏处，那就是我们从Factor的Tabular形式中想读出一些有价值的信息是比较困难的。这个困难有两个方面：&lt;/p&gt;&lt;p&gt;首先，因为不具有和为1的限制，所以我们想计算联合概率就比较抽象。这个大家去看几个真实的Factor就能明白了。再看看贝叶斯网络的CPD，你就会感慨还是CPD写的清楚啊。&lt;/p&gt;&lt;p&gt;其次，Factor的Tabular中记述的一些关系和全局状态下的一些关系有时是相反的。我们具体看某个Factor时，会觉得这些随机变量更有可能产生某几个数值，但是如果我们站在全局观察，把联合概率计算出来再去计算marginal probability，就会发生局部的关系可能是错误的。而CPD在这方面具有优势，局部的概率放在全局还是合理的。&lt;/p&gt;&lt;p&gt;说实话前面的知识量还是有点大，但是上面的就是无向图模型的基础，总结起来就是这些。&lt;/p&gt;&lt;h2&gt;Gibbs Distribution&lt;/h2&gt;&lt;p&gt;Gibbs Distribution就是利用Factor表示的无向图模型的概率分布，Gibbs Distribution的表示形式如下所示：&lt;/p&gt;&lt;equation&gt;P(X_1,X_2,...X_n)=\frac{1}{Z(X)}\tilde{P(X_1,X_2,...X_n)}&lt;/equation&gt;&lt;equation&gt;\tilde{P(X_1,X_2,...X_n)}=\prod_{i=k}^m\phi_i(X)&lt;/equation&gt;&lt;equation&gt;Z(X)=\sum \prod \phi_i(X)&lt;/equation&gt;&lt;p&gt;这也就是利用无向图模型表示联合概率的方式。&lt;/p&gt;&lt;h2&gt;Log-Linear Model&lt;/h2&gt;&lt;p&gt;上面的Gibbs Distribution实际上已经可以用了，但是它并不是十分好用。为什么呢？因为每一个Factor实际上还是需要采用Tabular的形式进行表达，这对我们建模还是个不小的负担。所以我们需要将这个形式进行一定的转换。&lt;/p&gt;&lt;p&gt;我们重新定义Factor：&lt;/p&gt;&lt;equation&gt;\phi(X)=exp(-\xi(X))&lt;/equation&gt;&lt;equation&gt;\xi(X)=-log(\phi(X))&lt;/equation&gt;&lt;p&gt;我们把&lt;equation&gt;\phi(X)&lt;/equation&gt;称作Factor function，把&lt;equation&gt;\xi(X)&lt;/equation&gt;称作Energy Function。在物理学中，能量越大的物质存在的概率越小，这样也可以解释这个崭新登场的函数。&lt;/p&gt;&lt;p&gt;为什么要定义这个函数呢？我们知道Factor function中的每一项都需要是非负的，这个限制也会对我们的建模造成困扰，因此利用指数，我们的Energy Function拜托了非负数的限制，现在变得可正可负。&lt;/p&gt;&lt;p&gt;另外一个十分重要的特性，是我们把原来的乘法关系变成了加法关系。我们现在有&lt;/p&gt;&lt;equation&gt;\tilde{P(X_1,X_2,...X_n)}=exp(\sum_{i=k}^m\xi_i(X))&lt;/equation&gt;&lt;p&gt;变成加法关系对我们建模求解来说都是一个令人兴奋的事情，因为加法的关系更利于求导化简。当然，模型形式到了这一步还不够，我们还要做进一步的化简，那就是引入Feature这个概念。&lt;/p&gt;&lt;p&gt;我们知道Factor的一般形式是Tabular的形式，但是很多时候我们的Tabular实际上是比较稀疏的。虽然参与一个Factor的随机变量很多，但是真正有意义的关系其实没几个。所以我们希望放弃Tabular的形式，转而使用Feature的形式进行表示，说白了就是尽可能地合并相同结果的表示条件。这样的话Factor的表示就会简洁很多。&lt;/p&gt;&lt;p&gt;于是我们就完成线性模型的构建。&lt;/p&gt;&lt;p&gt;CRF&lt;/p&gt;&lt;p&gt;CRF的全称是Conditional Random Field。它的形式如下所示：&lt;/p&gt;&lt;equation&gt;P(Y|X)=\frac{1}{Z(X)}\tilde{P}(Y,X)&lt;/equation&gt;&lt;equation&gt;\tilde{P}(Y,X)=exp(\sum_i w_i * f_i(Y,X))&lt;/equation&gt;&lt;equation&gt;Z(X)=\sum_Y exp(\sum_i w_i * f_i(Y,X))&lt;/equation&gt;&lt;p&gt;可以看出，条件随机场在建模的时候同样需要计算联合概率，只不过这一次参与计算的有两部分随机变量——X和Y。一般来说，我们把X称作观察变量，也就是已知的变量；Y称作目标变量或者隐含变量，是我们想知道的变量。&lt;/p&gt;&lt;p&gt;比方说图像分割的问题，X就是图像的像素，Y就是每个像素所归属的类别。当然对于二维的图像问题还是有点复杂，那么我们用一个简单的一维问题做了例子：比方说自然语言处理中的词性标注问题，那么它的建模形式如下所示：&lt;/p&gt;&lt;equation&gt;\tilde{P}(Y,X)=exp(\sum_i f_1(X_i,Y_i) + f_2(Y_i,Y_{i+1}))&lt;/equation&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;如果你在读这篇文章之前并不了解CRF，那么我相信这篇文章并不能让你对CRF有很深的印象，但是多多少少会有点了解。总体来说，采用无向图模型建模的CRF具有很强的灵活性和适应性，但是计算起来却不那么容易。所有的inference必须从求解联合概率入手，而且还要计算normalization那一项。所以计算是无向图模型的难题，后面我们也会深入计算这个问题，试图解决难以计算这个问题。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464581&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 03 Nov 2016 23:12:56 GMT</pubDate></item><item><title>FCN(1)——从分类问题出发</title><link>https://zhuanlan.zhihu.com/p/22464571</link><description>前面我们介绍了许多利用CNN进行分类任务的模型，今天我们来看看用CNN做分割的模型。我们的主要内容来自这篇文章——《Fully Convolutional Networks for Semantic Segmentation》。&lt;h2&gt;图像分割&lt;/h2&gt;&lt;p&gt;首先我们来简单看看图像分割的任务是什么样的。所谓的图像分割就是将一幅图像中各个实体的边界确定下来，这样我们可以通过寻找这个实体的边界确定实体的位置。在自然场景中我们可以根据画面中的实体进行划分，在文档图片中，我们也可以找出每一个文字的边界。这些都可以算作是完成分割任务。&lt;/p&gt;&lt;p&gt;当然，上面的描述还不太具有可操作性，我们可以把问题做进一步地转化。我们可以把问题转换成一个分类问题，我们判定每一个像素点的类别，然后将相邻且类别相同的像素点聚集起来，不就可以找到实体的边界了么？这样也就是从分类问题出发解决分割问题的方式。&lt;/p&gt;&lt;p&gt;但是这样解决分割问题的方法还是存在一些问题：之前我们对于一张图像，最终只输出一个结果或者几个结果，结果的数量并不多；而现在我们输出的数量和像素的数量相同，那么我们还能够获得和分类问题近似的精确率么？这里面存在着一些需要解决的问题，比如：&lt;/p&gt;&lt;p&gt;在分类问题中，为了解决图像的transform invariance，我们会加入pooling层减小维度，这个被减小的纬度如何能够再次放大呢？&lt;/p&gt;&lt;h2&gt;FCN的解决方案&lt;/h2&gt;&lt;p&gt;FCN，也就是Fully Convolutional Network，是一个不包含全连接层的网络。这里面所谓的不包含全连接层，实际上并不是标榜自己没有全连接层，而是为了保证计算过程中每一层数字的相对位置。在我们通常的印象中，全连接层需要把本来立体的图像拍平，这样原本存在的空间特性将被抹掉。为了确保我们识别出来的类别能和原来每一个像素点的位置对上，我们不能粗暴地把中间数据拍平，这也是网络中不使用全连接层的原因。&lt;/p&gt;&lt;p&gt;我们可以利用常规的分类CNN网络得到一个接近最终结果的中间层，这样层的数据往往已经可以代表了一些具有特定含义的特征，而不再像原始的像素亮度那样含义晦涩。因此我们可以从这样的信息出发得到一些分类信息，然后把这些分类信息重新映射到原始图片大小的区域上。&lt;/p&gt;&lt;p&gt;由于中间层的维度比原始图像小，那么恢复到原始大小必然意味着一些插值的工作。一旦使用了插值的算法（比方说bilinear），那么恢复的图像的精度一定会出现问题。所以大神们也曾经担心过这个问题。于是他们也想出过一些其他的方法。这些方法的特点就一条——&lt;/p&gt;&lt;p&gt;维度不缩小！&lt;/p&gt;&lt;p&gt;实际上想做到这一步从算法上来说也并不困难，但是这样的方法会增加不少计算量，而且也存在自己的问题。最终大神们选择了反卷积的方式完成了信息由小变大的工作。&lt;/p&gt;&lt;p&gt;反卷积可以让维度由小变大，而且我们还可以通过学习其中的参数让这个变化的过程变得不那么简单粗暴。但是小维度毕竟是小维度，谁也没法回避这个问题。就算采用反卷积的方式把维度扩大，精度损失的问题依旧无法避免。所以我们还需要其他的办法。&lt;/p&gt;&lt;p&gt;这个办法就是融合。我们不仅仅使用较深层的特征信息，还使用一些较浅的特征信息。我们知道较浅层的特征容易保留一些细节信息，比方说边缘信息，较深的特征容易保留一些类别信息，那么如果我们把这两部分信息融合起来，我们既可以保证找到的类别信息是准确的，同时我们也可以保证一些边缘的位置信息能够找准。这样我们也就解决了这个问题。&lt;/p&gt;&lt;p&gt;文章中还提到了一个有意思的问题，那就是采样的问题。一般来说我们的分类问题都是一张图一套类别信息，而现在的分割问题变成了一张图有许许多多的类别信息。这么多类别信息一起学习，会不会出现过拟合的问题？最终的实验结果告诉了我们，过拟合的问题似乎并不存在，但是当初大家还是担心了一下，也曾经想过采用采样的方式解决这个问题。对于最终输出的每一个像素的类别信息，我们并不把所有的像素点的结果计算到loss中进行反向传播，而是只取其中一部分的像素点。这个想法是有点道理的，因为每一个紧密相邻的像素点之前的特征差距可能并不大，如果每一个像素点都计算在内，那么就相当于我们对某一组特征增加了很高的权重。但好在我们对所有像素点都增加权重的话，这个影响还是会抵消的。&lt;/p&gt;&lt;p&gt;最后提一下分割问题和分类问题在evaluation方面的不同。对于经典的分类问题，我们常用的loss是cross entropy loss，精确率判断则是最终的类别判断的正确率。而在分割问题中，我们除了判断每一个像素点的“分类”loss和精确率，我们还可以计算IoU。它的全称是Intersection of Union。我们有模型预测的边界和Ground Truth的边界。我们计算两个边界相交和它们相并的比例，也可以判断最终的分割效果。如果以IOU作为评价标准，那么只要主体部分能够分割正确，那么我们就可以拿到比较高的分数。所以添加过多浅层的信息可能不会对最终结果造成很大的准确率提升。&lt;/p&gt;&lt;h2&gt;未来&lt;/h2&gt;&lt;p&gt;从上面的内容中，我们基本上完成了对FCN的基本介绍。FCN是一个将High-level问题的模型框架应用到Low-level问题的成功案例。但是，从已经走过的历史长河来看，这个方法开辟一片崭新的领域，但是并没有走在这个领域的前沿。所以相关的应用在此我们也不赘述了，留给大家自己开发了。&lt;/p&gt;&lt;p&gt;曾经在图像分割问题中，我们经常使用概率图模型的方式进行建模求解，正如一些童鞋所知。后来的大神们尝试了深度CNN和CRF模型结合的手段进行尝试，并获得了非常不错的成绩。在后面的篇幅里我们就来看看两种方法是如何结合在一起的。&lt;/p&gt;&lt;p&gt;当然，我们花了很长的时间介绍了CNN模型的方方面面，所以一路看下来的各位对CNN已经比较熟悉，所谓没吃过猪肉也见过猪跑了。但是CRF的内容在此之前我们并未涉及到，因此想搞清楚CRF对于初学者还是有点压力，下面我们会尽可能地用最通俗的语言介绍下CRF。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464571&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 30 Oct 2016 21:03:29 GMT</pubDate></item><item><title>Caffe中的SGD的变种优化算法(2)</title><link>https://zhuanlan.zhihu.com/p/22464551</link><description>接上回，我们继续来看优化的算法。&lt;h2&gt;Adam&lt;/h2&gt;&lt;p&gt;关于adam的算法，我觉得它的复杂程度比前面的算法更高，但是它还是主要使用了和计算动量相同的方法，并把这种方法应用到梯度的一阶总量和二阶总量上：&lt;/p&gt;&lt;equation&gt;m_{t+1} = \beta_1 * m_t+(1-\beta_1)*g&lt;/equation&gt;&lt;equation&gt;v_{t+1}=\beta_2*v_t+(1-\beta_2)*g^2&lt;/equation&gt;&lt;p&gt;同时作者发现这两个变量存在某种偏差，于是又给这两个变量加上了一定的修正量：&lt;/p&gt;&lt;equation&gt;\hat{m_t}=\frac{m_t}{1-\beta_1^t}&lt;/equation&gt;&lt;equation&gt;\hat{v_t}=\frac{v_t}{1 - \beta_2^t}&lt;/equation&gt;&lt;p&gt;最后我们将这两个变量融合起来得到最终的更新公式：&lt;/p&gt;&lt;equation&gt;x_{t+1}= x_t - lr*\frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \varepsilon }*g_t&lt;/equation&gt;&lt;p&gt;它的代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def adam(x_start, step, g, beta1 = 0.9, beta2 = 0.999,delta=1e-8):
    x = np.array(x_start, dtype='float64')
    sum_m = np.zeros_like(x)
    sum_v = np.zeros_like(x)
    passing_dot = [x.copy()]
    for i in range(50):
        grad = g(x)
        sum_m = beta1 * sum_m + (1 - beta1) * grad
        sum_v = beta2 * sum_v + (1 - beta2) * grad * grad
        correction = np.sqrt(1 - beta2 ** i) / (1 - beta1 ** i)
        x -= step * sum_m / (np.sqrt(sum_v + delta))
        passing_dot.append(x.copy())
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x, passing_dot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;前面也说过，实际上这些算法背后都有些原理的，但是这些原理在我们看来可能都比较抽象，所以再我们看完实验再看原理可能会更有感觉。下面我们看看它的表现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adam([5, 5], 0.1, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5dd1a9d087cd51deb2bd426bc7f7eeb6.png" data-rawwidth="878" data-rawheight="420"&gt;好吧，这一回它是唯一一个走得有点过头的算法，不过最后还是走上了正道。&lt;/p&gt;&lt;p&gt;这样除了Nesterov算法之外，我们把其他所有的算法都展示出来了。前面这个实验中，我们希望检验算法的“拐弯”能力。也就是说一开始算法冲着一个局部最优点而来，什么时候它会发现这是个骗局并掉头朝向真正的最优值呢？现在每一个算法都给我们交出了答案。仔细看来每个算法的表示还是不太一样的。有的算法比较灵敏，刚发现不对就调头逃跑，有的算法则是冲过了头才转过身来。有的算法做到这一切十分容易，并不需要很大的Learning rate，有的算法则需要强大的力量才能转过来，否则就会行动缓慢。&lt;/p&gt;&lt;p&gt;好了，各位童鞋，转弯赛到此结束，下面我们进入下一个比赛——爬坡赛。爬坡赛的比赛规则是，所有算法使用同样的参数，从鞍点附近出发，经过50轮迭代，看看它能走到哪里。&lt;/p&gt;&lt;h2&gt;爬坡赛&lt;/h2&gt;&lt;p&gt;首先是我们的老朋友梯度下降法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = gd([-0.23,0.0], 0.0008, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1801c65667491bcfb980ab62cf2501ec.png" data-rawwidth="872" data-rawheight="424"&gt;我们的老朋友在爬坡赛看来是要叫白卷了，说明它虽然很轻盈，但是动力不是很足啊。&lt;/p&gt;&lt;p&gt;下面是动量算法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = momentum([-0.23,0], 5e-4, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0a8e5a430e33f21cd6a79334a496130e.png" data-rawwidth="888" data-rawheight="423"&gt;比梯度下降好一点，但是好的有限。&lt;/p&gt;&lt;p&gt;然后是Adagrad：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adagrad([-0.23, 0], 1.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-63aaf5fdc0c324ca90369c0dad1ab46b.png" data-rawwidth="894" data-rawheight="425"&gt;好吧，表现得比转弯赛还好，省去夸赞的词语了。&lt;/p&gt;&lt;p&gt;下面是Rmsprop:&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = rmsprop([-0.23, 0], 0.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-59f403485d1ef7a948e3bd37a5b01799.png" data-rawwidth="879" data-rawheight="425"&gt;同样完成的不错！&lt;/p&gt;&lt;p&gt;下面我们再看AdaDelta：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adadelta([-0.23, 0], 0.4, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c59bcdecca66aeaf2e68f46876a2ddfb.png" data-rawwidth="884" data-rawheight="418"&gt;好吧，直接一骑绝尘而去了。&lt;/p&gt;&lt;p&gt;最后是我们今天登场的Adam：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adam([-0.23, 0], 0.1, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea60a426346859e020dafd83cb3a311e.png" data-rawwidth="882" data-rawheight="422"&gt;同样是跑出了我们原始规划的区域。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;经过两轮的比赛——转弯赛和爬坡赛，我们测试了几种算法在同一配置上的实力。测试的目标不是为了比较哪个算法更厉害，而是展示另外一个实力——哪个算法表现更稳定。&lt;/p&gt;&lt;p&gt;因为我们在实际使用过程中，learning rate是相对固定的，在优化过程中我们无法预料自己将会碰上什么样的情景，那么我们肯定希望一个算法又能转弯又能爬坡。如果一个算法转弯厉害但是爬坡比较弱，那一旦遇上爬坡它就会走得很慢，反之也是如此。&lt;/p&gt;&lt;p&gt;所以从刚才的表现来看，我们的新算法似乎表现更为平稳，现在也确实有越来越多的人抛弃了一些经典的算法，转而投入这些新算法的怀抱，而这些新算法也足够争气，拿出了它们的实力。关于算法性能的测试一直在进行，这些算法还需要经历更多的考验！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464551&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Wed, 26 Oct 2016 22:06:55 GMT</pubDate></item><item><title>Caffe中的SGD的变种优化算法(1)</title><link>https://zhuanlan.zhihu.com/p/22464537</link><description>Caffe中还有一个很重要的部分，那就是优化算法。在专栏的一开始，我们就从优化入手，聊了两个优化算法——随机梯度下降和动量（感谢众多知友指出我之前翻译的错误……）。下面我们沿着前面的步伐，继续前进。&lt;h2&gt;写在前面&lt;/h2&gt;&lt;p&gt;说实话设计优化算法和解释优化算法的合理性实际上是一个不太容易的事情。这里我们主要关注算法的实际效果，因此我们对于算法的理论推导就暂时放在一边了。而关于理论，我们只简单讲两句，不做过多的深入，重点是看算法在我们设计的场景下的表现。&lt;/p&gt;&lt;h2&gt;非凸函数&lt;/h2&gt;&lt;p&gt;之前在我们的算法演示中，我们展示的函数都是凸函数。凸函数有哪些好处，谁来讲一讲？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7ef6696a76e045a9006bc453d2cd3069.png" data-rawwidth="410" data-rawheight="246"&gt;（侵删）&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它通常有唯一的极值点。&lt;/li&gt;&lt;li&gt;Hessian矩阵正定。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;而下面我们将展示一个函数，这个函数可没有这么多好的性质了：&lt;/p&gt;&lt;equation&gt;f(x,y)=x^2-2x+100xy+10y^2+20y&lt;/equation&gt;&lt;p&gt;在等高线图上，它的样子是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6b3c9147f2fb44ded15d916f0a892223.png" data-rawwidth="787" data-rawheight="389"&gt;这里的配色采用绘图中的经典彩虹色，其中颜色越靠近蓝色表示函数值越大，颜色越靠近红色表示函数值越小。这和我们的直觉是相符的，一三象限的数字大，二四象限的数字小。&lt;/p&gt;&lt;p&gt;我们的第一个尝试就是从一个比较正常的位置出发——（5，5）点，看看各种算法的效果&lt;/p&gt;&lt;p&gt;在这个新的环境下，我们的老算法——梯度下降，动量，Nesterov算法会不会焕发第二春呢？&lt;/p&gt;&lt;h2&gt;有请老队员上场&lt;/h2&gt;&lt;p&gt;首先是梯度下降法，由于前面我们已经贴过代码了，这里我们就不再贴代码了，直接上结果：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = gd([5,5], 0.0008, g)
contour(X,Y,Z, x_arr)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0a2e3624b3c4dd427eea4ec6cc67e612.png" data-rawwidth="787" data-rawheight="382"&gt;为了保证算法经过50轮迭代不会超出我们限定的范围，我们调整了它的learning rate。可以看出算法在迭代过程中先是冲向了局部最优点，也就是我们常说的鞍点，然后发现不对，调转头冲向了真正的优化点，祝它一路走好……&lt;/p&gt;&lt;p&gt;然后是动量算法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = momentum([5, 5], 3e-4, g)
contour(X,Y,Z, x_arr)&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-5a5a0b0945b0dbde773bd57c732f9520.png" data-rawwidth="788" data-rawheight="380"&gt;&lt;p&gt;一切顺利！不同的演员同样的剧本，演出来的效果还是不错的。&lt;/p&gt;&lt;p&gt;好了，到这里我们的老队员登场完毕，下面看看新队员了。&lt;/p&gt;&lt;h2&gt;Adagrad&lt;/h2&gt;&lt;p&gt;Adagrad是一种自适应的梯度下降方法，它希望根据梯度的更新量调节梯度实际的影响值。如果一个变量的梯度更新量比较大，那么再后面的更新过程中我们会适当减少它的更新量，如果更新量比较小，那么我们可以适当地增加它的更新量。&lt;/p&gt;&lt;p&gt;它的参数更新公式如下所示：&lt;/p&gt;&lt;equation&gt;x -= lr * \frac{g}{\sqrt{\sum{g^2}} + \varepsilon }&lt;/equation&gt;&lt;p&gt;它的核心代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def adagrad(x_start, step, g, delta=1e-8):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    for i in range(50):
        grad = g(x)
        sum_grad += grad * grad
        x -= step * grad / (np.sqrt(sum_grad) + delta)
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们直接来看看它的表现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adagrad([5, 5], 1.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8c10fca60f1f0ed10c2751afa410c228.png" data-rawwidth="789" data-rawheight="375"&gt;看上去和前面的算法没什么差别嘛……&lt;/p&gt;&lt;h2&gt;Rmsprop&lt;/h2&gt;&lt;p&gt;前面的Adagrad有一个很大的问题，那就是随着优化的进行，更新公式的分母项会变得越来越大。所以理论上更新量会越来越小，下面的算法Rmsprop就试图解决这个问题。在它的算法中，分母的梯度平方和不再随优化而递增，而是做加权平均：&lt;/p&gt;&lt;equation&gt;G_{t+1}=\beta*G_{t}+(1-\beta)*g^2&lt;/equation&gt;&lt;equation&gt;x_{t+1} = x_t - lr * \frac{g}{\sqrt{G_{t+1}} + \varepsilon }&lt;/equation&gt;&lt;p&gt;它的代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def rmsprop(x_start, step, g, rms_decay = 0.9, delta=1e-8):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    passing_dot = [x.copy()]
    for i in range(50):
        grad = g(x)
        sum_grad = rms_decay * sum_grad + (1 - rms_decay) * grad * grad
        x -= step * grad / (np.sqrt(sum_grad) + delta)
        passing_dot.append(x.copy())        
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x, passing_dot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们同样来看看它的表现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = rmsprop([5, 5], 0.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-21acefc27d381f2d5dc25150975e573e.png" data-rawwidth="788" data-rawheight="383"&gt;看上去和前面的表现还是差不多的。&lt;/p&gt;&lt;h2&gt;AdaDelta&lt;/h2&gt;&lt;p&gt;Rmsprop已经很好地解决了一部分的问题，但是在AdaDelta的眼中似乎还不够。在介绍它的论文中，作者详细阐述了Adagrad算法中得到的参数更新量的“单位”已经不太对了，于是给出了一堆公式用以证明我们需要增加一些计算来使得计算单位恢复正常，于是就有了下面的公式：&lt;/p&gt;&lt;equation&gt;G_{t+1}=\beta*G_{t}+(1-\beta)*g^2&lt;/equation&gt;&lt;equation&gt;\delta_{t+1}=\sqrt{\frac{\Delta_t+\varepsilon}{G_{t+1}+ \varepsilon}  }*g&lt;/equation&gt;&lt;equation&gt;x_{t+1} = x_{t} - lr * \delta_{t+1}&lt;/equation&gt;&lt;equation&gt;\Delta_{t+1}=\beta*\Delta_t+(1-\beta)*\delta_{t+1}&lt;/equation&gt;&lt;p&gt;具体的代码如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def adadelta(x_start, step, g, momentum = 0.9, delta=1e-1):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    sum_diff = np.zeros_like(x)
    passing_dot = [x.copy()]
    for i in range(50):
        grad = g(x)
        sum_grad = momentum * sum_grad + (1 - momentum) * grad * grad
        diff = np.sqrt((sum_diff + delta) / (sum_grad + delta)) * grad
        x -= step * diff
        sum_diff = momentum * sum_diff + (1 - momentum) * (diff * diff)
        passing_dot.append(x.copy())
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x, passing_dot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;它的表现如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3b87268bf774661e93d11f73340f2cae.png" data-rawwidth="784" data-rawheight="381"&gt;虽然上面算法的表现差不多，但是它们的learning rate还是有些不同的，感兴趣的童鞋可以换一些learning rate去尝试。&lt;/p&gt;&lt;p&gt;看上去我们并没有把算法讲完，也没有把实验做完……没错，留下尾巴，剩下的事情我们下回再说。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464537&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 20 Oct 2016 23:01:44 GMT</pubDate></item><item><title>VAE（4）——实现</title><link>https://zhuanlan.zhihu.com/p/22684931</link><description>终于到了实现的地方。前面干燥乏味的公式推导和理论阐述已经让很多人昏昏欲睡了，下面我们要提起精神，来看看这个模型的一个比较不错的实现——&lt;a href="https://github.com/cdoersch/vae_tutorial" data-editable="true" data-title="GitHub - cdoersch/vae_tutorial: Caffe code to accompany my Tutorial on Variational Autoencoders" class=""&gt;GitHub - cdoersch/vae_tutorial: Caffe code to accompany my Tutorial on Variational Autoencoders&lt;/a&gt;，当然，这个实现也是一个配套tutorial文章的实现。感兴趣的童鞋也可以看看这篇tutorial，相信会对这个模型有更多的启发。&lt;p&gt;这个实现的目标数据集是MNIST，这和我们之前的DCGAN是一样的。当然，在他的tutorial中，他一共展现了3个模型。下面我们就从prototxt文件出发，先来看看我们最熟悉的经典VAE。&lt;/p&gt;&lt;h2&gt;VAE&lt;/h2&gt;&lt;p&gt;说实话一看他在github给出的那张图，即使是有一定的VAE模型基础的童鞋也一定会感觉有些发懵。我们将模型中的一些细节隐去，只留下核心的数据流动和loss计算部分，那么这个模型就变成了下面的样子：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-29f6a96fe0f698f255db5bddb4b40fe8.jpg" data-rawwidth="960" data-rawheight="1280"&gt;图中的黑色的框表示数据的流动，红色的框表示求loss的地方。双红线表示两个不同部分的数据共享。可以看出图的上边是encoder的部分，也就是从X到z的过程，下面是从z到X的过程。前面的文章中我们给出了求解的公式，现在我们给出了这个网络模型，我们可以把这两部分对照起来。&lt;/p&gt;&lt;p&gt;另外其中的encoder和decoder部分被省略了，在实际网络中，我们可以用一个深度神经网络模型代替。除此之外，图中还有三个主要部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先是q(z|X)的loss计算。&lt;/li&gt;&lt;li&gt;其次是z的随机生成。&lt;/li&gt;&lt;li&gt;最后是p(X|z)的loss计算。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这其中最复杂的就是第一项，q(z|X)的loss计算。由于caffe在实际计算过程中主要采用向量的计算方式，所以前面的公式需要进行一定的变换：&lt;/p&gt;&lt;equation&gt;KL(p1(\mu_1,\sigma_1)||N(0,I))=\frac{1}{2}[-\sum_i{log [(\sigma_{1i})}] - d + \sum_i(\sigma_{1i})+\mu_1^T \mu_1]&lt;/equation&gt;&lt;equation&gt;=\sum_{i=0}^{d}{-\frac{1}{2}log[std_{1i}^2]}+\sum_{i=0}^{d}(-\frac{1}{2})+\sum_{i=0}^d\frac{1}{2}(std_{1i}^2)+\sum_{i=0}^d\frac{1}{2}[\mu_{1i}^2]&lt;/equation&gt;&lt;equation&gt;=\sum_{i=0}^{d}[{-\frac{1}{2}log[std_{1i}^2]}+\frac{1}{2}(std_{1i}^2)+\frac{1}{2}[\mu_{1i}^2]+(-\frac{1}{2})]&lt;/equation&gt;&lt;p&gt;在完成了前面的向量计算后，最后一步是做Reduction，也就是完成加和的过程。这样就使得计算可以顺利完成。&lt;/p&gt;&lt;p&gt;看懂了这些部分，再加上前面我们对VAE的了解，相信我们对VAE模型有了更加清晰的认识。&lt;/p&gt;&lt;h2&gt;MNIST生成模型可视化&lt;/h2&gt;&lt;p&gt;下面这张图是一次实验过程中产生的，看上去有点像所有数字在一个平面的分布，数字与数字之间还存在着一定的过渡区域。那么这张图是如何产生的呢？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d7b3c6a70547ba8010249337c000a1b4.jpg" data-rawwidth="532" data-rawheight="532"&gt;一个比较简单的方法，就是把z的维度设为2。以下就是这幅图生成的过程：&lt;ol&gt;&lt;li&gt;利用VAE模型进行训练，得到了模型中的&lt;equation&gt;\mu&lt;/equation&gt;和&lt;equation&gt;\sigma&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;完成z的采样过程，我们在二维空间内按照N(0,I)有规律地进行采样noise，把noise、&lt;equation&gt;\mu&lt;/equation&gt;和&lt;equation&gt;\sigma&lt;/equation&gt;结合起来&lt;/li&gt;&lt;li&gt;把得到采样后的z，最后利用decoder把z转换成X，显示出来&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;经过这样几步我们就可以得到最终的图像了。实际上我们前面提过的GAN模型也可以用类似的方法生成这样的图像。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22684931&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 17 Oct 2016 20:57:51 GMT</pubDate></item><item><title>VAE(3)——公式与实现</title><link>https://zhuanlan.zhihu.com/p/22464768</link><description>前面两部分我们已经扫除了一些基本概念上的障碍，下面我们来直奔主题——VAE！&lt;p&gt;由于文章是一篇一篇写的，所以照顾到大家观看的情况，我们把前面介绍过的一些重要公式搬过来。&lt;/p&gt;&lt;p&gt;首先是系列第一篇的公式——多维高斯分布的KL散度计算公式：&lt;/p&gt;&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;p&gt;希望大家还有印象，如果没有印象就赶紧翻回去看看吧！&lt;/p&gt;&lt;p&gt;然后是上一回有关variational inference的推导公式：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{q(z) log p(X|z)}dz-KL(q(z)||p(z))&lt;/equation&gt;&lt;p&gt;还有上次的一句话：&lt;/p&gt;&lt;blockquote&gt;“VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数”&lt;/blockquote&gt;&lt;p&gt;好吧……专栏写成我这样也是醉了。为了保证每一篇文章的字数不要太长以至于让大家失去了看下去的耐心，这篇文章光是回顾已经花去了好大的篇幅。&lt;/p&gt;&lt;p&gt;好了，下面就是见证奇迹的时刻。&lt;/p&gt;&lt;h2&gt;Variational Autoencoder&lt;/h2&gt;&lt;p&gt;终于要见到正主了。让我们关注一下variational inference公式的右边，在此之前我们要对公式进行一定的变化，然后给出我们的详细建模过程。&lt;/p&gt;&lt;h2&gt;Reparameterization Trick&lt;/h2&gt;&lt;p&gt;为了更加方便地求解上面的公式，这里我们需要做一点小小的trick工作。上面提到了Q'(z|X)这个变分函数，它代表了当我们给定某个X的情况下z的分布情况。我们可以想象这里的z是满足某种分布的。那么我们从数值上可以把X抽离出来呢？&lt;/p&gt;&lt;p&gt;比方说我们有一个随机变量a服从高斯分布N(1,1)，根据定理我们可以定义一个随机变量b=a-1，那么它将服从高斯分布N(0,1)，换句话说，我们可以用一个均值为0，方差为1的随机变量加上1来表示现在的随机变量a。这样我们就把一个随机变量分成了两部分——一部分是确定的，一部分是随机的。&lt;/p&gt;&lt;p&gt;对于上面的Q'(z|X)，我们同样可以采用上面的方法完成。我们可以把一个服从这个条件概率的z拆分成两部分，一部分是一个复杂的函数&lt;equation&gt;g_\phi(X)&lt;/equation&gt;，它解决了确定部分的问题，我们再定义另外一个随机变量&lt;equation&gt;\varepsilon &lt;/equation&gt;，它负责随机的部分。为了书写的一致性，我们用&lt;equation&gt;g_\phi(X+\varepsilon )&lt;/equation&gt;来表示服从条件概率的z。&lt;/p&gt;&lt;p&gt;这样做有什么好处呢？现在我们知道了z条件概率值完全取决于生成它所使用的&lt;equation&gt;\varepsilon &lt;/equation&gt;的概率。也就是说如果&lt;equation&gt;z^{(i)}=g_\phi(X+\varepsilon^{(i)} )&lt;/equation&gt;，那么&lt;equation&gt;q(z^{(i)})=p(\varepsilon ^{(i)})&lt;/equation&gt;，那么上面关于变分推导的公式也就变成了下面的公式：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{p(\varepsilon ) log p(X|g_{\phi}(X,\varepsilon )  )}dz-KL(q(z|X,\varepsilon  )||p(z))&lt;/equation&gt;&lt;p&gt;这就是替换的一小步，求解的一大步！实际上到了这里，我们已经接近问题最终的答案了，剩下的只是我们的临门一脚——我们可不可以假设这个随机部分服从什么样的分布呢？&lt;/p&gt;&lt;p&gt;当然能！不过由于我们一般把z的先验假设成一个多维的独立高斯分布，为了KL计算的方便，也为了我们在前面的章节推导2个多维高斯分布的KL散度这件事情没有白做，我们决定在这里让这个替换后的随机部分同样服从多维的独立高斯分布。&lt;/p&gt;&lt;p&gt;下面我们来看看这个公式的两部分具体该如何计算。&lt;/p&gt;&lt;h2&gt;右边的第二项，KL散度部分——encoder&lt;/h2&gt;&lt;p&gt;首先来看看公式右边的第二项。刚才我们提到我们一般把z的先验假设成一个多维的独立高斯分布，这里我们可以给出一个更强的假设，那就是这个高斯分布的均值为0，方差为单位矩阵，那么我们前面提到的KL散度公式就从：&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;瞬间简化成为：&lt;/p&gt;&lt;equation&gt;KL(p1||N(0,I))=\frac{1}{2}[-log [det(\Sigma_1)] - d + tr(\Sigma_1)+\mu_1^T \mu_1]&lt;/equation&gt;&lt;p&gt;真的有种世界清静了的感觉……我们下面的目标就是利用encoder的部分根据X求解z的均值方差。这部分我们采用一个深度的神经网络就可以了。由于实际训练过程中我们采用的是batch的训练方法，因此我们需要输入一个batch的X信息，然后进行模型的计算和优化。&lt;/p&gt;&lt;p&gt;如果我们用一个向量&lt;equation&gt;\sigma_1&lt;/equation&gt;来表示上面协方差矩阵的主对角线，情况将会更加美好：&lt;/p&gt;&lt;equation&gt;KL(p1(\mu_1,\sigma_1)||N(0,I))=\frac{1}{2}[-\sum_i{log [(\sigma_{1i})}] - d + \sum_i(\sigma_{1i})+\mu_1^T \mu_1]&lt;/equation&gt;&lt;p&gt;到这里，关于这一部分的函数拟合已经比较清晰了，我们的函数输入输出已经非常清楚，我们的loss也化简到了一个比较简单的状态，下面就是具体的计算了。&lt;/p&gt;&lt;h2&gt;右边的第一项，期望部分——decoder&lt;/h2&gt;&lt;p&gt;从前面的KL散度公式优化中，我们可以看到，如果两个概率分布的KL散度值为0时，实际上就说明我们的随机部分的分布和我们z的先验分布相同了。&lt;/p&gt;&lt;p&gt;这带来一个好消息，就是我们可以直接使用上一步encoder得到的均值方差。这样，我们就需要另外一个深度函数decoder，帮助我们从z再变回X。前面我们说了我们的目标是最大化似然的期望，实际上就可以转换为采样一批X，先用encoder生成z‘的分布，然后通过优化使得p(X|z)的似然最大化。&lt;/p&gt;&lt;p&gt;关于如何最大化似然，我们有很多办法，这一部分将在实践的环节详细给出。&lt;/p&gt;&lt;p&gt;好了，到这里，实际上VAE的核心计算推导就结束了。我们已经花了3篇文章的时间把这个模型讲完了，怎么可以就这样结束呢？下一回我们来看看一个实现的代码，同时来看看基于经典VAE演变的一些模型是什么样子。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464768&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 13 Oct 2016 21:08:26 GMT</pubDate></item><item><title>聊点轻松的4——这回真的很轻松</title><link>https://zhuanlan.zhihu.com/p/22842859</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea12f296f26f16a28cdbf343098e66f9_r.jpg"&gt;&lt;/p&gt;等了好久终于等到今天——专栏关注人数突破4000人了！其实这篇文章是我在突破4000之前就写好了，希望各位童鞋多多关注，多多点赞，给我更多创作的动力！第一次这么赤裸裸地求关注求赞！&lt;p&gt;好了，再次回顾一下，专栏已经发表了33篇文章（包括这篇），除去一篇开篇文章和4篇“轻松”文，还有28篇干货文章，我们从神经网络的基础聊起，到Caffe的源码阅读，到axvier初始化，网络结构，反卷积，GAN，VAE，我们已经聊了许多和CNN相关的话题，后面我们还会继续聊更多CNN和其他机器学习算法，欢迎大家持续关注。&lt;/p&gt;&lt;p&gt;这一回我们来看看我们前面提过的这些知识点的作者，以及这些作者现在都在哪里发光发热。古龙先生的小说里曾经写过百晓生写兵器谱，我们这里也来一次挥斥方遒，指点江山，看看这些大神们现在都在哪门哪派，看看现在的江湖局势是什么样的。&lt;/p&gt;&lt;h2&gt;Yann LeCun&lt;/h2&gt;&lt;p&gt;没错就是这个在之前被大家玩坏了的名字……我们来看看知友们挡不住的起名热情……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-aa45eb1daadc1155f6af954d8b0fef81.png" data-rawwidth="152" data-rawheight="486"&gt;但当我今天在google上搜索时，google的翻译还是让我震惊了一下……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1c51a5db44796446a2b24475dca1a852.png" data-rawwidth="453" data-rawheight="155"&gt;忽然就觉得还是知友们的名字好听啊……&lt;/p&gt;&lt;p&gt;扯蛋的话到此结束，这位大神的事迹够多够牛，可以说CNN的诞生和他有密不可分的关系，他创建了江湖上第一个响当当的CNN模型——LeNet！乐网！希望这个名字将来不会被乐视抢注……&lt;/p&gt;&lt;p&gt;其他的事迹这里我们就不多说了，最后说一句，LeCun大叔任职Facebook AI Director。Facebook加一分！&lt;/p&gt;&lt;h1&gt;Yoshua Bengio&lt;/h1&gt;&lt;p&gt;关于这位大叔，我们该起一个什么“艺名”呢？&lt;/p&gt;&lt;p&gt;作为江湖上鼎鼎大名的殿堂级教授，Yoshua为我们贡献许多高质量的文章。我们前面聊过的“xavier”初始化算法就有他的身影。多说一句，这位Xavier，也就是介绍这个算法论文的一作Xavier Glorot在Google DeepMind。&lt;/p&gt;&lt;p&gt;除此之外，每年他也为深度学习界贡献大量优质的文章，这里就不赘述了。&lt;/p&gt;&lt;h2&gt;Geoffrey Hinton&lt;/h2&gt;&lt;p&gt;这位大叔同样是江湖中的传说人物。虽然在专栏中我们还没有直接接触到与他相关的内容，但是大名鼎鼎的Alexnet——可是有他的份的。他现在在Google兼职作Distinguished Researcher。&lt;/p&gt;&lt;h2&gt;Karen Simonyan&lt;/h2&gt;&lt;p&gt;这位大神是VGG Net的一作，GoogleScholar显示他在Google DeepMind&lt;/p&gt;&lt;h3&gt;Andrew Zisserman&lt;/h3&gt;&lt;p&gt;VGG Net的另一位作者，他竟然还是多目视觉经典著作"&lt;a href="http://www.robots.ox.ac.uk/~vgg/hzbook/" data-editable="true" data-title="Multiple View Geometry in Computer Vision"&gt;Multiple View Geometry in Computer Vision&lt;/a&gt;"的作者啊……&lt;/p&gt;&lt;h2&gt;Christian Szegedy&lt;/h2&gt;&lt;p&gt;GoogleNet的一作，当然是在Google了……&lt;/p&gt;&lt;h2&gt;何恺明&lt;/h2&gt;&lt;p&gt;ResNet的一作，曾经在MSRA，现在转投Facebook AI Research了。这位大神在拿奖方面可以说是相当厉害，他在个人主页上也小炫耀了一下——&lt;/p&gt;&lt;blockquote&gt;I have received &lt;b&gt;2 CVPR Best Paper Awards&lt;/b&gt; as the first author (2009 and 2016)&lt;/blockquote&gt;&lt;p&gt;看到这句话就有种要送膝盖的感觉啊……&lt;/p&gt;&lt;h2&gt;Ian Goodfellow&lt;/h2&gt;&lt;p&gt;Generative Adversarial Networks的一作，现在在OpenAI。大神还是Maxout模型论文的一作，这里我们又看到了尾作大神Yoshua了……&lt;/p&gt;&lt;h2&gt;Diederik P. Kingma&lt;/h2&gt;&lt;p&gt;Variational Autoencoder的作者，现在在OpenAI。&lt;/p&gt;&lt;p&gt;好了，暂时先到这里。首先感谢各位大神，没有自己的辛勤的工作写出这么多高质量的paper，就没有我在后面费劲地读各位的大作在这里写科普小文章。希望各位大神文章不要停，不然我就没得写啦！&lt;/p&gt;&lt;p&gt;另外我们也看到一点，那就是这些大牛不是在学校任教，就是在一线大公司工作。果然还是大公司有钱任性，能够招揽各路高手前来效力。看起来现在深度学习的成果主要还是被这些大公司获取了啊啊啊啊！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;既然大家都关注了我的专栏，不如顺道关注下我呗：&lt;a href="https://www.zhihu.com/people/3ef5c095dad2c81afb977771cfae27a6" data-hash="3ef5c095dad2c81afb977771cfae27a6" class="member_mention" data-title="@冯超" data-hovercard="p$b$3ef5c095dad2c81afb977771cfae27a6" data-editable="true"&gt;@冯超&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22842859&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 11 Oct 2016 21:19:39 GMT</pubDate></item><item><title>VAE(2)——基本思想</title><link>https://zhuanlan.zhihu.com/p/22464764</link><description>上一回我们花了很大的篇幅介绍了KL散度，这一回我们来看看VAE，尤其是深度模型下的VAE。&lt;p&gt;前面我们已经见过了许多优秀的深度学习模型，它们达到了非常好的精度和效果。众人曾十分认真地分析过为什么这些模型的效果这么好，结论是深度模型的非线性拟合能力确实很强。不管曾经多么复杂的问题，一个深度模型出马，立刻把问题解决的八九不离十。VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数，从而解决实际问题。&lt;/p&gt;&lt;p&gt;让我们先记住这个trick，后面我们会用到它。接下来我们要上场的是生成模型。前面我们看过的很多模型从原理上来说都是判别式模型。我们有一个等待判别的事物X，这个事物有一个类别y，我们来建立一个模型f(x;w)，使得p(y|X)的概率尽可能地大，换种方法说就是让f(x;w)尽可能地接近y。&lt;/p&gt;&lt;p&gt;如果我们想用生成式的模型去解决这个问题，就需要利用贝叶斯公式把这个问题转换过来：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{p(X)}&lt;/equation&gt;&lt;p&gt;为了遵从大多数教科书上的变量用法，这里将y变成了z。当然，这个时候的z可能比上面提到的“类别”y要复杂一些。在很多的生成模型中，我们把z称作隐含变量，把X称作观测变量。一般来说，我们可以比较容易地观察到X，但是X背后的z却不那么容易见到，而很多时候X是由z构造出来的，比方说一天的天气好与坏是由很多不易观察的因素决定的。于是我们自然而然就有了一个需求，当我们拿到这些X之后，我们想知道背后的z是什么，于是乎就有了上面那个公式。&lt;/p&gt;&lt;p&gt;对于一些简单的问题，上面的公式还是比较容易解出的，比方说朴素贝叶斯模型，但是还是有很多模型是不易解出的，尤其当隐含变量处于一个高维度的连续空间中：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{\int_z{p(X|z)p(z)dz}}&lt;/equation&gt;&lt;p&gt;这里的积分就没那么容易搞定了。于是乎，各路大神开始想尽一切办法让上面的式子变得好解些。&lt;/p&gt;&lt;p&gt;这时候我们难免会冒出一个问题，既然有了判别式模型可以直接求解式子左边的那个东西，为什么非要把它变成右边那一大堆东西，搞得自己不方便解呢？其实谁都不想给自己找麻烦，可问题是右边的这一堆除了能够解这个问题，它还有一个更加高级的功能，就是根据模型随机生成X。&lt;/p&gt;&lt;p&gt;我们可以想想看，如果我们只拥有式子左边的p(z|X)，我们想要生成一个符合某种z的X该怎么办？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一步，随机一个X；&lt;/li&gt;&lt;li&gt;第二步，用p(z|X)计算概率，如果概率满足，则结束，如果不满足，返回第一步；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;于是乎，用判别式模型生成X变成了人品游戏，谁也不知道自己什么时候能在第二步通过。而生成式模型就不同了，我们可以按需定制，首先确定好z，然后根据p(X|z)进行随机采样就行了，生成X的过程安全可控。&lt;/p&gt;&lt;p&gt;说了这么多，下面我们正式进入公式推导的部分。&lt;/p&gt;&lt;h2&gt;Variational Inference&lt;/h2&gt;&lt;p&gt;虽然我们鼓吹了很多生成模型的好处，但是面对等号右边那一堆东西，该束手无策还是束手无策。但是，前辈们还是想到了一些精妙的解法。既然用概率论的方法很难求出右边的东西，我们能不能做一些变换，比方说——（略显生硬地）我们用一个variational的函数q(z)去代替p(z|X)？别着急，后面我们会看到它带来的好处的。&lt;/p&gt;&lt;p&gt;这里的variational inference介绍的有点简单，有机会我们再详细介绍下。&lt;/p&gt;&lt;p&gt;既然要用q(z)这个新东西去代替p(z|X)，那么我们当然希望两个东西尽可能地相近，于是乎我们选择了KL散度这个指标用来衡量两者的相近程度。由于两边都是可以看作针对z的概率分布，因此用KL散度这个指标实际上非常合适。&lt;/p&gt;&lt;p&gt;所以就有了：&lt;/p&gt;&lt;equation&gt;KL(q(z)||p(z|X))=\int{q(z)log \frac{q(z)}{p(z|X)}}dz&lt;/equation&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(z|X)]}dz&lt;/equation&gt;&lt;p&gt;我们做一下贝叶斯公式的变换，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z) + logp(X)]}dz&lt;/equation&gt;&lt;p&gt;再将和z无关的项目从积分符号中拿出来，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z)]}dz + log p(X)&lt;/equation&gt;&lt;p&gt;左右整理一下，就得到了：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{q(z) log p(X|z)}dz-KL(q(z)||p(z))&lt;/equation&gt;&lt;p&gt;好吧，其实整理了一圈，这个公式还是很乱，不过因为KL散度的特殊关系，我们还是从这个公式中看到了一丝曙光：&lt;/p&gt;&lt;p&gt;我们虽然不大容易求出p(X)，但我们知道当X给定的情况下，p(X)是个固定值。那么如果我们希望KL(q(z)||p(z|X))尽可能地小，也就相当于让等号右边的那部分尽可能地大。其中等号右边的第一项实际上是基于q(z)的似然期望，第二项又是一个负的KL散度，所以我们可以认为，为了找到一个好的q(z)，使得它和p(z|X)尽可能地相近，我们需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;右边第一项的log似然的期望最大化&lt;/li&gt;&lt;li&gt;右边第二项的KL散度最小化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于VAE之前的variation inference（中文可以翻译成变分推断），到这里我们就要开始一段全新的公式推导了。比方说我们做一个mean-field assumption（说实话我不太知道mean-field怎么翻译更直观，于是就把英文放在这里了），于是乎对于多个隐含变量组成的z，分量相互之间是独立的，于是根据这个特性，我们又可以进行进一步地公式化简。由于我们今天的主题是VAE，所以关于这部分我们就不再赘述了。这时候我们又想起了文章开头我们提到的一句话：&lt;/p&gt;&lt;blockquote&gt;“VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数”&lt;/blockquote&gt;&lt;p&gt;那么是时候让这句话发挥作用了，不过关于它发挥的方法我们下回再说。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464764&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 09 Oct 2016 19:09:29 GMT</pubDate></item><item><title>VAE（1）——从KL说起</title><link>https://zhuanlan.zhihu.com/p/22464760</link><description>前面我们介绍了GAN——Generative Adversarial Network，这个网络组是站在对抗博弈的角度去展现生成模型和判别模型各自的威力的，下面我们来看看这种生成模型和判别模型组合的另一个套路——Variational autoencoder，简称VAE。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-856cfcd1acf4b155373953460cd7adbd.png" data-rawwidth="374" data-rawheight="601"&gt;突然想起来，他也叫VAE，我觉得他还是有点音乐才华的。不过我们今天不去讨论他。&lt;/p&gt;&lt;p&gt;Variational autoencoder的概念相对复杂一些，它涉及到一些比较复杂的公式推导。在开始正式的推导之前，我们先来看看一个基础概念——KL divergence，翻译过来叫做KL散度。&lt;/p&gt;&lt;h2&gt;什么是KL散度&lt;/h2&gt;&lt;p&gt;无论从概率论的角度，还是从信息论的角度，我们都可以很好地给出KL散度测量的意义。这里不是基础的概念介绍，所以有关KL的概念就不介绍了。在Variational Inference中，我们希望能够找到一个相对简单好算的概率分布q，使它尽可能地近似我们待分析的后验概率p(z|x)，其中z是隐变量，x是显变量。在这里我们的“loss函数”就是KL散度，他可以很好地测量两个概率分布之间的距离。如果两个分布越接近，那么KL散度越小，如果越远，KL散度就会越大。&lt;/p&gt;&lt;p&gt;KL散度的公式为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\sum{p(x)log\frac{p(x)}{q(x)}}&lt;/equation&gt;，这个是离散概率分布的公式，&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\int{p(x)log{\frac{p(x)}{q(x)}}dx}&lt;/equation&gt;，这个是连续概率分布的公式。&lt;/p&gt;&lt;p&gt;关于其他KL散度的性质，这里就不赘述了。&lt;/p&gt;&lt;h2&gt;KL散度的实战——1维高斯分布&lt;/h2&gt;&lt;p&gt;我们先来一个相对简单的例子。假设我们有两个随机变量x1,x2，各自服从一个高斯分布&lt;equation&gt;N_1(\mu_1,\sigma_1^2),N_2(\mu_2,\sigma_2^2)&lt;/equation&gt;，那么这两个分布的KL散度该怎么计算呢？&lt;/p&gt;&lt;p&gt;我们知道&lt;/p&gt;&lt;equation&gt;N(\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{(x-\mu)^2}{2\sigma^2}}&lt;/equation&gt;&lt;p&gt;那么KL(p1,p2)就等于&lt;/p&gt;&lt;equation&gt;\int{p_1(x)log\frac{p_1(x)}{p_2(x)}}dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log{p_1(x)}}dx-log{p_2(x)})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)}*(log{\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{\frac{(x-\mu_1)^2}{2\sigma_1^2}}}-log{\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{\frac{(x-\mu_2)^2}{2\sigma_2^2}}})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)
*(-\frac{1}{2}log2\pi-log{\sigma_1}-\frac{(x-\mu_1)^2}{2\sigma_1^2}}+
\frac{1}{2}log{2\pi}+log{\sigma_2}+\frac{(x-\mu_2)^2}{2\sigma_2^2})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log\frac{\sigma_2}{\sigma_1}+[\frac{(x-\mu_2)^2}{2\sigma_2^2}-\frac{(x-\mu_1)^2}{2\sigma_1^2}])dx}&lt;/equation&gt;&lt;equation&gt;=\int(log\frac{\sigma_2}{\sigma_1})p_1(x)dx+\int(\frac{(x-\mu_2)^2}{2\sigma_2^2})p_1(x)dx-\int(\frac{(x-\mu_1)^2}{2\sigma_1^2})p_1(x)dx&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2\sigma_1^2}\int((x-\mu_1)^2)p_1(x)dx&lt;/equation&gt;&lt;p&gt;（更新）到这里停一下，有童鞋问这里右边最后一项的化简，这时候积分符号里面的东西是不看着很熟悉？没错，就是我们常见的方差嘛，于是括号内外一约分，就得到了最终的结果——&lt;equation&gt;\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;好，继续。&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_1+\mu_1-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+\int{(\mu_1-\mu_2)^2}p_1(x)dx+2\int{(x-\mu_1)(\mu_1-\mu_2)]}p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+(\mu_1-\mu_2)^2]-\frac{1}{2}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}&lt;/equation&gt;&lt;p&gt;说实话一直以来我不是很喜欢写这种大段推导公式的文章，一来原创性比较差（都是前人推过的，我就是大自然的搬运工），二来其中的逻辑性太强，容易让人看蒙。不过最终的结论还是得出来了，我们假设N2是一个正态分布，也就是说&lt;equation&gt;\mu_2=0,\sigma_2^2=1&lt;/equation&gt;那么N1长成什么样子能够让KL散度尽可能地小呢？&lt;/p&gt;&lt;p&gt;也就是说&lt;equation&gt;KL(\mu_1,\sigma_1)=-log\sigma_1+\frac{\sigma_1^2+\mu_1^2}{2}-\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;我们用“肉眼”看一下就能猜测到当&lt;equation&gt;\mu_1=0,\sigma_1=1&lt;/equation&gt;时，KL散度最小。从公式中可以看出，如果&lt;equation&gt;\mu_1&lt;/equation&gt;偏离了0，那么KL散度一定会变大。而方差的变化则有些不同：&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;大于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越大，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越小；&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;小于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越小，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越大；&lt;/p&gt;&lt;p&gt;那么哪边的力量更强大呢？我们可以作图出来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0.5,2,100)
y = -np.log(x)+x*x/2-0.5
plt.plot(x,y)
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从图中可以看出&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-68395494c66b37550efea74beca61d7e.png" data-rawwidth="559" data-rawheight="431"&gt;二次项的威力更大，函数一直保持为非负，这和我们前面提到的关于非负的定义是完全一致的。&lt;/p&gt;&lt;p&gt;好了，看完了这个简单的例子，下面让我们再看一个复杂的例子。&lt;/p&gt;&lt;h2&gt;一个更为复杂的例子：多维高斯分布的KL散度&lt;/h2&gt;&lt;p&gt;上一回我们看过了1维高斯分布间的KL散度计算，下面我们来看看多维高斯分布的KL散度是什么样子？说实话，这一次的公式将在后面介绍VAE时发挥很重要的作用！&lt;/p&gt;&lt;p&gt;首先给出多维高斯分布的公式：&lt;/p&gt;&lt;equation&gt;p(x_1,x_2,...x_n)=\frac{1}{\sqrt{2\pi*det(\Sigma)}}e^{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}&lt;/equation&gt;&lt;p&gt;由于这次是多维变量，里面的大多数计算都变成了向量、矩阵之间的计算。我们常用的是各维间相互独立的分布，因此协方差矩阵实际上是个对角阵。&lt;/p&gt;&lt;p&gt;考虑到篇幅以及实际情况，下面直接给出结果，让我们忽略哪些恶心的推导过程:&lt;/p&gt;&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;p&gt;其实这一次我们并没有介绍关于KL的意义和作用，只是生硬地、莫名其妙地推导一堆公式，不过别着急，下一回，我们展示VAE效果的时候，就会让大家看到KL散度的作用。&lt;/p&gt;&lt;p&gt;坚持看到这里的童鞋是有福的，来展示一下VAE的解码器在MNIST数据库上产生的字符生成效果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-c622c0419cbffbdb66485c1966afa68e.png" data-rawwidth="298" data-rawheight="298"&gt;从这个效果上来看，它的功能和GAN是有点像的，那么让我们来进一步揭开它的庐山真面目吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464760&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 06 Oct 2016 19:06:42 GMT</pubDate></item></channel></rss>