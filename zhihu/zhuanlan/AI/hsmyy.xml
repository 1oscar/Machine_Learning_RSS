<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Tue, 06 Sep 2016 17:15:52 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>CNN--结构上的思考</title><link>https://zhuanlan.zhihu.com/p/22214112</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;前面我们通过几个数值展示了几个比较经典的网络的一些特性，下面我们就花一点时间来仔细观察CNN网络的变化，首先是VGG在网络结构上的一些思考，其次是Inception Module对于单层网络内部的扩展，最后我们再来看看ResidualNet对于网络计算的改变。当然，我们在介绍这些模型的同时还会聊一些同时代其他的模型。&lt;h2&gt;VGG模型&lt;/h2&gt;&lt;p&gt;介绍VGG模型的文章中自夸了VGG模型的几个特点，下面我们来仔细说说，&lt;/p&gt;&lt;p&gt;首先是卷积核变小。实际上在VGG之前已经有一些模型开始尝试小卷积核了，VGG模型只是成功案例之中的一个。&lt;/p&gt;&lt;p&gt;那么小卷积核有什么好处呢？文章中提出了两个好处，首先是参数数量变少，过去一个7*7的卷积核需要49个参数，而现在3个3*3的卷积核有27个参数，看上去参数数量降低了不少；第二是非线性层的增加，过去7*7的卷积层只有1层非线性层与其相配，现在有3个3*3的卷积层有3个非线性层。非线性层的增加会使模型变得更加复杂，因此模型的表现力也有了提高。&lt;/p&gt;&lt;p&gt;同时在文章还提出了VGG的模型收敛速度比之前的AlexNet还要快些，从后来人的角度来看，参数训练的速度和本层参数的数量相关。之前我们分析过CNN模型参数的方差，我们假设对于某一层，这层的输入维度为&lt;equation&gt;N_l&lt;/equation&gt;，输出维度为&lt;equation&gt;N_{l+1}&lt;/equation&gt;那么该层网络中每个参数的方差应该控制在&lt;equation&gt;\frac{2}{N_l+N_{l+1}}&lt;/equation&gt;。如果输入输出层的维度比较大，那么参数的理想方差就需要限定的更小，所以参数可以取值的范围就比较小，那么优化起来就比较费劲；如果输入输出维度比较小，那么每个参数的理想方差就会相对大一些，那么可以取值的范围就比较大，优化起来就相对容易些。从这个角度来看，减小每一层参数的数量对于优化来说是有意义的。&lt;/p&gt;&lt;p&gt;其次就是卷积层参数的规律。首先卷积层的操作不会改变输入数据的维度，这里的维度主要指feature map的长和宽。对于3*3的kernel，卷积层都会配一个大小为1的pad。同时stride被设为1。这样经过卷积层变换，长宽没有发生变化。这和之前的卷积层设计有些不同。而且每做一次pooling，feature map的长宽各缩小一倍，channel层就会增加一倍。这样的设计对于不同的feature map维度来说适配起来都比较容易。对于一些通过卷积减小维度的模型来说，对于不同的输入，卷积后的输出各不一样，所以适配起来有可能不太方便，而现在只有pooling层改变长宽维度，整体模型的维度计算就方便了许多。于是在论文中有输入为256和384等维度，模型不需要根据不同的输入维度设计不同的卷积结构，使用同样的结构或者直接加深网络深度就可以了。&lt;/p&gt;&lt;p&gt;此外，模型也提到了1*1的卷积核，这个卷积核我们在后面还会提到。这种卷积核也不会改变feature map的长宽，同时又可以进一步地增加模型的非线性层，也就增加了模型的表现能力。&lt;/p&gt;&lt;p&gt;上面就是VGGNet在架构上做的这些改变，这些改变也被后面一些的模型所接纳。&lt;/p&gt;&lt;h2&gt;丰富模型层的内部结构&lt;/h2&gt;&lt;p&gt;提到模型的内部结构，我们就来到了GoogLeNet模型（这个英文单词是在致敬LeNet？），模型中最核心的地方就是它的Inception Module。在此之前还有一个研究模型层内部结构的文章，叫做Network In Network，其中的道理也比较相似。&lt;/p&gt;&lt;p&gt;Network in Network和Inception Module这类结构主要看中的是模型在局部的拟合能力。有些模型在结构上是采用“一字长蛇阵”的方法，对于某一个特定的尺度，模型只采用一个特定尺度的卷积核进行处理，而上面两种模型却认为，采用一种尺度处理可能不太够，一张图象通常具有总体特征特征和细节特征这两类特征，我们用小卷积核能够更好地捕捉一些细节特征，而随着小卷积不断地卷下去，慢慢地一些总体特征也就被发现。&lt;/p&gt;&lt;p&gt;可是这里有一个问题，那就是我们在网络前段只有细节特征，后段才慢慢有一些总体特征，而有时候我们想让两方面的特征汇集在一起，同时出现发挥作用。那么采用单一的卷积核恐怕不太容易解决这样的问题。&lt;/p&gt;&lt;p&gt;于是上面两种模型开始考虑，与其把模型加深，不如把模型加厚（其实深度差不多），每一次feature map尺度的变化前后，我都尽可能地多做分析，把想得到的不同来源的信息都尽可能得到，这样的特征应该会更有价值吧！&lt;/p&gt;&lt;h2&gt;从乘法模型到加法模型&lt;/h2&gt;&lt;p&gt;ResNet的核心思路就是把曾经CNN模型中的乘法关系转变成加法关系,让模型有了点“Additive”的味道。关于这个问题，文章中采用一个极端的例子作说明。&lt;/p&gt;&lt;p&gt;假设我们已经有了一个较浅模型，我们的目标是去训练一个更深的模型。理论上如果我们能够找到一个靠谱的优化算法和足够的数据，那么这个更深的模型理论上应该比那个较浅的模型具有更好的表达能力。如果抛开优化和可能的过拟合问题不管，这个道理还是可以成立的。&lt;/p&gt;&lt;p&gt;就算较深的模型不能够超越较浅的模型，至少它是可以作到和具有较浅的模型同样的表达能力。如果我们把较深模型分成两部分——和较浅模型相同的部分，比较浅模型多出来的部分，那么我们保持和较浅模型相同的部分的参数完全相同，同时让多出来的模型部分“失效”，只原样传递数据而不做任何处理，那么较深模型就和较浅的模型完全一样了。在论文中，这些多出来的模型部分变成了“Identity Mapping”，也就是输入和输出完全一样。&lt;/p&gt;&lt;p&gt;好了，那么对于现在的架构来说，我们如何学习这些“Identity Mapping”呢？过去的学习方法就是按现在的乘法模式进行学习，我们一般的CNN模型都是一层套一层，层与层之间的关系是乘法，下一层的输出是上一层输入和卷积相乘得到的。学习这样的“Identity Mapping”还是有一点困难的，因为只要是想学到一个具体数值，它就具有一定的难度，不论是“Identity Mapping”还是其他。&lt;/p&gt;&lt;p&gt;于是，ResNet对上面的问题做了一些改变。既然是要学习“Identity Mapping”，那么我们能不能把过去的乘法转变为加法？我们假设多出来的层的函数形式是F(x)，那么乘法关系学习“Identity Mapping”就变成了&lt;equation&gt;F(x)=\sum{wx}=x&lt;/equation&gt;，由于学习的形式没有变，对于乘法我们学习起来同过去一样，但是对于加法就简单多了——&lt;equation&gt;F(x)=x+w&lt;/equation&gt;，只要将参数学习成0就可以了，0和其他数值相比具有很大的优势，这样训练难度就大大降低了。于是，我们也见到即使非常深的网络也可以训练，这也验证了将乘法关系改为加法关系后对模型训练带来的显著提升。&lt;/p&gt;&lt;p&gt;在ResNet之前，还有一些网络已经提出了类似的思想，比如Highway-Network。Highway-Network同样具有加法的特点，但是它并不是一个纯粹的加法，所以在优化过程总较ResNet弱一些。&lt;/p&gt;&lt;p&gt;这样我们就回顾完了上次我们提到的几个模型中的闪光点，如果想进一步地研究这些模型以及模型结构中的精妙之处，多多做实验多多分析数据才是王道。&lt;/p&gt;&lt;h2&gt;最后一点&lt;/h2&gt;&lt;p&gt;为什么GoogLeNet和ResNet的层数很深且参数很少？因为他们的全连接层比较少。为什么呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 03 Sep 2016 08:54:49 GMT</pubDate></item><item><title>CNN——架构上的一些数字</title><link>https://zhuanlan.zhihu.com/p/22197188</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/08cd1d56513335b13d6a93725db969ad" data-hash="08cd1d56513335b13d6a93725db969ad" class="member_mention" data-hovercard="p$b$08cd1d56513335b13d6a93725db969ad"&gt;@夏龙&lt;/a&gt;对本文的审阅。&lt;/p&gt;&lt;p&gt;前面说了很多关于CNN的数值上的事，下面我们来看看网络架构。网络架构也是CNN的一个核心部分，由于CNN的特点是它的深度，所以深度模型的网络架构给了人们无数的想象，于是也有了无数的前辈创造了各种各样的模型。我们今天来看看那些经典的模型，不是从感性的角度上去观看，而是从理性的角度——去尝试计算一些具体的数字，让我们描绘出这些模型的一个简单轮廓。&lt;/p&gt;&lt;p&gt;我们的目标问题是ImageNet分类问题，那么我们主要关注哪些问题呢？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;模型的深度，模型的核心层（卷积层、全连接层）的数量，这代表了模型的某种“能力”，基本上大家都有一个共识，那忽略优化问题的情况下，就是越深的模型在函数拟合方面效果越好。这里直接利用Caffe计算其中的layers_.size()，由于其中还包括data layer和loss layer，所以统计数会比实际的层数要多。&lt;/li&gt;&lt;li&gt;每层模型的参数数量，参数的总量，这代表了模型的复杂度。从机器学习的理论上讲，参数越多，模型的表达能力理论上也会“越强”。这里通过Caffe计算所有learnable_params的count总和表示。&lt;/li&gt;&lt;li&gt;模型前向的所需的内存量。也就是Caffe中计算的memory_used_变量值。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;AlexNet&lt;/h2&gt;&lt;p&gt;本文不是负责介绍历史的，所以不会花什么篇幅去聊故事。模型的prototxt来自：&lt;a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt" data-editable="true" data-title="caffe/train_val.prototxt at master · BVLC/caffe · GitHub" class=""&gt;https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;VGGNet&lt;/h2&gt;&lt;p&gt;VGGNet也是一个比较有代表性的网络，关于这个网络的“哲学”我们后面再开新贴去聊。利用论文和各处得到的信息，我们可以详细给出VGG19层模型的具体结构，参考的prototxt来自：&lt;a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md" data-editable="true" data-title="github.com 的页面" class=""&gt;https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="CS231n Convolutional Neural Networks for Visual Recognition" class=""&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;对VGG模型的内存占用量和参数数量做过一个计算，仅作参考：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见在计算过程中偏置项并没有被计算在其中。我们也要做一个详细的计算。&lt;/p&gt;&lt;h2&gt;GoogleNet&lt;/h2&gt;&lt;p&gt;GoogleNet作为Inception module的代表，同样取得了不错的成绩，我们的参考prototxt来自：&lt;a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt" data-editable="true" data-title="caffe/train_val.prototxt at master · BVLC/caffe · GitHub" class=""&gt;https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;ResNet&lt;/h2&gt;&lt;p&gt;ResNet作为新一代的模型霸主，其对模型构建的思想可谓又上了一个台阶。这里的ResNet我们参考的prototxt是&lt;a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt" data-editable="true" data-title="deep-residual-networks/ResNet-152-deploy.prototxt at master · KaimingHe/deep-residual-networks · GitHub" class=""&gt;https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最终结果&lt;/p&gt;&lt;p&gt;下面揭晓最终的实验结果，并附上当年论文中或者网络上给出的单模型的精度。如果数字有错误欢迎指出。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/75c149e547912931687a1f4b241a2c60.png" data-rawwidth="602" data-rawheight="154"&gt;我们一列一列来看，从模型层数来看，几年间模型的层数已经得到了爆炸式的增长，虽然GoogleNet的Inception Module和ResNet的Residual Module的网络层数都存在水分（GoogleNet官方宣称22层，ResNet官方宣称152层），但是总体上的趋势还是很明显的，那就是网络结构向着复杂的方向演变，层数也向着变深的方向演变。&lt;/p&gt;&lt;p&gt;对于Memory来说，除了GoogleNet（GoogleNet一般也是几个模型ensemble一起用），其他的模型的体量都比较大，在前向计算时所花费的存储还是很大的。&lt;/p&gt;&lt;p&gt;模型参数也是比较有意思的地方，实际上VGGNet的参数主要集中在全连接层上，而GoogleNet和ResNet在参数数量上并不算多，因为他们的层数实际上已经比较深，从层数的角度来看，模型的参数密度实际上是在减少的。&lt;/p&gt;&lt;p&gt;关于精度……这里就不细说了。&lt;/p&gt;&lt;p&gt;最后补充一句关于VGG的数据，上面的Memory计算的是1个batch所花费的内存，batch_size=256，想要对比上面的公式推演和代码计算的数字，需要把Memory的值除以batch_size。&lt;/p&gt;&lt;p&gt;好了，展示了这么多参数，实际上也是要说明CNN网络发展的趋势，那就是从Shallow and wide的模型转型成deep but thin的模型。模型的复杂程度不断增加，模型的拟合能力不断增强，但参数总量控制得很好，152层的ResNet和5层conv+3层fc的模型参数数量相近，其实这里面也说明了很多问题。&lt;/p&gt;&lt;p&gt;那么这些模型究竟是如何演化过来的呢？VGG的“模型哲学”，Inception Module的思想，ResNet对模型结构的颠覆都是如何影响我们对模型结构的三观呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;</description><author>冯超</author><pubDate>Mon, 29 Aug 2016 21:43:10 GMT</pubDate></item><item><title>CNN数值——ZCA</title><link>https://zhuanlan.zhihu.com/p/22148777</link><description>前面我们已经讲了很多有关参数合并的事情，反倒忘了介绍有关输入数据的事情，下面就来介绍一下对输入数据的初始化算法。&lt;p&gt;在Caffe的网络描述中，data layer的配置中有一项是用于配置mean_file，也就是数据的平均数值，在计算中每个数据在进入网络计算前会减去mean_file，以确保数据的整体均值为0，这样对于训练数据会更有帮助。&lt;/p&gt;&lt;p&gt;那么除了减去均值之外，还有什么初始化的方法呢？ZCA就是其中比较经典的初始化算法之一。&lt;/p&gt;&lt;h2&gt;Zero Component Analysis&lt;/h2&gt;&lt;p&gt;我们用一个例子来讲述这个初始化算法的过程。首先我们利用随机算法生成一个数据集。为了节目效果我们的数据集只有2维，而且两个维度之间还有相关关系。生成数据的代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;x = np.random.randn(200)
y = x * 2
err = np.random.rand(200) * 2
y += err
data = np.vstack((x,y))
plt.scatter(data[0,:], data[1,:])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的数据绘图的结果如下所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/69f28212ae5b64fbc3ca12fb9b1795d9.png" data-rawwidth="373" data-rawheight="255"&gt;我们可以求出两个特征的均值，再让全体数据减去均值，使得整体数据的均值为0：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;mean = np.mean(data, axis=1)
data -= mean.reshape((mean.shape[0],1))
plt.scatter(data[0,:], data[1,:])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ab025e16e33d313abacaeaf3b85c1741.png" data-rawwidth="375" data-rawheight="257"&gt;下面才是ZCA的关键部分。我们都知道训练数据中有时会出现特征之间相互关联的问题，对于图像数据，相互关联的问题则更为严重。虽然卷积层可以通过学习来解决这些局部相关性，但是通过学习来得到总是不够直接，如果直接对输入数据进行操作来解决一些数据相关性问题，一定会让训练更容易些。&lt;/p&gt;&lt;p&gt;为了解决数据相关问题，我们希望不同特征之间的协方差能够控制在一定范围内，我们首先来计算一下上面数据的协方差，由于我们的数据已经减去了均值，那么现在数据的均值为0，计算协方差就简化成了如下的计算：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;conv = np.dot(data, data.T) / (data.shape[1] - 1)
print conv
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[ 0.88200859  1.80316947]
 [ 1.80316947  4.033871  ]]&lt;/code&gt;&lt;/pre&gt;可以看出协方差是存在的……当然，我们在构造数据的时候就已经设置了这种相关性，所以看到这样的结果并不奇怪。下面我们就要用ZCA的方法减小协方差，我们的目标是：&lt;blockquote&gt;让每个特征自身的方差变为1，让特征之间的协方差变为0。&lt;/blockquote&gt;&lt;p&gt;为了达到这个效果，我们可以给每个输入数据做一次线性变换，使最终的结果满足我们所设定的效果。那么就有：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;# 如果有数据矩阵x，那么我们要寻找一个线性变换矩阵W，满足
Y = np.dot(W, X)
# 且
np.dot(Y, Y.T) / (Y.shape[1] - 1) == np.eye(Y.shape[0])&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了达到这个目标，我们首先做如下的假设&lt;/p&gt;&lt;p&gt;线性变换矩阵W是一个对称矩阵：&lt;equation&gt;W=W^T&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;我们的目标是令&lt;equation&gt;YY^T=(n-1)I&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是有：&lt;/p&gt;&lt;equation&gt;WXX^TW^T=(n-1)I&lt;/equation&gt;&lt;equation&gt;W^TWXX^TW^T=(n-1)W^T&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;W^2XX^TW^T=(n-1)W^T&lt;/equation&gt;，同时去掉左右两式右边的&lt;equation&gt;W^T&lt;/equation&gt;，有&lt;/p&gt;&lt;equation&gt;W^2XX^T=(n-1)I&lt;/equation&gt;,我们假设X不全为0，那么&lt;equation&gt;XX^T&lt;/equation&gt;就是一个正定矩阵，满足可逆性（这里也需要些推导），所以：&lt;equation&gt;W^2=(n-1)(XX^T)^{-1}&lt;/equation&gt;，&lt;equation&gt;W=\sqrt{n-1}(XX^T)^{-1/2}&lt;/equation&gt;&lt;p&gt;由于&lt;equation&gt;XX^T&lt;/equation&gt;是一个对称矩阵，对称矩阵具有一个特性。我们先求出&lt;equation&gt;XX^T&lt;/equation&gt;的特征值和特征向量：&lt;/p&gt;&lt;equation&gt;XX^TS=S\Lambda &lt;/equation&gt;&lt;p&gt;对称矩阵具有一个特性，它的特征向量可以构成一个标准正交矩阵，根据标准正交矩阵的特性，于是我们可以得到：&lt;/p&gt;&lt;equation&gt;XX^T=S\Lambda S^T&lt;/equation&gt;（关于这个定理，我们可以等后续进行证明，在此先直接使用）&lt;p&gt;继续推导，可以得到：&lt;/p&gt;&lt;equation&gt;(XX^T)^{-1/2}=(S\Lambda S)^{-1/2}=S\Lambda^{-1/2} S&lt;/equation&gt;（这里其实也稍有跳跃，再后续证明）&lt;p&gt;于是最终的&lt;equation&gt;W=\sqrt{n-1}S\Lambda^{-1/2} S&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;实际上上面的推导还缺少一些细节，比如对称矩阵相关的一些性质，对于这一部分的详细无脑推导我们之后可以再详细叙述。以下是根据定义对应的代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;# 由于conv中已经除掉了1/(Y.shape[1] - 1),所以后面的计算中我们将不再去除它
eig_val, eig_vec = np.linalg.eig(conv)
S_sqrt = np.sqrt(np.diag(eig_val))
W = np.dot(eig_vec, np.dot(np.linalg.inv(S_sqrt), eig_vec.T))
print W
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果得到&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[ 3.37642486 -1.32714676]
 [-1.32714676  1.05662959]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在我们得到了W，就可以进行线性变换，可以得到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Y = np.dot(W, data)
plt.scatter(Y[0,:], Y[1,:])
conv2 = np.dot(Y, Y.T) / (data.shape[1] - 1)
print conv2
&lt;/code&gt;&lt;/pre&gt;相对应的结果为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/fb14365d656ababf37caeda147b0f15c.png" data-rawwidth="369" data-rawheight="252"&gt;&lt;p&gt;此时对应的协方差为&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[  1.00000000e+00  -1.29433036e-16]
 [ -1.29433036e-16   1.00000000e+00]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;无论从图像结果还是协方差的数值结果上看，ZCA都完成了我们想要的目标。以上的ZCA算法推导来自论文《Learning Multiple Layers of Features from Tiny Images》的附录。&lt;/p&gt;&lt;p&gt;ZCA初始化在一些经典的数据集（比方说cifar10）已经得到验证，采用这样的初始化可以得到更好地训练精度。不妨动手一试吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;</description><author>冯超</author><pubDate>Fri, 26 Aug 2016 23:48:40 GMT</pubDate></item><item><title>CNN——L1正则的稀疏性</title><link>https://zhuanlan.zhihu.com/p/22099871</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;这一回我们把目光转向正则化。Caffe中提供了两种正则化：L2和L1，也是大家最耳熟能详的正则化算法了。从刚开始学习机器学习开始，有关经验风险和结构风险，bias and variance，过拟合等一系列的概念都和正则化有着密切的关系。&lt;p&gt;在当年经典的机器学习课本中，大师们经常用这样一张图来教导我们：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/045228bde397817c9b2f1588cdcbd32a.png" data-rawwidth="623" data-rawheight="367"&gt;（图片来自网络，侵删）&lt;/p&gt;&lt;p&gt;从上面这张图可以看出，损失函数的主体是一个凸函数，它的等高线均匀地向外扩散。因此正方形的L1正则更容达到参数的稀疏性，而圆形的L2正则则不太容易达到这个效果。所以多年以来，大家一直在心中默默记住：L1可以达到参数稀疏化的效果。而且，在浅层模型中，大多数情况下L1的稀疏效果还是不错的。当然，理论上依然存在使用L1正则却无法达到稀疏效果的情况，这个比较特殊就不细说了。&lt;/p&gt;&lt;p&gt;当然，如果要详细讲述L1正则相关的故事，恐怕相当于又打开了一个新天地。恐怕其中所涉及到的各位大师前辈的精妙算法又是数不胜数。关于这些话题有时间我们慢慢再聊，现在我们来看看L1在CNN上的表现。&lt;/p&gt;&lt;h2&gt;MNIST的L1正则实验&lt;/h2&gt;&lt;p&gt;我们这一次同样采用MNIST作为实验。训练数据和测试数据一切正常，所采用的模型也是我们之前所提到的那个正常的模型，非线性部分采用ReLU。唯一不同的是我们讲正则化的方法改为L1。我们依然使用Caffe进行训练，经过10000轮的训练，我们得到了如下的精确率：&lt;/p&gt;&lt;p&gt;0.9766&lt;/p&gt;&lt;p&gt;看上去比L2正则在10000轮训练后的精度0.9912要差一些啊，宝宝不开心。&lt;/p&gt;&lt;p&gt;既然精度已经差一些了，那么我们看看L1的正则在稀疏性上的表现。这时候我们要拿L2正则训练出的参数和L1正则训练的参数进行比较，比较的方式也是直接利用参数的数值去看他们的数据分布情况：&lt;/p&gt;&lt;p&gt;首先是L1正则的两张图，我们只关注极小的某个区间下的参数直方图，首先是conv1层的参数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d9e22ba7abae62b92e0986f3d9d210c5.png" data-rawwidth="769" data-rawheight="328"&gt;可以看出在1e-7的范围下，没有参数等于0。然后是conv2层：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e968f88f617be05f808e2ca969a9c9e6.png" data-rawwidth="764" data-rawheight="322"&gt;在1e-9的范围下，只有一个参数等于0。然后是L2正则的参数结果，首先是conv1层：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a813f692162db3f893f7dbf539fb50e7.png" data-rawwidth="775" data-rawheight="310"&gt;&lt;p&gt;然后是conv2层：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0e42cbabf70bc52881422d71407ddb5e.png" data-rawwidth="783" data-rawheight="315"&gt;&lt;p&gt;从分布中可以看出，L2参数没有像L1那样特别小的参数，L1参数确实比L2更靠近0，但是并没有到达0这个地方。&lt;/p&gt;&lt;p&gt;这是为什么呢？&lt;/p&gt;&lt;p&gt;首先，CNN的参数和Loss关系并不是凸函数，而教科书上讲的例子都是凸函数，所以从这个初始条件来看就不一样，所以最终的结果也自然不一样。对于非凸的函数，情况比上面的图复杂的多。&lt;/p&gt;&lt;p&gt;其次，Caffe中求解L1的算法使用的是最基础的subgradient descent，这种方法在求解的绝对稀疏性上是有劣势的，对于这个问题，我们可以举一个简单的例子做展示。&lt;/p&gt;&lt;p&gt;我们可以简单地列出subgradient_descent的python代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def subgradient_descent(x, grad, lr, lambda):
    x -= lr * (grad + lambda * sign(x))&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们初始化一个小量的数据集，用于L1正则的训练：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def genData(n, p, s):
    A = np.random.normal(0, 1, (n,p))
    opt_x = np.zeros(p)
    random_index_list = random.sample(range(p), s)
    for i in random_index_list:
	opt_x[i] = np.random.normal(0,10)
    e = np.random.normal(0,1,n)      
    b = np.dot(A,opt_x.T) + e.T
    return A, b

A, b = genData(100, 50, 20)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的代码中理想的最优参数共有50维，其中有30维都被设置为0。我们希望subgradient的优化算法可以学到这样的稀疏性。&lt;/p&gt;&lt;p&gt;不过实际情况是，当训练结束时，没有一个参数真正等于0，只是接近0。&lt;/p&gt;&lt;p&gt;然而我们采用其他的subgradient算法时，参数可以优化到0。（比方说proximal gradient descent）&lt;/p&gt;&lt;p&gt;举一次优化的结果为例，理想参数如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;opt_x 
[  0.00000000e+00   0.00000000e+00  -8.89068011e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   9.79677398e+00   0.00000000e+00  -1.53016548e+01
   1.01055968e+01   2.08989507e+01   0.00000000e+00  -4.16464326e+00
   1.76916005e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.00900104e+00
  -9.35102012e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00  -7.13791761e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00  -2.79487453e+01   3.11610679e+00   0.00000000e+00
   0.00000000e+00   2.14217260e+00   6.00126451e+00   6.58569031e+00
   4.52257852e+00   0.00000000e+00  -2.68944021e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00  -5.51801700e+00   1.31847462e+00
  -1.81286404e+01   0.00000000e+00]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;subgradient给出的结果是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;subgradient_process
[  1.71781432e-07  -1.47628627e-07  -8.74652150e+00  -1.72402237e-07
   5.39554174e-08   2.85199917e-07   2.09696332e-07   5.44626181e-08
   1.42748391e-07   9.09983860e+00  -1.34269176e-07  -1.48617035e+01
   9.73235939e+00   2.00436077e+01   2.01599098e-07  -4.07292753e+00
   1.69462946e+01   2.24365336e-01  -1.00920339e-04  -5.21839170e-03
  -9.68581468e-04  -1.93440885e-07   3.31274645e-01   4.36451104e+00
  -9.05097771e-01  -1.58529600e-07   9.90021826e-08  -2.60871187e-01
   1.40025608e-07  -6.60147937e+00  -7.96634585e-05   1.24702253e-07
   4.38641043e-08  -2.70520316e+01   2.73945615e+00   2.80055535e-07
   2.02949781e-07   2.52015283e+00   5.89734589e+00   6.33872176e+00
   4.64813524e+00   1.17230844e-07  -1.24874200e-01  -2.53462109e-02
   3.01514888e-07   2.72601191e-07  -5.18595641e+00   1.24439798e+00
  -1.77537445e+01  -3.96211798e-08]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而proximal gradient descent给出的是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;proximal_gradient_process
[ -0.           0.          -8.65370345   0.           
   0.           0.           0.           0.           
   0.           9.30621127  -0.         -15.07056666
   9.76129302  20.45180052   0.          -4.11430611  
  17.16825959   0.          -0.         -0.          
  -0.          -0.           0.           4.55458453
  -0.81317833   0.          -0.          -0.07547525  
  -0.          -6.73065609  -0.          -0.          
  -0.         -27.54904074   2.87046799   0.           
   0.           2.23054671   5.8947849    6.36406299   
   4.61893832  -0.          -0.          -0.
   0.           0.          -5.26100017   1.19033131 
 -17.77335708  -0.        ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从稀疏性的效果来看一目了然。&lt;/p&gt;&lt;p&gt;那么在Caffe中想利用L1正则来获得稀疏性是不太可能的。当然，还有一个更重要的问题，L1的稀疏性真的是我们需要的么？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;564533376&lt;/p&gt;&lt;p&gt;——现在加入“我爱机器学习”QQ群，开启机器学习开挂的人生~&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 24 Aug 2016 22:06:45 GMT</pubDate></item><item><title>聊点轻松的2——斗图篇</title><link>https://zhuanlan.zhihu.com/p/22112582</link><description>非常感谢各位看官的支持和鼓励！不知不觉我的“无痛的机器学习”专栏即将迎来第2000个看官。22天前，我写了“聊点轻松的”系列篇1，纪念了1000个小伙伴的达成之日，今天就来提前庆祝下2000小伙伴这个milestone。（发文时已经破2000了啊～）&lt;p&gt;感谢的话有很多，但是专栏毕竟不是颁奖典礼，没有干货的煽情鸡汤文还是留到适合的场合去说吧。今天我们来看一个开源代码的实验效果——当然，其实重点不是去看开源代码，而是去看看ImageNet图片库里的奇葩图给大家轻松一下。&lt;/p&gt;&lt;p&gt;首先我们来正经一下，这篇论文叫《Visualizing and Understanding Convolutional Networks》，是一篇讲述CNN模型可视化相关的论文。论文的具体内容后面有机会可以细聊，今天单单看一下它的实验结果——也就是前向计算得到的feature经过反向传播会变成什么样子。&lt;/p&gt;&lt;p&gt;（以下图片一律侵删）&lt;/p&gt;&lt;p&gt;下面请擦亮眼睛，因为亮点实在太多了！（以下是手纸系列）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/978ca34ecc06f2310230e61caeaefd8c.jpg" data-rawwidth="454" data-rawheight="227"&gt;这干净的背景，销魂的表情，手纸，啤酒……感觉我的脑中已经有画面了……（突然一本正经地）旁边的reconstruction的主要目标是人脸和手纸，说明你已经成功引起了模型的注意。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/63ae9422588c51dcffc99e7e8519f2f1.jpg" data-rawwidth="454" data-rawheight="227"&gt;这么有内涵的手纸，难怪CNN只顾着捕捉地图信息了，万一上面有藏宝图呢！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6c4832ff234bf29a5aa26944c7155521.jpg" data-rawwidth="454" data-rawheight="227"&gt;这真是有故事的一群人啊，你看CNN都看不过来了，满屏都是重点！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/198096b39f3e88cb0eb44ea21e932d58.jpg" data-rawwidth="454" data-rawheight="227"&gt;什么叫亲民的总统？笑口常开啊！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1a3e4d39024734b62c957fded059bf41.jpg" data-rawwidth="454" data-rawheight="227"&gt;世界不是缺少美，而是缺少——可以发现美的手纸眼镜……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ba2a0e60660b536d4a46222d769f1c1a.jpg" data-rawwidth="454" data-rawheight="227"&gt;呃……我不知道如何用语言形容了……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0b4653051b88a14017892279500d985b.jpg" data-rawwidth="454" data-rawheight="227"&gt;呃……嗯……这个……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/aa20d8f10e5c2f70993adb2dde2862ff.jpg" data-rawwidth="454" data-rawheight="227"&gt;这个广告语还不错嘛，挺有创意的……&lt;/p&gt;&lt;p&gt;还有更多精彩图片，欢迎大家自行挖掘，以下就是ImageNet中第1000类——toilet tissue, toilet paper, bathroom tissue中的精彩图片。我们的模型就是要从这些逗逼的图片中认出主题来，也怪难为它们的了……&lt;/p&gt;&lt;h2&gt;今天没有私货～&lt;/h2&gt;</description><author>冯超</author><pubDate>Sun, 21 Aug 2016 19:45:22 GMT</pubDate></item><item><title>CNN数值——xavier（下）</title><link>https://zhuanlan.zhihu.com/p/22044472</link><description>上回说到我们从前向的方向推导，发现了这些0均值的随机变量在计算过程中会产生方差扩散的问题，我们并且从前向的方向给出了解决的办法。既然在刚才的句子中我们反复提到了前向这两个字，那肯定是在别有用心地告诉大家——还有后向呗。&lt;p&gt;后向的计算公式其实和前向类似，忘记的可以顺便去前面的文章中回顾一下（顺便顺手点个赞啊～），这里我们还是用比较抽象的方式去表示，假设我们还是一个k层的网络，现在我们得到了第k层的梯度&lt;equation&gt;\frac{\partial Loss}{\partial x^k}&lt;/equation&gt;,那么对于第k-1层输入的梯度，有&lt;/p&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x^{k-1}_j}=\sum_{i=1}^n{\frac{\partial Loss}{\partial x^k_i}*w^k_j}&lt;/equation&gt;&lt;p&gt;好了，这个公式的精髓还是在意不在形，也就是说，K-1层一个数值的梯度，相当于上一层的n个参数的乘加。这个n个参数的计算方式和之前方式一样，只是表示了输出端的数据维度，在此先不去赘述了。&lt;/p&gt;&lt;p&gt;然后我们又到了反向传播到非线性函数的地方了，时间一长洗脑可能会失效，让我们再次催眠自己，想象非线性函数像线性函数一样飘过，飘过……&lt;/p&gt;&lt;p&gt;于是我们如果假设每一层的参数也服从某种均值为0，方差为某值的分布，利用这种来自东方的神秘力量，我们又可以写出一个神奇的公式：&lt;/p&gt;&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{k-1}_j})=n^k*Var({\frac{\partial Loss}{\partial x^k_i})*\sigma_w^k}&lt;/equation&gt;&lt;p&gt;于是乎，对于这个k层网络，我们又可以推导出一个神奇的公式：&lt;/p&gt;&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{1}_j})=Var({\frac{\partial Loss}{\partial x^k_i})*\prod_{i=1}^{k-1}n^i*\sigma_w^i}&lt;/equation&gt;&lt;p&gt;好了，上次我说过的话不会重复再说一遍了。这次我们考虑后向操作是为了什么呢？前面我们前向传播data，我们做到了数值的稳定，现在反向传播如果不能做到同样的数值稳定，那么被diff更新过的data不再服从这种神奇的力量怎么办？要命了。&lt;/p&gt;&lt;p&gt;所以为了服从神奇的力量，我们又可以得到：&lt;/p&gt;&lt;p&gt;为了&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{k-1}_j})=Var({\frac{\partial Loss}{\partial x^k_i}})&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^k}&lt;/equation&gt;&lt;p&gt;咦，好像我们两次推导得到了同样的结果，大功告成了？如果仔细看一下这两个公式，我们就会发现两个n实际上不是同一个n。对于全连接来说，前向操作时，n表示了输入的维度，而后向操作时，n表示了输出的维度。而输出的维度也可以等于下一层的输入维度。所以两个公式实际上可以写作：&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^k}&lt;/equation&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^{k+1}}&lt;/equation&gt;&lt;p&gt;这么看上去前向后向不是很统一啊，但是大功快要告成，怎么也得再糊弄一回了，于是我们把两个公式揉合以下，就成了：&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{2}{n^{k+1}+n^k}&lt;/equation&gt;&lt;p&gt;下面就是对这个方差的具体使用了。没错，前辈思来想去，决定使用均匀分布进行初始化，我们设定我们要初始化的范围是[-a,a]。熟悉均匀分布和不熟悉均匀分布的各位，都可以看一下上述的范围下，均匀分布的方差为：&lt;/p&gt;&lt;equation&gt;Var(uniform)=\frac{(a-(-a))^2}{12}=\frac{a^2}{3}=\sigma^k_w&lt;/equation&gt;&lt;p&gt;将上面两个公式合并一下，就可以得到：&lt;/p&gt;&lt;equation&gt;a=\sqrt{\frac{6}{n^{k+1}+n^k}}&lt;/equation&gt;&lt;p&gt;于是，我们的xavier初始化方法横空出世，那就是把参数初始化成&lt;equation&gt;[-\sqrt{\frac{6}{n^{k+1}+n^k}},\sqrt{\frac{6}{n^{k+1}+n^k}}]&lt;/equation&gt;范围内的均匀分布。&lt;/p&gt;&lt;p&gt;看完了这段晕晕忽忽地演绎，再看看最终的结果，和源代码，有没有一种搞了半天就弄出点这的感觉？&lt;/p&gt;&lt;p&gt;没错，这个初始化的公式不难，但是想这样推导出来还是让前辈们付出了巨大的心血。后人在使用这个初始化方法的时候，理所当然地使用了这些方法，但是很少去理会这些推导背后的真正含义。&lt;/p&gt;&lt;p&gt;虽然前面用了大量戏谑的语言来说明一些假设的不合理性，但是如果没有这些假设，我们也无法得出这样精彩而且实用的结论。其实数学模型的世界经常就是会用到一些抽象这件工具，只有把一些不太好把握的地方抽象掉，才能更容易地抓住事物的本质，找到事物的核心规律。所以在这里还是要由衷的给这个初始化算法的作者点个赞。&lt;/p&gt;&lt;h2&gt;向更远方前进&lt;/h2&gt;&lt;p&gt;如果熟悉Caffe源码的同学，在看到xavier的源码后，会看到下面还有一个类似结构的初始化方法——MSRAFiller。这个初始化方法来自《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》，不同的是，这篇文章的主要目标是基于ReLU的初始化算法，实际上它的推导过程和我们看过的xavier的方法类似，只是在一些细节处有所不同。如果你理解了xavier的演绎思想，不妨去看看这篇文章的推导过程，相信你会很轻松地理解这一路研究初始化算法的思路。总之，能够出现在应用中的算法都是经过一定实践检验的算法，已经被人证明了它在理论和实践上的可行性，是完全值得去深入了解的。&lt;/p&gt;&lt;p&gt;除了这两篇文章，还有很多大牛写了关于初始化的文章，以它们的角度讲述了它们心中初始化的样子。后面有时间我们还会继续去看这些文章，不过我们要暂时停下脚步了，因为还有在其他方向努力的前辈们要急着登场了，它们又会给我们带来一个全新的角度去理解CNN……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;h2&gt;私货时间2&lt;/h2&gt;&lt;p&gt;支持王宝强快速走出阴影！&lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 20 Aug 2016 23:21:22 GMT</pubDate></item><item><title>CNN数值——xavier（上）</title><link>https://zhuanlan.zhihu.com/p/22028079</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@Express&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/08cd1d56513335b13d6a93725db969ad" data-hash="08cd1d56513335b13d6a93725db969ad" class="member_mention" data-hovercard="p$b$08cd1d56513335b13d6a93725db969ad"&gt;@夏龙&lt;/a&gt;对本文的审阅。欢迎大家多多提出宝贵意见。&lt;/p&gt;上一回我们做了三个小实验。第一个是正常的实验，表现优异；第二个实验我们把初始化调整得很奇葩（为什么奇葩？），最终训练结果弱爆了；第三个实验我们把非线性函数重新换回sigmoid，模型奇迹般地回血，虽然表现不够完美，但也算是十分优异了。&lt;p&gt;于是ReLU被众人推到墙角，开始质问。&lt;/p&gt;&lt;p&gt;其实ReLU也是很委屈的，前面说过他的优势在与模型前向后向计算的过程中，它可以更好地传递数据，不会像sigmoid那样有梯度传递的问题，但是它又缺少了sigmoid的重要特性，那就是对数据的控制力。&lt;/p&gt;&lt;p&gt;我们知道sigmoid可以把任意维度的数据压缩到0到1之间，这是它最强力的一个特点。所以在使用sigmoid时，我们不用太担心数据的幅度问题，因为只要使用一个sigmoid，数据的幅度就会得到良好的控制（当然了，全是正数这件事其实也有点不太靠谱，要是像tanh那样有正有负就更好了）。而我们从上一次的实验中可以看出，采用ReLU的非线性函数，数据的维度完全没有得到控制。有的幅度到达了上千，有的依然是一个极小的小数。这说明ReLU在压缩数据幅度方面存在劣势。&lt;/p&gt;&lt;p&gt;于是乎我们有了以下的经验总结：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;sigmoid在压缩数据幅度方面有优势，对于深度网络，使用sigmoid可以保证数据幅度不会有问题，这样数据幅度稳住了就不会出现太大的失误。&lt;/li&gt;&lt;li&gt;但是sigmoid存在梯度消失的问题，在反向传播上有劣势，所以在优化的过程中存在不足&lt;/li&gt;&lt;li&gt;relu不会对数据做幅度压缩，所以如果数据的幅度不断扩张，那么模型的层数越深，幅度的扩张也会越厉害，最终会影响模型的表现。&lt;/li&gt;&lt;li&gt;但是relu在反向传导方面可以很好地将“原汁原味”的梯度传到后面，这样在学习的过程中可以更好地发挥出来。（这个“原汁原味”只可意会，不必深究）&lt;/li&gt;&lt;/ol&gt;这么来看，sigmoid前向更靠谱，relu后向更强。这么一看似乎一切又回到了起点，到底哪个非线性函数更好呢？&lt;p&gt;要评判哪个非线性函数更好，不但要看自己本身，还要看它们和整体模型阵型的搭配情况（BP很重要！）sigmoid在学习方面存在弱点，有什么办法能帮助它呢？这个我们后面再说。那relu的数据幅度呢？有没有什么办法能够帮助它解决呢？&lt;/p&gt;&lt;p&gt;众人想了好久，又重新看向了初始化……&lt;/p&gt;&lt;p&gt;初始化：（黑人问号脸）？&lt;/p&gt;&lt;h2&gt;xavier&lt;/h2&gt;&lt;p&gt;大家突然想起来，刚才和relu完美配合的那个初始化叫什么来着？哦对，xavier。我们就来看看这个初始化方法的由来。xavier诞生时并没有用relu做例子，但是实际效果中xavier还是和relu很搭配的。&lt;/p&gt;&lt;p&gt;xavier是如何完成初始化工作的呢？它的初始化公式如下所示：&lt;/p&gt;&lt;p&gt;定义参数所在层的输入维度为n，输出维度为m，那么参数将以均匀分布的方式在&lt;equation&gt;[-\sqrt{\frac{6}{m+n}},\sqrt{\frac{6}{m+n}}]&lt;/equation&gt;的范围内进行初始化。&lt;/p&gt;&lt;p&gt;那么这个公式是如何计算出来的呢？关于这个问题我们需要一段漫长的推导。在推导之前我们要强调一个关键点，就是参数的标准差，或者方差。前面我们提到了Caffe中的debug_info主要展示了数据的L1 norm，对于均值为0的数据来说，这个L1 norm可以近似表示标准差。&lt;/p&gt;&lt;p&gt;我们将用到以下和方差相关的定理：&lt;/p&gt;&lt;p&gt;假设有随机变量x和w，它们都服从均值为0，方差为&lt;equation&gt;\sigma&lt;/equation&gt;的分布，那么：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;w*x就会服从均值为0，方差为&lt;equation&gt;\sigma^2&lt;/equation&gt;的分布&lt;/li&gt;&lt;li&gt;w*x+w*x就会服从均值为0，方差为&lt;equation&gt;2*\sigma^2&lt;/equation&gt;的分布&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以下内容主要来自论文《Understanding the difficulty of training deep feedforward neural network》的理解，这里将以我个人的理解做一下解读，如果有错欢迎来喷。&lt;/p&gt;&lt;p&gt;前面两个定理的变量名称是不是有点熟悉？没错，下面我们说的就是参数w和x。这里暂时将偏置项放在一边，同时我们还要把一个部分放在一边，那就是非线性部分。这篇论文心目中的理想非线性函数是tanh。为啥呢？&lt;/p&gt;&lt;p&gt;在大神的假想世界中，x和w都是靠近0的比较小的数字，那么它们最终计算出来的数字也应该是一个靠近0，比较小的数字。我们再看一眼tanh函数和它对应的梯度函数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/9b7de17962ef31509e41957d0f318042.png" data-rawwidth="378" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4657a64ed40e2a4de13e4b0a4fc01342.png" data-rawwidth="375" data-rawheight="259"&gt;这两张图有点大，不过可以看出来，如果数值集中在0附近，我们可以发现，前向时tanh靠近0的地方斜率接近1，所以前辈告诉我们，把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;下面这张梯度的图像也是一样，靠近0的地方斜率接近1，所以前辈又一次告诉我们，把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;什么，你不信？&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数……&lt;/p&gt;&lt;p&gt;好了，现在这个挡在中间的非线性函数硬生生掰成一个线性函数了，为了理论的完美我们也是什么也不顾了。下面就要面对一个问题，如何让深层网络在学习过程中的表现像浅层网络？&lt;/p&gt;&lt;p&gt;我们的脑中迅速回忆起我们接触过的浅层模型——logistic regression，SVM。为了它们的表现能够更好，我们都会把特征做初始化，细心处理，比方说做白化处理，使他的均值方差保持好，然后用浅层模型一波训练完成。现在我们采用了深层模型，输入的第一层我们是可以做到数据的白化的——减去均值，除以一个标准差。但是里面层次的数据，你总不好伸手进入把它们也搞白化吧！（当然，后来真的有人伸进去了，还做得不错）那我们看看如果在中间层不做处理会发生什么？&lt;/p&gt;&lt;p&gt;我们假设所有的输入数据x满足均值为0，方差为&lt;equation&gt;\sigma_x&lt;/equation&gt;的分布，我们再将参数w以均值为0，方差为&lt;equation&gt;\sigma_w&lt;/equation&gt;的方式进行初始化。我们假设第一次是大家喜闻乐见的卷积层，卷积层共有n个参数（n=channel*kernel_h*kernel_w），于是为了计算出一个线性部分的结果，我们有：&lt;/p&gt;&lt;equation&gt;z_j=\sum^n_i{w_i*x_i}&lt;/equation&gt;&lt;p&gt;这个公式的下标不准确，大家姑且这么看了，也就是说，线性输出部分的一个结果值，实际上是由n个乘加计算出来的，那么下面是一道抢答题，按照我们刚才对x和w的定义，加上前面我们说过的两个方差计算公式，这个z会服从一个什么分布呢？&lt;/p&gt;&lt;p&gt;均值肯定还是0嘛，没得说。&lt;/p&gt;&lt;p&gt;方差好像积累了一大堆东西：&lt;equation&gt;n*\sigma_x*\sigma_w&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然后我们通过那个靠意念构建的具有“线性特征”的非线性层，奇迹般地发现一切都没有变化，那么下一层的数据就成了均值为0，方差为&lt;equation&gt;n*\sigma_x*\sigma_w&lt;/equation&gt;的“随机变量”（姑且称之为随机变量吧）。&lt;/p&gt;&lt;p&gt;为了更好地表达，我们将层号写在变量的上标处，于是就有：&lt;/p&gt;&lt;equation&gt;\sigma^2_x=n^1*\sigma^1_x*\sigma^1_w&lt;/equation&gt;&lt;p&gt;我们将卷积层和全连接层统一考虑成n个参数的一层，于是接着就有：&lt;/p&gt;&lt;equation&gt;\sigma^3_x=n^2*\sigma^2_x*\sigma^2_w&lt;/equation&gt;&lt;p&gt;如果我们是一个k层的网络（这里主要值卷积层+全连接层的总和数），我们就有&lt;/p&gt;&lt;equation&gt;\sigma^k_x=n^{k-1}*\sigma^{k-1}_x*\sigma^{k-1}_w=n^{k-1}*n^{k-2}*\sigma^{k-2}_x*\sigma^{k-2}_w*\sigma^{k-1}_w&lt;/equation&gt;&lt;p&gt;继续把这个公式展开，就会得到它的最终形态：&lt;/p&gt;&lt;equation&gt;\sigma^k_x=\sigma^1_x*\prod_{i=1}^{k-1}n^i*\sigma^i_w &lt;/equation&gt;&lt;p&gt;可以看出，后面的那个连乘实际上看着就像个定时炸弹（相信看到这，我应该能成功地吸引大家的注意力，帮助大家把非线性函数线性化的事情忘掉了……），如果&lt;equation&gt;n^i*\sigma^i_w&lt;/equation&gt;总是大于1，那么随着层数越深，数值的方差会越来越大，反过来如果乘积小于1，那么随着层数越深，数值的方差就会越来越小。&lt;/p&gt;&lt;p&gt;越来越大，就容易Hold不住导致溢出，越来越小，就容易导致数据差异小而不易产生有力的梯度。这就是深层模型的一大命门。&lt;/p&gt;&lt;p&gt;公式推到这里，我们不妨回头看看这个公式：&lt;/p&gt;&lt;equation&gt;\sigma^2_x=n^1*\sigma^1_x*\sigma^1_w&lt;/equation&gt;&lt;p&gt;你一定会有这样一个想法（一定会有！），如果&lt;equation&gt;\sigma_x^2=\sigma_x^1&lt;/equation&gt;，接着我们保证每一层输入的方差都保持一致，那么数值的幅度不就可以解决了么？于是乎：&lt;/p&gt;&lt;equation&gt;\sigma^1_w=\frac{1}{n^1}&lt;/equation&gt;&lt;p&gt;我们用均值为1，方差为上式的那个数字做初始化，不就可以解决了？&lt;/p&gt;&lt;p&gt;不错，从理论上讲是这个思路，不过，这只是这个思路的开始……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;p&gt;（其实重要的事情也可以说两遍，吼吼～）&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 17 Aug 2016 20:40:49 GMT</pubDate></item><item><title>CNN的数值实验</title><link>https://zhuanlan.zhihu.com/p/22027076</link><description>前面我们聊过了Caffe的整体架构，相信各位对Caffe的结构已经比较熟悉了，下面我们就来看看CNN中的一些细节问题，也顺着前面的思路进一步进行。&lt;h2&gt;序&lt;/h2&gt;&lt;p&gt;在好久之前介绍ReLU的时候，我们曾经提到ReLU相比于Sigmoid的一些优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sigmoid整体的梯度偏小，在外向传导的过程中会不断让梯度的幅度变小；&lt;/li&gt;&lt;li&gt;Sigmoid存在一个梯度饱和问题：那就是如果非线性部分的输出靠近0或者1，那么反向传导到了这里，Sigmoid的梯度会接近于0，这样就会导致梯度到这里就传导不下去了。从前面我们的那张图中可以看得出来。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有了这两个原因，我们认为ReLU这样的非线性函数在前向后向的传递性方面效果更好，如果CNN的网络层数比较深，ReLU更能够保证梯度的无损传递。&lt;/p&gt;&lt;p&gt;当我们的前辈完成了这个重大发现之后，大家都觉得“这次终于稳了”，AlexNet也横空出世，让CNN大放异彩。那个时候AlexNet已经算是非常"Deep"的网络了……&lt;/p&gt;&lt;p&gt;可是后来，大家对CNN模型能力的诉求越来越高，大家通过不断地实验，慢慢发现，现在的模型还不够深，想要模型够牛逼，模型的深度还得继续增加啊！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1049017686e7ebefdae7367397fa052e.jpg" data-rawwidth="296" data-rawheight="220"&gt;可是随着后面模型的深度不断增加，我们发现模型的训练过程变得越来越不可控，有时候一个不留神模型就训练溢出了，有时候模型训练着训练着就不动了。训练模型也变得越来越像炼丹了……&lt;/p&gt;&lt;p&gt;炼丹不是一件让人开心的事情，能炼好固然让人开心，但是炼不好就会让宝宝有情绪了。于是各位前辈又重新出发，试图找出新的方法解决这些恼人的问题。&lt;/p&gt;&lt;p&gt;当年众位大神齐聚一堂，开始讨论究竟什么样招式能破解这个困局。有的大神选择从数值的方向入手，有的大神选择从模型结构的方向入手，有的大神选择从模型结构入手，还有的大神走了其他的方法。一场“拯救大兵CNN”的好戏也由此上演……&lt;/p&gt;&lt;h2&gt;一个数值的小实验&lt;/h2&gt;&lt;p&gt;好了，让我们回到MNIST这种比较简单的问题上。我们也曾经展示过Caffe中MNIST的例子，以下这个模型是我们曾经用过的模型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Conv 20*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Conv 50*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Ip  500 - ReLU&lt;/li&gt;&lt;li&gt;Ip 10&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这一次我们将debug_info打开，记录一下网络中每一层数据的数值信息。Caffe中的debug_info默认可以给出所有数据的L1 norm，也就是绝对值的和。这个数值可以在一定程度上反映数据的幅度。如果数据比较大，那么说明整体的数值比较大。&lt;/p&gt;&lt;p&gt;完成了10000轮模型的训练，模型最终在test集合得到了0.991的精度，可以说精度已经很高了。那么各个层的数据表现如何呢？这里我们主要关注以下的数值：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;两个conv层和两个ip层的top data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的diff信息&lt;/li&gt;&lt;li&gt;所有参数的data和diff的L1 norm和L2 norm&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先是top data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8397d75b85e7c1af80e554bf62ff6e6a.png" data-rawwidth="918" data-rawheight="433"&gt;&lt;p&gt;可以看出除了一开始的数值波动，后面的迭代中数值整体表现比较稳定，稳中有升。&lt;/p&gt;&lt;p&gt;其次是w的data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/dc0702632a1d330e7919ce4ce414f281.png" data-rawwidth="917" data-rawheight="437"&gt;&lt;p&gt;可以看出w整体表现不算稳定，不过考虑整个优化的过程就是在改变它，所以它的大幅波动也是可以理解的。&lt;/p&gt;&lt;p&gt;接下来是w的diff&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/72b4e4e7458586f86af667ea2262e1d4.png" data-rawwidth="919" data-rawheight="435"&gt;&lt;p&gt;可以看出diff在数值上表现得也比较稳定，基本上处于一个小的区间之中。&lt;/p&gt;&lt;p&gt;最后是L1 norm和L2 norm&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7ebf6288c1845e5a000dfaae4c8e37f3.png" data-rawwidth="914" data-rawheight="428"&gt;&lt;p&gt;norm和diff的表现总体上和前面展现的一致。&lt;/p&gt;&lt;p&gt;总体来说，topdata和w的diff数值没有太大的波动，保持在一个稳定幅度上，所以整体的数值比较稳定，无论是前向的top data还是后向的diff data没有出现太大的数值波动或者数值幅度问题。&lt;/p&gt;&lt;p&gt;看完了这个成功案例，我们再看看失败案例，当然这个失败案例一般不会出现，只是做个效果而已，不过这个效果也足以说明一些问题。&lt;/p&gt;&lt;p&gt;这个失败案例直接来自于刚才的例子。我们在刚才的模型上只修改一个地方：四个核心层（conv1,conv2,ip1,ip2）的w参数的初始化方法。&lt;/p&gt;&lt;p&gt;原来的初始化方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "xavier"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们将它们改成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "gaussian"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样的改法是比较脑残的，这相当于对参数采取均值为0，方差为1的高斯分布对参数进行初始化……&lt;/p&gt;&lt;p&gt;不用说，这个模型会死的很难看，10000轮训练结束后，最终的精度为0.1135。要知道一共只有10个数字，猜也可以达到0.1的精度。这个模型采用了各种高科技，结果却只比猜稍微好一点，简直弱到家了。&lt;/p&gt;&lt;p&gt;那么它在数值上的表现是怎样呢？直接来看图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/955f787ae8dc46b137e3f7fb077ad6d5.png" data-rawwidth="916" data-rawheight="419"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e2e676bf99194f1c89d7a6263d30a7c9.png" data-rawwidth="920" data-rawheight="435"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/54a5a4b652a4d1d4d6630d87c93cf593.png" data-rawwidth="916" data-rawheight="428"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7e5b81d4199d1658ef8f0e419ea1a7f5.png" data-rawwidth="914" data-rawheight="429"&gt;&lt;p&gt;从图中可以看出，各个数据在幅度上的差距都非常大。尤其是最后一张图，data和diff在数值上的差距拉得非常大，那么小量的diff对大量的data完全起不到改变的作用。这么看来一定是初始化的锅了。&lt;/p&gt;&lt;p&gt;那真的完全是初始化的锅么？众人问：初始化同学，你还有没有什么要补充的？&lt;/p&gt;&lt;p&gt;初始化同学：其实是因为我和ReLU同学不熟，你们换Sigmoid试试……&lt;/p&gt;&lt;p&gt;众人：当真？&lt;/p&gt;&lt;p&gt;于是乎我们开始了第三个实验……&lt;/p&gt;&lt;h2&gt;第三个实验&lt;/h2&gt;&lt;p&gt;为了洗刷初始化的清白，我们在保持初始化的基础上，将3个非线性的部分重新换成sigmoid，由sigmoid再度出马。sigmoid表示这种小场面它Hold住……&lt;/p&gt;&lt;p&gt;于是我们来看看sigmoid的最终结果：0.9538，相关的数值图在这里就不全贴了，只贴一张L1 L2 norm的图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6f231a2392460c8b6b0b89c64ce3b312.png" data-rawwidth="914" data-rawheight="425"&gt;&lt;p&gt;从这个图上来看，虽然data的幅度很大，但是diff的幅度也很大，基本上算旗鼓相当。&lt;/p&gt;&lt;p&gt;这个最终的精确率不能算特别高，但是起码说明模型还是学到了很多有用的知识。而且和0.11的精度相比，简直是一个天上一个地下，完全没得比啊！大家纷纷表示关键时候还得靠老将才能扛得住，初始化同学是被冤枉了。&lt;/p&gt;&lt;p&gt;这时候众人重新以怀疑的眼光看着ReLU，原来真正的问题出你这啊！你的表现有点浪啊！&lt;/p&gt;&lt;p&gt;ReLU委屈地表示：我还能再说一句么？&lt;/p&gt;&lt;p&gt;众人表示：这次的版面已经不够了，有话下次再说……&lt;/p&gt;&lt;h2&gt;继续广告……&lt;/h2&gt;&lt;p&gt;欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;</description><author>冯超</author><pubDate>Tue, 16 Aug 2016 19:17:23 GMT</pubDate></item><item><title>Caffe代码阅读——Solver</title><link>https://zhuanlan.zhihu.com/p/21800004</link><description>前面我们聊了Net组装的内容，接下来我们来看看Solver的内容。Solver主体有两部分：初始化和训练。初始化内容相对比较简单，这里就不说了；下面我们来说说训练中的几个关键函数。&lt;h2&gt;核心函数：Step&lt;/h2&gt;&lt;p&gt;真正的训练在Step函数内，这里有多卡训练的关键回调函数：on_start()和on_gradient_ready()，具体的调用方法我们后面再说，在这两个回调函数中间有两个重要的过程：ForwardBackward和UpdateSmoothedLoss。在on_gradient_ready之后有一个关键函数ApplyUpdate()，这里面的代码在Sgd_solver中。下面我们详细看一下。&lt;/p&gt;&lt;h2&gt;ForwardBackward&lt;/h2&gt;&lt;p&gt;这里主要调用了Net中的代码，主要完成了前向后向的计算，前向用于计算模型的最终输出和Loss，后向用于计算每一层网络和参数的梯度。对于前向后向的具体内容这里需要详细叙述了，唯一值得一提的是前向的Loss计算，这部分代码实际上实在Layer里面，具体涉及到loss_weight这个参数相关的初始化和loss()的判断，同时还有Loss_Layer在Setup函数中的初始化。&lt;/p&gt;&lt;h2&gt;UpdateSmoothedLoss&lt;/h2&gt;&lt;p&gt;这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题。代码中的平滑方法比较简单，大家一看便知。&lt;/p&gt;&lt;p&gt;下面就是ApplyUpdate函数，这个函数真正完成了参数更新的任务。Caffe的参数更新只利用了模型的梯度信息，没有利用二阶信息。下面就详细介绍下更新参数的几个过程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;GetLearningRate&lt;/li&gt;&lt;li&gt;ClipGradients&lt;/li&gt;&lt;li&gt;Normalize&lt;/li&gt;&lt;li&gt;Regularize&lt;/li&gt;&lt;li&gt;ComputeUpdateValue&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;GetLearningRate&lt;/h2&gt;&lt;p&gt;learning rate的故事我们前面已经聊过了，在CNN训练中这确实是个大问题。Caffe为了让learning rate的设计更灵活，提供了一系列的learning rate方案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;fixed&lt;/b&gt;：lr永远不变&lt;/li&gt;&lt;li&gt;&lt;b&gt;step&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter / stepsize}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;exp&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;inv&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1+gamma*iter)^{-power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;multistep&lt;/b&gt;：直接写iter在某个范围内时lr应该是多少&lt;/li&gt;&lt;li&gt;&lt;b&gt;poly&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1-\frac{iter}{maxiter})^{power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;sigmoid&lt;/b&gt;：&lt;equation&gt;lr=baselr*\frac{1}{1+e^{-gamma*(iter-stepsize)}}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;这些方案各有优劣，选择自己顺手的就好。&lt;h2&gt;ClipGradients&lt;/h2&gt;&lt;p&gt;这一步主要是对梯度值做一个限制，如果梯度值过大，那么这里就会对梯度做一个修剪，对所有的参数乘以一个缩放因子，使得所有参数的平方和不超过参数中设定的梯度总值。这个功能感觉上像是对全局函数设置了一个Trust Region，可以防止更新的量过大二导致梯度发散。我认为这一步的想法是很好的，但是实际操作中可能会有问题。实际中可能只有部分参数的梯度比较大，而其他参数的梯度本身比较小，那么对所有的参数乘以相同的因子会让一些本来比较小的参数变得更小，这样会带来一些不公平。&lt;/p&gt;&lt;h2&gt;Normalize&lt;/h2&gt;&lt;p&gt;这一步同样考虑了一些单一Batch不足以完成训练的问题，通过限制每个Batch的更新量来控制更新总量，代码比较简单。&lt;/p&gt;&lt;h2&gt;Regularize&lt;/h2&gt;&lt;p&gt;到这一步终于要计算正则项的梯度了。Caffe提供两种正则方法——L2和L1，其中L2采用了标准的梯度下降方法，L1采用了sub-gradient的计算方法。L2的优化计算比较简单，没有什么好说的，但是L1的计算还是有点值得玩味的地方的。这里采用的sub-gradient方法其实本身没有什么问题，不过Lasso的优化还可以有其他的方法，这个问题以后可以再细聊。&lt;/p&gt;&lt;h2&gt;ComputeUpdateValue&lt;/h2&gt;&lt;p&gt;到这里，我们终于来到了梯度计算的最后一站，这时候我们终于完成了对梯度的计算，下面该考虑lr和梯度结合起来如何计算最终的梯度优化值了。sgd方法主要采用momentum加梯度的优化方法。关于momentum的优势我们前面已经聊过了。除此之外，Caffe还提供了一系列的梯度计算方法，这些优化方法各有特点，以后我们可以慢慢来看。&lt;/p&gt;&lt;p&gt;当计算完这一步，我们就可以调用Blob中的Update把每个参数的data和diff进行相加，计算出最终的结果。这样，整个优化过程就完成了。至于剩下的一些内容都不是核心过程，就略去不看了。&lt;/p&gt;&lt;p&gt;如果我们采用单卡训练的策略，那么阅读代码到这里也差不多了。不过多卡训练对于大规模的训练任务来说是必不可少的，所以我们接下来趁热打铁地看看Caffe的多卡训练。&lt;/p&gt;&lt;h2&gt;多卡训练算法&lt;/h2&gt;&lt;p&gt;Caffe的多卡训练算法总体思路是数据并行，我们用不同的GPU处理不同的数据，然后将所有的梯度更新汇总。由于Solver在训练中给了两个回调函数，多卡训练也主要利用了这两个回调函数进行：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;on_start()：将参数拷贝到每一个GPU中。&lt;/li&gt;&lt;li&gt;ForwardBackward()：每个GPU各自计算自己的前向后向结果。&lt;/li&gt;&lt;li&gt;on_gradient_ready()：将反向梯度汇总到一起。&lt;/li&gt;&lt;li&gt;ApplyUpdate()：在汇总的线程上进行参数更新&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中第2步由每一个CPU线程和自己的GPU并行完成，第4步由汇总的CPU和自己的GPU完成，剩下的1，3两步主要是完成数据传输的任务，也是多卡计算中主要完成的部分。&lt;/p&gt;&lt;p&gt;Caffe采用树型结构进行参数传递，其中一个CPU线程和GPU作为树型结构的根，其他的则作为根下面的节点。为了更快地传输GPU数据，树型结构的构建要考虑GPU之间是否相近，比方说两个GPU之间是否可以进行P2P的直传。在前面的翻译博客中我们已经聊过GPU之间数据传输的问题了，这里的树形结构也主要以此做考虑。&lt;/p&gt;&lt;p&gt;我们假设4块GPU的拓扑结构如下：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3  
GPU0   X     PHB    SOC    SOC    
GPU1   PHB    X     SOC    SOC    
GPU2   SOC    SOC    X     PHB   
GPU3   SOC    SOC    PHB    X     &lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;那么我们构造出的树型结构如下所示，数据传输也是按照这样的结构传输：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2b57bad80fd32d75715706d642836647.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;这样1，3的数据传递就解决了，具体的过程请详细阅读代码，这里就不叙述了。&lt;/p&gt;&lt;p&gt;对Caffe代码的基本介绍就到这里了，我们对代码的整体结构有了比较清晰的认识，下面我们将分析模型中各个部分的特性。&lt;/p&gt;&lt;h2&gt;最后的私货&lt;/h2&gt;&lt;p&gt;欢迎大家加入“我爱机器学习”QQ群：564533376，（省略情怀文若干字……）一起学习机器学习，一起成长！&lt;/p&gt;</description><author>冯超</author><pubDate>Thu, 11 Aug 2016 20:27:25 GMT</pubDate></item><item><title>[翻译]Exploring the Complexities of PCIe Connectivity and Peer-to-Peer Communication</title><link>https://zhuanlan.zhihu.com/p/21908564</link><description>本来准备今天开始写Caffe代码阅读的第三部分的，结果找到了一片介绍GPU通信的文章，觉得这篇文章的内容讲得非常好，而这方面的背景知识也是非常缺乏的，另外Caffe多卡训练的一些细节和这里面的知识也有关系，所以今天就来翻译一下这篇文章。&lt;p&gt;当然了，翻译之前先扯一段。读大学的时候我曾经报名过一节英文翻译课，当时教过我的一些具体的翻译小技巧我已经忘光了，但是有一段话我还记得，那就是翻译的三个目标——信、达、雅。翻译首先要准确，然后要通顺，最后要能做到优美那就再好不过了。这里不免想吐槽一下一些文章翻译，在达的方面做得实在不够好，我都可以从中文翻译中读出原文定语从句的味道……我个人希望自己能做到达，最后不需要做到雅，做到俗也是极好的。&lt;/p&gt;&lt;p&gt;文章出处：&lt;a href="http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/" class=""&gt;http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原作者：Ross Walker（玫瑰行走者？）&lt;/p&gt;&lt;p&gt;这篇文章我们来深入地看看PCI-E总线上数据通信方面存在的一些瓶颈，以及我们Exxact公司最新的牛逼系统是如何搞定这些问题，使得像机器学习这样严重依赖GPU的工作可以轻松点。&lt;/p&gt;&lt;h2&gt;传统套路&lt;/h2&gt;&lt;p&gt;PCI-E是个啥本文是没有兴趣给你讲的，我们直接来谈谈以它为主线的架构，以及架构的优势和劣势（主要是劣势）。在此之前，我们首先要明白一件事，那就是为啥PCI-E总线的数据传输速度是十分重要的。在GPU计算革命到来之前，PCI总线主要是用于和磁盘做数据通信，或者计算机结点间通信的（HPC小红人infiniband）。一个经典的PCI-E总线架构图就长成这个样子：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8e11038678ca5afe5dd2aa485bc3f8e2.jpg" data-rawwidth="1025" data-rawheight="363"&gt;以前这个架构用得很好没问题，但后来GPU计算时代来临，我们搞GPU计算的朋友开始有了新的诉求，我们希望主存数据和GPU显存的数据传输能够尽量快点，于是乎，传统设计就暴露出了三个问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;GPU被分在了两个区域中，就像上面的图片，它们分别被不同的MCH（Memory Controller Hub）控制，连接在不同的CPU数据接口上。然后，不同区域的CPU在通过一个叫QPI的东西连接起来，以保证不同区域的数据也能够通信。当然这样只能说解决了数据可以传输的问题，但问题是，对于处于不同区域的GPU，比方说GPU0和GPU2，如果它们两之间想传递数据（多卡训练的时候你会遇到的）,就必须翻山越岭穿越层层障碍，绕个大圈才能把数据传过来。所以……它就是慢嘛，数据吞吐量和延时的数据肯定就不好看了。&lt;/li&gt;&lt;li&gt;前面看到同一区域的CPU和GPU之间都是通过PCI-E的通道连接，但是CPU的PCI-E通道是有限的，这个问题相信不少自己装机搞深度学习环境的朋友都遇到过，如果想搞多GPU，CPU的PCI-E通道数一定不能少！那么对于现在这一代的Intel CPU架构Haswell,它是拥有两个数据接口系统，像上面的图一样，每个接口的PCI-E通道数是40个，那么它最多只能接受4个GPU（要多少是多啊……），因为我们现在的GPU每个都需要连16个数据接口，那么一边最多也就连2块GPU。万一你要是觉得4块不够多，想再多搞点，对不起，那也不行，请走分布式的路子，再部署一台机器去，然后让机器间想办法互联（infiniband）。不过很显然，这比单机要慢一些了。而且40个通道已经被GPU分走32个了，inifiband也需要PCI-E通道的，就给inifiniband分剩下的8个？太瞧不起infiniband了吧？万一跨机间的数据传输速度慢了，infiniband表示这锅它不背。&lt;/li&gt;&lt;li&gt;你以为同在一个区域的GPU就好过了？MCH照样会把它们之间的传输速度拉下去（具体原理我也不懂……）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;“这病听上去感觉还挺严重的，那大夫该怎么判断我有没有得这个病呢？”&lt;/p&gt;&lt;p&gt;“老司机早已为你准备好药方，一个命令+一个程序”&lt;/p&gt;&lt;p&gt;首先是&lt;b&gt;nvidia-smi&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3   CPU Affinity
GPU0   X     PHB    SOC    SOC    0-9,20-29
GPU1   PHB    X     SOC    SOC    0-9,20-29
GPU2   SOC    SOC    X     PHB    10-19,30-39
GPU3   SOC    SOC    PHB    X     10-19,30-39

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的结果可以看到，PHB表示了PCI-E Host Bridge，就是我们上面图中的连接方式，而SOC就是要通过GPU-CPU-QPI-CPU-GPU这样长途跋涉才能传输的连接方式。&lt;/p&gt;&lt;p&gt;其次是CUDA中自带的一个程序，在examples/1_Utilities/p2pBandwidthLatencyTest，编译一下就可以运行，亲测会输出很多内容，以下只显示一部分：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;p2pBandwidthLatencyTest
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   19.18  12.22  11.77
     1  19.17     X   17.07  11.81
     2  12.23  12.17     X   19.17
     3  11.73  11.88  19.18     X
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的数据可以一目了然的看出，在一个区域和在不同区域的带宽差是比较明显的。当然如果不是MCH背锅，这个差距还可以更亮眼一点，16通道的PCI-E理论传输上限是32GB/s。所以不论是同一区域，还是不同区域，这个数据都不够好。&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   3.39   8.18  16.86  16.26
     1   7.22   3.74  13.56  16.54
     2  16.27  16.06   3.52   5.81
     3  15.98  15.92   6.62   3.20
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;传输时延的结果也十分明显，就不说了。&lt;/p&gt;&lt;h2&gt;重新思考以GPU为核心的三个代表母版设计思想&lt;/h2&gt;&lt;p&gt;前面喷了那么多，下面该说说我们设计方案了，首先展示的是我们的关键道具，PCI-E switch chip(PLX)，这家伙的通道数有48，80，96条，这样4块GPU就不用分到2个区域了，而且它可以让机器上GPU的数据传输更快！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5d9371405090e491c0a1ba477119b20d.jpg" data-rawwidth="567" data-rawheight="178"&gt;&lt;h2&gt;产品1： Deep Learning Dev Box&lt;/h2&gt;&lt;p&gt;废话不多说了，直接上架构图……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b499f46fc46e4aa7221eacee961e8e5e.jpg" data-rawwidth="1007" data-rawheight="513"&gt;我们略去浮华的产品介绍，我们的产品加上深度学习软件包，心动价只卖8999刀！8999你买不了吃亏，8999你买不了上当！下面直接看下数据：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
      GPU0  GPU1  GPU2  GPU3  CPU Affinity
GPU0  X    PIX   PHB   PHB   0-11
GPU1  PIX   X    PHB   PHB   0-11
GPU2  PHB   PHB   X    PIX   0-11
GPU3  PHB   PHB   PIX   X    0-11
 

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="text"&gt;Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   26.13  20.31  20.32
     1  25.97     X   20.31  20.32
     2  20.32  20.32     X   26.12
     3  20.32  20.32  26.12     X&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;可以看到，这数据好得吓人，要知道我们距离理论极限32GB/s已经不远了。再看看时延：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   4.15   6.10   6.27   6.05
     1   6.10   4.13   6.12   6.00
     2   6.31   5.96   4.19   6.04
     3   6.07   5.97   6.15   4.09

&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;在数据面前一切语言都显得那么的苍白，不要犹豫，赶紧订购吧！&lt;/p&gt;&lt;h2&gt;产品2，3，4&lt;/h2&gt;&lt;p&gt;你以为我还会一个挨一个的翻译这些产品介绍？想看就去看原文吧[微笑][微笑]&lt;/p&gt;&lt;h2&gt;总结（非翻译）&lt;/h2&gt;&lt;p&gt;抛开广告的环节，这篇文章很好地向我们解释了多GPU间数据传输这个问题。这个问题总体上算是个硬件层次的问题，那么对于软件方面，我们能做些什么呢？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;让你的CPU和GPU在同一个区域内，减少跨区域的数据拷贝传输&lt;/li&gt;&lt;li&gt;如果涉及到跨区域的GPU数据传输，尽量减少传输的次数，不然真的有可能会让程序慢下来&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;带着这两个结论，我们就可以更好地去看Caffe多卡训练中的一些问题了。&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 10 Aug 2016 00:01:01 GMT</pubDate></item></channel></rss>