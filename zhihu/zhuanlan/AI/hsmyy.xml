<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Sun, 28 Aug 2016 20:15:56 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>CNN数值——ZCA</title><link>https://zhuanlan.zhihu.com/p/22148777</link><description>前面我们已经讲了很多有关参数合并的事情，反倒忘了介绍有关输入数据的事情，下面就来介绍一下对输入数据的初始化算法。&lt;p&gt;在Caffe的网络描述中，data layer的配置中有一项是用于配置mean_file，也就是数据的平均数值，在计算中每个数据在进入网络计算前会减去mean_file，以确保数据的整体均值为0，这样对于训练数据会更有帮助。&lt;/p&gt;&lt;p&gt;那么除了减去均值之外，还有什么初始化的方法呢？ZCA就是其中比较经典的初始化算法之一。&lt;/p&gt;&lt;h2&gt;Zero Component Analysis&lt;/h2&gt;&lt;p&gt;我们用一个例子来讲述这个初始化算法的过程。首先我们利用随机算法生成一个数据集。为了节目效果我们的数据集只有2维，而且两个维度之间还有相关关系。生成数据的代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;x = np.random.randn(200)
y = x * 2
err = np.random.rand(200) * 2
y += err
data = np.vstack((x,y))
plt.scatter(data[0,:], data[1,:])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的数据绘图的结果如下所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/69f28212ae5b64fbc3ca12fb9b1795d9.png" data-rawwidth="373" data-rawheight="255"&gt;我们可以求出两个特征的均值，再让全体数据减去均值，使得整体数据的均值为0：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;mean = np.mean(data, axis=1)
data -= mean.reshape((mean.shape[0],1))
plt.scatter(data[0,:], data[1,:])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ab025e16e33d313abacaeaf3b85c1741.png" data-rawwidth="375" data-rawheight="257"&gt;下面才是ZCA的关键部分。我们都知道训练数据中有时会出现特征之间相互关联的问题，对于图像数据，相互关联的问题则更为严重。虽然卷积层可以通过学习来解决这些局部相关性，但是通过学习来得到总是不够直接，如果直接对输入数据进行操作来解决一些数据相关性问题，一定会让训练更容易些。&lt;/p&gt;&lt;p&gt;为了解决数据相关问题，我们希望不同特征之间的协方差能够控制在一定范围内，我们首先来计算一下上面数据的协方差，由于我们的数据已经减去了均值，那么现在数据的均值为0，计算协方差就简化成了如下的计算：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;conv = np.dot(data, data.T) / (data.shape[1] - 1)
print conv
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[ 0.88200859  1.80316947]
 [ 1.80316947  4.033871  ]]&lt;/code&gt;&lt;/pre&gt;可以看出协方差是存在的……当然，我们在构造数据的时候就已经设置了这种相关性，所以看到这样的结果并不奇怪。下面我们就要用ZCA的方法减小协方差，我们的目标是：&lt;blockquote&gt;让每个特征自身的方差变为1，让特征之间的协方差变为0。&lt;/blockquote&gt;&lt;p&gt;为了达到这个效果，我们可以给每个输入数据做一次线性变换，使最终的结果满足我们所设定的效果。那么就有：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;# 如果有数据矩阵x，那么我们要寻找一个线性变换矩阵W，满足
Y = np.dot(W, X)
# 且
np.dot(Y, Y.T) / (Y.shape[1] - 1) == np.eye(Y.shape[0])&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了达到这个目标，我们首先做如下的假设&lt;/p&gt;&lt;p&gt;线性变换矩阵W是一个对称矩阵：&lt;equation&gt;W=W^T&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;我们的目标是令&lt;equation&gt;YY^T=(n-1)I&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;于是有：&lt;/p&gt;&lt;equation&gt;WXX^TW^T=(n-1)I&lt;/equation&gt;&lt;equation&gt;W^TWXX^TW^T=(n-1)W^T&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;W^2XX^TW^T=(n-1)W^T&lt;/equation&gt;，同时去掉左右两式右边的&lt;equation&gt;W^T&lt;/equation&gt;，有&lt;/p&gt;&lt;equation&gt;W^2XX^T=(n-1)I&lt;/equation&gt;,我们假设X不全为0，那么&lt;equation&gt;XX^T&lt;/equation&gt;就是一个正定矩阵，满足可逆性（这里也需要些推导），所以：&lt;equation&gt;W^2=(n-1)(XX^T)^{-1}&lt;/equation&gt;，&lt;equation&gt;W=\sqrt{n-1}(XX^T)^{-1/2}&lt;/equation&gt;&lt;p&gt;由于&lt;equation&gt;XX^T&lt;/equation&gt;是一个对称矩阵，对称矩阵具有一个特性。我们先求出&lt;equation&gt;XX^T&lt;/equation&gt;的特征值和特征向量：&lt;/p&gt;&lt;equation&gt;XX^TS=S\Lambda &lt;/equation&gt;&lt;p&gt;对称矩阵具有一个特性，它的特征向量可以构成一个标准正交矩阵，根据标准正交矩阵的特性，于是我们可以得到：&lt;/p&gt;&lt;equation&gt;XX^T=S\Lambda S^T&lt;/equation&gt;（关于这个定理，我们可以等后续进行证明，在此先直接使用）&lt;p&gt;继续推导，可以得到：&lt;/p&gt;&lt;equation&gt;(XX^T)^{-1/2}=(S\Lambda S)^{-1/2}=S\Lambda^{-1/2} S&lt;/equation&gt;（这里其实也稍有跳跃，再后续证明）&lt;p&gt;于是最终的&lt;equation&gt;W=\sqrt{n-1}S\Lambda^{-1/2} S&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;实际上上面的推导还缺少一些细节，比如对称矩阵相关的一些性质，对于这一部分的详细无脑推导我们之后可以再详细叙述。以下是根据定义对应的代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;# 由于conv中已经除掉了1/(Y.shape[1] - 1),所以后面的计算中我们将不再去除它
eig_val, eig_vec = np.linalg.eig(conv)
S_sqrt = np.sqrt(np.diag(eig_val))
W = np.dot(eig_vec, np.dot(np.linalg.inv(S_sqrt), eig_vec.T))
print W
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果得到&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[ 3.37642486 -1.32714676]
 [-1.32714676  1.05662959]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在我们得到了W，就可以进行线性变换，可以得到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Y = np.dot(W, data)
plt.scatter(Y[0,:], Y[1,:])
conv2 = np.dot(Y, Y.T) / (data.shape[1] - 1)
print conv2
&lt;/code&gt;&lt;/pre&gt;相对应的结果为&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/fb14365d656ababf37caeda147b0f15c.png" data-rawwidth="369" data-rawheight="252"&gt;&lt;p&gt;此时对应的协方差为&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[[  1.00000000e+00  -1.29433036e-16]
 [ -1.29433036e-16   1.00000000e+00]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;无论从图像结果还是协方差的数值结果上看，ZCA都完成了我们想要的目标。以上的ZCA算法推导来自论文《Learning Multiple Layers of Features from Tiny Images》的附录。&lt;/p&gt;&lt;p&gt;ZCA初始化在一些经典的数据集（比方说cifar10）已经得到验证，采用这样的初始化可以得到更好地训练精度。不妨动手一试吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;</description><author>冯超</author><pubDate>Fri, 26 Aug 2016 23:48:40 GMT</pubDate></item><item><title>CNN——L1正则的稀疏性</title><link>https://zhuanlan.zhihu.com/p/22099871</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;这一回我们把目光转向正则化。Caffe中提供了两种正则化：L2和L1，也是大家最耳熟能详的正则化算法了。从刚开始学习机器学习开始，有关经验风险和结构风险，bias and variance，过拟合等一系列的概念都和正则化有着密切的关系。&lt;p&gt;在当年经典的机器学习课本中，大师们经常用这样一张图来教导我们：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/045228bde397817c9b2f1588cdcbd32a.png" data-rawwidth="623" data-rawheight="367"&gt;（图片来自网络，侵删）&lt;/p&gt;&lt;p&gt;从上面这张图可以看出，损失函数的主体是一个凸函数，它的等高线均匀地向外扩散。因此正方形的L1正则更容达到参数的稀疏性，而圆形的L2正则则不太容易达到这个效果。所以多年以来，大家一直在心中默默记住：L1可以达到参数稀疏化的效果。而且，在浅层模型中，大多数情况下L1的稀疏效果还是不错的。当然，理论上依然存在使用L1正则却无法达到稀疏效果的情况，这个比较特殊就不细说了。&lt;/p&gt;&lt;p&gt;当然，如果要详细讲述L1正则相关的故事，恐怕相当于又打开了一个新天地。恐怕其中所涉及到的各位大师前辈的精妙算法又是数不胜数。关于这些话题有时间我们慢慢再聊，现在我们来看看L1在CNN上的表现。&lt;/p&gt;&lt;h2&gt;MNIST的L1正则实验&lt;/h2&gt;&lt;p&gt;我们这一次同样采用MNIST作为实验。训练数据和测试数据一切正常，所采用的模型也是我们之前所提到的那个正常的模型，非线性部分采用ReLU。唯一不同的是我们讲正则化的方法改为L1。我们依然使用Caffe进行训练，经过10000轮的训练，我们得到了如下的精确率：&lt;/p&gt;&lt;p&gt;0.9766&lt;/p&gt;&lt;p&gt;看上去比L2正则在10000轮训练后的精度0.9912要差一些啊，宝宝不开心。&lt;/p&gt;&lt;p&gt;既然精度已经差一些了，那么我们看看L1的正则在稀疏性上的表现。这时候我们要拿L2正则训练出的参数和L1正则训练的参数进行比较，比较的方式也是直接利用参数的数值去看他们的数据分布情况：&lt;/p&gt;&lt;p&gt;首先是L1正则的两张图，我们只关注极小的某个区间下的参数直方图，首先是conv1层的参数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d9e22ba7abae62b92e0986f3d9d210c5.png" data-rawwidth="769" data-rawheight="328"&gt;可以看出在1e-7的范围下，没有参数等于0。然后是conv2层：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e968f88f617be05f808e2ca969a9c9e6.png" data-rawwidth="764" data-rawheight="322"&gt;在1e-9的范围下，只有一个参数等于0。然后是L2正则的参数结果，首先是conv1层：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a813f692162db3f893f7dbf539fb50e7.png" data-rawwidth="775" data-rawheight="310"&gt;&lt;p&gt;然后是conv2层：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0e42cbabf70bc52881422d71407ddb5e.png" data-rawwidth="783" data-rawheight="315"&gt;&lt;p&gt;从分布中可以看出，L2参数没有像L1那样特别小的参数，L1参数确实比L2更靠近0，但是并没有到达0这个地方。&lt;/p&gt;&lt;p&gt;这是为什么呢？&lt;/p&gt;&lt;p&gt;首先，CNN的参数和Loss关系并不是凸函数，而教科书上讲的例子都是凸函数，所以从这个初始条件来看就不一样，所以最终的结果也自然不一样。对于非凸的函数，情况比上面的图复杂的多。&lt;/p&gt;&lt;p&gt;其次，Caffe中求解L1的算法使用的是最基础的subgradient descent，这种方法在求解的绝对稀疏性上是有劣势的，对于这个问题，我们可以举一个简单的例子做展示。&lt;/p&gt;&lt;p&gt;我们可以简单地列出subgradient_descent的python代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def subgradient_descent(x, grad, lr, lambda):
    x -= lr * (grad + lambda * sign(x))&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们初始化一个小量的数据集，用于L1正则的训练：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def genData(n, p, s):
    A = np.random.normal(0, 1, (n,p))
    opt_x = np.zeros(p)
    random_index_list = random.sample(range(p), s)
    for i in random_index_list:
	opt_x[i] = np.random.normal(0,10)
    e = np.random.normal(0,1,n)      
    b = np.dot(A,opt_x.T) + e.T
    return A, b

A, b = genData(100, 50, 20)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的代码中理想的最优参数共有50维，其中有30维都被设置为0。我们希望subgradient的优化算法可以学到这样的稀疏性。&lt;/p&gt;&lt;p&gt;不过实际情况是，当训练结束时，没有一个参数真正等于0，只是接近0。&lt;/p&gt;&lt;p&gt;然而我们采用其他的subgradient算法时，参数可以优化到0。（比方说proximal gradient descent）&lt;/p&gt;&lt;p&gt;举一次优化的结果为例，理想参数如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;opt_x 
[  0.00000000e+00   0.00000000e+00  -8.89068011e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   9.79677398e+00   0.00000000e+00  -1.53016548e+01
   1.01055968e+01   2.08989507e+01   0.00000000e+00  -4.16464326e+00
   1.76916005e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.00900104e+00
  -9.35102012e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00  -7.13791761e+00   0.00000000e+00   0.00000000e+00
   0.00000000e+00  -2.79487453e+01   3.11610679e+00   0.00000000e+00
   0.00000000e+00   2.14217260e+00   6.00126451e+00   6.58569031e+00
   4.52257852e+00   0.00000000e+00  -2.68944021e-02   0.00000000e+00
   0.00000000e+00   0.00000000e+00  -5.51801700e+00   1.31847462e+00
  -1.81286404e+01   0.00000000e+00]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;subgradient给出的结果是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;subgradient_process
[  1.71781432e-07  -1.47628627e-07  -8.74652150e+00  -1.72402237e-07
   5.39554174e-08   2.85199917e-07   2.09696332e-07   5.44626181e-08
   1.42748391e-07   9.09983860e+00  -1.34269176e-07  -1.48617035e+01
   9.73235939e+00   2.00436077e+01   2.01599098e-07  -4.07292753e+00
   1.69462946e+01   2.24365336e-01  -1.00920339e-04  -5.21839170e-03
  -9.68581468e-04  -1.93440885e-07   3.31274645e-01   4.36451104e+00
  -9.05097771e-01  -1.58529600e-07   9.90021826e-08  -2.60871187e-01
   1.40025608e-07  -6.60147937e+00  -7.96634585e-05   1.24702253e-07
   4.38641043e-08  -2.70520316e+01   2.73945615e+00   2.80055535e-07
   2.02949781e-07   2.52015283e+00   5.89734589e+00   6.33872176e+00
   4.64813524e+00   1.17230844e-07  -1.24874200e-01  -2.53462109e-02
   3.01514888e-07   2.72601191e-07  -5.18595641e+00   1.24439798e+00
  -1.77537445e+01  -3.96211798e-08]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而proximal gradient descent给出的是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;proximal_gradient_process
[ -0.           0.          -8.65370345   0.           
   0.           0.           0.           0.           
   0.           9.30621127  -0.         -15.07056666
   9.76129302  20.45180052   0.          -4.11430611  
  17.16825959   0.          -0.         -0.          
  -0.          -0.           0.           4.55458453
  -0.81317833   0.          -0.          -0.07547525  
  -0.          -6.73065609  -0.          -0.          
  -0.         -27.54904074   2.87046799   0.           
   0.           2.23054671   5.8947849    6.36406299   
   4.61893832  -0.          -0.          -0.
   0.           0.          -5.26100017   1.19033131 
 -17.77335708  -0.        ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从稀疏性的效果来看一目了然。&lt;/p&gt;&lt;p&gt;那么在Caffe中想利用L1正则来获得稀疏性是不太可能的。当然，还有一个更重要的问题，L1的稀疏性真的是我们需要的么？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;564533376&lt;/p&gt;&lt;p&gt;——现在加入“我爱机器学习”QQ群，开启机器学习开挂的人生~&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 24 Aug 2016 22:06:45 GMT</pubDate></item><item><title>聊点轻松的2——斗图篇</title><link>https://zhuanlan.zhihu.com/p/22112582</link><description>非常感谢各位看官的支持和鼓励！不知不觉我的“无痛的机器学习”专栏即将迎来第2000个看官。22天前，我写了“聊点轻松的”系列篇1，纪念了1000个小伙伴的达成之日，今天就来提前庆祝下2000小伙伴这个milestone。（发文时已经破2000了啊～）&lt;p&gt;感谢的话有很多，但是专栏毕竟不是颁奖典礼，没有干货的煽情鸡汤文还是留到适合的场合去说吧。今天我们来看一个开源代码的实验效果——当然，其实重点不是去看开源代码，而是去看看ImageNet图片库里的奇葩图给大家轻松一下。&lt;/p&gt;&lt;p&gt;首先我们来正经一下，这篇论文叫《Visualizing and Understanding Convolutional Networks》，是一篇讲述CNN模型可视化相关的论文。论文的具体内容后面有机会可以细聊，今天单单看一下它的实验结果——也就是前向计算得到的feature经过反向传播会变成什么样子。&lt;/p&gt;&lt;p&gt;（以下图片一律侵删）&lt;/p&gt;&lt;p&gt;下面请擦亮眼睛，因为亮点实在太多了！（以下是手纸系列）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/978ca34ecc06f2310230e61caeaefd8c.jpg" data-rawwidth="454" data-rawheight="227"&gt;这干净的背景，销魂的表情，手纸，啤酒……感觉我的脑中已经有画面了……（突然一本正经地）旁边的reconstruction的主要目标是人脸和手纸，说明你已经成功引起了模型的注意。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/63ae9422588c51dcffc99e7e8519f2f1.jpg" data-rawwidth="454" data-rawheight="227"&gt;这么有内涵的手纸，难怪CNN只顾着捕捉地图信息了，万一上面有藏宝图呢！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6c4832ff234bf29a5aa26944c7155521.jpg" data-rawwidth="454" data-rawheight="227"&gt;这真是有故事的一群人啊，你看CNN都看不过来了，满屏都是重点！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/198096b39f3e88cb0eb44ea21e932d58.jpg" data-rawwidth="454" data-rawheight="227"&gt;什么叫亲民的总统？笑口常开啊！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1a3e4d39024734b62c957fded059bf41.jpg" data-rawwidth="454" data-rawheight="227"&gt;世界不是缺少美，而是缺少——可以发现美的手纸眼镜……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ba2a0e60660b536d4a46222d769f1c1a.jpg" data-rawwidth="454" data-rawheight="227"&gt;呃……我不知道如何用语言形容了……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0b4653051b88a14017892279500d985b.jpg" data-rawwidth="454" data-rawheight="227"&gt;呃……嗯……这个……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/aa20d8f10e5c2f70993adb2dde2862ff.jpg" data-rawwidth="454" data-rawheight="227"&gt;这个广告语还不错嘛，挺有创意的……&lt;/p&gt;&lt;p&gt;还有更多精彩图片，欢迎大家自行挖掘，以下就是ImageNet中第1000类——toilet tissue, toilet paper, bathroom tissue中的精彩图片。我们的模型就是要从这些逗逼的图片中认出主题来，也怪难为它们的了……&lt;/p&gt;&lt;h2&gt;今天没有私货～&lt;/h2&gt;</description><author>冯超</author><pubDate>Sun, 21 Aug 2016 19:45:22 GMT</pubDate></item><item><title>CNN数值——xavier（下）</title><link>https://zhuanlan.zhihu.com/p/22044472</link><description>上回说到我们从前向的方向推导，发现了这些0均值的随机变量在计算过程中会产生方差扩散的问题，我们并且从前向的方向给出了解决的办法。既然在刚才的句子中我们反复提到了前向这两个字，那肯定是在别有用心地告诉大家——还有后向呗。&lt;p&gt;后向的计算公式其实和前向类似，忘记的可以顺便去前面的文章中回顾一下（顺便顺手点个赞啊～），这里我们还是用比较抽象的方式去表示，假设我们还是一个k层的网络，现在我们得到了第k层的梯度&lt;equation&gt;\frac{\partial Loss}{\partial x^k}&lt;/equation&gt;,那么对于第k-1层输入的梯度，有&lt;/p&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x^{k-1}_j}=\sum_{i=1}^n{\frac{\partial Loss}{\partial x^k_i}*w^k_j}&lt;/equation&gt;&lt;p&gt;好了，这个公式的精髓还是在意不在形，也就是说，K-1层一个数值的梯度，相当于上一层的n个参数的乘加。这个n个参数的计算方式和之前方式一样，只是表示了输出端的数据维度，在此先不去赘述了。&lt;/p&gt;&lt;p&gt;然后我们又到了反向传播到非线性函数的地方了，时间一长洗脑可能会失效，让我们再次催眠自己，想象非线性函数像线性函数一样飘过，飘过……&lt;/p&gt;&lt;p&gt;于是我们如果假设每一层的参数也服从某种均值为0，方差为某值的分布，利用这种来自东方的神秘力量，我们又可以写出一个神奇的公式：&lt;/p&gt;&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{k-1}_j})=n^k*Var({\frac{\partial Loss}{\partial x^k_i})*\sigma_w^k}&lt;/equation&gt;&lt;p&gt;于是乎，对于这个k层网络，我们又可以推导出一个神奇的公式：&lt;/p&gt;&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{1}_j})=Var({\frac{\partial Loss}{\partial x^k_i})*\prod_{i=1}^{k-1}n^i*\sigma_w^i}&lt;/equation&gt;&lt;p&gt;好了，上次我说过的话不会重复再说一遍了。这次我们考虑后向操作是为了什么呢？前面我们前向传播data，我们做到了数值的稳定，现在反向传播如果不能做到同样的数值稳定，那么被diff更新过的data不再服从这种神奇的力量怎么办？要命了。&lt;/p&gt;&lt;p&gt;所以为了服从神奇的力量，我们又可以得到：&lt;/p&gt;&lt;p&gt;为了&lt;equation&gt;Var(\frac{\partial Loss}{\partial x^{k-1}_j})=Var({\frac{\partial Loss}{\partial x^k_i}})&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^k}&lt;/equation&gt;&lt;p&gt;咦，好像我们两次推导得到了同样的结果，大功告成了？如果仔细看一下这两个公式，我们就会发现两个n实际上不是同一个n。对于全连接来说，前向操作时，n表示了输入的维度，而后向操作时，n表示了输出的维度。而输出的维度也可以等于下一层的输入维度。所以两个公式实际上可以写作：&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^k}&lt;/equation&gt;&lt;equation&gt;\sigma^k_w=\frac{1}{n^{k+1}}&lt;/equation&gt;&lt;p&gt;这么看上去前向后向不是很统一啊，但是大功快要告成，怎么也得再糊弄一回了，于是我们把两个公式揉合以下，就成了：&lt;/p&gt;&lt;equation&gt;\sigma^k_w=\frac{2}{n^{k+1}+n^k}&lt;/equation&gt;&lt;p&gt;下面就是对这个方差的具体使用了。没错，前辈思来想去，决定使用均匀分布进行初始化，我们设定我们要初始化的范围是[-a,a]。熟悉均匀分布和不熟悉均匀分布的各位，都可以看一下上述的范围下，均匀分布的方差为：&lt;/p&gt;&lt;equation&gt;Var(uniform)=\frac{(a-(-a))^2}{12}=\frac{a^2}{3}=\sigma^k_w&lt;/equation&gt;&lt;p&gt;将上面两个公式合并一下，就可以得到：&lt;/p&gt;&lt;equation&gt;a=\sqrt{\frac{6}{n^{k+1}+n^k}}&lt;/equation&gt;&lt;p&gt;于是，我们的xavier初始化方法横空出世，那就是把参数初始化成&lt;equation&gt;[-\sqrt{\frac{6}{n^{k+1}+n^k}},\sqrt{\frac{6}{n^{k+1}+n^k}}]&lt;/equation&gt;范围内的均匀分布。&lt;/p&gt;&lt;p&gt;看完了这段晕晕忽忽地演绎，再看看最终的结果，和源代码，有没有一种搞了半天就弄出点这的感觉？&lt;/p&gt;&lt;p&gt;没错，这个初始化的公式不难，但是想这样推导出来还是让前辈们付出了巨大的心血。后人在使用这个初始化方法的时候，理所当然地使用了这些方法，但是很少去理会这些推导背后的真正含义。&lt;/p&gt;&lt;p&gt;虽然前面用了大量戏谑的语言来说明一些假设的不合理性，但是如果没有这些假设，我们也无法得出这样精彩而且实用的结论。其实数学模型的世界经常就是会用到一些抽象这件工具，只有把一些不太好把握的地方抽象掉，才能更容易地抓住事物的本质，找到事物的核心规律。所以在这里还是要由衷的给这个初始化算法的作者点个赞。&lt;/p&gt;&lt;h2&gt;向更远方前进&lt;/h2&gt;&lt;p&gt;如果熟悉Caffe源码的同学，在看到xavier的源码后，会看到下面还有一个类似结构的初始化方法——MSRAFiller。这个初始化方法来自《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》，不同的是，这篇文章的主要目标是基于ReLU的初始化算法，实际上它的推导过程和我们看过的xavier的方法类似，只是在一些细节处有所不同。如果你理解了xavier的演绎思想，不妨去看看这篇文章的推导过程，相信你会很轻松地理解这一路研究初始化算法的思路。总之，能够出现在应用中的算法都是经过一定实践检验的算法，已经被人证明了它在理论和实践上的可行性，是完全值得去深入了解的。&lt;/p&gt;&lt;p&gt;除了这两篇文章，还有很多大牛写了关于初始化的文章，以它们的角度讲述了它们心中初始化的样子。后面有时间我们还会继续去看这些文章，不过我们要暂时停下脚步了，因为还有在其他方向努力的前辈们要急着登场了，它们又会给我们带来一个全新的角度去理解CNN……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;h2&gt;私货时间2&lt;/h2&gt;&lt;p&gt;支持王宝强快速走出阴影！&lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 20 Aug 2016 23:21:22 GMT</pubDate></item><item><title>CNN数值——xavier（上）</title><link>https://zhuanlan.zhihu.com/p/22028079</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1"&gt;@Express&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/08cd1d56513335b13d6a93725db969ad" data-hash="08cd1d56513335b13d6a93725db969ad" class="member_mention" data-hovercard="p$b$08cd1d56513335b13d6a93725db969ad"&gt;@夏龙&lt;/a&gt;对本文的审阅。欢迎大家多多提出宝贵意见。&lt;/p&gt;上一回我们做了三个小实验。第一个是正常的实验，表现优异；第二个实验我们把初始化调整得很奇葩（为什么奇葩？），最终训练结果弱爆了；第三个实验我们把非线性函数重新换回sigmoid，模型奇迹般地回血，虽然表现不够完美，但也算是十分优异了。&lt;p&gt;于是ReLU被众人推到墙角，开始质问。&lt;/p&gt;&lt;p&gt;其实ReLU也是很委屈的，前面说过他的优势在与模型前向后向计算的过程中，它可以更好地传递数据，不会像sigmoid那样有梯度传递的问题，但是它又缺少了sigmoid的重要特性，那就是对数据的控制力。&lt;/p&gt;&lt;p&gt;我们知道sigmoid可以把任意维度的数据压缩到0到1之间，这是它最强力的一个特点。所以在使用sigmoid时，我们不用太担心数据的幅度问题，因为只要使用一个sigmoid，数据的幅度就会得到良好的控制（当然了，全是正数这件事其实也有点不太靠谱，要是像tanh那样有正有负就更好了）。而我们从上一次的实验中可以看出，采用ReLU的非线性函数，数据的维度完全没有得到控制。有的幅度到达了上千，有的依然是一个极小的小数。这说明ReLU在压缩数据幅度方面存在劣势。&lt;/p&gt;&lt;p&gt;于是乎我们有了以下的经验总结：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;sigmoid在压缩数据幅度方面有优势，对于深度网络，使用sigmoid可以保证数据幅度不会有问题，这样数据幅度稳住了就不会出现太大的失误。&lt;/li&gt;&lt;li&gt;但是sigmoid存在梯度消失的问题，在反向传播上有劣势，所以在优化的过程中存在不足&lt;/li&gt;&lt;li&gt;relu不会对数据做幅度压缩，所以如果数据的幅度不断扩张，那么模型的层数越深，幅度的扩张也会越厉害，最终会影响模型的表现。&lt;/li&gt;&lt;li&gt;但是relu在反向传导方面可以很好地将“原汁原味”的梯度传到后面，这样在学习的过程中可以更好地发挥出来。（这个“原汁原味”只可意会，不必深究）&lt;/li&gt;&lt;/ol&gt;这么来看，sigmoid前向更靠谱，relu后向更强。这么一看似乎一切又回到了起点，到底哪个非线性函数更好呢？&lt;p&gt;要评判哪个非线性函数更好，不但要看自己本身，还要看它们和整体模型阵型的搭配情况（BP很重要！）sigmoid在学习方面存在弱点，有什么办法能帮助它呢？这个我们后面再说。那relu的数据幅度呢？有没有什么办法能够帮助它解决呢？&lt;/p&gt;&lt;p&gt;众人想了好久，又重新看向了初始化……&lt;/p&gt;&lt;p&gt;初始化：（黑人问号脸）？&lt;/p&gt;&lt;h2&gt;xavier&lt;/h2&gt;&lt;p&gt;大家突然想起来，刚才和relu完美配合的那个初始化叫什么来着？哦对，xavier。我们就来看看这个初始化方法的由来。xavier诞生时并没有用relu做例子，但是实际效果中xavier还是和relu很搭配的。&lt;/p&gt;&lt;p&gt;xavier是如何完成初始化工作的呢？它的初始化公式如下所示：&lt;/p&gt;&lt;p&gt;定义参数所在层的输入维度为n，输出维度为m，那么参数将以均匀分布的方式在&lt;equation&gt;[-\sqrt{\frac{6}{m+n}},\sqrt{\frac{6}{m+n}}]&lt;/equation&gt;的范围内进行初始化。&lt;/p&gt;&lt;p&gt;那么这个公式是如何计算出来的呢？关于这个问题我们需要一段漫长的推导。在推导之前我们要强调一个关键点，就是参数的标准差，或者方差。前面我们提到了Caffe中的debug_info主要展示了数据的L1 norm，对于均值为0的数据来说，这个L1 norm可以近似表示标准差。&lt;/p&gt;&lt;p&gt;我们将用到以下和方差相关的定理：&lt;/p&gt;&lt;p&gt;假设有随机变量x和w，它们都服从均值为0，方差为&lt;equation&gt;\sigma&lt;/equation&gt;的分布，那么：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;w*x就会服从均值为0，方差为&lt;equation&gt;\sigma^2&lt;/equation&gt;的分布&lt;/li&gt;&lt;li&gt;w*x+w*x就会服从均值为0，方差为&lt;equation&gt;2*\sigma^2&lt;/equation&gt;的分布&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以下内容主要来自论文《Understanding the difficulty of training deep feedforward neural network》的理解，这里将以我个人的理解做一下解读，如果有错欢迎来喷。&lt;/p&gt;&lt;p&gt;前面两个定理的变量名称是不是有点熟悉？没错，下面我们说的就是参数w和x。这里暂时将偏置项放在一边，同时我们还要把一个部分放在一边，那就是非线性部分。这篇论文心目中的理想非线性函数是tanh。为啥呢？&lt;/p&gt;&lt;p&gt;在大神的假想世界中，x和w都是靠近0的比较小的数字，那么它们最终计算出来的数字也应该是一个靠近0，比较小的数字。我们再看一眼tanh函数和它对应的梯度函数：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/9b7de17962ef31509e41957d0f318042.png" data-rawwidth="378" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4657a64ed40e2a4de13e4b0a4fc01342.png" data-rawwidth="375" data-rawheight="259"&gt;这两张图有点大，不过可以看出来，如果数值集中在0附近，我们可以发现，前向时tanh靠近0的地方斜率接近1，所以前辈告诉我们，把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;下面这张梯度的图像也是一样，靠近0的地方斜率接近1，所以前辈又一次告诉我们，把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;什么，你不信？&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数。&lt;/p&gt;&lt;p&gt;把它想象成一个线性函数……&lt;/p&gt;&lt;p&gt;好了，现在这个挡在中间的非线性函数硬生生掰成一个线性函数了，为了理论的完美我们也是什么也不顾了。下面就要面对一个问题，如何让深层网络在学习过程中的表现像浅层网络？&lt;/p&gt;&lt;p&gt;我们的脑中迅速回忆起我们接触过的浅层模型——logistic regression，SVM。为了它们的表现能够更好，我们都会把特征做初始化，细心处理，比方说做白化处理，使他的均值方差保持好，然后用浅层模型一波训练完成。现在我们采用了深层模型，输入的第一层我们是可以做到数据的白化的——减去均值，除以一个标准差。但是里面层次的数据，你总不好伸手进入把它们也搞白化吧！（当然，后来真的有人伸进去了，还做得不错）那我们看看如果在中间层不做处理会发生什么？&lt;/p&gt;&lt;p&gt;我们假设所有的输入数据x满足均值为0，方差为&lt;equation&gt;\sigma_x&lt;/equation&gt;的分布，我们再将参数w以均值为0，方差为&lt;equation&gt;\sigma_w&lt;/equation&gt;的方式进行初始化。我们假设第一次是大家喜闻乐见的卷积层，卷积层共有n个参数（n=channel*kernel_h*kernel_w），于是为了计算出一个线性部分的结果，我们有：&lt;/p&gt;&lt;equation&gt;z_j=\sum^n_i{w_i*x_i}&lt;/equation&gt;&lt;p&gt;这个公式的下标不准确，大家姑且这么看了，也就是说，线性输出部分的一个结果值，实际上是由n个乘加计算出来的，那么下面是一道抢答题，按照我们刚才对x和w的定义，加上前面我们说过的两个方差计算公式，这个z会服从一个什么分布呢？&lt;/p&gt;&lt;p&gt;均值肯定还是0嘛，没得说。&lt;/p&gt;&lt;p&gt;方差好像积累了一大堆东西：&lt;equation&gt;n*\sigma_x*\sigma_w&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;然后我们通过那个靠意念构建的具有“线性特征”的非线性层，奇迹般地发现一切都没有变化，那么下一层的数据就成了均值为0，方差为&lt;equation&gt;n*\sigma_x*\sigma_w&lt;/equation&gt;的“随机变量”（姑且称之为随机变量吧）。&lt;/p&gt;&lt;p&gt;为了更好地表达，我们将层号写在变量的上标处，于是就有：&lt;/p&gt;&lt;equation&gt;\sigma^2_x=n^1*\sigma^1_x*\sigma^1_w&lt;/equation&gt;&lt;p&gt;我们将卷积层和全连接层统一考虑成n个参数的一层，于是接着就有：&lt;/p&gt;&lt;equation&gt;\sigma^3_x=n^2*\sigma^2_x*\sigma^2_w&lt;/equation&gt;&lt;p&gt;如果我们是一个k层的网络（这里主要值卷积层+全连接层的总和数），我们就有&lt;/p&gt;&lt;equation&gt;\sigma^k_x=n^{k-1}*\sigma^{k-1}_x*\sigma^{k-1}_w=n^{k-1}*n^{k-2}*\sigma^{k-2}_x*\sigma^{k-2}_w*\sigma^{k-1}_w&lt;/equation&gt;&lt;p&gt;继续把这个公式展开，就会得到它的最终形态：&lt;/p&gt;&lt;equation&gt;\sigma^k_x=\sigma^1_x*\prod_{i=1}^{k-1}n^i*\sigma^i_w &lt;/equation&gt;&lt;p&gt;可以看出，后面的那个连乘实际上看着就像个定时炸弹（相信看到这，我应该能成功地吸引大家的注意力，帮助大家把非线性函数线性化的事情忘掉了……），如果&lt;equation&gt;n^i*\sigma^i_w&lt;/equation&gt;总是大于1，那么随着层数越深，数值的方差会越来越大，反过来如果乘积小于1，那么随着层数越深，数值的方差就会越来越小。&lt;/p&gt;&lt;p&gt;越来越大，就容易Hold不住导致溢出，越来越小，就容易导致数据差异小而不易产生有力的梯度。这就是深层模型的一大命门。&lt;/p&gt;&lt;p&gt;公式推到这里，我们不妨回头看看这个公式：&lt;/p&gt;&lt;equation&gt;\sigma^2_x=n^1*\sigma^1_x*\sigma^1_w&lt;/equation&gt;&lt;p&gt;你一定会有这样一个想法（一定会有！），如果&lt;equation&gt;\sigma_x^2=\sigma_x^1&lt;/equation&gt;，接着我们保证每一层输入的方差都保持一致，那么数值的幅度不就可以解决了么？于是乎：&lt;/p&gt;&lt;equation&gt;\sigma^1_w=\frac{1}{n^1}&lt;/equation&gt;&lt;p&gt;我们用均值为1，方差为上式的那个数字做初始化，不就可以解决了？&lt;/p&gt;&lt;p&gt;不错，从理论上讲是这个思路，不过，这只是这个思路的开始……&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;p&gt;想获得更多机器学习相关的干货？欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;&lt;p&gt;（其实重要的事情也可以说两遍，吼吼～）&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 17 Aug 2016 20:40:49 GMT</pubDate></item><item><title>CNN的数值实验</title><link>https://zhuanlan.zhihu.com/p/22027076</link><description>前面我们聊过了Caffe的整体架构，相信各位对Caffe的结构已经比较熟悉了，下面我们就来看看CNN中的一些细节问题，也顺着前面的思路进一步进行。&lt;h2&gt;序&lt;/h2&gt;&lt;p&gt;在好久之前介绍ReLU的时候，我们曾经提到ReLU相比于Sigmoid的一些优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sigmoid整体的梯度偏小，在外向传导的过程中会不断让梯度的幅度变小；&lt;/li&gt;&lt;li&gt;Sigmoid存在一个梯度饱和问题：那就是如果非线性部分的输出靠近0或者1，那么反向传导到了这里，Sigmoid的梯度会接近于0，这样就会导致梯度到这里就传导不下去了。从前面我们的那张图中可以看得出来。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有了这两个原因，我们认为ReLU这样的非线性函数在前向后向的传递性方面效果更好，如果CNN的网络层数比较深，ReLU更能够保证梯度的无损传递。&lt;/p&gt;&lt;p&gt;当我们的前辈完成了这个重大发现之后，大家都觉得“这次终于稳了”，AlexNet也横空出世，让CNN大放异彩。那个时候AlexNet已经算是非常"Deep"的网络了……&lt;/p&gt;&lt;p&gt;可是后来，大家对CNN模型能力的诉求越来越高，大家通过不断地实验，慢慢发现，现在的模型还不够深，想要模型够牛逼，模型的深度还得继续增加啊！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1049017686e7ebefdae7367397fa052e.jpg" data-rawwidth="296" data-rawheight="220"&gt;可是随着后面模型的深度不断增加，我们发现模型的训练过程变得越来越不可控，有时候一个不留神模型就训练溢出了，有时候模型训练着训练着就不动了。训练模型也变得越来越像炼丹了……&lt;/p&gt;&lt;p&gt;炼丹不是一件让人开心的事情，能炼好固然让人开心，但是炼不好就会让宝宝有情绪了。于是各位前辈又重新出发，试图找出新的方法解决这些恼人的问题。&lt;/p&gt;&lt;p&gt;当年众位大神齐聚一堂，开始讨论究竟什么样招式能破解这个困局。有的大神选择从数值的方向入手，有的大神选择从模型结构的方向入手，有的大神选择从模型结构入手，还有的大神走了其他的方法。一场“拯救大兵CNN”的好戏也由此上演……&lt;/p&gt;&lt;h2&gt;一个数值的小实验&lt;/h2&gt;&lt;p&gt;好了，让我们回到MNIST这种比较简单的问题上。我们也曾经展示过Caffe中MNIST的例子，以下这个模型是我们曾经用过的模型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Conv 20*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Conv 50*5*5 - ReLU - Pooling2*2,stride2&lt;/li&gt;&lt;li&gt;Ip  500 - ReLU&lt;/li&gt;&lt;li&gt;Ip 10&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这一次我们将debug_info打开，记录一下网络中每一层数据的数值信息。Caffe中的debug_info默认可以给出所有数据的L1 norm，也就是绝对值的和。这个数值可以在一定程度上反映数据的幅度。如果数据比较大，那么说明整体的数值比较大。&lt;/p&gt;&lt;p&gt;完成了10000轮模型的训练，模型最终在test集合得到了0.991的精度，可以说精度已经很高了。那么各个层的数据表现如何呢？这里我们主要关注以下的数值：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;两个conv层和两个ip层的top data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的data信息&lt;/li&gt;&lt;li&gt;两个conv层和两个ip层参数w的diff信息&lt;/li&gt;&lt;li&gt;所有参数的data和diff的L1 norm和L2 norm&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先是top data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8397d75b85e7c1af80e554bf62ff6e6a.png" data-rawwidth="918" data-rawheight="433"&gt;&lt;p&gt;可以看出除了一开始的数值波动，后面的迭代中数值整体表现比较稳定，稳中有升。&lt;/p&gt;&lt;p&gt;其次是w的data&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/dc0702632a1d330e7919ce4ce414f281.png" data-rawwidth="917" data-rawheight="437"&gt;&lt;p&gt;可以看出w整体表现不算稳定，不过考虑整个优化的过程就是在改变它，所以它的大幅波动也是可以理解的。&lt;/p&gt;&lt;p&gt;接下来是w的diff&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/72b4e4e7458586f86af667ea2262e1d4.png" data-rawwidth="919" data-rawheight="435"&gt;&lt;p&gt;可以看出diff在数值上表现得也比较稳定，基本上处于一个小的区间之中。&lt;/p&gt;&lt;p&gt;最后是L1 norm和L2 norm&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7ebf6288c1845e5a000dfaae4c8e37f3.png" data-rawwidth="914" data-rawheight="428"&gt;&lt;p&gt;norm和diff的表现总体上和前面展现的一致。&lt;/p&gt;&lt;p&gt;总体来说，topdata和w的diff数值没有太大的波动，保持在一个稳定幅度上，所以整体的数值比较稳定，无论是前向的top data还是后向的diff data没有出现太大的数值波动或者数值幅度问题。&lt;/p&gt;&lt;p&gt;看完了这个成功案例，我们再看看失败案例，当然这个失败案例一般不会出现，只是做个效果而已，不过这个效果也足以说明一些问题。&lt;/p&gt;&lt;p&gt;这个失败案例直接来自于刚才的例子。我们在刚才的模型上只修改一个地方：四个核心层（conv1,conv2,ip1,ip2）的w参数的初始化方法。&lt;/p&gt;&lt;p&gt;原来的初始化方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "xavier"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们将它们改成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;weight_filler {
  type: "gaussian"
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样的改法是比较脑残的，这相当于对参数采取均值为0，方差为1的高斯分布对参数进行初始化……&lt;/p&gt;&lt;p&gt;不用说，这个模型会死的很难看，10000轮训练结束后，最终的精度为0.1135。要知道一共只有10个数字，猜也可以达到0.1的精度。这个模型采用了各种高科技，结果却只比猜稍微好一点，简直弱到家了。&lt;/p&gt;&lt;p&gt;那么它在数值上的表现是怎样呢？直接来看图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/955f787ae8dc46b137e3f7fb077ad6d5.png" data-rawwidth="916" data-rawheight="419"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e2e676bf99194f1c89d7a6263d30a7c9.png" data-rawwidth="920" data-rawheight="435"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/54a5a4b652a4d1d4d6630d87c93cf593.png" data-rawwidth="916" data-rawheight="428"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7e5b81d4199d1658ef8f0e419ea1a7f5.png" data-rawwidth="914" data-rawheight="429"&gt;&lt;p&gt;从图中可以看出，各个数据在幅度上的差距都非常大。尤其是最后一张图，data和diff在数值上的差距拉得非常大，那么小量的diff对大量的data完全起不到改变的作用。这么看来一定是初始化的锅了。&lt;/p&gt;&lt;p&gt;那真的完全是初始化的锅么？众人问：初始化同学，你还有没有什么要补充的？&lt;/p&gt;&lt;p&gt;初始化同学：其实是因为我和ReLU同学不熟，你们换Sigmoid试试……&lt;/p&gt;&lt;p&gt;众人：当真？&lt;/p&gt;&lt;p&gt;于是乎我们开始了第三个实验……&lt;/p&gt;&lt;h2&gt;第三个实验&lt;/h2&gt;&lt;p&gt;为了洗刷初始化的清白，我们在保持初始化的基础上，将3个非线性的部分重新换成sigmoid，由sigmoid再度出马。sigmoid表示这种小场面它Hold住……&lt;/p&gt;&lt;p&gt;于是我们来看看sigmoid的最终结果：0.9538，相关的数值图在这里就不全贴了，只贴一张L1 L2 norm的图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6f231a2392460c8b6b0b89c64ce3b312.png" data-rawwidth="914" data-rawheight="425"&gt;&lt;p&gt;从这个图上来看，虽然data的幅度很大，但是diff的幅度也很大，基本上算旗鼓相当。&lt;/p&gt;&lt;p&gt;这个最终的精确率不能算特别高，但是起码说明模型还是学到了很多有用的知识。而且和0.11的精度相比，简直是一个天上一个地下，完全没得比啊！大家纷纷表示关键时候还得靠老将才能扛得住，初始化同学是被冤枉了。&lt;/p&gt;&lt;p&gt;这时候众人重新以怀疑的眼光看着ReLU，原来真正的问题出你这啊！你的表现有点浪啊！&lt;/p&gt;&lt;p&gt;ReLU委屈地表示：我还能再说一句么？&lt;/p&gt;&lt;p&gt;众人表示：这次的版面已经不够了，有话下次再说……&lt;/p&gt;&lt;h2&gt;继续广告……&lt;/h2&gt;&lt;p&gt;欢迎加入“我爱机器学习”QQ群，564533376～&lt;/p&gt;</description><author>冯超</author><pubDate>Tue, 16 Aug 2016 19:17:23 GMT</pubDate></item><item><title>Caffe代码阅读——Solver</title><link>https://zhuanlan.zhihu.com/p/21800004</link><description>前面我们聊了Net组装的内容，接下来我们来看看Solver的内容。Solver主体有两部分：初始化和训练。初始化内容相对比较简单，这里就不说了；下面我们来说说训练中的几个关键函数。&lt;h2&gt;核心函数：Step&lt;/h2&gt;&lt;p&gt;真正的训练在Step函数内，这里有多卡训练的关键回调函数：on_start()和on_gradient_ready()，具体的调用方法我们后面再说，在这两个回调函数中间有两个重要的过程：ForwardBackward和UpdateSmoothedLoss。在on_gradient_ready之后有一个关键函数ApplyUpdate()，这里面的代码在Sgd_solver中。下面我们详细看一下。&lt;/p&gt;&lt;h2&gt;ForwardBackward&lt;/h2&gt;&lt;p&gt;这里主要调用了Net中的代码，主要完成了前向后向的计算，前向用于计算模型的最终输出和Loss，后向用于计算每一层网络和参数的梯度。对于前向后向的具体内容这里需要详细叙述了，唯一值得一提的是前向的Loss计算，这部分代码实际上实在Layer里面，具体涉及到loss_weight这个参数相关的初始化和loss()的判断，同时还有Loss_Layer在Setup函数中的初始化。&lt;/p&gt;&lt;h2&gt;UpdateSmoothedLoss&lt;/h2&gt;&lt;p&gt;这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题。代码中的平滑方法比较简单，大家一看便知。&lt;/p&gt;&lt;p&gt;下面就是ApplyUpdate函数，这个函数真正完成了参数更新的任务。Caffe的参数更新只利用了模型的梯度信息，没有利用二阶信息。下面就详细介绍下更新参数的几个过程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;GetLearningRate&lt;/li&gt;&lt;li&gt;ClipGradients&lt;/li&gt;&lt;li&gt;Normalize&lt;/li&gt;&lt;li&gt;Regularize&lt;/li&gt;&lt;li&gt;ComputeUpdateValue&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;GetLearningRate&lt;/h2&gt;&lt;p&gt;learning rate的故事我们前面已经聊过了，在CNN训练中这确实是个大问题。Caffe为了让learning rate的设计更灵活，提供了一系列的learning rate方案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;fixed&lt;/b&gt;：lr永远不变&lt;/li&gt;&lt;li&gt;&lt;b&gt;step&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter / stepsize}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;exp&lt;/b&gt;：&lt;equation&gt;lr=baselr*gamma^{iter}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;inv&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1+gamma*iter)^{-power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;multistep&lt;/b&gt;：直接写iter在某个范围内时lr应该是多少&lt;/li&gt;&lt;li&gt;&lt;b&gt;poly&lt;/b&gt;：&lt;equation&gt;lr=baselr*(1-\frac{iter}{maxiter})^{power}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;sigmoid&lt;/b&gt;：&lt;equation&gt;lr=baselr*\frac{1}{1+e^{-gamma*(iter-stepsize)}}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;这些方案各有优劣，选择自己顺手的就好。&lt;h2&gt;ClipGradients&lt;/h2&gt;&lt;p&gt;这一步主要是对梯度值做一个限制，如果梯度值过大，那么这里就会对梯度做一个修剪，对所有的参数乘以一个缩放因子，使得所有参数的平方和不超过参数中设定的梯度总值。这个功能感觉上像是对全局函数设置了一个Trust Region，可以防止更新的量过大二导致梯度发散。我认为这一步的想法是很好的，但是实际操作中可能会有问题。实际中可能只有部分参数的梯度比较大，而其他参数的梯度本身比较小，那么对所有的参数乘以相同的因子会让一些本来比较小的参数变得更小，这样会带来一些不公平。&lt;/p&gt;&lt;h2&gt;Normalize&lt;/h2&gt;&lt;p&gt;这一步同样考虑了一些单一Batch不足以完成训练的问题，通过限制每个Batch的更新量来控制更新总量，代码比较简单。&lt;/p&gt;&lt;h2&gt;Regularize&lt;/h2&gt;&lt;p&gt;到这一步终于要计算正则项的梯度了。Caffe提供两种正则方法——L2和L1，其中L2采用了标准的梯度下降方法，L1采用了sub-gradient的计算方法。L2的优化计算比较简单，没有什么好说的，但是L1的计算还是有点值得玩味的地方的。这里采用的sub-gradient方法其实本身没有什么问题，不过Lasso的优化还可以有其他的方法，这个问题以后可以再细聊。&lt;/p&gt;&lt;h2&gt;ComputeUpdateValue&lt;/h2&gt;&lt;p&gt;到这里，我们终于来到了梯度计算的最后一站，这时候我们终于完成了对梯度的计算，下面该考虑lr和梯度结合起来如何计算最终的梯度优化值了。sgd方法主要采用momentum加梯度的优化方法。关于momentum的优势我们前面已经聊过了。除此之外，Caffe还提供了一系列的梯度计算方法，这些优化方法各有特点，以后我们可以慢慢来看。&lt;/p&gt;&lt;p&gt;当计算完这一步，我们就可以调用Blob中的Update把每个参数的data和diff进行相加，计算出最终的结果。这样，整个优化过程就完成了。至于剩下的一些内容都不是核心过程，就略去不看了。&lt;/p&gt;&lt;p&gt;如果我们采用单卡训练的策略，那么阅读代码到这里也差不多了。不过多卡训练对于大规模的训练任务来说是必不可少的，所以我们接下来趁热打铁地看看Caffe的多卡训练。&lt;/p&gt;&lt;h2&gt;多卡训练算法&lt;/h2&gt;&lt;p&gt;Caffe的多卡训练算法总体思路是数据并行，我们用不同的GPU处理不同的数据，然后将所有的梯度更新汇总。由于Solver在训练中给了两个回调函数，多卡训练也主要利用了这两个回调函数进行：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;on_start()：将参数拷贝到每一个GPU中。&lt;/li&gt;&lt;li&gt;ForwardBackward()：每个GPU各自计算自己的前向后向结果。&lt;/li&gt;&lt;li&gt;on_gradient_ready()：将反向梯度汇总到一起。&lt;/li&gt;&lt;li&gt;ApplyUpdate()：在汇总的线程上进行参数更新&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中第2步由每一个CPU线程和自己的GPU并行完成，第4步由汇总的CPU和自己的GPU完成，剩下的1，3两步主要是完成数据传输的任务，也是多卡计算中主要完成的部分。&lt;/p&gt;&lt;p&gt;Caffe采用树型结构进行参数传递，其中一个CPU线程和GPU作为树型结构的根，其他的则作为根下面的节点。为了更快地传输GPU数据，树型结构的构建要考虑GPU之间是否相近，比方说两个GPU之间是否可以进行P2P的直传。在前面的翻译博客中我们已经聊过GPU之间数据传输的问题了，这里的树形结构也主要以此做考虑。&lt;/p&gt;&lt;p&gt;我们假设4块GPU的拓扑结构如下：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3  
GPU0   X     PHB    SOC    SOC    
GPU1   PHB    X     SOC    SOC    
GPU2   SOC    SOC    X     PHB   
GPU3   SOC    SOC    PHB    X     &lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;那么我们构造出的树型结构如下所示，数据传输也是按照这样的结构传输：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2b57bad80fd32d75715706d642836647.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;这样1，3的数据传递就解决了，具体的过程请详细阅读代码，这里就不叙述了。&lt;/p&gt;&lt;p&gt;对Caffe代码的基本介绍就到这里了，我们对代码的整体结构有了比较清晰的认识，下面我们将分析模型中各个部分的特性。&lt;/p&gt;&lt;h2&gt;最后的私货&lt;/h2&gt;&lt;p&gt;欢迎大家加入“我爱机器学习”QQ群：564533376，（省略情怀文若干字……）一起学习机器学习，一起成长！&lt;/p&gt;</description><author>冯超</author><pubDate>Thu, 11 Aug 2016 20:27:25 GMT</pubDate></item><item><title>[翻译]Exploring the Complexities of PCIe Connectivity and Peer-to-Peer Communication</title><link>https://zhuanlan.zhihu.com/p/21908564</link><description>本来准备今天开始写Caffe代码阅读的第三部分的，结果找到了一片介绍GPU通信的文章，觉得这篇文章的内容讲得非常好，而这方面的背景知识也是非常缺乏的，另外Caffe多卡训练的一些细节和这里面的知识也有关系，所以今天就来翻译一下这篇文章。&lt;p&gt;当然了，翻译之前先扯一段。读大学的时候我曾经报名过一节英文翻译课，当时教过我的一些具体的翻译小技巧我已经忘光了，但是有一段话我还记得，那就是翻译的三个目标——信、达、雅。翻译首先要准确，然后要通顺，最后要能做到优美那就再好不过了。这里不免想吐槽一下一些文章翻译，在达的方面做得实在不够好，我都可以从中文翻译中读出原文定语从句的味道……我个人希望自己能做到达，最后不需要做到雅，做到俗也是极好的。&lt;/p&gt;&lt;p&gt;文章出处：&lt;a href="http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/" class=""&gt;http://exxactcorp.com/blog/exploring-the-complexities-of-pcie-connectivity-and-peer-to-peer-communication/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原作者：Ross Walker（玫瑰行走者？）&lt;/p&gt;&lt;p&gt;这篇文章我们来深入地看看PCI-E总线上数据通信方面存在的一些瓶颈，以及我们Exxact公司最新的牛逼系统是如何搞定这些问题，使得像机器学习这样严重依赖GPU的工作可以轻松点。&lt;/p&gt;&lt;h2&gt;传统套路&lt;/h2&gt;&lt;p&gt;PCI-E是个啥本文是没有兴趣给你讲的，我们直接来谈谈以它为主线的架构，以及架构的优势和劣势（主要是劣势）。在此之前，我们首先要明白一件事，那就是为啥PCI-E总线的数据传输速度是十分重要的。在GPU计算革命到来之前，PCI总线主要是用于和磁盘做数据通信，或者计算机结点间通信的（HPC小红人infiniband）。一个经典的PCI-E总线架构图就长成这个样子：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8e11038678ca5afe5dd2aa485bc3f8e2.jpg" data-rawwidth="1025" data-rawheight="363"&gt;以前这个架构用得很好没问题，但后来GPU计算时代来临，我们搞GPU计算的朋友开始有了新的诉求，我们希望主存数据和GPU显存的数据传输能够尽量快点，于是乎，传统设计就暴露出了三个问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;GPU被分在了两个区域中，就像上面的图片，它们分别被不同的MCH（Memory Controller Hub）控制，连接在不同的CPU数据接口上。然后，不同区域的CPU在通过一个叫QPI的东西连接起来，以保证不同区域的数据也能够通信。当然这样只能说解决了数据可以传输的问题，但问题是，对于处于不同区域的GPU，比方说GPU0和GPU2，如果它们两之间想传递数据（多卡训练的时候你会遇到的）,就必须翻山越岭穿越层层障碍，绕个大圈才能把数据传过来。所以……它就是慢嘛，数据吞吐量和延时的数据肯定就不好看了。&lt;/li&gt;&lt;li&gt;前面看到同一区域的CPU和GPU之间都是通过PCI-E的通道连接，但是CPU的PCI-E通道是有限的，这个问题相信不少自己装机搞深度学习环境的朋友都遇到过，如果想搞多GPU，CPU的PCI-E通道数一定不能少！那么对于现在这一代的Intel CPU架构Haswell,它是拥有两个数据接口系统，像上面的图一样，每个接口的PCI-E通道数是40个，那么它最多只能接受4个GPU（要多少是多啊……），因为我们现在的GPU每个都需要连16个数据接口，那么一边最多也就连2块GPU。万一你要是觉得4块不够多，想再多搞点，对不起，那也不行，请走分布式的路子，再部署一台机器去，然后让机器间想办法互联（infiniband）。不过很显然，这比单机要慢一些了。而且40个通道已经被GPU分走32个了，inifiband也需要PCI-E通道的，就给inifiniband分剩下的8个？太瞧不起infiniband了吧？万一跨机间的数据传输速度慢了，infiniband表示这锅它不背。&lt;/li&gt;&lt;li&gt;你以为同在一个区域的GPU就好过了？MCH照样会把它们之间的传输速度拉下去（具体原理我也不懂……）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;“这病听上去感觉还挺严重的，那大夫该怎么判断我有没有得这个病呢？”&lt;/p&gt;&lt;p&gt;“老司机早已为你准备好药方，一个命令+一个程序”&lt;/p&gt;&lt;p&gt;首先是&lt;b&gt;nvidia-smi&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
       GPU0   GPU1   GPU2   GPU3   CPU Affinity
GPU0   X     PHB    SOC    SOC    0-9,20-29
GPU1   PHB    X     SOC    SOC    0-9,20-29
GPU2   SOC    SOC    X     PHB    10-19,30-39
GPU3   SOC    SOC    PHB    X     10-19,30-39

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的结果可以看到，PHB表示了PCI-E Host Bridge，就是我们上面图中的连接方式，而SOC就是要通过GPU-CPU-QPI-CPU-GPU这样长途跋涉才能传输的连接方式。&lt;/p&gt;&lt;p&gt;其次是CUDA中自带的一个程序，在examples/1_Utilities/p2pBandwidthLatencyTest，编译一下就可以运行，亲测会输出很多内容，以下只显示一部分：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;p2pBandwidthLatencyTest
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   19.18  12.22  11.77
     1  19.17     X   17.07  11.81
     2  12.23  12.17     X   19.17
     3  11.73  11.88  19.18     X
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;从上面的数据可以一目了然的看出，在一个区域和在不同区域的带宽差是比较明显的。当然如果不是MCH背锅，这个差距还可以更亮眼一点，16通道的PCI-E理论传输上限是32GB/s。所以不论是同一区域，还是不同区域，这个数据都不够好。&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   3.39   8.18  16.86  16.26
     1   7.22   3.74  13.56  16.54
     2  16.27  16.06   3.52   5.81
     3  15.98  15.92   6.62   3.20
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;传输时延的结果也十分明显，就不说了。&lt;/p&gt;&lt;h2&gt;重新思考以GPU为核心的三个代表母版设计思想&lt;/h2&gt;&lt;p&gt;前面喷了那么多，下面该说说我们设计方案了，首先展示的是我们的关键道具，PCI-E switch chip(PLX)，这家伙的通道数有48，80，96条，这样4块GPU就不用分到2个区域了，而且它可以让机器上GPU的数据传输更快！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5d9371405090e491c0a1ba477119b20d.jpg" data-rawwidth="567" data-rawheight="178"&gt;&lt;h2&gt;产品1： Deep Learning Dev Box&lt;/h2&gt;&lt;p&gt;废话不多说了，直接上架构图……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b499f46fc46e4aa7221eacee961e8e5e.jpg" data-rawwidth="1007" data-rawheight="513"&gt;我们略去浮华的产品介绍，我们的产品加上深度学习软件包，心动价只卖8999刀！8999你买不了吃亏，8999你买不了上当！下面直接看下数据：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;nvidia-smi topo -m
      GPU0  GPU1  GPU2  GPU3  CPU Affinity
GPU0  X    PIX   PHB   PHB   0-11
GPU1  PIX   X    PHB   PHB   0-11
GPU2  PHB   PHB   X    PIX   0-11
GPU3  PHB   PHB   PIX   X    0-11
 

Legend:
  X   = Self
  SOC = Path traverses a socket-level link (e.g. QPI)
  PHB = Path traverses a PCIe host bridge
  PXB = Path traverses multiple PCIe internal switches
  PIX = Path traverses a PCIe internal switch
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="text"&gt;Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3
     0     X   26.13  20.31  20.32
     1  25.97     X   20.31  20.32
     2  20.32  20.32     X   26.12
     3  20.32  20.32  26.12     X&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;可以看到，这数据好得吓人，要知道我们距离理论极限32GB/s已经不远了。再看看时延：&lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&lt;code lang="text"&gt;P2P=Enabled Latency Matrix (us)
   D\D     0      1      2      3
     0   4.15   6.10   6.27   6.05
     1   6.10   4.13   6.12   6.00
     2   6.31   5.96   4.19   6.04
     3   6.07   5.97   6.15   4.09

&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;在数据面前一切语言都显得那么的苍白，不要犹豫，赶紧订购吧！&lt;/p&gt;&lt;h2&gt;产品2，3，4&lt;/h2&gt;&lt;p&gt;你以为我还会一个挨一个的翻译这些产品介绍？想看就去看原文吧[微笑][微笑]&lt;/p&gt;&lt;h2&gt;总结（非翻译）&lt;/h2&gt;&lt;p&gt;抛开广告的环节，这篇文章很好地向我们解释了多GPU间数据传输这个问题。这个问题总体上算是个硬件层次的问题，那么对于软件方面，我们能做些什么呢？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;让你的CPU和GPU在同一个区域内，减少跨区域的数据拷贝传输&lt;/li&gt;&lt;li&gt;如果涉及到跨区域的GPU数据传输，尽量减少传输的次数，不然真的有可能会让程序慢下来&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;带着这两个结论，我们就可以更好地去看Caffe多卡训练中的一些问题了。&lt;/p&gt;</description><author>冯超</author><pubDate>Wed, 10 Aug 2016 00:01:01 GMT</pubDate></item><item><title>Caffe源码阅读——Net组装</title><link>https://zhuanlan.zhihu.com/p/21875025</link><description>&lt;p&gt;最近忙着看TI没有及时写文章，今天赶紧补一篇……&lt;/p&gt;Net是Caffe代码中一个比较核心的类，往下看它封装了所有的Layer，构建起了整个神经网络；往上看它对外提供了前向后向计算，以及核心数据结构的访问结构，使得再上层的Solver可以利用Net比较轻松地实现Train和Test的策略。当然，正是因为它的重要性，组装Net是一个比较复杂的部分。这一回我们就来看看Net的内容。&lt;p&gt;当然，说在前面，看Net组装的代码有两个目的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;了解作为一个成熟的CNN模型框架需要考虑的一些问题；&lt;/li&gt;&lt;li&gt;如果想对网络结构做扩展，如写一个新的Layer，其中的一些数据是如何在Layer和Net之间流动的&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;首先，为了使问题不那么复杂，我们先从训练模型时输出的log看看Net组装的几个关键步骤，然后再把这个过程慢慢展开，了解组装的所有细节。&lt;/p&gt;&lt;h2&gt;Log眼中的Net组装&lt;/h2&gt;&lt;p&gt;为了更好地展示Net组装的一些细节，我们在这里选取了一个实际例子，就是Caffe的examples里面的siamese model。关于这个model的细节这里就不多说了，感兴趣的可以去看官方或者非官方的文档，这里只提一点：这个网络除了包含其他正常网络中的一些特性之外，还具有网络参数复用的特点，在后面的分析中我们会用到。&lt;/p&gt;&lt;p&gt;下面我们要看的就是Net组装的Log。这段Log一般都是大家在训练网络时一闪而过的大段Log，当然如果它没有一闪而过而是停下来了，有可能是你的网络定义有问题爆出了错误。这段Log内容比较多，总体来说就是Train阶段和Test阶段的两个网络组装起来。我们重点关注其中的几个片段，来大概了解Net组装的一些核心内容，也是那些比较值得打印出来的内容。&lt;/p&gt;&lt;p&gt;首先是一个正常的卷积层conv1，Log如下所示（以下代码的行号可能会有不同，但位置是相近的）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer conv1
net.cpp:92] Creating Layer conv1
net.cpp:428] conv1 &amp;lt;- data
net.cpp:402] conv1 -&amp;gt; conv1
net.cpp:144] Setting up conv1
net.cpp:151] Top shape: 64 20 24 24 (737280)
net.cpp:159] Memory required for data: 3752192
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这其中第一行是创建这个Layer实例的代码，具体的创建过程在layer_factory里面。为了方便创建Layer，Caffe采用了工厂方法的设计模式，只要提供Layer的名字（在配置文件中参数叫type），就可以根据名字和对应参数实例化一个Layer。这部分的细节只要认真看一下就会明白。&lt;/p&gt;&lt;p&gt;第3，4行显示了创建当前层的bottom和top数据的过程。这里涉及到net.cpp中的AppendBottom和AppendTop两个方法，因为每一个bottom blob和top blob都有名字，这里就将他们之间的关系输出在了这里。&lt;/p&gt;&lt;p&gt;第5行看上去没什么干货，但是它代表了Layer的Setup函数已经调用完成（或者Layer被share）。Layer的Setup函数是Layer初始化的关键函数，这里面涉及到以下几个具体的操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;CheckBlobCounts(bottom, top);
LayerSetUp(bottom, top);
Reshape(bottom, top);
SetLossWeights(top);&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;总结地说，这四句完成了：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对bottom blob, top blob数量的检查，父类实现。&lt;/li&gt;&lt;li&gt;对Layer内部相关变量的初始化，由具体的子类实现&lt;/li&gt;&lt;li&gt;传入时bottom blob的维度已经确定，Layer需要根据自己要做的计算确定top blob的纬度。比方说这一层是卷积层，维度是20*5*5，输入图像是1*28*28，也就是bottom blob的维度，那么输入就是20*24*24，这也是上面log里面算出的结果，只不过还加了一个batch size。这个函数由具体的子类实现。&lt;/li&gt;&lt;li&gt;对Layer是否输出loss以及输出loss要做的操作进行初始化。父类实现。必须说一句，Caffe中关于Loss Layer中Loss_weight，loss_，top.cpu_diff的数据设定还是有点绕且有点trick的。&lt;/li&gt;&lt;/ol&gt;好了回到上面的log。接下来的那一句告诉了我们top层应该输出的维度。这里输出了维度就是为了让不放心的朋友算一下，看看和你想的是否一样。当然，输出这句log的循环不是只做了这件事，它的主要工作就是设置top blob的loss_weight。 &lt;p&gt;最后一句计算了该层top blob所占用的内存。可以看出截至到这一层，内存消耗大约是3M多，还不算大。&lt;/p&gt;&lt;p&gt;好，这就是一个最典型的Layer的初始化，下面这个ReLU层就稍微有些不同了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer relu1
net.cpp:92] Creating Layer relu1
net.cpp:428] relu1 &amp;lt;- ip1
net.cpp:389] relu1 -&amp;gt; ip1 (in-place)
net.cpp:144] Setting up relu1
net.cpp:151] Top shape: 64 500 (32000)
net.cpp:159] Memory required for data: 5769472
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里面最不同的就是第4行结尾的（in-place），这说明relu的bottom blob和top blob是同一个数据，这和我们在网络中的定义是一样的。in-place的好处就是减少内存的操作，但是这里在统计内存消耗时并没有考虑in-place带来的节省。&lt;/p&gt;&lt;p&gt;接下来就是共享网络的conv1_p了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer_factory.hpp:77] Creating layer conv1_p
net.cpp:92] Creating Layer conv1_p
net.cpp:428] conv1_p &amp;lt;- data_p
net.cpp:402] conv1_p -&amp;gt; conv1_p
net.cpp:144] Setting up conv1_p
net.cpp:151] Top shape: 64 20 24 24 (737280)
net.cpp:159] Memory required for data: 8721664
net.cpp:488] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
net.cpp:488] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这一段最有特点的是最后两句“Sharing”，因为siamese model中拥有参数完全相同的两个网络，所以在构建时候，第二个网络检测到参数名字已经存在，说明该层的参数和其他层共享，于是在这里打印出来告诉用户这一点。当然，这一句之前没有打印出来的内容告诉了我们，实际上Net类中还负责了参数相关的初始化。这部分的内容实际上还挺多，除了参数共享，还有对参数learning rate，weight decay的设定。&lt;/p&gt;&lt;p&gt;最后是最特别的一层：loss层&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;net.cpp:92] Creating Layer loss
net.cpp:428] loss &amp;lt;- feat
net.cpp:428] loss &amp;lt;- feat_p
net.cpp:428] loss &amp;lt;- sim
net.cpp:402] loss -&amp;gt; loss
net.cpp:144] Setting up loss
net.cpp:151] Top shape: (1)
net.cpp:154]     with loss weight 1
net.cpp:159] Memory required for data: 10742020
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这一层看上去没有什么特别，该有的和前面一样，但是唯一不同的就是它的倒数第二行，这说明这一层是有loss weight的。至于有loss weight有什么用，以后我们会详细说这个事情。这里简单说一下，有loss weight表示这个blob会被用于计算loss。&lt;/p&gt;&lt;p&gt;前面的log主要解决了网络的组装和前向的一些计算，从log中，我们可以看出Net完成了以下的事情：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;实例化Layer&lt;/li&gt;&lt;li&gt;创建bottom blob,top blob&lt;/li&gt;&lt;li&gt;Setup Layer（初始化Layer，确定top blob维度）&lt;/li&gt;&lt;li&gt;确定layer的loss_weight&lt;/li&gt;&lt;li&gt;确定layer的参数是否共享，不共享则创建新的&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;从上面的过程也可以看出，整个网络中所有的流动性变量（bottom blob,top blob）都保存在Net中，同时对于各层的参数，根据各层的共享关系做了标记。这样好处是集中管理了网络中的数据，方便对数据进行操作。&lt;/p&gt;&lt;p&gt;再往下面，我们可以截取一小段log来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;net.cpp:220] pool1 needs backward computation.
net.cpp:220] conv1 needs backward computation.
net.cpp:222] slice_pair does not need backward computation.
net.cpp:222] pair_data does not need backward computation.
net.cpp:264] This network produces output loss
net.cpp:277] Network initialization done.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来是统计一个层次是否需要进行反向传播的计算。一般来说我们的层是都需要计算的，但是也会有一些层不需要计算，比方说数据层，就像上面的log那样，还有就是一些希望固定的层，这个一般在finetune网络的时候用的上。因为反向计算一般比前向计算慢，如果有不需要计算的Layer，直接跳过计算是可以节省时间的。&lt;/p&gt;&lt;p&gt;最后是整个网络产生的输出，这个输出会在训练迭代中显示出来的。&lt;/p&gt;&lt;p&gt;了解了这些，我们就对Net装载有了大概的了解，再去看它的代码就会轻松些。&lt;/p&gt;&lt;p&gt;最后，关于Net类中所有的成员变量与它们之间的关系，我们可以用下面的一张图来理解就好：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/66911c674bd5aae86a9eb088949f3e53.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;把Net的初始化理解后，其实Net以下的架构方面的问题就不多了。下面我再看看Net以上的东西，Solver以及Caffe里“简单”的多卡训练。&lt;/p&gt;</description><author>冯超</author><pubDate>Sat, 06 Aug 2016 22:44:29 GMT</pubDate></item><item><title>Caffe代码阅读——层次结构</title><link>https://zhuanlan.zhihu.com/p/21796890</link><description>Caffe是一款优秀的深度神经网络的开源软件，下面我们来聊聊它的源代码以及它的实现。Caffe的代码整体上可读性很好，架构比较清晰，阅读代码并不算是一件很困难的事情。不过在阅读代码之前还是要回答两个问题：&lt;ol&gt;&lt;li&gt;阅读代码是为了什么？&lt;/li&gt;&lt;li&gt;阅读到什么程度？（这个问题实际上和前面的问题相关）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;阅读代码大体上来说有下面几个目的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;搞清楚代码所实现的算法或者功能。对算法本身不是很了解，希望通过阅读代码了解算法。&lt;/li&gt;&lt;li&gt;搞清楚代码在实现算法过程中的细节。这种情况下，一般对算法已经有大概的了解，读代码是为了了解代码中对算法细节的考量。当然，如果想使用代码，了解代码细节是很有帮助的。&lt;/li&gt;&lt;li&gt;扩展代码。在开源代码的基础上，利用已有的框架，增加或者修改功能，来实现自己想要的功能。这个就需要对代码的架构细节有更加深入的了解。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们的目标是扩展代码。Caffe中主要的扩展点就是Layer和Solver，当然其他的部分也可以扩展，只不过要改动的代码会多一些。&lt;/p&gt;&lt;p&gt;当确定了上面第一个问题，下面就是第二个问题了。读代码要读到什么程度？一般来说，我觉得阅读代码这件事情可以用一个Logistic型的函数来表示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/65a395cb2d773de04047c35af8f320fd.png" data-rawwidth="348" data-rawheight="232"&gt;&lt;p&gt;这个图上，横轴是阅读代码花费的时间，纵轴是阅读代码带来的效果。对于代码量比较大的项目，一开始阅读肯定是蒙的，需要花一定的时间梳理清楚各个文件，各个模块之间的关系。随着结构关系逐渐清晰，读者开始领会代码中所表达的含义，阅读代码的效果直线上升。然而当我们把代码主线和重要支线弄懂后，再读一些小支线的收益就不会太大。所以根据阅读代码的性价比和Caffe代码自身的特点，我们只会将主线和一些重要支线阅读完，估计也就是整体代码量的一半。&lt;/p&gt;&lt;h2&gt;Caffe代码的主线结构抽象&lt;/h2&gt;&lt;p&gt;不同于其他的一些框架，Caffe没有采用符号计算的模式进行编写，整体上的架构以系统级的抽象为主。所谓的抽象，就是逐层地封装一些细节问题，让上层的代码变得更加清晰。那么就让我们来顺着Caffe的抽象层级看看Caffe的主线结构：&lt;/p&gt;&lt;p&gt;&lt;b&gt;SyncedMem：&lt;/b&gt;这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&amp;gt;CPU内存-&amp;gt;GPU内存-&amp;gt;CPU内存-&amp;gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Blob：&lt;/b&gt;这个类做了两个封装：一个是操作数据的封装。在这里使用Blob，我们可以操纵高维的数据，可以快速访问其中的数据，变换数据的维度等等；另一个是对原始数据和更新量的封装。每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff用于存储反。向传播的梯度更新值。Blob使用了SyncedMem，这样也得到了不同处理端访问的便利。这样Blob就基本实现了整个Caffe数据部分结构的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。&lt;/p&gt;&lt;p&gt;数据的抽象到这个就可以了，接下来是层级的抽象。前面我们也分析过，神经网络的前后向计算可以做到层与层之间完全独立，那么每个层只要依照一定的接口规则实现，就可以确保整个网络的正确性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Layer：&lt;/b&gt;Caffe实现了一个基础的层级类&lt;b&gt;Layer&lt;/b&gt;，对于一些特殊种类还会有自己的抽象类（比如base_conv_layer），这些类主要采用了模板的设计模式（Template）,也就是说一些必须的代码在基类写好，一些具体的内容在子类中实现。比方说在Layer的Setup中，函数中包括Setup的几个步骤，其中的一些步骤由基类完成，一些步骤由子类完成。还有十分重要的Forward和Backward，基类实现了其中需要的一些逻辑，但是真正的运算部分则交给了子类。这样当我们需要实现一个新的层时，我们不需要管理琐碎的事物，只要关系好层的初始化和前后向即可。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Net：&lt;/b&gt;Net将数据和层组合起来做进一步的封装，对外暴露了初始化和前后向的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样。同时值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，所以我们可以通过配置实现多个神经网络层之间共享参数的功能，这也增强了我们对网络结构的想象力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Solver&lt;/b&gt;：有了Net我们实际上就可以进行网络的前向后向计算了，但是关于网络的学习训练的功能还有些缺乏，于是在此之上，&lt;b&gt;Solver&lt;/b&gt;类进一步封装了训练和预测相关的一些功能。与此同时，它还开放了两类接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如大家喜闻乐见的Momentum，Nesterov，Adagrad等。这样使得不同的优化算法能够应用其中。另外一个是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多卡训练算法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;IO&lt;/b&gt;：有了上面的东西就够了？还不够，我们还需要输入数据和参数，正所谓巧妇难为无米之炊，没有数据都是白搭。&lt;b&gt;DataReader&lt;/b&gt;和&lt;b&gt;DataTransformer&lt;/b&gt;帮助准备输入数据，&lt;b&gt;Filler&lt;/b&gt;对参数进行初始化。一些&lt;b&gt;Snapshot&lt;/b&gt;方法帮助模型的持久化，这样模型和数据的IO问题也解决了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;多卡&lt;/b&gt;：对于单GPU训练来说，基本的层次关系到这里也就结束了，如果要进行多GPU训练，那么上层还会有&lt;b&gt;InternalThread&lt;/b&gt;和&lt;b&gt;P2PSync&lt;/b&gt;两个类，这两个类属于最上层的类了，而他们所调用的也只有Solver和一些参数类。&lt;/p&gt;&lt;p&gt;其实到这里，Caffe的主线也就基本走完了。我们可以画一张图把Caffe的整体层次关系展示出来：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c6f445ed948594f81b3b2e35ce4c70f0.jpg" data-rawwidth="4032" data-rawheight="3024"&gt;&lt;p&gt;如果对这张图和图中的一些细节比较清楚的话，那么你对Caffe的了解应该已经不错了。后面关于Caffe源码分析的文章就可以不看了。如果没有，那么我们还是可以继续关注一下。当然如果想真正理解这张图中所表达的含义，还是要真正地读一下代码，去理解一些细节。但是有些细节这里就不做详细的分析了，下一回我们会站在Layer的角度去看一个Layer在训练过程的全部经历。&lt;/p&gt;</description><author>冯超</author><pubDate>Sun, 31 Jul 2016 19:23:46 GMT</pubDate></item></channel></rss>