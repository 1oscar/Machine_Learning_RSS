<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Sun, 23 Oct 2016 08:15:54 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Caffe中的SGD的变种优化算法(1)</title><link>https://zhuanlan.zhihu.com/p/22464537</link><description>Caffe中还有一个很重要的部分，那就是优化算法。在专栏的一开始，我们就从优化入手，聊了两个优化算法——随机梯度下降和动量（感谢众多知友指出我之前翻译的错误……）。下面我们沿着前面的步伐，继续前进。&lt;h2&gt;写在前面&lt;/h2&gt;&lt;p&gt;说实话设计优化算法和解释优化算法的合理性实际上是一个不太容易的事情。这里我们主要关注算法的实际效果，因此我们对于算法的理论推导就暂时放在一边了。而关于理论，我们只简单讲两句，不做过多的深入，重点是看算法在我们设计的场景下的表现。&lt;/p&gt;&lt;h2&gt;非凸函数&lt;/h2&gt;&lt;p&gt;之前在我们的算法演示中，我们展示的函数都是凸函数。凸函数有哪些好处，谁来讲一讲？&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7ef6696a76e045a9006bc453d2cd3069.png" data-rawwidth="410" data-rawheight="246"&gt;（侵删）&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它通常有唯一的极值点。&lt;/li&gt;&lt;li&gt;Hessian矩阵正定。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;而下面我们将展示一个函数，这个函数可没有这么多好的性质了：&lt;/p&gt;&lt;equation&gt;f(x,y)=x^2-2x+100xy+10y^2+20y&lt;/equation&gt;&lt;p&gt;在等高线图上，它的样子是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6b3c9147f2fb44ded15d916f0a892223.png" data-rawwidth="787" data-rawheight="389"&gt;这里的配色采用绘图中的经典彩虹色，其中颜色越靠近蓝色表示函数值越大，颜色越靠近红色表示函数值越小。这和我们的直觉是相符的，一三象限的数字大，二四象限的数字小。&lt;/p&gt;&lt;p&gt;我们的第一个尝试就是从一个比较正常的位置出发——（5，5）点，看看各种算法的效果&lt;/p&gt;&lt;p&gt;在这个新的环境下，我们的老算法——梯度下降，动量，Nesterov算法会不会焕发第二春呢？&lt;/p&gt;&lt;h2&gt;有请老队员上场&lt;/h2&gt;&lt;p&gt;首先是梯度下降法，由于前面我们已经贴过代码了，这里我们就不再贴代码了，直接上结果：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = gd([5,5], 0.0008, g)
contour(X,Y,Z, x_arr)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0a2e3624b3c4dd427eea4ec6cc67e612.png" data-rawwidth="787" data-rawheight="382"&gt;为了保证算法经过50轮迭代不会超出我们限定的范围，我们调整了它的learning rate。可以看出算法在迭代过程中先是冲向了局部最优点，也就是我们常说的鞍点，然后发现不对，调转头冲向了真正的优化点，祝它一路走好……&lt;/p&gt;&lt;p&gt;然后是动量算法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = momentum([5, 5], 3e-4, g)
contour(X,Y,Z, x_arr)&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5a5a0b0945b0dbde773bd57c732f9520.png" data-rawwidth="788" data-rawheight="380"&gt;&lt;p&gt;一切顺利！不同的演员同样的剧本，演出来的效果还是不错的。&lt;/p&gt;&lt;p&gt;好了，到这里我们的老队员登场完毕，下面看看新队员了。&lt;/p&gt;&lt;h2&gt;Adagrad&lt;/h2&gt;&lt;p&gt;Adagrad是一种自适应的梯度下降方法，它希望根据梯度的更新量调节梯度实际的影响值。如果一个变量的梯度更新量比较大，那么再后面的更新过程中我们会适当减少它的更新量，如果更新量比较小，那么我们可以适当地增加它的更新量。&lt;/p&gt;&lt;p&gt;它的参数更新公式如下所示：&lt;/p&gt;&lt;equation&gt;x -= lr * \frac{g}{\sqrt{\sum{g^2}} + \varepsilon }&lt;/equation&gt;&lt;p&gt;它的核心代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def adagrad(x_start, step, g, delta=1e-8):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    for i in range(50):
        grad = g(x)
        sum_grad += grad * grad
        x -= step * grad / (np.sqrt(sum_grad) + delta)
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们直接来看看它的表现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = adagrad([5, 5], 1.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8c10fca60f1f0ed10c2751afa410c228.png" data-rawwidth="789" data-rawheight="375"&gt;看上去和前面的算法没什么差别嘛……&lt;/p&gt;&lt;h2&gt;Rmsprop&lt;/h2&gt;&lt;p&gt;前面的Adagrad有一个很大的问题，那就是随着优化的进行，更新公式的分母项会变得越来越大。所以理论上更新量会越来越小，下面的算法Rmsprop就试图解决这个问题。在它的算法中，分母的梯度平方和不再随优化而递增，而是做加权平均：&lt;/p&gt;&lt;equation&gt;G_{t+1}=\beta*G_{t}+(1-\beta)*g^2&lt;/equation&gt;&lt;equation&gt;x_{t+1} = x_t - lr * \frac{g}{\sqrt{G_{t+1}} + \varepsilon }&lt;/equation&gt;&lt;p&gt;它的代码如下所示：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def rmsprop(x_start, step, g, rms_decay = 0.9, delta=1e-8):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    passing_dot = [x.copy()]
    for i in range(50):
        grad = g(x)
        sum_grad = rms_decay * sum_grad + (1 - rms_decay) * grad * grad
        x -= step * grad / (np.sqrt(sum_grad) + delta)
        passing_dot.append(x.copy())        
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x, passing_dot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们同样来看看它的表现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;res, x_arr = rmsprop([5, 5], 0.3, g)
contour(X,Y,Z, x_arr)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-21acefc27d381f2d5dc25150975e573e.png" data-rawwidth="788" data-rawheight="383"&gt;看上去和前面的表现还是差不多的。&lt;/p&gt;&lt;h2&gt;AdaDelta&lt;/h2&gt;&lt;p&gt;Rmsprop已经很好地解决了一部分的问题，但是在AdaDelta的眼中似乎还不够。在介绍它的论文中，作者详细阐述了Adagrad算法中得到的参数更新量的“单位”已经不太对了，于是给出了一堆公式用以证明我们需要增加一些计算来使得计算单位恢复正常，于是就有了下面的公式：&lt;/p&gt;&lt;equation&gt;G_{t+1}=\beta*G_{t}+(1-\beta)*g^2&lt;/equation&gt;&lt;equation&gt;\delta_{t+1}=\sqrt{\frac{\Delta_t+\varepsilon}{G_{t+1}+ \varepsilon}  }*g&lt;/equation&gt;&lt;equation&gt;x_{t+1} = x_{t} - lr * \delta_{t+1}&lt;/equation&gt;&lt;equation&gt;\Delta_{t+1}=\beta*\Delta_t+(1-\beta)*\delta_{t+1}&lt;/equation&gt;&lt;p&gt;具体的代码如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def adadelta(x_start, step, g, momentum = 0.9, delta=1e-1):
    x = np.array(x_start, dtype='float64')
    sum_grad = np.zeros_like(x)
    sum_diff = np.zeros_like(x)
    passing_dot = [x.copy()]
    for i in range(50):
        grad = g(x)
        sum_grad = momentum * sum_grad + (1 - momentum) * grad * grad
        diff = np.sqrt((sum_diff + delta) / (sum_grad + delta)) * grad
        x -= step * diff
        sum_diff = momentum * sum_diff + (1 - momentum) * (diff * diff)
        passing_dot.append(x.copy())
        if abs(sum(grad)) &amp;lt; 1e-6:
            break;
    return x, passing_dot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;它的表现如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3b87268bf774661e93d11f73340f2cae.png" data-rawwidth="784" data-rawheight="381"&gt;虽然上面算法的表现差不多，但是它们的learning rate还是有些不同的，感兴趣的童鞋可以换一些learning rate去尝试。&lt;/p&gt;&lt;p&gt;看上去我们并没有把算法讲完，也没有把实验做完……没错，留下尾巴，剩下的事情我们下回再说。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464537&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 20 Oct 2016 23:01:44 GMT</pubDate></item><item><title>VAE（4）——实现</title><link>https://zhuanlan.zhihu.com/p/22684931</link><description>终于到了实现的地方。前面干燥乏味的公式推导和理论阐述已经让很多人昏昏欲睡了，下面我们要提起精神，来看看这个模型的一个比较不错的实现——&lt;a href="https://github.com/cdoersch/vae_tutorial" data-editable="true" data-title="GitHub - cdoersch/vae_tutorial: Caffe code to accompany my Tutorial on Variational Autoencoders" class=""&gt;GitHub - cdoersch/vae_tutorial: Caffe code to accompany my Tutorial on Variational Autoencoders&lt;/a&gt;，当然，这个实现也是一个配套tutorial文章的实现。感兴趣的童鞋也可以看看这篇tutorial，相信会对这个模型有更多的启发。&lt;p&gt;这个实现的目标数据集是MNIST，这和我们之前的DCGAN是一样的。当然，在他的tutorial中，他一共展现了3个模型。下面我们就从prototxt文件出发，先来看看我们最熟悉的经典VAE。&lt;/p&gt;&lt;h2&gt;VAE&lt;/h2&gt;&lt;p&gt;说实话一看他在github给出的那张图，即使是有一定的VAE模型基础的童鞋也一定会感觉有些发懵。我们将模型中的一些细节隐去，只留下核心的数据流动和loss计算部分，那么这个模型就变成了下面的样子：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-29f6a96fe0f698f255db5bddb4b40fe8.jpg" data-rawwidth="960" data-rawheight="1280"&gt;图中的黑色的框表示数据的流动，红色的框表示求loss的地方。双红线表示两个不同部分的数据共享。可以看出图的上边是encoder的部分，也就是从X到z的过程，下面是从z到X的过程。前面的文章中我们给出了求解的公式，现在我们给出了这个网络模型，我们可以把这两部分对照起来。&lt;/p&gt;&lt;p&gt;另外其中的encoder和decoder部分被省略了，在实际网络中，我们可以用一个深度神经网络模型代替。除此之外，图中还有三个主要部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先是q(z|X)的loss计算。&lt;/li&gt;&lt;li&gt;其次是z的随机生成。&lt;/li&gt;&lt;li&gt;最后是p(X|z)的loss计算。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这其中最复杂的就是第一项，q(z|X)的loss计算。由于caffe在实际计算过程中主要采用向量的计算方式，所以前面的公式需要进行一定的变换：&lt;/p&gt;&lt;equation&gt;KL(p1(\mu_1,\sigma_1)||N(0,I))=\frac{1}{2}[-\sum_i{log [(\sigma_{1i})}] - d + \sum_i(\sigma_{1i})+\mu_1^T \mu_1]&lt;/equation&gt;&lt;equation&gt;=\sum_{i=0}^{d}{-\frac{1}{2}log[std_{1i}^2]}+\sum_{i=0}^{d}(-\frac{1}{2})+\sum_{i=0}^d\frac{1}{2}(std_{1i}^2)+\sum_{i=0}^d\frac{1}{2}[\mu_{1i}^2]&lt;/equation&gt;&lt;equation&gt;=\sum_{i=0}^{d}[{-\frac{1}{2}log[std_{1i}^2]}+\frac{1}{2}(std_{1i}^2)+\frac{1}{2}[\mu_{1i}^2]+(-\frac{1}{2})]&lt;/equation&gt;&lt;p&gt;在完成了前面的向量计算后，最后一步是做Reduction，也就是完成加和的过程。这样就使得计算可以顺利完成。&lt;/p&gt;&lt;p&gt;看懂了这些部分，再加上前面我们对VAE的了解，相信我们对VAE模型有了更加清晰的认识。&lt;/p&gt;&lt;h2&gt;MNIST生成模型可视化&lt;/h2&gt;&lt;p&gt;下面这张图是一次实验过程中产生的，看上去有点像所有数字在一个平面的分布，数字与数字之间还存在着一定的过渡区域。那么这张图是如何产生的呢？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-d7b3c6a70547ba8010249337c000a1b4.jpg" data-rawwidth="532" data-rawheight="532"&gt;一个比较简单的方法，就是把z的维度设为2。以下就是这幅图生成的过程：&lt;ol&gt;&lt;li&gt;利用VAE模型进行训练，得到了模型中的&lt;equation&gt;\mu&lt;/equation&gt;和&lt;equation&gt;\sigma&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;完成z的采样过程，我们在二维空间内按照N(0,I)有规律地进行采样noise，把noise、&lt;equation&gt;\mu&lt;/equation&gt;和&lt;equation&gt;\sigma&lt;/equation&gt;结合起来&lt;/li&gt;&lt;li&gt;把得到采样后的z，最后利用decoder把z转换成X，显示出来&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;经过这样几步我们就可以得到最终的图像了。实际上我们前面提过的GAN模型也可以用类似的方法生成这样的图像。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22684931&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 17 Oct 2016 20:57:51 GMT</pubDate></item><item><title>VAE(3)——公式与实现</title><link>https://zhuanlan.zhihu.com/p/22464768</link><description>前面两部分我们已经扫除了一些基本概念上的障碍，下面我们来直奔主题——VAE！&lt;p&gt;由于文章是一篇一篇写的，所以照顾到大家观看的情况，我们把前面介绍过的一些重要公式搬过来。&lt;/p&gt;&lt;p&gt;首先是系列第一篇的公式——多维高斯分布的KL散度计算公式：&lt;/p&gt;&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;p&gt;希望大家还有印象，如果没有印象就赶紧翻回去看看吧！&lt;/p&gt;&lt;p&gt;然后是上一回有关variational inference的推导公式：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{q(z) log p(X|z)}dz-KL(q(z)||p(z))&lt;/equation&gt;&lt;p&gt;还有上次的一句话：&lt;/p&gt;&lt;blockquote&gt;“VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数”&lt;/blockquote&gt;&lt;p&gt;好吧……专栏写成我这样也是醉了。为了保证每一篇文章的字数不要太长以至于让大家失去了看下去的耐心，这篇文章光是回顾已经花去了好大的篇幅。&lt;/p&gt;&lt;p&gt;好了，下面就是见证奇迹的时刻。&lt;/p&gt;&lt;h2&gt;Variational Autoencoder&lt;/h2&gt;&lt;p&gt;终于要见到正主了。让我们关注一下variational inference公式的右边，在此之前我们要对公式进行一定的变化，然后给出我们的详细建模过程。&lt;/p&gt;&lt;h2&gt;Reparameterization Trick&lt;/h2&gt;&lt;p&gt;为了更加方便地求解上面的公式，这里我们需要做一点小小的trick工作。上面提到了Q'(z|X)这个变分函数，它代表了当我们给定某个X的情况下z的分布情况。我们可以想象这里的z是满足某种分布的。那么我们从数值上可以把X抽离出来呢？&lt;/p&gt;&lt;p&gt;比方说我们有一个随机变量a服从高斯分布N(1,1)，根据定理我们可以定义一个随机变量b=a-1，那么它将服从高斯分布N(0,1)，换句话说，我们可以用一个均值为0，方差为1的随机变量加上1来表示现在的随机变量a。这样我们就把一个随机变量分成了两部分——一部分是确定的，一部分是随机的。&lt;/p&gt;&lt;p&gt;对于上面的Q'(z|X)，我们同样可以采用上面的方法完成。我们可以把一个服从这个条件概率的z拆分成两部分，一部分是一个复杂的函数&lt;equation&gt;g_\phi(X)&lt;/equation&gt;，它解决了确定部分的问题，我们再定义另外一个随机变量&lt;equation&gt;\varepsilon &lt;/equation&gt;，它负责随机的部分。为了书写的一致性，我们用&lt;equation&gt;g_\phi(X+\varepsilon )&lt;/equation&gt;来表示服从条件概率的z。&lt;/p&gt;&lt;p&gt;这样做有什么好处呢？现在我们知道了z条件概率值完全取决于生成它所使用的&lt;equation&gt;\varepsilon &lt;/equation&gt;的概率。也就是说如果&lt;equation&gt;z^{(i)}=g_\phi(X+\varepsilon^{(i)} )&lt;/equation&gt;，那么&lt;equation&gt;q(z^{(i)})=p(\varepsilon ^{(i)})&lt;/equation&gt;，那么上面关于变分推导的公式也就变成了下面的公式：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{p(\varepsilon ) log p(X|g_{\phi}(X,\varepsilon )  )}dz-KL(q(z|X,\varepsilon  )||p(z))&lt;/equation&gt;&lt;p&gt;这就是替换的一小步，求解的一大步！实际上到了这里，我们已经接近问题最终的答案了，剩下的只是我们的临门一脚——我们可不可以假设这个随机部分服从什么样的分布呢？&lt;/p&gt;&lt;p&gt;当然能！不过由于我们一般把z的先验假设成一个多维的独立高斯分布，为了KL计算的方便，也为了我们在前面的章节推导2个多维高斯分布的KL散度这件事情没有白做，我们决定在这里让这个替换后的随机部分同样服从多维的独立高斯分布。&lt;/p&gt;&lt;p&gt;下面我们来看看这个公式的两部分具体该如何计算。&lt;/p&gt;&lt;h2&gt;右边的第二项，KL散度部分——encoder&lt;/h2&gt;&lt;p&gt;首先来看看公式右边的第二项。刚才我们提到我们一般把z的先验假设成一个多维的独立高斯分布，这里我们可以给出一个更强的假设，那就是这个高斯分布的均值为0，方差为单位矩阵，那么我们前面提到的KL散度公式就从：&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;瞬间简化成为：&lt;/p&gt;&lt;equation&gt;KL(p1||N(0,I))=\frac{1}{2}[-log [det(\Sigma_1)] - d + tr(\Sigma_1)+\mu_1^T \mu_1]&lt;/equation&gt;&lt;p&gt;真的有种世界清静了的感觉……我们下面的目标就是利用encoder的部分根据X求解z的均值方差。这部分我们采用一个深度的神经网络就可以了。由于实际训练过程中我们采用的是batch的训练方法，因此我们需要输入一个batch的X信息，然后进行模型的计算和优化。&lt;/p&gt;&lt;p&gt;如果我们用一个向量&lt;equation&gt;\sigma_1&lt;/equation&gt;来表示上面协方差矩阵的主对角线，情况将会更加美好：&lt;/p&gt;&lt;equation&gt;KL(p1(\mu_1,\sigma_1)||N(0,I))=\frac{1}{2}[-\sum_i{log [(\sigma_{1i})}] - d + \sum_i(\sigma_{1i})+\mu_1^T \mu_1]&lt;/equation&gt;&lt;p&gt;到这里，关于这一部分的函数拟合已经比较清晰了，我们的函数输入输出已经非常清楚，我们的loss也化简到了一个比较简单的状态，下面就是具体的计算了。&lt;/p&gt;&lt;h2&gt;右边的第一项，期望部分——decoder&lt;/h2&gt;&lt;p&gt;从前面的KL散度公式优化中，我们可以看到，如果两个概率分布的KL散度值为0时，实际上就说明我们的随机部分的分布和我们z的先验分布相同了。&lt;/p&gt;&lt;p&gt;这带来一个好消息，就是我们可以直接使用上一步encoder得到的均值方差。这样，我们就需要另外一个深度函数decoder，帮助我们从z再变回X。前面我们说了我们的目标是最大化似然的期望，实际上就可以转换为采样一批X，先用encoder生成z‘的分布，然后通过优化使得p(X|z)的似然最大化。&lt;/p&gt;&lt;p&gt;关于如何最大化似然，我们有很多办法，这一部分将在实践的环节详细给出。&lt;/p&gt;&lt;p&gt;好了，到这里，实际上VAE的核心计算推导就结束了。我们已经花了3篇文章的时间把这个模型讲完了，怎么可以就这样结束呢？下一回我们来看看一个实现的代码，同时来看看基于经典VAE演变的一些模型是什么样子。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"5群已经准确就绪：583914960，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;583914960！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464768&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 13 Oct 2016 21:08:26 GMT</pubDate></item><item><title>聊点轻松的4——这回真的很轻松</title><link>https://zhuanlan.zhihu.com/p/22842859</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea12f296f26f16a28cdbf343098e66f9_r.jpg"&gt;&lt;/p&gt;等了好久终于等到今天——专栏关注人数突破4000人了！其实这篇文章是我在突破4000之前就写好了，希望各位童鞋多多关注，多多点赞，给我更多创作的动力！第一次这么赤裸裸地求关注求赞！&lt;p&gt;好了，再次回顾一下，专栏已经发表了33篇文章（包括这篇），除去一篇开篇文章和4篇“轻松”文，还有28篇干货文章，我们从神经网络的基础聊起，到Caffe的源码阅读，到axvier初始化，网络结构，反卷积，GAN，VAE，我们已经聊了许多和CNN相关的话题，后面我们还会继续聊更多CNN和其他机器学习算法，欢迎大家持续关注。&lt;/p&gt;&lt;p&gt;这一回我们来看看我们前面提过的这些知识点的作者，以及这些作者现在都在哪里发光发热。古龙先生的小说里曾经写过百晓生写兵器谱，我们这里也来一次挥斥方遒，指点江山，看看这些大神们现在都在哪门哪派，看看现在的江湖局势是什么样的。&lt;/p&gt;&lt;h2&gt;Yann LeCun&lt;/h2&gt;&lt;p&gt;没错就是这个在之前被大家玩坏了的名字……我们来看看知友们挡不住的起名热情……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-aa45eb1daadc1155f6af954d8b0fef81.png" data-rawwidth="152" data-rawheight="486"&gt;但当我今天在google上搜索时，google的翻译还是让我震惊了一下……&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1c51a5db44796446a2b24475dca1a852.png" data-rawwidth="453" data-rawheight="155"&gt;忽然就觉得还是知友们的名字好听啊……&lt;/p&gt;&lt;p&gt;扯蛋的话到此结束，这位大神的事迹够多够牛，可以说CNN的诞生和他有密不可分的关系，他创建了江湖上第一个响当当的CNN模型——LeNet！乐网！希望这个名字将来不会被乐视抢注……&lt;/p&gt;&lt;p&gt;其他的事迹这里我们就不多说了，最后说一句，LeCun大叔任职Facebook AI Director。Facebook加一分！&lt;/p&gt;&lt;h1&gt;Yoshua Bengio&lt;/h1&gt;&lt;p&gt;关于这位大叔，我们该起一个什么“艺名”呢？&lt;/p&gt;&lt;p&gt;作为江湖上鼎鼎大名的殿堂级教授，Yoshua为我们贡献许多高质量的文章。我们前面聊过的“xavier”初始化算法就有他的身影。多说一句，这位Xavier，也就是介绍这个算法论文的一作Xavier Glorot在Google DeepMind。&lt;/p&gt;&lt;p&gt;除此之外，每年他也为深度学习界贡献大量优质的文章，这里就不赘述了。&lt;/p&gt;&lt;h2&gt;Geoffrey Hinton&lt;/h2&gt;&lt;p&gt;这位大叔同样是江湖中的传说人物。虽然在专栏中我们还没有直接接触到与他相关的内容，但是大名鼎鼎的Alexnet——可是有他的份的。他现在在Google兼职作Distinguished Researcher。&lt;/p&gt;&lt;h2&gt;Karen Simonyan&lt;/h2&gt;&lt;p&gt;这位大神是VGG Net的一作，GoogleScholar显示他在Google DeepMind&lt;/p&gt;&lt;h3&gt;Andrew Zisserman&lt;/h3&gt;&lt;p&gt;VGG Net的另一位作者，他竟然还是多目视觉经典著作"&lt;a href="http://www.robots.ox.ac.uk/~vgg/hzbook/" data-editable="true" data-title="Multiple View Geometry in Computer Vision"&gt;Multiple View Geometry in Computer Vision&lt;/a&gt;"的作者啊……&lt;/p&gt;&lt;h2&gt;Christian Szegedy&lt;/h2&gt;&lt;p&gt;GoogleNet的一作，当然是在Google了……&lt;/p&gt;&lt;h2&gt;何恺明&lt;/h2&gt;&lt;p&gt;ResNet的一作，曾经在MSRA，现在转投Facebook AI Research了。这位大神在拿奖方面可以说是相当厉害，他在个人主页上也小炫耀了一下——&lt;/p&gt;&lt;blockquote&gt;I have received &lt;b&gt;2 CVPR Best Paper Awards&lt;/b&gt; as the first author (2009 and 2016)&lt;/blockquote&gt;&lt;p&gt;看到这句话就有种要送膝盖的感觉啊……&lt;/p&gt;&lt;h2&gt;Ian Goodfellow&lt;/h2&gt;&lt;p&gt;Generative Adversarial Networks的一作，现在在OpenAI。大神还是Maxout模型论文的一作，这里我们又看到了尾作大神Yoshua了……&lt;/p&gt;&lt;h2&gt;Diederik P. Kingma&lt;/h2&gt;&lt;p&gt;Variational Autoencoder的作者，现在在OpenAI。&lt;/p&gt;&lt;p&gt;好了，暂时先到这里。首先感谢各位大神，没有自己的辛勤的工作写出这么多高质量的paper，就没有我在后面费劲地读各位的大作在这里写科普小文章。希望各位大神文章不要停，不然我就没得写啦！&lt;/p&gt;&lt;p&gt;另外我们也看到一点，那就是这些大牛不是在学校任教，就是在一线大公司工作。果然还是大公司有钱任性，能够招揽各路高手前来效力。看起来现在深度学习的成果主要还是被这些大公司获取了啊啊啊啊！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;既然大家都关注了我的专栏，不如顺道关注下我呗：&lt;a href="https://www.zhihu.com/people/3ef5c095dad2c81afb977771cfae27a6" data-hash="3ef5c095dad2c81afb977771cfae27a6" class="member_mention" data-title="@冯超" data-hovercard="p$b$3ef5c095dad2c81afb977771cfae27a6" data-editable="true"&gt;@冯超&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22842859&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 11 Oct 2016 21:19:39 GMT</pubDate></item><item><title>VAE(2)——基本思想</title><link>https://zhuanlan.zhihu.com/p/22464764</link><description>上一回我们花了很大的篇幅介绍了KL散度，这一回我们来看看VAE，尤其是深度模型下的VAE。&lt;p&gt;前面我们已经见过了许多优秀的深度学习模型，它们达到了非常好的精度和效果。众人曾十分认真地分析过为什么这些模型的效果这么好，结论是深度模型的非线性拟合能力确实很强。不管曾经多么复杂的问题，一个深度模型出马，立刻把问题解决的八九不离十。VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数，从而解决实际问题。&lt;/p&gt;&lt;p&gt;让我们先记住这个trick，后面我们会用到它。接下来我们要上场的是生成模型。前面我们看过的很多模型从原理上来说都是判别式模型。我们有一个等待判别的事物X，这个事物有一个类别y，我们来建立一个模型f(x;w)，使得p(y|X)的概率尽可能地大，换种方法说就是让f(x;w)尽可能地接近y。&lt;/p&gt;&lt;p&gt;如果我们想用生成式的模型去解决这个问题，就需要利用贝叶斯公式把这个问题转换过来：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{p(X)}&lt;/equation&gt;&lt;p&gt;为了遵从大多数教科书上的变量用法，这里将y变成了z。当然，这个时候的z可能比上面提到的“类别”y要复杂一些。在很多的生成模型中，我们把z称作隐含变量，把X称作观测变量。一般来说，我们可以比较容易地观察到X，但是X背后的z却不那么容易见到，而很多时候X是由z构造出来的，比方说一天的天气好与坏是由很多不易观察的因素决定的。于是我们自然而然就有了一个需求，当我们拿到这些X之后，我们想知道背后的z是什么，于是乎就有了上面那个公式。&lt;/p&gt;&lt;p&gt;对于一些简单的问题，上面的公式还是比较容易解出的，比方说朴素贝叶斯模型，但是还是有很多模型是不易解出的，尤其当隐含变量处于一个高维度的连续空间中：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{\int_z{p(X|z)p(z)dz}}&lt;/equation&gt;&lt;p&gt;这里的积分就没那么容易搞定了。于是乎，各路大神开始想尽一切办法让上面的式子变得好解些。&lt;/p&gt;&lt;p&gt;这时候我们难免会冒出一个问题，既然有了判别式模型可以直接求解式子左边的那个东西，为什么非要把它变成右边那一大堆东西，搞得自己不方便解呢？其实谁都不想给自己找麻烦，可问题是右边的这一堆除了能够解这个问题，它还有一个更加高级的功能，就是根据模型随机生成X。&lt;/p&gt;&lt;p&gt;我们可以想想看，如果我们只拥有式子左边的p(z|X)，我们想要生成一个符合某种z的X该怎么办？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一步，随机一个X；&lt;/li&gt;&lt;li&gt;第二步，用p(z|X)计算概率，如果概率满足，则结束，如果不满足，返回第一步；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;于是乎，用判别式模型生成X变成了人品游戏，谁也不知道自己什么时候能在第二步通过。而生成式模型就不同了，我们可以按需定制，首先确定好z，然后根据p(X|z)进行随机采样就行了，生成X的过程安全可控。&lt;/p&gt;&lt;p&gt;说了这么多，下面我们正式进入公式推导的部分。&lt;/p&gt;&lt;h2&gt;Variational Inference&lt;/h2&gt;&lt;p&gt;虽然我们鼓吹了很多生成模型的好处，但是面对等号右边那一堆东西，该束手无策还是束手无策。但是，前辈们还是想到了一些精妙的解法。既然用概率论的方法很难求出右边的东西，我们能不能做一些变换，比方说——（略显生硬地）我们用一个variational的函数q(z)去代替p(z|X)？别着急，后面我们会看到它带来的好处的。&lt;/p&gt;&lt;p&gt;这里的variational inference介绍的有点简单，有机会我们再详细介绍下。&lt;/p&gt;&lt;p&gt;既然要用q(z)这个新东西去代替p(z|X)，那么我们当然希望两个东西尽可能地相近，于是乎我们选择了KL散度这个指标用来衡量两者的相近程度。由于两边都是可以看作针对z的概率分布，因此用KL散度这个指标实际上非常合适。&lt;/p&gt;&lt;p&gt;所以就有了：&lt;/p&gt;&lt;equation&gt;KL(q(z)||p(z|X))=\int{q(z)log \frac{q(z)}{p(z|X)}}dz&lt;/equation&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(z|X)]}dz&lt;/equation&gt;&lt;p&gt;我们做一下贝叶斯公式的变换，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z) + logp(X)]}dz&lt;/equation&gt;&lt;p&gt;再将和z无关的项目从积分符号中拿出来，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z)]}dz + log p(X)&lt;/equation&gt;&lt;p&gt;左右整理一下，就得到了：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{q(z) log p(X|z)}dz-KL(q(z)||p(z))&lt;/equation&gt;&lt;p&gt;好吧，其实整理了一圈，这个公式还是很乱，不过因为KL散度的特殊关系，我们还是从这个公式中看到了一丝曙光：&lt;/p&gt;&lt;p&gt;我们虽然不大容易求出p(X)，但我们知道当X给定的情况下，p(X)是个固定值。那么如果我们希望KL(q(z)||p(z|X))尽可能地小，也就相当于让等号右边的那部分尽可能地大。其中等号右边的第一项实际上是基于q(z)的似然期望，第二项又是一个负的KL散度，所以我们可以认为，为了找到一个好的q(z)，使得它和p(z|X)尽可能地相近，我们需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;右边第一项的log似然的期望最大化&lt;/li&gt;&lt;li&gt;右边第二项的KL散度最小化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于VAE之前的variation inference（中文可以翻译成变分推断），到这里我们就要开始一段全新的公式推导了。比方说我们做一个mean-field assumption（说实话我不太知道mean-field怎么翻译更直观，于是就把英文放在这里了），于是乎对于多个隐含变量组成的z，分量相互之间是独立的，于是根据这个特性，我们又可以进行进一步地公式化简。由于我们今天的主题是VAE，所以关于这部分我们就不再赘述了。这时候我们又想起了文章开头我们提到的一句话：&lt;/p&gt;&lt;blockquote&gt;“VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数”&lt;/blockquote&gt;&lt;p&gt;那么是时候让这句话发挥作用了，不过关于它发挥的方法我们下回再说。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464764&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 09 Oct 2016 19:09:29 GMT</pubDate></item><item><title>VAE（1）——从KL说起</title><link>https://zhuanlan.zhihu.com/p/22464760</link><description>前面我们介绍了GAN——Generative Adversarial Network，这个网络组是站在对抗博弈的角度去展现生成模型和判别模型各自的威力的，下面我们来看看这种生成模型和判别模型组合的另一个套路——Variational autoencoder，简称VAE。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-856cfcd1acf4b155373953460cd7adbd.png" data-rawwidth="374" data-rawheight="601"&gt;突然想起来，他也叫VAE，我觉得他还是有点音乐才华的。不过我们今天不去讨论他。&lt;/p&gt;&lt;p&gt;Variational autoencoder的概念相对复杂一些，它涉及到一些比较复杂的公式推导。在开始正式的推导之前，我们先来看看一个基础概念——KL divergence，翻译过来叫做KL散度。&lt;/p&gt;&lt;h2&gt;什么是KL散度&lt;/h2&gt;&lt;p&gt;无论从概率论的角度，还是从信息论的角度，我们都可以很好地给出KL散度测量的意义。这里不是基础的概念介绍，所以有关KL的概念就不介绍了。在Variational Inference中，我们希望能够找到一个相对简单好算的概率分布q，使它尽可能地近似我们待分析的后验概率p(z|x)，其中z是隐变量，x是显变量。在这里我们的“loss函数”就是KL散度，他可以很好地测量两个概率分布之间的距离。如果两个分布越接近，那么KL散度越小，如果越远，KL散度就会越大。&lt;/p&gt;&lt;p&gt;KL散度的公式为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\sum{p(x)log\frac{p(x)}{q(x)}}&lt;/equation&gt;，这个是离散概率分布的公式，&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\int{p(x)log{\frac{p(x)}{q(x)}}dx}&lt;/equation&gt;，这个是连续概率分布的公式。&lt;/p&gt;&lt;p&gt;关于其他KL散度的性质，这里就不赘述了。&lt;/p&gt;&lt;h2&gt;KL散度的实战——1维高斯分布&lt;/h2&gt;&lt;p&gt;我们先来一个相对简单的例子。假设我们有两个随机变量x1,x2，各自服从一个高斯分布&lt;equation&gt;N_1(\mu_1,\sigma_1^2),N_2(\mu_2,\sigma_2^2)&lt;/equation&gt;，那么这两个分布的KL散度该怎么计算呢？&lt;/p&gt;&lt;p&gt;我们知道&lt;/p&gt;&lt;equation&gt;N(\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{(x-\mu)^2}{2\sigma^2}}&lt;/equation&gt;&lt;p&gt;那么KL(p1,p2)就等于&lt;/p&gt;&lt;equation&gt;\int{p_1(x)log\frac{p_1(x)}{p_2(x)}}dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log{p_1(x)}}dx-log{p_2(x)})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)}*(log{\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{\frac{(x-\mu_1)^2}{2\sigma_1^2}}}-log{\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{\frac{(x-\mu_2)^2}{2\sigma_2^2}}})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)
*(-\frac{1}{2}log2\pi-log{\sigma_1}-\frac{(x-\mu_1)^2}{2\sigma_1^2}}+
\frac{1}{2}log{2\pi}+log{\sigma_2}+\frac{(x-\mu_2)^2}{2\sigma_2^2})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log\frac{\sigma_2}{\sigma_1}+[\frac{(x-\mu_2)^2}{2\sigma_2^2}-\frac{(x-\mu_1)^2}{2\sigma_1^2}])dx}&lt;/equation&gt;&lt;equation&gt;=\int(log\frac{\sigma_2}{\sigma_1})p_1(x)dx+\int(\frac{(x-\mu_2)^2}{2\sigma_2^2})p_1(x)dx-\int(\frac{(x-\mu_1)^2}{2\sigma_1^2})p_1(x)dx&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2\sigma_1^2}\int((x-\mu_1)^2)p_1(x)dx&lt;/equation&gt;&lt;p&gt;（更新）到这里停一下，有童鞋问这里右边最后一项的化简，这时候积分符号里面的东西是不看着很熟悉？没错，就是我们常见的方差嘛，于是括号内外一约分，就得到了最终的结果——&lt;equation&gt;\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;好，继续。&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_1+\mu_1-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+\int{(\mu_1-\mu_2)^2}p_1(x)dx+2\int{(x-\mu_1)(\mu_1-\mu_2)]}p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+(\mu_1-\mu_2)^2]-\frac{1}{2}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}&lt;/equation&gt;&lt;p&gt;说实话一直以来我不是很喜欢写这种大段推导公式的文章，一来原创性比较差（都是前人推过的，我就是大自然的搬运工），二来其中的逻辑性太强，容易让人看蒙。不过最终的结论还是得出来了，我们假设N2是一个正态分布，也就是说&lt;equation&gt;\mu_2=0,\sigma_2^2=1&lt;/equation&gt;那么N1长成什么样子能够让KL散度尽可能地小呢？&lt;/p&gt;&lt;p&gt;也就是说&lt;equation&gt;KL(\mu_1,\sigma_1)=-log\sigma_1+\frac{\sigma_1^2+\mu_1^2}{2}-\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;我们用“肉眼”看一下就能猜测到当&lt;equation&gt;\mu_1=0,\sigma_1=1&lt;/equation&gt;时，KL散度最小。从公式中可以看出，如果&lt;equation&gt;\mu_1&lt;/equation&gt;偏离了0，那么KL散度一定会变大。而方差的变化则有些不同：&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;大于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越大，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越小；&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;小于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越小，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越大；&lt;/p&gt;&lt;p&gt;那么哪边的力量更强大呢？我们可以作图出来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0.5,2,100)
y = -np.log(x)+x*x/2-0.5
plt.plot(x,y)
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从图中可以看出&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-68395494c66b37550efea74beca61d7e.png" data-rawwidth="559" data-rawheight="431"&gt;二次项的威力更大，函数一直保持为非负，这和我们前面提到的关于非负的定义是完全一致的。&lt;/p&gt;&lt;p&gt;好了，看完了这个简单的例子，下面让我们再看一个复杂的例子。&lt;/p&gt;&lt;h2&gt;一个更为复杂的例子：多维高斯分布的KL散度&lt;/h2&gt;&lt;p&gt;上一回我们看过了1维高斯分布间的KL散度计算，下面我们来看看多维高斯分布的KL散度是什么样子？说实话，这一次的公式将在后面介绍VAE时发挥很重要的作用！&lt;/p&gt;&lt;p&gt;首先给出多维高斯分布的公式：&lt;/p&gt;&lt;equation&gt;p(x_1,x_2,...x_n)=\frac{1}{\sqrt{2\pi*det(\Sigma)}}e^{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}&lt;/equation&gt;&lt;p&gt;由于这次是多维变量，里面的大多数计算都变成了向量、矩阵之间的计算。我们常用的是各维间相互独立的分布，因此协方差矩阵实际上是个对角阵。&lt;/p&gt;&lt;p&gt;考虑到篇幅以及实际情况，下面直接给出结果，让我们忽略哪些恶心的推导过程:&lt;/p&gt;&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;p&gt;其实这一次我们并没有介绍关于KL的意义和作用，只是生硬地、莫名其妙地推导一堆公式，不过别着急，下一回，我们展示VAE效果的时候，就会让大家看到KL散度的作用。&lt;/p&gt;&lt;p&gt;坚持看到这里的童鞋是有福的，来展示一下VAE的解码器在MNIST数据库上产生的字符生成效果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c622c0419cbffbdb66485c1966afa68e.png" data-rawwidth="298" data-rawheight="298"&gt;从这个效果上来看，它的功能和GAN是有点像的，那么让我们来进一步揭开它的庐山真面目吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464760&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 06 Oct 2016 19:06:42 GMT</pubDate></item><item><title>寻找CNN的弱点</title><link>https://zhuanlan.zhihu.com/p/22464575</link><description>CNN是现在十分火热的模型，在很多图像检索问题上，CNN模型的效果在以往的基础上有了很大的提高，但是CNN毕竟没有把这些问题完全解决，CNN还是有它自己的弱点的。这个弱点也不能算作是它独有的问题，但是由于它的效果实在太好了，很多人甚至对它产生了迷信，因此这盆冷水就泼到它身上了。&lt;p&gt;大神们看到了CNN模型的强大，但忍不住提出一个问题：CNN有没有什么搞不定的地方？比方说我们用CNN构建了一个人脸识别的模型，在训练数据集和测试数据集上表现良好，但是会不会有一些用例是它会误判的，而且我们可以找到规律生成这些用例？&lt;/p&gt;&lt;p&gt;我们可以想象，如果我们对之前识别正确的数据做轻微的改动，那么它还是有可能识别正确的。于是我们就有了一个方案，我们每将图像做一点改动，就把图像传入CNN做一下测试，然后看看CNN的预测结果有没有发生改变，如果没有发生改变，我们就保存这个图像，接着我们再进行下一轮的改动，经过若干轮的改动后，我们把生成的图像输出出来看看图像会变成什么样子。&lt;/p&gt;&lt;p&gt;这里我们将采用MNIST为例，以下的就是我们的改动方案：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用MNIST的训练集训练一个CNN的模型，我们的CNN模型结构是：conv32*3*3-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;conv64*3*6-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;fc256-&amp;gt;dropout0.5-&amp;gt;fc10。&lt;/li&gt;&lt;li&gt;找到一个训练数据，将其数据范围限定在0到1之间，我们对每一个像素点随机增减-0.1到0.1之间的一个数，这样得到64个随机的图像，然后经过CNN模型预测得到这64个图像的预测label，从中选择一个和原始label相同的图像。经过若干轮迭代后，我们就可以看看这个随机改变的数字变成了什么样子。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们选择了一个数字0：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1a666d86f42923d245036bec25270c59.png" data-rawwidth="421" data-rawheight="424"&gt;经过50轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c3725d6bbb1cde8c2f236cf24a63b5f4.png" data-rawwidth="423" data-rawheight="423"&gt;&lt;p&gt;经过100轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/38c4258464d5ff070a64f3a636a0864b.png" data-rawwidth="421" data-rawheight="421"&gt;&lt;p&gt;经过150轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/06d7faa4bd8245d98ec1ae265c98cd48.png" data-rawwidth="424" data-rawheight="426"&gt;&lt;p&gt;经过200轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/232da9eb664724791b4181a2a636edad.png" data-rawwidth="426" data-rawheight="425"&gt;到此为止，可以看出这个数字还是隐约可见，但是实际上图像已经变得模糊不清，大量的杂乱信息混入其中，已经和原始的数字完全不同。&lt;/p&gt;&lt;p&gt;这个套路被称作“fool CNN”，用东北话说就是忽悠。继续迭代下去，我们还能生成出更精彩的图像。当然这也只是忽悠CNN模型的一种办法，我们还有其他的办法来生成图像。其他的办法这里就不再介绍了。关于这种忽悠，大神们也给出了和机器学习有关的解释：&lt;/p&gt;&lt;p&gt;CNN的模型说到底还是个判别式模型，如果我们把图像设为X，label设为y，CNN的模型就相当于求p(y|X)的值。判别式模型相当于描述“什么样的图像是这个label的图像”，而满足了这些条件的图像有时并不是具有真实label的那个图像。而上面的忽悠套路就是利用了这个漏洞。&lt;/p&gt;&lt;p&gt;上面的例子中，我们用这种fool的方法让一张模糊不清的图像保持了原来的label，同时我们也可以让一张不算模糊的图像被CNN错认成另外一个label。&lt;/p&gt;&lt;p&gt;比方说下面这张经过40轮迭代的图像被认成了6：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2444cb2b26a5ee30177d5eb47f4b5abb.png" data-rawwidth="419" data-rawheight="422"&gt;这些套路的出现都让我们对CNN有了一些警惕，如果想让CNN对手写数字完全hold住，我们还需要其他的方法辅助，不然的话这种意外总会发生。&lt;/p&gt;&lt;p&gt;那么有没有什么方法能解决这样的问题呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464575&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 29 Sep 2016 22:48:26 GMT</pubDate></item><item><title>DCGAN的小尝试（2）</title><link>https://zhuanlan.zhihu.com/p/22389906</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-editable="true" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;上一回我们只是简单地展示了基于keras框架、MNIST数据集的DCGAN模型的结果，下面我们来详细地看一下这个代码的实现。&lt;h2&gt;生成模型的结构&lt;/h2&gt;&lt;pre&gt;&lt;code lang="text"&gt;def generator_model():
    model = Sequential()
    model.add(Dense(input_dim=100, output_dim=1024))
    model.add(Activation('tanh'))
    model.add(Dense(out_dim=128*7*7))
    model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=64, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=1, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;直接上代码了。keras的代码总体上比较直观，我在里面加了一些参数对应的描述，应该编译不过，但是会比较好理解。&lt;/p&gt;&lt;p&gt;这里需要说明的一点是，这个实现中的激活函数都是双曲正切，和论文中的描述不一样。当然，和论文中的模型架构也不一样，不过两者的数据集也不一样。&lt;/p&gt;&lt;h2&gt;判别模型的结构&lt;/h2&gt;&lt;p&gt;判别模型的结构如下所示，仔细地读一遍就可以理解，这里不需要赘述了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def discriminator_model():
    model = Sequential()
    model.add(Convolution2D(
                        64, 5, 5,
                        border_mode='same',
                        input_shape=(1, 28, 28)))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(128, 5, 5))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('tanh'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    return model&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;训练&lt;/h2&gt;&lt;p&gt;这里的训练的一轮迭代可以用下面的流程表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一批generated_image&lt;/li&gt;&lt;li&gt;将真实的数据和generated_image合并，并放入D中进行一轮训练，其中真实数据的label为1，generated_image的label为0。&lt;/li&gt;&lt;li&gt;利用G再生成一批generated_image&lt;/li&gt;&lt;li&gt;这一次将G和D连起来，并给第3步的gereated_image的label设为1，固定D的参数不变，进行一轮训练。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;可以看出第1，2步是为了优化D，第3，4步是为了优化G，而两者之间还是存在着紧密的联系。&lt;/p&gt;&lt;h2&gt;图像生成&lt;/h2&gt;&lt;p&gt;图像生成的过程可以用如下两步表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一大批generated_image&lt;/li&gt;&lt;li&gt;利用D计算这些generated_image的分类结果，把得分高的一批选出来&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;最终我们看到的就是从D的眼皮下逃出的优质的生成数据。&lt;/p&gt;&lt;p&gt;好了，前面对代码的几个核心部分做了介绍，下面我们来看看实验过程中的一些问题。&lt;/p&gt;&lt;h2&gt;图像的演变过程&lt;/h2&gt;&lt;p&gt;在优化刚开始时，从随机生成的100维向量生成的图像是这样子的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dee6ed0084ef2fbf17d76e2b0e947e9b.png" data-rawwidth="308" data-rawheight="336"&gt;其实就是噪声。&lt;/p&gt;&lt;p&gt;经过400轮的迭代，生成模型可以生成下面的图像了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/30549e49b67fb85af5acb25cd90042fa.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出数字的大体结构已经形成，但是能够表征数字细节的特征还没有出现。&lt;/p&gt;&lt;p&gt;经过10个Epoch后，生成模型的作品：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/93ba20c5958eee5a8db956743a580c40.png" data-rawwidth="308" data-rawheight="336"&gt;这时候有些数字已经成型，但是还有一些数字仍然存在欠缺。&lt;/p&gt;&lt;p&gt;然后是20轮Epoch的结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/10e43906800c936ea133757c98fa83be.png" data-rawwidth="308" data-rawheight="336"&gt;这个时候的数字已经具有很强的辨识度，但是与其同时，我们发现生成的数字中有大量的“1”。&lt;/p&gt;&lt;p&gt;当完成了所有的训练，我们拿出生成模型在最后一轮生成的图像，可以看到：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2b4d98d848f9d2da7a304f0fd0bc7f01.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出这里面的数字质量更高一些，但是里面的1也更多了。&lt;/p&gt;&lt;p&gt;从这个演化过程中，我们可以看出，从一开始的数字生成质量都很差但生成数字的多样性比较好，到后来的数字质量比较高但数字的多样性比较差，模型的特性在不断地发生变化。这也和两个模型的对抗有关系，而这个演变也和增强学习中的“探索-利用”困境有关系。&lt;/p&gt;&lt;p&gt;我们站在生成模型的角度去想，一开始生成模型会尽可能地生成各种各样形状的数字，而判别模型会识别出一些形状较差的模型，而放过一些形状较好的模型，随着学习的进程不断推进，判别模型的能力也在不断地加强，生成模型慢慢发现有一些固定的套路比较容易通过，而其他一些套路不那么容易通过，于是它就会尽可能地增大这些“套路”出现的概率，让自己的loss变小。这样，一个从探索为主的模型变成了一个以利用为主的模型，实际上它的数据分布已经不那么均匀了。&lt;/p&gt;&lt;p&gt;如果这个模型继续训练下去，生成模型有可能进一步地利用这个“套路”，这和我们传统意义上的过拟合也有很相近的地方。所以我们希望能够避免这样的过拟合。&lt;/p&gt;&lt;p&gt;这个小实验也到此结束了，下面我们将看看论文中关于DCGAN的介绍，以及关于模型的一些改进方案和其他框架的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22389906&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 26 Sep 2016 09:55:52 GMT</pubDate></item><item><title>DCGAN的小尝试（1）</title><link>https://zhuanlan.zhihu.com/p/22386494</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;话说当今的深度学习网络框架世界，除了Caffe，还有很多不错的框架。这一次为了省事，我们直接找一个开源的应用进行分析和尝试。而这次的框架主角是keras，一个拥有简洁API的框架。而我们今天的主角来自深度学习界的大神Yann LeCun（我比较喜欢叫他颜乐存，哈哈……）在Quora上的对这个问题的回答：&lt;p&gt;&lt;a class="" href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning" data-editable="true" data-title="What are some recent and potentially upcoming breakthroughs in deep learning?"&gt;What are some recent and potentially upcoming breakthroughs in deep learning?&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;There are many interesting recent development in deep learning, probably too many for me to describe them all here. But there are a few ideas that caught my attention enough for me to get personally involved in research projects.&lt;/p&gt;&lt;p&gt;The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks).&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;总结成一句话：深度学习的未来，就是干（GAN）！&lt;/p&gt;&lt;p&gt;听上去就让人热血沸腾啊～&lt;/p&gt;&lt;p&gt;抱着极大的好奇心，我们开始对GAN的探险之旅。&lt;/p&gt;&lt;h2&gt;DCGAN&lt;/h2&gt;&lt;p&gt;如果从GAN的起点开始聊起，那么等我们聊到正题，估计好几集都过去了。所以让我们忘掉前面的种种解法，直接来到我们的深度学习部分：DCGAN。全称是Deep Convolution GAN。也就是用深度卷积网络进行对抗生成网络的建模。&lt;/p&gt;&lt;p&gt;对抗神经网络（GAN）有两个主角——&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个是G（Generator）,也就是生成模型；它的输入是一个随机生成的向量，长度不定，输出是一个具有一定大小的图像（N*N*3）和（N*N*1）。&lt;/li&gt;&lt;li&gt;一个是D（Discriminator），也就是判别模型。在我们接下来介绍的模型中，它的输入维度和G的输出一样，输出是一个长度为1 的向量，数字的范围从0到1，表示图像像一个正常图片的程度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;G的输入和输出都比较好理解，D的输入也比较好理解，那么D的输出是什么含义呢？它表示了对给定输出是否像我们给定的标准的输入数据。这句话可能有点绕口，我们可以把判别模型理解成一个解决分类问题的模型，那么在这个问题中判别模型的结果就是区分一个输入属于下面两个类别中的哪个——“正常输入”和“非正常输入”。&lt;/p&gt;&lt;p&gt;举个更具体的例子。对于MNIST数据集来说，每一个手写的数字都可以认为是一个“正常输入”，而随便生成的一个不像手写数字的输入都可以认为是一个“非正常输入”。而我们的判别模型就是要判断这个问题，我们学习的目标也是学习出一个能够解决这个问题的模型。&lt;/p&gt;&lt;p&gt;那么，我们的生成模型的目标呢？就是我们能够从一个随机生成的向量生成一样“正常输入”的图像。听上去有点神奇吧，不过现实中这个效果是可以实现的。我们可以想象我们的输入空间是满足某种分布的一个空间，对于空间中的每一个点，我们都可以利用生成模型将其映射成为一个图像，现在我们限定了生成的图像必须是“正常输入”，那么输入和输出在某种程度上已经确定，我们就可以用监督学习的方式进行学习了。不过对于生成模型来说，我们的loss是生成图像的likelihood，这个和判别模型的loss不太一样。&lt;/p&gt;&lt;p&gt;好了，两个模型的输入输出已经说完了，下面还有两个问题需要解决：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;判别模型的训练数据该如何准备？正例可以用现有数据，那负例呢？&lt;/li&gt;&lt;li&gt;生成模型的loss该如何计算？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其实要想解决这两个问题，我们需要把两个模型连起来。因为生成模型和输出和判别模型的输入在维度和含义上都是相同的，这样连起来我们就可以解决上面的两个问题。我们利用判别模型去判断生成模型的likelihood，而用生成模型产生的结果去做判别模型的负例，这样就把上面的两个问题解决了。&lt;/p&gt;&lt;p&gt;当然，关于把生成模型的输出作负例这件事，听上去还是有点奇怪的。生成模型的目标是生成“正常输入”，那么生成了“正常输入”还被当成负例，也是够冤的。不过这种矛盾的关系在机器学习中经常存在，就像优化目标中的loss项和正则项一样，这两个目标往往也是一对矛盾体。所以这种矛盾的存在并不奇怪，这也是这个模型被称为“对抗”的原因。&lt;/p&gt;&lt;p&gt;大家都喜欢用警察和小偷的关系来比喻生成模型和判别模型之间的“对抗”关系，我觉得可以用“魔高一尺，道高一丈”，“道高一丈，魔高十丈”来解释两个模型随着对抗不断强化的关系。判别模型在进化中能够捕捉不像“正常输入”的所有细节，而生成模型则会尽全力地模仿判别模型心中“正常输入”的形象。&lt;/p&gt;&lt;p&gt;好了，说了这么多，我们来看看针对“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” 这篇论文的keras版“实现”：&lt;a href="https://github.com/jacobgil/keras-dcgan" data-editable="true" data-title="GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks" class=""&gt;GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks&lt;/a&gt;，说是“实现”是因为这个实现实际上和论文中期望的有点小不同。&lt;/p&gt;&lt;p&gt;这个代码使用的数据集是MNIST，经典的小数据集，手写数字。在我的实验结果中，生成模型生成的手写数字是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/65660a40ec7d70c5798737fd357a5aac.png" data-rawwidth="308" data-rawheight="336"&gt;除了个别数字之外，大多数的数字生成得还是有模有样的嘛！&lt;/p&gt;&lt;p&gt;另外我们看一下两个模型在训练过程中的Loss：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2ab472cb9adaa293b3544b6406530fd7.png" data-rawwidth="976" data-rawheight="309"&gt;其中蓝色是生成模型的loss，绿色是判别模型的loss，可以看出两个模型的Loss都存在一定程度的抖动，也可以算是对抗过程中的此消彼长吧。&lt;/p&gt;&lt;p&gt;最后套用乐存大师的话做结尾：&lt;/p&gt;&lt;blockquote&gt;It seems like a rather technical issue, but I really think it opens the door to an entire world of possibilities.&lt;/blockquote&gt;&lt;p&gt;既然乐存老师都这么说了，我们真得好好看看这个模型了。下一会我们来看看前面提到的论文和上面的提到的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22386494&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 20 Sep 2016 08:52:22 GMT</pubDate></item><item><title>Caffe源码阅读——DataLayer&amp;Data Transformer</title><link>https://zhuanlan.zhihu.com/p/22404295</link><description>又一次回到了Caffe的源码阅读的环节，这一次瞄准的目标是网络的输入，现在的CNN网络百花齐放，各种各样的网络结构搭配各种各样的输入让人眼花缭乱，所以我们也必要研究一下输入的代码结构。&lt;p&gt;Caffe的DataLayer基础版的主要目标是读入两种DB的训练数据作为输入，而两种DB内存储的格式默认是一种叫Datum的数据结构。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;message Datum {
  optional int32 channels = 1;
  optional int32 height = 2;
  optional int32 width = 3;
  // the actual image data, in bytes
  optional bytes data = 4;
  optional int32 label = 5;
  // Optionally, the datum could also hold float data.
  repeated float float_data = 6;
  // If true data contains an encoded image that need to be decoded
  optional bool encoded = 7 [default = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看出，这种Datum的结构主要的服务对象是经典的图像分类任务。我们同时输入两部分信息：图像信息data和类别信息label。对于其他的信息来说，使用这个结构进行存储就显得有些困难了。比方说Object Detection的任务，其中还涉及到许多BoundingBox的信息，存储的结构要比这个更复杂。比方说一个知名的图像物体检测的网络结构SSD的作者开源实现就用到了一种自定义的训练数据存储方式：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;// An extension of Datum which contains "rich" annotations.
message AnnotatedDatum {
  enum AnnotationType {
    BBOX = 0;
  }
  optional Datum datum = 1;
  // If there are "rich" annotations, specify the type of annotation.
  // Currently it only supports bounding box.
  // If there are no "rich" annotations, use label in datum instead.
  optional AnnotationType type = 2;
  // Each group contains annotation for a particular class.
  repeated AnnotationGroup annotation_group = 3;
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然，创建一个新的Datum类型只是开始，我们还需要围绕着这个新的Datum创造相关的读取数据的C++类。当然，在创建这些类之前，我们当然需要了解一下Caffe自身的数据层的机制。&lt;/p&gt;&lt;p&gt;为了更好地理解这部分有点绕弯的关系，我们先来上一张几个相关类的关系图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a30df15ae709d83b0915fc23245f0959.jpg" data-rawwidth="1280" data-rawheight="960"&gt;这其中涉及到了两个线程和一个先向计算的过程，我们一一仔细看下。&lt;/p&gt;&lt;h2&gt;DataReader Thread&lt;/h2&gt;&lt;p&gt;DataReader是Caffe封装的读取两种DB的数据的类，这一步仅仅是把数据从DB中读取出来，也是上面图中右下角那个红框所展示的内容。这部分会给每一个读入的数据源创建一个独立的线程，专门负责这个数据源的读入工作。如果我们有多个Solver，比方说工作在多GPU下，而读入的数据源只有一份（比方说Train的DB只有一个），那么这一个读取数据的线程将会给这些Solver一并服务，这其中的原理可以详细看看DataReader这一部分。&lt;/p&gt;&lt;p&gt;最终每一个Solver里面的Net对象的DataLayer都会有一个自己的DataReader对象，其中会有一对变量：free和full。DataReader线程作为生产者将读入的数据放入full中，而下游的BasePrefetchingDataLayer的线程（后面会提到）将作为消费者将full中的内容取走。Caffe中继续使用BlockingQueue作为生产者和消费者之间同步的结构，并且设置两个队列的容量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每一次DataReader将free中已经被消费过的对象取出，填上新的数据，然后将其塞入full中；&lt;/li&gt;&lt;li&gt;每一次BasePrefetchingDataLayer将full中的对象取出并消费，然后将其塞入free中。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这样就保证了两边通信没有问题。&lt;/p&gt;&lt;h2&gt;BasePrefetchingDataLayer Thread&lt;/h2&gt;&lt;p&gt;BasePrefetchingDataLayer从名字上来看就是一个具有预先取出数据功能的数据层。每次前向计算时，我们并不需要在取数据这一步等待，我们完成可以把数据事先取好，等用的时候直接拿出来。这就是它们以线程的形式独立启动的原因。实际上DataReader的主要工作是把原始的数据从DB中取出，而BasePrefetchingDataLayer类要做的就是数据的加工了。这一部分主要完成两件事：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;确定数据层最终的输出（可以不输出label的）&lt;/li&gt;&lt;li&gt;完成数据层预处理（通常要做一些白化数据的简单工作，比如减均值，乘系数）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;前面已经提过，这一部分将消费DataReader的输入，同时这一部分将产生可以供上层计算网络直接使用的数据，这样在它的前向计算中，我们直接将BasePrefetchingDataLayer的输出拷贝到top_data就可以了，这样就节省了一定的时间。一般来说由GPU完成计算，由CPU完成数据准备，两者之间也不会出现严重的资源竞争。&lt;/p&gt;&lt;h2&gt;写个新结构？&lt;/h2&gt;&lt;p&gt;从上面的介绍中，我们看出：DataReader基本上不用变，我们只要根据不用的Datum类型创建不同的泛型类就好了，这部分的代码逻辑是固定的；而BasePrefetchingDataLayer的部分是有可能发生变化的。这也是一个新的Datum要面对的主要部分。而BasePrefetchingDataLayer也采用的Template的设计模式把不变的流程代码准备好了，一般来说只要实现两个函数即可：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DataLayerSetUp&lt;/li&gt;&lt;li&gt;load_batch&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一个是把类内的一些变量的维度进行初始化，一个是实现如何把一个DataReader返回的raw data转化为上层网络要的数据。&lt;/p&gt;&lt;p&gt;知道了这些，我们就可以看看SSD中的annotated_data_layer的实现了，它的label中需要存入8个信息：&lt;/p&gt;&lt;p&gt;[item_id, group_label, instance_id, xmin, ymin, xmax, ymax, diff]&lt;/p&gt;&lt;p&gt;所以相对应的load_batch部分也要做许多计算和准备。具体的计算内容我们可以以后再看，总之通过这两个部分的修改——prototxt中的数据结构定义和DataLayer部分相关位置的修改，我们就可以使得网络输入多种多样的数据，我们的Caffe也就可以完成更多有挑战的事情了。&lt;/p&gt;&lt;h2&gt;Data Transformer&lt;/h2&gt;&lt;p&gt;这一段新加入的，本来希望能单独写一篇，后来发现字数不够多，就把这部分和这篇合并起来了。我们来看一看Data Transformer的内容。这一部分的实现在C++和python上有所不同。一般来说，我们用C++的代码做训练，用python的代码做预测（当然现在用python做训练的也越来越多）。在C++中这是DataLayer中的一个小部分，而在python中这是一个独立的部分。&lt;/p&gt;&lt;h2&gt;C++中的DataTransformer&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Crop&lt;/b&gt;：这是在训练过程中经常用到的一种增强数据的方式。在train的过程中Caffe会进行随机crop，在test的过程中只会保留中间的部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mirror&lt;/b&gt;：做一个x轴的翻转&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mean&lt;/b&gt;: 给每个像素减去一个均值&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scale&lt;/b&gt;：给每个像素值乘以一个系数&lt;/p&gt;&lt;h2&gt;Python中的DataTransformer&lt;/h2&gt;&lt;p&gt;python的Transformer就有些复杂了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Resize&lt;/b&gt;：将输入数据缩放到指定的长宽比例&lt;/p&gt;&lt;p&gt;&lt;b&gt;Transpose&lt;/b&gt;：转换输入数据的维度。因为经过skimage读入后数据的维度是(Height * Width * Channel)，需要将数据的维度转换到(Channel*Height*Width)&lt;/p&gt;&lt;p&gt;&lt;b&gt;Channel&lt;/b&gt;&lt;b&gt;swap&lt;/b&gt;：这个主要针对彩色图的输入，不同的图像处理库对channel的处理顺序有所不同。像opencv，C++的主要合作伙伴，它的默认装载顺序是BGR——Blue,Green,Red。而skimage读入的是RGB，所以为了保证和C++训练的模型一致，所以这一步也是很必要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raw scale——Mean——Input scale&lt;/b&gt;：这里是将每个像素乘以一个Raw scale，减去一个Mean，再乘以一个Input scale。&lt;/p&gt;&lt;p&gt;python的版本中还包含一个&lt;b&gt;deprocess&lt;/b&gt;，用于做图像的反向处理。&lt;/p&gt;&lt;p&gt;python版本的crop和mirror功能在python/caffe/io.py中的oversample函数，不过他的逻辑和C++的逻辑不太一样了。实际使用中还可以针对自己的使用情况进行修改。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22404295&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Fri, 16 Sep 2016 08:43:04 GMT</pubDate></item></channel></rss>