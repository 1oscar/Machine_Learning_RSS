<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>无痛的机器学习 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/hsmyy</link><description>专栏主营业务：让更多人能看的懂的机器学习科普+进阶文章。欢迎各位大神投稿或协助审阅。</description><lastBuildDate>Mon, 10 Oct 2016 18:16:01 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>VAE(2)——基本思想</title><link>https://zhuanlan.zhihu.com/p/22464764</link><description>上一回我们花了很大的篇幅介绍了KL散度，这一回我们来看看VAE，尤其是深度模型下的VAE。&lt;p&gt;前面我们已经见过了许多优秀的深度学习模型，它们达到了非常好的精度和效果。众人曾十分认真地分析过为什么这些模型的效果这么好，结论是深度模型的非线性拟合能力确实很强。不管曾经多么复杂的问题，一个深度模型出马，立刻把问题解决的八九不离十。VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数，从而解决实际问题。&lt;/p&gt;&lt;p&gt;让我们先记住这个trick，后面我们会用到它。接下来我们要上场的是生成模型。前面我们看过的很多模型从原理上来说都是判别式模型。我们有一个等待判别的事物X，这个事物有一个类别y，我们来建立一个模型f(x;w)，使得p(y|X)的概率尽可能地大，换种方法说就是让f(x;w)尽可能地接近y。&lt;/p&gt;&lt;p&gt;如果我们想用生成式的模型去解决这个问题，就需要利用贝叶斯公式把这个问题转换过来：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{p(X)}&lt;/equation&gt;&lt;p&gt;为了遵从大多数教科书上的变量用法，这里将y变成了z。当然，这个时候的z可能比上面提到的“类别”y要复杂一些。在很多的生成模型中，我们把z称作隐含变量，把X称作观测变量。一般来说，我们可以比较容易地观察到X，但是X背后的z却不那么容易见到，而很多时候X是由z构造出来的，比方说一天的天气好与坏是由很多不易观察的因素决定的。于是我们自然而然就有了一个需求，当我们拿到这些X之后，我们想知道背后的z是什么，于是乎就有了上面那个公式。&lt;/p&gt;&lt;p&gt;对于一些简单的问题，上面的公式还是比较容易解出的，比方说朴素贝叶斯模型，但是还是有很多模型是不易解出的，尤其当隐含变量处于一个高维度的连续空间中：&lt;/p&gt;&lt;equation&gt;p(z|X)=\frac{p(X|z)p(z)}{\int_z{p(X|z)p(z)dz}}&lt;/equation&gt;&lt;p&gt;这里的积分就没那么容易搞定了。于是乎，各路大神开始想尽一切办法让上面的式子变得好解些。&lt;/p&gt;&lt;p&gt;这时候我们难免会冒出一个问题，既然有了判别式模型可以直接求解式子左边的那个东西，为什么非要把它变成右边那一大堆东西，搞得自己不方便解呢？其实谁都不想给自己找麻烦，可问题是右边的这一堆除了能够解这个问题，它还有一个更加高级的功能，就是根据模型随机生成X。&lt;/p&gt;&lt;p&gt;我们可以想想看，如果我们只拥有式子左边的p(z|X)，我们想要生成一个符合某种z的X该怎么办？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一步，随机一个X；&lt;/li&gt;&lt;li&gt;第二步，用p(z|X)计算概率，如果概率满足，则结束，如果不满足，返回第一步；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;于是乎，用判别式模型生成X变成了人品游戏，谁也不知道自己什么时候能在第二步通过。而生成式模型就不同了，我们可以按需定制，首先确定好z，然后根据p(X|z)进行随机采样就行了，生成X的过程安全可控。&lt;/p&gt;&lt;p&gt;说了这么多，下面我们正式进入公式推导的部分。&lt;/p&gt;&lt;h2&gt;Variational Inference&lt;/h2&gt;&lt;p&gt;虽然我们鼓吹了很多生成模型的好处，但是面对等号右边那一堆东西，该束手无策还是束手无策。但是，前辈们还是想到了一些精妙的解法。既然用概率论的方法很难求出右边的东西，我们能不能做一些变换，比方说——（略显生硬地）我们用一个variational的函数q(z)去代替p(z|X)？别着急，后面我们会看到它带来的好处的。&lt;/p&gt;&lt;p&gt;这里的variational inference介绍的有点简单，有机会我们再详细介绍下。&lt;/p&gt;&lt;p&gt;既然要用q(z)这个新东西去代替p(z|X)，那么我们当然希望两个东西尽可能地相近，于是乎我们选择了KL散度这个指标用来衡量两者的相近程度。由于两边都是可以看作针对z的概率分布，因此用KL散度这个指标实际上非常合适。&lt;/p&gt;&lt;p&gt;所以就有了：&lt;/p&gt;&lt;equation&gt;KL(q(z)||p(z|X))=\int{q(z)log \frac{q(z)}{p(z|X)}}dz&lt;/equation&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(z|X)]}dz&lt;/equation&gt;&lt;p&gt;我们做一下贝叶斯公式的变换，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z) + logp(X)]}dz&lt;/equation&gt;&lt;p&gt;再将和z无关的项目从积分符号中拿出来，就得到了：&lt;/p&gt;&lt;equation&gt;=\int{q(z)[log q(z) - log p(X|z) - log p(z)]}dz + log p(X)&lt;/equation&gt;&lt;p&gt;左右整理一下，就得到了：&lt;/p&gt;&lt;equation&gt;log p(X) - KL(q(z)||p(z|X))=\int{q(z) log p(X|z)}dz-KL(q(z)||p(z))&lt;/equation&gt;&lt;p&gt;好吧，其实整理了一圈，这个公式还是很乱，不过因为KL散度的特殊关系，我们还是从这个公式中看到了一丝曙光：&lt;/p&gt;&lt;p&gt;我们虽然不大容易求出p(X)，但我们知道当X给定的情况下，p(X)是个固定值。那么如果我们希望KL(q(z)||p(z|X))尽可能地小，也就相当于让等号右边的那部分尽可能地大。其中等号右边的第一项实际上是基于q(z)的似然期望，第二项又是一个负的KL散度，所以我们可以认为，为了找到一个好的q(z)，使得它和p(z|X)尽可能地相近，我们需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;右边第一项的log似然的期望最大化&lt;/li&gt;&lt;li&gt;右边第二项的KL散度最小化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于VAE之前的variation inference（中文可以翻译成变分推断），到这里我们就要开始一段全新的公式推导了。比方说我们做一个mean-field assumption（说实话我不太知道mean-field怎么翻译更直观，于是就把英文放在这里了），于是乎对于多个隐含变量组成的z，分量相互之间是独立的，于是根据这个特性，我们又可以进行进一步地公式化简。由于我们今天的主题是VAE，所以关于这部分我们就不再赘述了。这时候我们又想起了文章开头我们提到的一句话：&lt;/p&gt;&lt;blockquote&gt;“VAE也是利用了这个特点，我们用深度模型去拟合一些复杂的函数”&lt;/blockquote&gt;&lt;p&gt;那么是时候让这句话发挥作用了，不过关于它发挥的方法我们下回再说。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464764&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 09 Oct 2016 19:09:29 GMT</pubDate></item><item><title>VAE（1）——从KL说起</title><link>https://zhuanlan.zhihu.com/p/22464760</link><description>前面我们介绍了GAN——Generative Adversarial Network，这个网络组是站在对抗博弈的角度去展现生成模型和判别模型各自的威力的，下面我们来看看这种生成模型和判别模型组合的另一个套路——Variational autoencoder，简称VAE。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-856cfcd1acf4b155373953460cd7adbd.png" data-rawwidth="374" data-rawheight="601"&gt;突然想起来，他也叫VAE，我觉得他还是有点音乐才华的。不过我们今天不去讨论他。&lt;/p&gt;&lt;p&gt;Variational autoencoder的概念相对复杂一些，它涉及到一些比较复杂的公式推导。在开始正式的推导之前，我们先来看看一个基础概念——KL divergence，翻译过来叫做KL散度。&lt;/p&gt;&lt;h2&gt;什么是KL散度&lt;/h2&gt;&lt;p&gt;无论从概率论的角度，还是从信息论的角度，我们都可以很好地给出KL散度测量的意义。这里不是基础的概念介绍，所以有关KL的概念就不介绍了。在Variational Inference中，我们希望能够找到一个相对简单好算的概率分布q，使它尽可能地近似我们待分析的后验概率p(z|x)，其中z是隐变量，x是显变量。在这里我们的“loss函数”就是KL散度，他可以很好地测量两个概率分布之间的距离。如果两个分布越接近，那么KL散度越小，如果越远，KL散度就会越大。&lt;/p&gt;&lt;p&gt;KL散度的公式为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\sum{p(x)log\frac{p(x)}{q(x)}}&lt;/equation&gt;，这个是离散概率分布的公式，&lt;/p&gt;&lt;p&gt;&lt;equation&gt;KL(p||q)=\int{p(x)log{\frac{p(x)}{q(x)}}dx}&lt;/equation&gt;，这个是连续概率分布的公式。&lt;/p&gt;&lt;p&gt;关于其他KL散度的性质，这里就不赘述了。&lt;/p&gt;&lt;h2&gt;KL散度的实战——1维高斯分布&lt;/h2&gt;&lt;p&gt;我们先来一个相对简单的例子。假设我们有两个随机变量x1,x2，各自服从一个高斯分布&lt;equation&gt;N_1(\mu_1,\sigma_1^2),N_2(\mu_2,\sigma_2^2)&lt;/equation&gt;，那么这两个分布的KL散度该怎么计算呢？&lt;/p&gt;&lt;p&gt;我们知道&lt;/p&gt;&lt;equation&gt;N(\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{(x-\mu)^2}{2\sigma^2}}&lt;/equation&gt;&lt;p&gt;那么KL(p1,p2)就等于&lt;/p&gt;&lt;equation&gt;\int{p_1(x)log\frac{p_1(x)}{p_2(x)}}dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log{p_1(x)}}dx-log{p_2(x)})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)}*(log{\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{\frac{(x-\mu_1)^2}{2\sigma_1^2}}}-log{\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{\frac{(x-\mu_2)^2}{2\sigma_2^2}}})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)
*(-\frac{1}{2}log2\pi-log{\sigma_1}-\frac{(x-\mu_1)^2}{2\sigma_1^2}}+
\frac{1}{2}log{2\pi}+log{\sigma_2}+\frac{(x-\mu_2)^2}{2\sigma_2^2})dx&lt;/equation&gt;&lt;equation&gt;=\int{p_1(x)(log\frac{\sigma_2}{\sigma_1}+[\frac{(x-\mu_2)^2}{2\sigma_2^2}-\frac{(x-\mu_1)^2}{2\sigma_1^2}])dx}&lt;/equation&gt;&lt;equation&gt;=\int(log\frac{\sigma_2}{\sigma_1})p_1(x)dx+\int(\frac{(x-\mu_2)^2}{2\sigma_2^2})p_1(x)dx-\int(\frac{(x-\mu_1)^2}{2\sigma_1^2})p_1(x)dx&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2\sigma_1^2}\int((x-\mu_1)^2)p_1(x)dx&lt;/equation&gt;&lt;p&gt;（更新）到这里停一下，有童鞋问这里右边最后一项的化简，这时候积分符号里面的东西是不看着很熟悉？没错，就是我们常见的方差嘛，于是括号内外一约分，就得到了最终的结果——&lt;equation&gt;\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;好，继续。&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}\int((x-\mu_1+\mu_1-\mu_2)^2)p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+\int{(\mu_1-\mu_2)^2}p_1(x)dx+2\int{(x-\mu_1)(\mu_1-\mu_2)]}p_1(x)dx-\frac{1}{2}&lt;/equation&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{1}{2\sigma_2^2}[\int{(x-\mu_1)^2}p_1(x)dx+(\mu_1-\mu_2)^2]-\frac{1}{2}&lt;/equation&gt;&lt;/p&gt;&lt;equation&gt;=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}&lt;/equation&gt;&lt;p&gt;说实话一直以来我不是很喜欢写这种大段推导公式的文章，一来原创性比较差（都是前人推过的，我就是大自然的搬运工），二来其中的逻辑性太强，容易让人看蒙。不过最终的结论还是得出来了，我们假设N2是一个正态分布，也就是说&lt;equation&gt;\mu_2=0,\sigma_2^2=1&lt;/equation&gt;那么N1长成什么样子能够让KL散度尽可能地小呢？&lt;/p&gt;&lt;p&gt;也就是说&lt;equation&gt;KL(\mu_1,\sigma_1)=-log\sigma_1+\frac{\sigma_1^2+\mu_1^2}{2}-\frac{1}{2}&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;我们用“肉眼”看一下就能猜测到当&lt;equation&gt;\mu_1=0,\sigma_1=1&lt;/equation&gt;时，KL散度最小。从公式中可以看出，如果&lt;equation&gt;\mu_1&lt;/equation&gt;偏离了0，那么KL散度一定会变大。而方差的变化则有些不同：&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;大于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越大，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越小；&lt;/p&gt;&lt;p&gt;当&lt;equation&gt;\sigma_1&lt;/equation&gt;小于1时，&lt;equation&gt;\frac{1}{2}\sigma_1^2&lt;/equation&gt;将越变越小，而&lt;equation&gt;-log\sigma_1&lt;/equation&gt;越变越大；&lt;/p&gt;&lt;p&gt;那么哪边的力量更强大呢？我们可以作图出来：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0.5,2,100)
y = -np.log(x)+x*x/2-0.5
plt.plot(x,y)
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从图中可以看出&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-68395494c66b37550efea74beca61d7e.png" data-rawwidth="559" data-rawheight="431"&gt;二次项的威力更大，函数一直保持为非负，这和我们前面提到的关于非负的定义是完全一致的。&lt;/p&gt;&lt;p&gt;好了，看完了这个简单的例子，下面让我们再看一个复杂的例子。&lt;/p&gt;&lt;h2&gt;一个更为复杂的例子：多维高斯分布的KL散度&lt;/h2&gt;&lt;p&gt;上一回我们看过了1维高斯分布间的KL散度计算，下面我们来看看多维高斯分布的KL散度是什么样子？说实话，这一次的公式将在后面介绍VAE时发挥很重要的作用！&lt;/p&gt;&lt;p&gt;首先给出多维高斯分布的公式：&lt;/p&gt;&lt;equation&gt;p(x_1,x_2,...x_n)=\frac{1}{\sqrt{2\pi*det(\Sigma)}}e^{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}&lt;/equation&gt;&lt;p&gt;由于这次是多维变量，里面的大多数计算都变成了向量、矩阵之间的计算。我们常用的是各维间相互独立的分布，因此协方差矩阵实际上是个对角阵。&lt;/p&gt;&lt;p&gt;考虑到篇幅以及实际情况，下面直接给出结果，让我们忽略哪些恶心的推导过程:&lt;/p&gt;&lt;equation&gt;KL(p1||p2)=\frac{1}{2}[log \frac{det(\Sigma_2)}{det(\Sigma_1)} - d + tr(\Sigma_2^{-1}\Sigma_1)+(\mu_2-\mu_1)^T \Sigma_2^{-1}(\mu_2-\mu_1)]&lt;/equation&gt;&lt;p&gt;其实这一次我们并没有介绍关于KL的意义和作用，只是生硬地、莫名其妙地推导一堆公式，不过别着急，下一回，我们展示VAE效果的时候，就会让大家看到KL散度的作用。&lt;/p&gt;&lt;p&gt;坚持看到这里的童鞋是有福的，来展示一下VAE的解码器在MNIST数据库上产生的字符生成效果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c622c0419cbffbdb66485c1966afa68e.png" data-rawwidth="298" data-rawheight="298"&gt;从这个效果上来看，它的功能和GAN是有点像的，那么让我们来进一步揭开它的庐山真面目吧！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464760&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 06 Oct 2016 19:06:42 GMT</pubDate></item><item><title>寻找CNN的弱点</title><link>https://zhuanlan.zhihu.com/p/22464575</link><description>CNN是现在十分火热的模型，在很多图像检索问题上，CNN模型的效果在以往的基础上有了很大的提高，但是CNN毕竟没有把这些问题完全解决，CNN还是有它自己的弱点的。这个弱点也不能算作是它独有的问题，但是由于它的效果实在太好了，很多人甚至对它产生了迷信，因此这盆冷水就泼到它身上了。&lt;p&gt;大神们看到了CNN模型的强大，但忍不住提出一个问题：CNN有没有什么搞不定的地方？比方说我们用CNN构建了一个人脸识别的模型，在训练数据集和测试数据集上表现良好，但是会不会有一些用例是它会误判的，而且我们可以找到规律生成这些用例？&lt;/p&gt;&lt;p&gt;我们可以想象，如果我们对之前识别正确的数据做轻微的改动，那么它还是有可能识别正确的。于是我们就有了一个方案，我们每将图像做一点改动，就把图像传入CNN做一下测试，然后看看CNN的预测结果有没有发生改变，如果没有发生改变，我们就保存这个图像，接着我们再进行下一轮的改动，经过若干轮的改动后，我们把生成的图像输出出来看看图像会变成什么样子。&lt;/p&gt;&lt;p&gt;这里我们将采用MNIST为例，以下的就是我们的改动方案：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用MNIST的训练集训练一个CNN的模型，我们的CNN模型结构是：conv32*3*3-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;conv64*3*6-&amp;gt;relu-&amp;gt;maxpool2*2-&amp;gt;fc256-&amp;gt;dropout0.5-&amp;gt;fc10。&lt;/li&gt;&lt;li&gt;找到一个训练数据，将其数据范围限定在0到1之间，我们对每一个像素点随机增减-0.1到0.1之间的一个数，这样得到64个随机的图像，然后经过CNN模型预测得到这64个图像的预测label，从中选择一个和原始label相同的图像。经过若干轮迭代后，我们就可以看看这个随机改变的数字变成了什么样子。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们选择了一个数字0：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1a666d86f42923d245036bec25270c59.png" data-rawwidth="421" data-rawheight="424"&gt;经过50轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c3725d6bbb1cde8c2f236cf24a63b5f4.png" data-rawwidth="423" data-rawheight="423"&gt;&lt;p&gt;经过100轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/38c4258464d5ff070a64f3a636a0864b.png" data-rawwidth="421" data-rawheight="421"&gt;&lt;p&gt;经过150轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/06d7faa4bd8245d98ec1ae265c98cd48.png" data-rawwidth="424" data-rawheight="426"&gt;&lt;p&gt;经过200轮迭代，我们得到了这样的图像：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/232da9eb664724791b4181a2a636edad.png" data-rawwidth="426" data-rawheight="425"&gt;到此为止，可以看出这个数字还是隐约可见，但是实际上图像已经变得模糊不清，大量的杂乱信息混入其中，已经和原始的数字完全不同。&lt;/p&gt;&lt;p&gt;这个套路被称作“fool CNN”，用东北话说就是忽悠。继续迭代下去，我们还能生成出更精彩的图像。当然这也只是忽悠CNN模型的一种办法，我们还有其他的办法来生成图像。其他的办法这里就不再介绍了。关于这种忽悠，大神们也给出了和机器学习有关的解释：&lt;/p&gt;&lt;p&gt;CNN的模型说到底还是个判别式模型，如果我们把图像设为X，label设为y，CNN的模型就相当于求p(y|X)的值。判别式模型相当于描述“什么样的图像是这个label的图像”，而满足了这些条件的图像有时并不是具有真实label的那个图像。而上面的忽悠套路就是利用了这个漏洞。&lt;/p&gt;&lt;p&gt;上面的例子中，我们用这种fool的方法让一张模糊不清的图像保持了原来的label，同时我们也可以让一张不算模糊的图像被CNN错认成另外一个label。&lt;/p&gt;&lt;p&gt;比方说下面这张经过40轮迭代的图像被认成了6：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2444cb2b26a5ee30177d5eb47f4b5abb.png" data-rawwidth="419" data-rawheight="422"&gt;这些套路的出现都让我们对CNN有了一些警惕，如果想让CNN对手写数字完全hold住，我们还需要其他的方法辅助，不然的话这种意外总会发生。&lt;/p&gt;&lt;p&gt;那么有没有什么方法能解决这样的问题呢？&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22464575&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Thu, 29 Sep 2016 22:48:26 GMT</pubDate></item><item><title>DCGAN的小尝试（2）</title><link>https://zhuanlan.zhihu.com/p/22389906</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-editable="true" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;上一回我们只是简单地展示了基于keras框架、MNIST数据集的DCGAN模型的结果，下面我们来详细地看一下这个代码的实现。&lt;h2&gt;生成模型的结构&lt;/h2&gt;&lt;pre&gt;&lt;code lang="text"&gt;def generator_model():
    model = Sequential()
    model.add(Dense(input_dim=100, output_dim=1024))
    model.add(Activation('tanh'))
    model.add(Dense(out_dim=128*7*7))
    model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=64, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(out_channel=1, kernel_height=5, kernel_width=5, border_mode='same'))
    model.add(Activation('tanh'))
    return model
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;直接上代码了。keras的代码总体上比较直观，我在里面加了一些参数对应的描述，应该编译不过，但是会比较好理解。&lt;/p&gt;&lt;p&gt;这里需要说明的一点是，这个实现中的激活函数都是双曲正切，和论文中的描述不一样。当然，和论文中的模型架构也不一样，不过两者的数据集也不一样。&lt;/p&gt;&lt;h2&gt;判别模型的结构&lt;/h2&gt;&lt;p&gt;判别模型的结构如下所示，仔细地读一遍就可以理解，这里不需要赘述了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def discriminator_model():
    model = Sequential()
    model.add(Convolution2D(
                        64, 5, 5,
                        border_mode='same',
                        input_shape=(1, 28, 28)))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(128, 5, 5))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('tanh'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    return model&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;训练&lt;/h2&gt;&lt;p&gt;这里的训练的一轮迭代可以用下面的流程表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一批generated_image&lt;/li&gt;&lt;li&gt;将真实的数据和generated_image合并，并放入D中进行一轮训练，其中真实数据的label为1，generated_image的label为0。&lt;/li&gt;&lt;li&gt;利用G再生成一批generated_image&lt;/li&gt;&lt;li&gt;这一次将G和D连起来，并给第3步的gereated_image的label设为1，固定D的参数不变，进行一轮训练。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;可以看出第1，2步是为了优化D，第3，4步是为了优化G，而两者之间还是存在着紧密的联系。&lt;/p&gt;&lt;h2&gt;图像生成&lt;/h2&gt;&lt;p&gt;图像生成的过程可以用如下两步表示：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;利用G生成一大批generated_image&lt;/li&gt;&lt;li&gt;利用D计算这些generated_image的分类结果，把得分高的一批选出来&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;最终我们看到的就是从D的眼皮下逃出的优质的生成数据。&lt;/p&gt;&lt;p&gt;好了，前面对代码的几个核心部分做了介绍，下面我们来看看实验过程中的一些问题。&lt;/p&gt;&lt;h2&gt;图像的演变过程&lt;/h2&gt;&lt;p&gt;在优化刚开始时，从随机生成的100维向量生成的图像是这样子的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/dee6ed0084ef2fbf17d76e2b0e947e9b.png" data-rawwidth="308" data-rawheight="336"&gt;其实就是噪声。&lt;/p&gt;&lt;p&gt;经过400轮的迭代，生成模型可以生成下面的图像了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/30549e49b67fb85af5acb25cd90042fa.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出数字的大体结构已经形成，但是能够表征数字细节的特征还没有出现。&lt;/p&gt;&lt;p&gt;经过10个Epoch后，生成模型的作品：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/93ba20c5958eee5a8db956743a580c40.png" data-rawwidth="308" data-rawheight="336"&gt;这时候有些数字已经成型，但是还有一些数字仍然存在欠缺。&lt;/p&gt;&lt;p&gt;然后是20轮Epoch的结果：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/10e43906800c936ea133757c98fa83be.png" data-rawwidth="308" data-rawheight="336"&gt;这个时候的数字已经具有很强的辨识度，但是与其同时，我们发现生成的数字中有大量的“1”。&lt;/p&gt;&lt;p&gt;当完成了所有的训练，我们拿出生成模型在最后一轮生成的图像，可以看到：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2b4d98d848f9d2da7a304f0fd0bc7f01.png" data-rawwidth="308" data-rawheight="336"&gt;可以看出这里面的数字质量更高一些，但是里面的1也更多了。&lt;/p&gt;&lt;p&gt;从这个演化过程中，我们可以看出，从一开始的数字生成质量都很差但生成数字的多样性比较好，到后来的数字质量比较高但数字的多样性比较差，模型的特性在不断地发生变化。这也和两个模型的对抗有关系，而这个演变也和增强学习中的“探索-利用”困境有关系。&lt;/p&gt;&lt;p&gt;我们站在生成模型的角度去想，一开始生成模型会尽可能地生成各种各样形状的数字，而判别模型会识别出一些形状较差的模型，而放过一些形状较好的模型，随着学习的进程不断推进，判别模型的能力也在不断地加强，生成模型慢慢发现有一些固定的套路比较容易通过，而其他一些套路不那么容易通过，于是它就会尽可能地增大这些“套路”出现的概率，让自己的loss变小。这样，一个从探索为主的模型变成了一个以利用为主的模型，实际上它的数据分布已经不那么均匀了。&lt;/p&gt;&lt;p&gt;如果这个模型继续训练下去，生成模型有可能进一步地利用这个“套路”，这和我们传统意义上的过拟合也有很相近的地方。所以我们希望能够避免这样的过拟合。&lt;/p&gt;&lt;p&gt;这个小实验也到此结束了，下面我们将看看论文中关于DCGAN的介绍，以及关于模型的一些改进方案和其他框架的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"4群已经准确就绪：466461154，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;466461154！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22389906&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Mon, 26 Sep 2016 09:55:52 GMT</pubDate></item><item><title>DCGAN的小尝试（1）</title><link>https://zhuanlan.zhihu.com/p/22386494</link><description>&lt;p&gt;感谢&lt;a href="https://www.zhihu.com/people/03675ab7bf1c28d3d71d2154abb3ddd1" data-hash="03675ab7bf1c28d3d71d2154abb3ddd1" class="member_mention" data-hovercard="p$b$03675ab7bf1c28d3d71d2154abb3ddd1" data-title="@我爱机器学习"&gt;@我爱机器学习&lt;/a&gt;对本文的审阅。&lt;/p&gt;话说当今的深度学习网络框架世界，除了Caffe，还有很多不错的框架。这一次为了省事，我们直接找一个开源的应用进行分析和尝试。而这次的框架主角是keras，一个拥有简洁API的框架。而我们今天的主角来自深度学习界的大神Yann LeCun（我比较喜欢叫他颜乐存，哈哈……）在Quora上的对这个问题的回答：&lt;p&gt;&lt;a class="" href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning" data-editable="true" data-title="What are some recent and potentially upcoming breakthroughs in deep learning?"&gt;What are some recent and potentially upcoming breakthroughs in deep learning?&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;There are many interesting recent development in deep learning, probably too many for me to describe them all here. But there are a few ideas that caught my attention enough for me to get personally involved in research projects.&lt;/p&gt;&lt;p&gt;The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks).&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;总结成一句话：深度学习的未来，就是干（GAN）！&lt;/p&gt;&lt;p&gt;听上去就让人热血沸腾啊～&lt;/p&gt;&lt;p&gt;抱着极大的好奇心，我们开始对GAN的探险之旅。&lt;/p&gt;&lt;h2&gt;DCGAN&lt;/h2&gt;&lt;p&gt;如果从GAN的起点开始聊起，那么等我们聊到正题，估计好几集都过去了。所以让我们忘掉前面的种种解法，直接来到我们的深度学习部分：DCGAN。全称是Deep Convolution GAN。也就是用深度卷积网络进行对抗生成网络的建模。&lt;/p&gt;&lt;p&gt;对抗神经网络（GAN）有两个主角——&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个是G（Generator）,也就是生成模型；它的输入是一个随机生成的向量，长度不定，输出是一个具有一定大小的图像（N*N*3）和（N*N*1）。&lt;/li&gt;&lt;li&gt;一个是D（Discriminator），也就是判别模型。在我们接下来介绍的模型中，它的输入维度和G的输出一样，输出是一个长度为1 的向量，数字的范围从0到1，表示图像像一个正常图片的程度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;G的输入和输出都比较好理解，D的输入也比较好理解，那么D的输出是什么含义呢？它表示了对给定输出是否像我们给定的标准的输入数据。这句话可能有点绕口，我们可以把判别模型理解成一个解决分类问题的模型，那么在这个问题中判别模型的结果就是区分一个输入属于下面两个类别中的哪个——“正常输入”和“非正常输入”。&lt;/p&gt;&lt;p&gt;举个更具体的例子。对于MNIST数据集来说，每一个手写的数字都可以认为是一个“正常输入”，而随便生成的一个不像手写数字的输入都可以认为是一个“非正常输入”。而我们的判别模型就是要判断这个问题，我们学习的目标也是学习出一个能够解决这个问题的模型。&lt;/p&gt;&lt;p&gt;那么，我们的生成模型的目标呢？就是我们能够从一个随机生成的向量生成一样“正常输入”的图像。听上去有点神奇吧，不过现实中这个效果是可以实现的。我们可以想象我们的输入空间是满足某种分布的一个空间，对于空间中的每一个点，我们都可以利用生成模型将其映射成为一个图像，现在我们限定了生成的图像必须是“正常输入”，那么输入和输出在某种程度上已经确定，我们就可以用监督学习的方式进行学习了。不过对于生成模型来说，我们的loss是生成图像的likelihood，这个和判别模型的loss不太一样。&lt;/p&gt;&lt;p&gt;好了，两个模型的输入输出已经说完了，下面还有两个问题需要解决：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;判别模型的训练数据该如何准备？正例可以用现有数据，那负例呢？&lt;/li&gt;&lt;li&gt;生成模型的loss该如何计算？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其实要想解决这两个问题，我们需要把两个模型连起来。因为生成模型和输出和判别模型的输入在维度和含义上都是相同的，这样连起来我们就可以解决上面的两个问题。我们利用判别模型去判断生成模型的likelihood，而用生成模型产生的结果去做判别模型的负例，这样就把上面的两个问题解决了。&lt;/p&gt;&lt;p&gt;当然，关于把生成模型的输出作负例这件事，听上去还是有点奇怪的。生成模型的目标是生成“正常输入”，那么生成了“正常输入”还被当成负例，也是够冤的。不过这种矛盾的关系在机器学习中经常存在，就像优化目标中的loss项和正则项一样，这两个目标往往也是一对矛盾体。所以这种矛盾的存在并不奇怪，这也是这个模型被称为“对抗”的原因。&lt;/p&gt;&lt;p&gt;大家都喜欢用警察和小偷的关系来比喻生成模型和判别模型之间的“对抗”关系，我觉得可以用“魔高一尺，道高一丈”，“道高一丈，魔高十丈”来解释两个模型随着对抗不断强化的关系。判别模型在进化中能够捕捉不像“正常输入”的所有细节，而生成模型则会尽全力地模仿判别模型心中“正常输入”的形象。&lt;/p&gt;&lt;p&gt;好了，说了这么多，我们来看看针对“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” 这篇论文的keras版“实现”：&lt;a href="https://github.com/jacobgil/keras-dcgan" data-editable="true" data-title="GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks" class=""&gt;GitHub - jacobgil/keras-dcgan: Keras implementation of Deep Convolutional Generative Adversarial Networks&lt;/a&gt;，说是“实现”是因为这个实现实际上和论文中期望的有点小不同。&lt;/p&gt;&lt;p&gt;这个代码使用的数据集是MNIST，经典的小数据集，手写数字。在我的实验结果中，生成模型生成的手写数字是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/65660a40ec7d70c5798737fd357a5aac.png" data-rawwidth="308" data-rawheight="336"&gt;除了个别数字之外，大多数的数字生成得还是有模有样的嘛！&lt;/p&gt;&lt;p&gt;另外我们看一下两个模型在训练过程中的Loss：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2ab472cb9adaa293b3544b6406530fd7.png" data-rawwidth="976" data-rawheight="309"&gt;其中蓝色是生成模型的loss，绿色是判别模型的loss，可以看出两个模型的Loss都存在一定程度的抖动，也可以算是对抗过程中的此消彼长吧。&lt;/p&gt;&lt;p&gt;最后套用乐存大师的话做结尾：&lt;/p&gt;&lt;blockquote&gt;It seems like a rather technical issue, but I really think it opens the door to an entire world of possibilities.&lt;/blockquote&gt;&lt;p&gt;既然乐存老师都这么说了，我们真得好好看看这个模型了。下一会我们来看看前面提到的论文和上面的提到的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22386494&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 20 Sep 2016 08:52:22 GMT</pubDate></item><item><title>Caffe源码阅读——DataLayer&amp;Data Transformer</title><link>https://zhuanlan.zhihu.com/p/22404295</link><description>又一次回到了Caffe的源码阅读的环节，这一次瞄准的目标是网络的输入，现在的CNN网络百花齐放，各种各样的网络结构搭配各种各样的输入让人眼花缭乱，所以我们也必要研究一下输入的代码结构。&lt;p&gt;Caffe的DataLayer基础版的主要目标是读入两种DB的训练数据作为输入，而两种DB内存储的格式默认是一种叫Datum的数据结构。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;message Datum {
  optional int32 channels = 1;
  optional int32 height = 2;
  optional int32 width = 3;
  // the actual image data, in bytes
  optional bytes data = 4;
  optional int32 label = 5;
  // Optionally, the datum could also hold float data.
  repeated float float_data = 6;
  // If true data contains an encoded image that need to be decoded
  optional bool encoded = 7 [default = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看出，这种Datum的结构主要的服务对象是经典的图像分类任务。我们同时输入两部分信息：图像信息data和类别信息label。对于其他的信息来说，使用这个结构进行存储就显得有些困难了。比方说Object Detection的任务，其中还涉及到许多BoundingBox的信息，存储的结构要比这个更复杂。比方说一个知名的图像物体检测的网络结构SSD的作者开源实现就用到了一种自定义的训练数据存储方式：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;// An extension of Datum which contains "rich" annotations.
message AnnotatedDatum {
  enum AnnotationType {
    BBOX = 0;
  }
  optional Datum datum = 1;
  // If there are "rich" annotations, specify the type of annotation.
  // Currently it only supports bounding box.
  // If there are no "rich" annotations, use label in datum instead.
  optional AnnotationType type = 2;
  // Each group contains annotation for a particular class.
  repeated AnnotationGroup annotation_group = 3;
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然，创建一个新的Datum类型只是开始，我们还需要围绕着这个新的Datum创造相关的读取数据的C++类。当然，在创建这些类之前，我们当然需要了解一下Caffe自身的数据层的机制。&lt;/p&gt;&lt;p&gt;为了更好地理解这部分有点绕弯的关系，我们先来上一张几个相关类的关系图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a30df15ae709d83b0915fc23245f0959.jpg" data-rawwidth="1280" data-rawheight="960"&gt;这其中涉及到了两个线程和一个先向计算的过程，我们一一仔细看下。&lt;/p&gt;&lt;h2&gt;DataReader Thread&lt;/h2&gt;&lt;p&gt;DataReader是Caffe封装的读取两种DB的数据的类，这一步仅仅是把数据从DB中读取出来，也是上面图中右下角那个红框所展示的内容。这部分会给每一个读入的数据源创建一个独立的线程，专门负责这个数据源的读入工作。如果我们有多个Solver，比方说工作在多GPU下，而读入的数据源只有一份（比方说Train的DB只有一个），那么这一个读取数据的线程将会给这些Solver一并服务，这其中的原理可以详细看看DataReader这一部分。&lt;/p&gt;&lt;p&gt;最终每一个Solver里面的Net对象的DataLayer都会有一个自己的DataReader对象，其中会有一对变量：free和full。DataReader线程作为生产者将读入的数据放入full中，而下游的BasePrefetchingDataLayer的线程（后面会提到）将作为消费者将full中的内容取走。Caffe中继续使用BlockingQueue作为生产者和消费者之间同步的结构，并且设置两个队列的容量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每一次DataReader将free中已经被消费过的对象取出，填上新的数据，然后将其塞入full中；&lt;/li&gt;&lt;li&gt;每一次BasePrefetchingDataLayer将full中的对象取出并消费，然后将其塞入free中。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这样就保证了两边通信没有问题。&lt;/p&gt;&lt;h2&gt;BasePrefetchingDataLayer Thread&lt;/h2&gt;&lt;p&gt;BasePrefetchingDataLayer从名字上来看就是一个具有预先取出数据功能的数据层。每次前向计算时，我们并不需要在取数据这一步等待，我们完成可以把数据事先取好，等用的时候直接拿出来。这就是它们以线程的形式独立启动的原因。实际上DataReader的主要工作是把原始的数据从DB中取出，而BasePrefetchingDataLayer类要做的就是数据的加工了。这一部分主要完成两件事：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;确定数据层最终的输出（可以不输出label的）&lt;/li&gt;&lt;li&gt;完成数据层预处理（通常要做一些白化数据的简单工作，比如减均值，乘系数）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;前面已经提过，这一部分将消费DataReader的输入，同时这一部分将产生可以供上层计算网络直接使用的数据，这样在它的前向计算中，我们直接将BasePrefetchingDataLayer的输出拷贝到top_data就可以了，这样就节省了一定的时间。一般来说由GPU完成计算，由CPU完成数据准备，两者之间也不会出现严重的资源竞争。&lt;/p&gt;&lt;h2&gt;写个新结构？&lt;/h2&gt;&lt;p&gt;从上面的介绍中，我们看出：DataReader基本上不用变，我们只要根据不用的Datum类型创建不同的泛型类就好了，这部分的代码逻辑是固定的；而BasePrefetchingDataLayer的部分是有可能发生变化的。这也是一个新的Datum要面对的主要部分。而BasePrefetchingDataLayer也采用的Template的设计模式把不变的流程代码准备好了，一般来说只要实现两个函数即可：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DataLayerSetUp&lt;/li&gt;&lt;li&gt;load_batch&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一个是把类内的一些变量的维度进行初始化，一个是实现如何把一个DataReader返回的raw data转化为上层网络要的数据。&lt;/p&gt;&lt;p&gt;知道了这些，我们就可以看看SSD中的annotated_data_layer的实现了，它的label中需要存入8个信息：&lt;/p&gt;&lt;p&gt;[item_id, group_label, instance_id, xmin, ymin, xmax, ymax, diff]&lt;/p&gt;&lt;p&gt;所以相对应的load_batch部分也要做许多计算和准备。具体的计算内容我们可以以后再看，总之通过这两个部分的修改——prototxt中的数据结构定义和DataLayer部分相关位置的修改，我们就可以使得网络输入多种多样的数据，我们的Caffe也就可以完成更多有挑战的事情了。&lt;/p&gt;&lt;h2&gt;Data Transformer&lt;/h2&gt;&lt;p&gt;这一段新加入的，本来希望能单独写一篇，后来发现字数不够多，就把这部分和这篇合并起来了。我们来看一看Data Transformer的内容。这一部分的实现在C++和python上有所不同。一般来说，我们用C++的代码做训练，用python的代码做预测（当然现在用python做训练的也越来越多）。在C++中这是DataLayer中的一个小部分，而在python中这是一个独立的部分。&lt;/p&gt;&lt;h2&gt;C++中的DataTransformer&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Crop&lt;/b&gt;：这是在训练过程中经常用到的一种增强数据的方式。在train的过程中Caffe会进行随机crop，在test的过程中只会保留中间的部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mirror&lt;/b&gt;：做一个x轴的翻转&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mean&lt;/b&gt;: 给每个像素减去一个均值&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scale&lt;/b&gt;：给每个像素值乘以一个系数&lt;/p&gt;&lt;h2&gt;Python中的DataTransformer&lt;/h2&gt;&lt;p&gt;python的Transformer就有些复杂了：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Resize&lt;/b&gt;：将输入数据缩放到指定的长宽比例&lt;/p&gt;&lt;p&gt;&lt;b&gt;Transpose&lt;/b&gt;：转换输入数据的维度。因为经过skimage读入后数据的维度是(Height * Width * Channel)，需要将数据的维度转换到(Channel*Height*Width)&lt;/p&gt;&lt;p&gt;&lt;b&gt;Channel&lt;/b&gt;&lt;b&gt;swap&lt;/b&gt;：这个主要针对彩色图的输入，不同的图像处理库对channel的处理顺序有所不同。像opencv，C++的主要合作伙伴，它的默认装载顺序是BGR——Blue,Green,Red。而skimage读入的是RGB，所以为了保证和C++训练的模型一致，所以这一步也是很必要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raw scale——Mean——Input scale&lt;/b&gt;：这里是将每个像素乘以一个Raw scale，减去一个Mean，再乘以一个Input scale。&lt;/p&gt;&lt;p&gt;python的版本中还包含一个&lt;b&gt;deprocess&lt;/b&gt;，用于做图像的反向处理。&lt;/p&gt;&lt;p&gt;python版本的crop和mirror功能在python/caffe/io.py中的oversample函数，不过他的逻辑和C++的逻辑不太一样了。实际使用中还可以针对自己的使用情况进行修改。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22404295&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Fri, 16 Sep 2016 08:43:04 GMT</pubDate></item><item><title>聊点轻松的3——什么是学习</title><link>https://zhuanlan.zhihu.com/p/22421787</link><description>之前给自己定下了一个目标，专栏的关注者每突破一千就写一篇和技术没有太大关系但是又比较有意义的文章。第一个1000人，我聊了点专栏未来的发展方向，第二个1000人，我放了一些“有趣”但实际上挺恶俗的图片，到了第三个1000人，我想了好久才想到了这个话题，于是让我们一起聊一聊什么是学习。&lt;p&gt;写在前面，专栏里正常的文章还是怀着科学的精神来写的，不过这个“聊点轻松的”支线剧情就不管那么多了，玄学为主哈。&lt;/p&gt;&lt;p&gt;我们聊了好多有关机器学习的事情，那么让我们回到自己身上，人又是怎么学习的？从人学习方式来看，我们有哪些优秀的模式可以利用到机器上呢？从机器学习的角度，人类的学习又该如何进行呢？（好玄学的问题啊）&lt;/p&gt;&lt;h2&gt;学习方法&lt;/h2&gt;&lt;p&gt;说到这个让我们首先来看看机器界的三种常见的学习方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;监督学习&lt;/li&gt;&lt;li&gt;非监督学习&lt;/li&gt;&lt;li&gt;增强学习&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;排名不分先后。这几种学习在我们的日常生活中会不会经常出现呢？当然会。下面我们来举几个例子：&lt;/p&gt;&lt;p&gt;监督学习：1+1等于几？要是答等于2就对了，答其他就错了（这里是严肃的1+1问题）。这里问题有，答案也有，我们的目标是让自己做桥梁，连通问题和答案；&lt;/p&gt;&lt;p&gt;非监督学习：多半来自脑中突然出现的一些宇宙终极问题，还有一些课外兴趣小组中偶然碰到的问题（当然还有现在工作研究中的一些问题），没人知道正确答案，能不能答对全看自己的思考分析了。（XX猜想……）当然一般这些问题最后慢慢都有了答案，不过当初解决问题时确实没有答案。&lt;/p&gt;&lt;p&gt;增强学习：生活小经验，生活小技能。人在江湖飘，哪能不挨刀，每一次成功与失败，老天都会给你一个“奖励”，并让这个世界稍微改变一点……当然这个奖励有时候不是物质的，比方说“心里乐开了花”这种……&lt;/p&gt;&lt;p&gt;从某种意义上看，机器学习的方式和人类学习的方式其实差不多，机器学习也可以算是门仿生学了。我们走进校园，在课堂上被老师进行监督学习，自己无聊时做些思考，进行一些非监督学习，其他的大部分时间，进行很多增强学习。于是学校里也经常出现各种各样的传奇人物：&lt;/p&gt;&lt;p&gt;学霸：典型的监督学习产物，一天刷一本练习册，这不是海量训练数据样本+高性能计算是什么？&lt;/p&gt;&lt;p&gt;学神：思考一些神秘的问题，经常有规律的进行非监督学习……当然学神一般是在完成了监督学习之后才进行这项高级活动的。&lt;/p&gt;&lt;p&gt;老（lao）江（si）湖（ji）：社会活动多，社交经验丰富，审时度势，察言观色，调节气氛，推动剧情的好帮手。&lt;/p&gt;&lt;p&gt;好吧，胡扯到此结束。不过似乎每个人都有自己擅长的学习方式，也有自己不擅长的学习方式，可能所谓的“因材施教”和这个也有关系吧。&lt;/p&gt;&lt;h2&gt;论语和机器学习&lt;/h2&gt;&lt;p&gt;玄学第二弹，论语！你没有看错，几千年前的孔老夫子，中华文化的形象代言人，已经在他的著作中告诉了我们很多机器学习的“套路”！已经忘了？没关系，让我们来一起复习下：&lt;/p&gt;&lt;p&gt;“学而时习之，不亦说乎？”&lt;/p&gt;&lt;p&gt;——训练数据要记得经常回测一下，以确保模型的泛化性，别捡了芝麻丢了西瓜。&lt;/p&gt;&lt;p&gt;“温故而知新，可以为师矣”&lt;/p&gt;&lt;p&gt;——要在旧数据上finetune，也要记得在新数据上继续提升哦～&lt;/p&gt;&lt;p&gt;——要是能通过研究旧数据就能产生新数据，那就逆天了！Deep Generative Model，我来了！（歪解）&lt;/p&gt;&lt;p&gt;“吾尝终日不食，终夜不寝，以思，无益，不如学也”&lt;/p&gt;&lt;p&gt;——跑了三天无监督训练精度还是上不去，还是去标点数据跑监督学习吧。&lt;/p&gt;&lt;p&gt;（镜像贴：《荀子  劝学》“吾尝终日而思，不如须臾之所学也”）&lt;/p&gt;&lt;h2&gt;有痛的学习&lt;/h2&gt;&lt;p&gt;专栏的名字叫做“无痛的机器学习”，可实际上谁都知道学习有痛。最近还被罗振宇罗胖打了脸。他的一期《罗辑思维》中《怎么样成为一个高手》&lt;a href="http://v.youku.com/v_show/id_XMTY4OTY3NjU3Ng==.html?from=y1.6-91.3.1.5bdbf57c947311e3b8b7" data-editable="true" data-title="罗辑思维"&gt;罗辑思维&lt;/a&gt;中提到了，学习是要专找自己不会的地方“刻意练习”。从机器学习的角度解释再简单不过了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;只有自己不会的地方才会有较大的loss梯度，才有优化目标函数的必要，总是找自己已经会的，都没有梯度可以下降，还优化个毛线啊……&lt;/li&gt;&lt;li&gt;刻意练习，反复练习。哪有一轮迭代就可以完美梯度下降的优化算法，兄弟一定要多跑几轮啊！&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实说到这里想想学习这个事情，还是听悲哀的。学习本来就是个苦差事，现在的中文中，从学习两个字不太能看出它的痛苦，主要是能看出它的手段——学和习，不过我中华上国，少年学习不谈苦和累，只谈使命和责任；不过我们的邻居就比较诚实了：&lt;/p&gt;&lt;p&gt;日语中的学习写作：“&lt;b&gt;勉強する&lt;/b&gt;”，我去太直白了，就是勉强嘛，凡事不能勉强啊……&lt;/p&gt;&lt;p&gt;韩语中的学习写作：“&lt;b&gt;공부하다&lt;/b&gt;”，后面两个字算是词缀，和日语的“する”类似，前面两个字念出来就是“恐怖”，恐怖……&lt;/p&gt;&lt;p&gt;所以多学点语言了解不同的国家人的内心还是很有必要的……一个是做勉强的事情，一个是做恐怖的事情，我们还在天真地认为那只是在知识的海洋遨游……&lt;/p&gt;&lt;p&gt;虽然学习是有痛的，但是“学海无涯苦作舟”的时代已经过去，现在的时代信息交流已经变得非常方便，我们可以寻找到很多方法让自己在学到一定知识的基础上减少痛苦，或者在一定痛苦的基础上多学知识。随着社会的发展，人类要学的会越来越多，不过人类学习的痛苦也一定会越来越轻的。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22421787&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Tue, 13 Sep 2016 00:00:31 GMT</pubDate></item><item><title>CNN-卷积反卷积（2）</title><link>https://zhuanlan.zhihu.com/p/22293817</link><description>在前面的斗图篇我们提过这篇文章《Visualizing and understanding convolutional networks》，这是一片介绍反卷积和可视化的文章，今天我们就来详细看看这篇文章的一个开源实现——来自&lt;a href="https://github.com/piergiaj/caffe-deconvnet" data-editable="true" data-title="GitHub - piergiaj/caffe-deconvnet: A deconvolutional network in caffe"&gt;GitHub - piergiaj/caffe-deconvnet: A deconvolutional network in caffe&lt;/a&gt;。&lt;p&gt;首先我们给出上面这篇论文的网络结构架构：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/06d0d9ad21e2f6d6039c3c6dec164652.jpg" data-rawwidth="649" data-rawheight="564"&gt;&lt;p&gt;从结构中可以看出，网络首先进行前向计算，在前向计算中收集一些数据，然后将这些数据塞入反向网络中进行反向计算，从而得到最终的反卷积结果。&lt;/p&gt;&lt;p&gt;这其中还包括一些传统网络中没有的层结构——当然不是网络中带有参数的核心层了。以下这张图是这个网络的结构展示以及在前向计算过程中每个层的维度大小：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/067af47f556e22c96654b4acd4e11fdc.jpg" data-rawwidth="960" data-rawheight="1280"&gt;&lt;p&gt;从这张图可以看出它就是一个经典的AlexNet结构的网络，但是这个层中有一些特殊的层，它们是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PoolingSwitches&lt;/li&gt;&lt;li&gt;SliceHalf&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个层有什么作用呢？实际上它就是一个Pooling层，只不过会多输出一些信息。对于max_pooling来说，它会输出正常的max_pooling值，以及对应的max_pooling的index。这个index信息将会被用在反向计算中。因为反向计算时我们要将diff传输到pooling层前指定的index上，这里要做一个记录。&lt;/p&gt;&lt;p&gt;其中PoolingSwitches就是记录max pooling的值和max pooling选中的index，而SliceHalf则将这两部分分成两个输出，属于SliceLayer的一个特殊实现。&lt;/p&gt;&lt;p&gt;反向计算就是把之前的参数再传输回去，在前面我们已经介绍过这其中的思想了。返回的网络结构如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0819bac8ccd184778b1393007b509642.jpg" data-rawwidth="3024" data-rawheight="4032"&gt;可以看出除了去掉了relu层，反向的结构和前向的结构完全一样。反向中的InvPooling层做了Pooling的的反向计算，其中利用到了前向计算中保存的index值。&lt;/p&gt;&lt;p&gt;其中的一些效果可以参见前面的斗图篇，这里就不多介绍了。链接是：&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22112582" data-editable="true" data-title="聊点轻松的2——斗图篇 - 无痛的机器学习 - 知乎专栏"&gt;聊点轻松的2——斗图篇 - 无痛的机器学习 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;感兴趣的童鞋也可以自己尝试一下。&lt;/p&gt;&lt;p&gt;总体来说这个方法可以帮助我们观察出图像中的一些响应特点，在论文中作者还提出，他们通过可视化的方法发现了AlexNet中的一些不足，并提出了一些改进方法，也最终形成了在江湖上有一定影响力的ZF模型。&lt;/p&gt;&lt;p&gt;除了这种反卷积的可视化方法，其实还有其他的可视化方法。后面我们再慢慢介绍。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22293817&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Sun, 11 Sep 2016 20:23:47 GMT</pubDate></item><item><title>CNN-反卷积（1）</title><link>https://zhuanlan.zhihu.com/p/22245268</link><description>关于CNN的内容已经说了很多，虽然我们无法把这个长得像黑盒的东西完全摸清楚，但是我们多多少少也对它的外部结构有了一定的了解。下面我们来看看大神们对进一步探究CNN内部世界所做的工作。&lt;p&gt;这部分工作有一个响亮的Title，那就是，CNN的网络到底学到了什么？&lt;/p&gt;&lt;p&gt;对于浅层网络，尤其是一层网络，上面这个问题非常好回答。我们知道模型输入的特征和分布，我们也知道输出的特征和分布（这里特指监督学习），那么模型的目标就是把输入空间的数据映射到输出空间，而且映射的结果是正确的。如果我们把浅层网络替换成深层网络，还把它当作一个不可分割的整体，那么它的目标和浅层网络是完全一致的。&lt;/p&gt;&lt;p&gt;但问题是，我们并没有把深层网络当成浅层网络，因为深度学习涉及了浅层网络学习时代的两个部分——构建特征和从特征到结果。所以我们心里一定在想，这么多层，哪些层是在构建特征，哪些层是在把特征转换到结果？在CNN网络模型发展的初期，人们倾向于把这个分界点设立在卷积层和全连接层的交界处——卷积层负责收集特征信息，全连接层和早年的神经网络一样，只负责特征的处理。&lt;/p&gt;&lt;p&gt;如果你接受了这样的概念，那么我们下一个问题就来了。卷积层是如何把一张图片或者其他的东西转换成了特征，这些特征是如何表达我们的图片呢？这还是一个没有解决的问题。&lt;/p&gt;&lt;p&gt;于是很多大神开始了各种尝试，其中一种尝试就是反卷积操作。&lt;/p&gt;&lt;h2&gt;反卷积&lt;/h2&gt;&lt;p&gt;反卷积是什么？是一个新概念呢？其实不是。其实在CNN中Deconvolution的计算和图像的反卷积操作还是有一点不同，这种Deconvolution是将Convolution的方向反过来——没错，前向变后向，后向变前向。实际上关于它的名称还有很多，像back convolution, transpose convolution等。我个人比较喜欢back convolution。关于前向后向的计算可以参考我们前面对于卷积层的计算推导，这里就不再多说了。那么我们就从这个角度去看看反卷积究竟做了什么。&lt;/p&gt;&lt;p&gt;想知道反卷积的前向操作做了什么，我们可以去看卷积的后向操作做了什么。卷积的后向操作是为了计算卷积层的Loss对参数和输入的梯度，那么梯度是什么含义？负梯度表示了函数下降最快的方向，为了使函数值（也就是Loss）尽可能地小，我们希望梯度尽可能地小。但是如果某个参数的梯度非常大，能说明什么呢？说明当前函数参数的变动对函数造成的影响非常大。&lt;/p&gt;&lt;p&gt;举个例子，比方说有这样一个函数&lt;equation&gt;y=w_1*x_1+w_2*x_2&lt;/equation&gt;，y最终还要经过计算得到最终的loss，那么我们计算这个函数的梯度：&lt;/p&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x_1}=\frac{\partial Loss}{\partial y}*w_1&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial x_2}=\frac{\partial Loss}{\partial y}*w_2&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial w_1}=\frac{\partial Loss}{\partial y}*x_1&lt;/equation&gt;&lt;equation&gt;\frac{\partial Loss}{\partial w_2}=\frac{\partial Loss}{\partial y}*x_2&lt;/equation&gt;&lt;p&gt;如果w1非常大而w2非常小，那么x1的梯度就会远大于x2，如果x1做点小改动，它会对最终结果产生比较大的影响；而x2做点小改动，对应的影响就没有那么严重了；&lt;/p&gt;&lt;p&gt;如果x1非常大而x2非常小，那么w1的梯度就会远大雨w2，如果w1做点小改动，它会对最终结果产生比较大的影响；而w2做点小改动，对应的影响就没有那么严重了；&lt;/p&gt;&lt;p&gt;那么我们可以想想看，对结果产生较大影响以为着什么？对于一个人脸识别系统，上面公式中那些对结果影响比较大的因子往往对应着人脸中富有关键特征的地方，因为如果这些地方发生了变化，我们识别这个人的关键信息就变动了，换言之——我们就有可能认错人。&lt;/p&gt;&lt;p&gt;而对于那些对结果影响较小的因子，往往对应着一些不太重要的地方，比方说背景——不论你变成蓝色，红色，白色，我都能认出这个人。&lt;/p&gt;&lt;p&gt;所以我们可以得出一个推论，那就是梯度大的因子是当前这个这套模型和数据组成的整体的关键！对于w，它就是当前输入数据的关键参数，对于x，他就是当前模型参数的关键输入！&lt;/p&gt;&lt;p&gt;所以，如果我们能够找到对于某一个模型某一特定输入下梯度比较大的输入因子和参数因子，我们就能知道输入的哪些部分是模型最关心的，而模型的哪些部分是输入最关心的。&lt;/p&gt;&lt;p&gt;一般来说，由于输入的图像大多具备一定的可视性，所以我们观察“输入的哪些部分是模型最关心的”是相对容易的。&lt;/p&gt;&lt;p&gt;那么我们就可以开始我们的实验了，其实并不需要真的构建一个反卷积层，我们理论上只要把数据从前向传一遍，后向传一遍，然后分析Loss对输入的梯度，找出梯度最大的像素点，就是模型最关心的地方。这样是不是就可以完成任务？&lt;/p&gt;&lt;p&gt;理论上是这样的，那么下一回我们看看具体的实现。&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;"我爱机器学习"3群已经准确就绪：252089415，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252089415！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22245268&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Fri, 09 Sep 2016 21:14:47 GMT</pubDate></item><item><title>CNN--两个Loss层计算的数值问题</title><link>https://zhuanlan.zhihu.com/p/22260935</link><description>写在前面，这篇文章的原创性比较差，因为里面聊的已经是老生长谈的事情，但是为了保持对CNN问题的完整性，还是把它单独拿出来写一篇了。已经知道的童鞋可以忽略，没看过的童鞋可以来瞧瞧。&lt;p&gt;这次我们来聊一聊在计算Loss部分是可能出现的一些小问题以及现在的解决方法。其实也是仔细阅读下Caffe代码中有关Softmax loss和sigmoid cross entropy loss两个部分的真实计算方法。&lt;/p&gt;&lt;h2&gt;Softmax&lt;/h2&gt;&lt;p&gt;有关Softmax的起源以及深层含义这里不多说了，我们直接来看看从定义出发的计算方法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def naive_softmax(x):
    y = np.exp(x)
    return y / np.sum(y)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;随便生成一组数据，计算一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)
print a
print naive_softmax(a)

[ 0.67362493  0.20352691  0.02024274  0.29988184  0.2319521           
  0.43930833  0.98219225  0.54569955  0.00298489  0.83399241]
[ 0.12203807  0.07626659  0.06349434  0.08398094  0.07846559   
  0.09654569  0.16615155  0.10738362  0.06240797  0.14326563]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从结果来看比较正常，符合预期，但是如果我们的输入不那么正常呢？&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;b = np.random.rand(10) * 1000
print b
print naive_softmax(b)

[ 497.46732916  227.75385779  537.82669096  787.54950048  663.13861524
  224.69389572  958.39441314  139.09633232  381.35034548  604.08586655]
[  0.   0.   0.  nan   0.   0.  nan   0.   0.   0.]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们发现数值溢出了，因为指数函数是一个很容易让数值爆炸的函数，那么输入大概到多少会溢出呢？蛋疼的我还是做了一个实验：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;np.exp(709)
8.2184074615549724e+307
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这是在python能够正常输出的单一数字的极限了。实际上这接近double类型的数值极限了。&lt;/p&gt;&lt;p&gt;虽然我们前面讲过有一些方法可以控制住数字，使输出不会那么大，但是终究难免会有个别大数字使得计算溢出。而且实际场景中计算softmax的向量维度可能会比较大，大家累积起来的数字有时还是挺吓人的。&lt;/p&gt;&lt;p&gt;那么如何解决呢？我们只要给每个数字除以一个大数，保证它不溢出，问题不就解决了？老司机给出的方案是找出输入数据中最大的数，然后除以e的最大数次幂，相当于下面的代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def high_level_softmax(x):
    max_val = np.max(x)
    x -= max_val
    return naive_softmax(x)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样一来，之前的问题就解决了，数值不再溢出了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;b = np.random.rand(10) * 1000
print b
print high_level_softmax(b)

[ 903.27437996  260.68316085   22.31677464  544.80611744  506.26848644
  698.38019158  833.72024087  200.55675076  924.07740602  909.39841128]

[  9.23337324e-010   7.79004225e-289   0.00000000e+000   
   1.92562645e-165   3.53094986e-182   9.57072864e-099   
   5.73299537e-040   6.01134555e-315   9.99999577e-001   
   4.21690097e-007]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;虽然不溢出了，但是这个结果看着还是有点怪。上面的例子中最大的数字924.07740602的结果高达0.99999，而其他一众数字经过softmax之后都小的可怜，小到我们用肉眼无法从坐标轴上把它们区分出来，这说明softmax的最终结果和scale有很大的关系。&lt;/p&gt;&lt;p&gt;为了让这些小的可怜的数字不那么可怜，使用一点平滑的小技巧还是很有必要的，于是代码又变成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def practical_softmax(x):
    max_val = np.max(x)
    x -= max_val
    y = np.exp(x)
    y[y &amp;lt; 1e-20] = 1e-20
    return y / np.sum(y)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结果变成了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[  9.23337325e-10   9.99999577e-21   9.99999577e-21   9.99999577e-21
   9.99999577e-21   9.99999577e-21   9.99999577e-21   9.99999577e-21
   9.99999577e-01   4.21690096e-07]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看上去比上面的还是要好一些，虽然不能扭转一家独大的局面。&lt;/p&gt;&lt;h2&gt;Sigmoid Cross Entropy Loss&lt;/h2&gt;&lt;p&gt;从上面的例子我们可以看出，exp这个函数实在是有毒。下面又轮到另外一个中毒专业户sigmoid出厂了。这里我们同样不解释算法原理，直接出代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def naive_sigmoid_loss(x, t):
    y = 1 / (1 + np.exp(-x))
    return -np.sum(t * np.log(y) + (1 - t) * np.log(1 - y)) / y.shape[0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们给出一个温和的例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)
b = a &amp;gt; 0.5
print a
print b
print naive_sigmoid_loss(a,b)

[ 0.39962673  0.04308825  0.18672843  0.05445796  0.82770513  
  0.16295996  0.18544111  0.57409273  0.63078192  0.62763516]
[False False False False  True False False  True  True  True]
0.63712381656
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下面自然是一个暴力的例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)* 1000
b = a &amp;gt; 500
print a
print b
print naive_sigmoid_loss(a,b)

[  63.20798359  958.94378279  250.75385942  895.49371345  965.62635077
   81.1217712   423.36466749  532.20604694  333.45425951  185.72621262]
[False  True False  True  True False False  True False False]
nan
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;果然不出所料，我们的程序又一次溢出了。&lt;/p&gt;&lt;p&gt;那怎么办呢？这里节省点笔墨，直接照搬老司机的推导过程：（侵删，我就自己推一遍了……）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/70203ed967a41892e424dfea23ffd2c6.jpg" data-rawwidth="1229" data-rawheight="1325"&gt;于是，代码变成了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;def high_level_sigmoid_loss(x, t):
    first = (t - (x &amp;gt; 0)) * x
    second = np.log(1 + np.exp(x - 2 * x * (x &amp;gt; 0)))
    return -np.sum(first - second) / x.shape[0]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;举一个例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;a = np.random.rand(10)* 1000 - 500
b = a &amp;gt; 0
print a
print b
print high_level_sigmoid_loss(a,b)

[-173.48716596  462.06216262 -417.78666769    6.10480948  340.13986055
   23.64615392  256.33358957 -332.46689674  416.88593348 -246.51402684]
[False  True False  True  True  True  True False  True False]
0.000222961919658
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样一来数值的问题也就解决了！&lt;/p&gt;&lt;h2&gt;就剩一句话了&lt;/h2&gt;&lt;p&gt;计算中遇到Exp要小心溢出！&lt;/p&gt;&lt;h2&gt;私货时间&lt;/h2&gt;&lt;p&gt;“我爱机器学习”1群快要装满，2群已经准确就绪：252085834，欢迎大家赶紧上车！&lt;/p&gt;&lt;p&gt;252085834！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22260935&amp;pixel&amp;useReferer"/&gt;</description><author>冯超</author><pubDate>Wed, 07 Sep 2016 09:04:13 GMT</pubDate></item></channel></rss>