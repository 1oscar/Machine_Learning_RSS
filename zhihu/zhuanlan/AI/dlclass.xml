<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>深度学习大讲堂 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/dlclass</link><description>推送深度学习的最新消息，包括最新技术进展，使用以及活动</description><lastBuildDate>Thu, 20 Oct 2016 16:15:27 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>IJCAI16论文速读：Deep Learning论文选读（上）</title><link>https://zhuanlan.zhihu.com/p/23037608</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1d697eb1755f2db92d1ba82e202d4ad1_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;摘要&lt;/b&gt;&lt;p&gt;选读两篇IJCAI2016的深度学习论文，IBM东京研究院提出的基于模型性能预测的深度网络超参数快速选择算法和厦门大学纪荣嵘组提出的基于全局误差重构的深度网络模型压缩方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;IJCAI16会议介绍：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国际人工智能联合会议（ International Joint Conference on Artificial Intelligence，IJCAI ）是聚集人工智能领域研究者和从业者的盛会，也是人工智能领域中最主要的学术会议之一。1969 年到 2015 年，该大会在每个奇数年举办，现已举办了 24 届。随着近几年来人工智能领域的研究和应用的持续升温，从 2016 年开始，IJCAI 大会将变成每年举办一次的年度盛会；今年是该大会第一次在偶数年举办。第 25 届 IJCAI 大会于 7 月 9 日- 15 日在纽约举办。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor导读：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本届会议的举办地在繁华喧嚣的纽约时代广场附近，正映衬了人工智能领域几年来的火热氛围。此次大会包括7场特邀演讲、4场获奖演讲、551篇同行评议论文的presentation，41场workshop、37堂tutorial、22个demo等。深度学习成为了IJCAI 2016的关键词之一，以深度学习为主题的论文报告session共计有3个。本期我们从中选择了两篇篇深度学习领域的相关论文进行选读，组织了相关领域的博士研究生，介绍论文的主要思想，并对论文的贡献进行点评。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Weight Features for Predicting Future Model Performance of Deep Neural Networks&lt;/b&gt;&lt;/p&gt;&lt;p&gt;IBM东京研究院的研究者研究了一个有趣且非常实用的问题，在深度学习的调参中如何快速的丢弃一些超参数的组合来加速调参。传统的方法只利用了网络训练过程中的learning curve（不同训练epoch的模型在测试集上最终性能构成的曲线）, 却没有考虑网络模型参数与最终模型性能的相关性。本文提出利用网络训练阶段的参数作为特征，采用random forest学习回回归函数，直接建模参数和参数变化与最终模型性能的关系，预测网络的最终性能。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-53856b1a0b3b52b2301e9bf79063082b.png"&gt;&lt;b&gt;方法框架：&lt;/b&gt;特征设计和性能预测函数。特征设计部分参考了手工描述子的设计方式，比如对卷积核权重提取了等均值、方法、峰度、散度等特征，性能预测函数部分则直接使用了随机森林算法。通过在已有的learning curve上训练分类器器，可以在其他超参数组合训练的早期（比如前10个epoch）预测其最终模型的性能。&lt;/p&gt;&lt;p&gt;部分实验设计如表1，给出了ImageNet数据集上的超参数搜索空间。需要指出的是，在实践中，卷Act. func会默认为ReLU，大量实验表明ReLU效果更好。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dbea167900f8c169f93ce9ab03c41a79.png"&gt;下图所示为在三个数数据集的实验，虽然相比learning curve的方式，该方法的性能有显著的提高，但是分类正确率或者top-1错误率估计的均方误差（RMSE）还是达到0.13，这对于锱铢必较的细粒度调参来说暂时还达不到实用程度。但是一定意义上，通过文中方法early stop掉一些明显最终性能预期不好的超参数组合对于加速超参数搜索依旧有其实用价值。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-994a66bd9479d9da4c5a2a87e1c5a55b.png"&gt;总结起来文章提供了一个通过模型参数来预测模型最高性能的方法，提供了超参数选择的一种思路。但是目前的实验结果尚不足以代精细化的调参，比如learning rate 0.01, 0.02….的细粒度调整。&lt;/p&gt;&lt;p&gt;本文方法的未来改进的空间在于由目前手工设计特征+分类器的两段式方法到非端到端的深度学习方法。此外，考虑实际应用中细粒度调参的需要也是本文方法的一个可能改进方向。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Towards Convolutional Neural Networks Compression via Global Error Reconstruction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大规模深度卷积神经网络，例如VGGNet面临参数量大和计算复杂度高两个问题，现有的网络压缩方法如low-rank分层分解会带来层间的累积误差问题，影响网络全局逼近的精度。厦门大学纪荣嵘组的这篇论文设计了一种两段式网络压缩方法，首先对全连接层进行low–rank分解，接着引入了一个全局重构误差最小化的策略，通过最小化网络的重构误差来对压缩后的网络进行fine-tune，从而有效缓解了分层逼近带来的累积误差问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方法框架：&lt;/b&gt;全连接层low-rank分解的方法图示如下，全连接矩阵W被分解为矩阵P和Q。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cc0ffdcf5461eddcc28c06e5dd1e4700.png"&gt;Low-rank分解的形式化如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cade3cf7af2cd36b773668e5e82463f0.png"&gt;进一步的，基于全局误差最小化的优化目标，对low-rank分解后每一层的参数P1和Q1用误差反向传播算法进行更新。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5bd08b55775087c43b14d05e41a6c7ff.png"&gt;在VGGNet上的实验结果如下图所示，文章提出的GER算法取得了state-of-the-art的性能。当然高压缩率情况下的GER明显的性能优势，理论上的意义更大一些，因为此时的性能损失已经太大。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3e203dcb858aef895ce4816fee8b5c9b.jpg"&gt;从实用性的角度，单独压缩全连接层虽然可以显著减少参数，但是理论意义更大，原因有两点：1）实践中已经越来越少使用全连接层，例如ResNet中就没有全连接层（分类器层除外）。2）卷积层占了主要的计算量，虽然显著压缩了参数，但并不能明显改善速度。&lt;/p&gt;&lt;p&gt;本文的潜在优势在于，卷积层的运算实际上可以写成patch展开后的矩阵（Caffe中的Im2Col操作）和kernel matrix的矩阵相乘，也可以采用low-rank的方法逼近，因此本文方法如果扩展到卷积层，则可以直接降低卷积层计算量和参数量，这也是GER在实际问题中的潜力所在。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参与人员：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;胡兰青，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;尹肖贻，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘昊淼，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;刘   昕，&lt;/b&gt;中科院计算所VIPL研究组博士研究生&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Guest Editor：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-50f36023f36d78ecb565c4cf89f5a509.jpg" data-rawwidth="118" data-rawheight="117"&gt;朱鹏飞，&lt;/b&gt;天津大学机器学习与数据挖掘实验室副教授，硕士生导师。分别于2009和2011年在哈尔滨工业大学能源科学与工程学院获得学士和硕士学位，2015年于香港理工大学电子计算学系获得博士学位。目前，在机器学习与计算机视觉国际顶级会议和期刊上发表论文20余篇，包括AAAI、IJCAI、ICCV、ECCV以及IEEE Transactions on Information Forensics and Security等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325557&amp;amp;idx=1&amp;amp;sn=362d476d3b3820ea56e4672369565e4f&amp;amp;chksm=f235a53fc5422c2939f76b7e8f5265333f3159b0ec4275fe733d27e7a03f17395b0460a318d2&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="IJCAI16论文速读：Deep Learning论文选读（上）"&gt;IJCAI16论文速读：Deep Learning论文选读（上）&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23037608&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 18 Oct 2016 17:35:58 GMT</pubDate></item><item><title>深度学习在文本简化中的应用进展</title><link>https://zhuanlan.zhihu.com/p/22956057</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c925d01a44f7371afa5589b02c3a013a_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;b&gt;背景与介绍&lt;/b&gt;&lt;p&gt;近年来，机器翻译任务依靠深度学习技术取得了重大突破。最先进的神经机器翻译模型已经能够在多种语言上超越传统统计机器翻译模型的性能。在传统统计机器翻译模型上积累深厚的谷歌，也终于开始将最新的神经机器翻译系统逐步上线。&lt;/p&gt;&lt;p&gt;目前神经机器翻译的技术基础是端到端的编码器-解码器架构，以源语言句子作为输入，目标语言同义句作为输出。容易想象，只要具备充足的训练数据，类似架构完全有可能推广到其他涉及文本改写的任务上。例如，输入一段文字，希望系统输出一小段核心语义不变、但更为简洁的表达。这样的改写统称为文本简化（text simplification）。近两年深度学习技术应用相对较多的是其中的一个实例，在自然语言生成研究中一般称为语句压缩（sentence compression）或语句简化（sentence simplification），即输入和输出均为语句。语句简化任务要求神经网络结构能够编码输入句中的核心语义信息，才能够提炼出不改变原句主要意义的更简洁表达。&lt;/p&gt;&lt;p&gt;深度学习技术在语句简化上的一个典型应用是新闻标题生成（headline generation）。新闻文章通常具有较为规范的写作形式：文章首句或者首段对新闻内容进行概括介绍，新闻标题则进一步精炼概括出新闻核心事件。目前基于深度学习技术的新闻标题生成研究中，一般以新闻文章的首句作为输入，生成该新闻文章的标题。现有的基于深度学习的新闻标题生成工作通常采用和神经机器翻译类似的编码器-解码器架构，一般不需要手动提取特征或语句改写文法。最常见的仍然是序列到序列（sequence-to-sequence, 简记为seq2seq）模型。典型的序列到序列模型如图1所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-62cb9e88d7414159a64a094b1c652b65.jpg" data-rawwidth="666" data-rawheight="198"&gt;&lt;p&gt;在图1中，以文本序列A-B-C-&amp;lt;EOS&amp;gt;输入给一个编码器，编码器用于将输入文本编码成一个语义向量表达（图中对应第四个节点处的隐状态）。输入序列的语义向量表达进而交由解码器用于目标文本序列W-X-Y-Z-&amp;lt;EOS&amp;gt;的生成。这里的&amp;lt;EOS&amp;gt;表示序列结束符号（end of sequence），标志着输入序列或输出序列的结束。解码器接收&amp;lt;EOS&amp;gt;符号后，开始解码的过程，直到生成&amp;lt;EOS&amp;gt;符号标志着解码过程的结束。序列到序列模型是一个按照字符（或单词）序列逐个处理的过程，编码过程中编码器逐个接收输入字符，解码过程中解码器逐个输出生成的字符。在最原始的模型训练过程中，解码器每次接收答案序列中的一个字符（例：W），预测应该输出的下一个字符（例：X）。编码器-解码器架构的经典训练目标，是在给定编码器输入后，使解码器输出的结果能够最大程度地拟合训练集中的答案，在概率模型下即最大化数据似然。在模型预测阶段，答案序列未知，解码器接收&amp;lt;EOS&amp;gt;作为解码开始符，并生成一个输出字符，然后将模型预测出的输出字符作为解码器的下一个输入，重复这个过程直到解码器生成&amp;lt;EOS&amp;gt;符号为止。预测阶段的一般目标是，给定输入句编码后，根据当前模型选择概率最大的解码器输出结果。精确搜索这个最优解一般复杂度极高，所以在实际应用中解码过程通常应用集束搜索（beam search，也可译作柱搜索）近似求解：在每一步保留K个最高得分的输出，最后从K个输出结果中选择得分最高的作为最终的输出。&lt;/p&gt;这样的编码器-解码器模型一般可以处理变长的输入和输出序列，使得它可以被应用于多种文本改写任务上。形式上，给定一个包含M个词的输入文本序列x={x1,x2,…,xM}，在模型中将每个词xt表示成一个向量。词的向量表示会在模型中进行学习，可以用无监督训练得到的一般word embedding向量作为初始化。语句简化的目标是生成输入句x的一个简化y={y1,y2,…,yN}，一般要求y的长度比输入句更短，即N&amp;lt;M。标题生成的目标是寻找y ̂使得给定x的条件下y的条件概率最大化，即：y ̂=arg⁡maxy⁡〖&lt;em&gt;P&lt;/em&gt;(y|x;&lt;em&gt;θ&lt;/em&gt;)〗 ，其中&lt;em&gt;θ&lt;/em&gt;代表需要学习的模型参数。条件概率&lt;em&gt;P&lt;/em&gt;(y|x;&lt;em&gt;θ&lt;/em&gt;)可以由链式法则分解为：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-81b9e8cc835358a137d9cff5113448e0.jpg" data-rawwidth="468" data-rawheight="90"&gt;编码器一般能够处理长度不确定的输入文本序列，将每个词的词向量表示汇总，编码成一个定长的输入文本向量表示。这个编码过程可以采用不同的编码器，如卷积神经网络（CNN），循环神经网络（RNN）等。而解码的过程是根据输入文本序列生成输出文本序列的过程，在大多数模型中，解码器使用的是RNN，常用的RNN节点包括标准的RNN单元以及像LSTM、GRU这样记忆能力更强的带门限单元等。RNN对序列中的每一个单元执行相同的运算过程，从而可以接受任意长的序列作为输入。具体来说，一个标准的RNN以及其按照输入序列展开形式如图2所示。在图2中，xi是第i个输入词语，hi是接收xi之后RNN隐单元的状态。hi+1基于前一个隐状态hi和当前的输入xi+1得到，即hi+1=&lt;em&gt;f&lt;/em&gt;(Uxi+1+whi)。&lt;em&gt;f&lt;/em&gt;是非线性函数，如tanh或者sigmoid。标准的RNN单元在每一步输出yi+1=g(Vhi+1)，g是非线性函数。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-2f4d30016a8f4c3d31cb0e61baf615bb.jpg" data-rawwidth="683" data-rawheight="252"&gt;在序列到序列模型中，如果选用RNN作为编码器，这一部分RNN的输出（yi）一般被忽略；而RNN作为解码器时，每一步输出yi+1对应规模为V的词表上所有词语的概率分布（通常选用softmax函数将V维得分向量标准化得到），产生yi+1的过程依赖于前一步状态hi以及前一步的输出yi。解码过程中，生成单词yi+1的方法是 :&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8572518c594e5d77b3f69c3e6c2aa242.jpg" data-rawwidth="544" data-rawheight="70"&gt;早期的编码器-解码器模型中，要求编码器结构的最后一个单元能很好地保留输入文本的信息编码。而在实际应用中，这样的定长文本编码并不一定能够捕捉输入句的所有重要信息，尤其是在输入文本较长的情况下。为解决这个问题，有研究工作(Bahdanau et al., 2015)在序列到序列神经机器翻译模型中引入了“注意力”（attention）机制，用于在生成目标文本序列的过程中，为生成每个目标词确定一个有注意力偏差的输入文本编码，使得模型可以学习输出序列到输入序列的一个软对齐（soft alignment）。注意力机制的主要思想是：在每一步生成不同的yi+1时，侧重使用编码器中对应x的不同部分的隐状态信息，即使用编码器中各隐状态ht的加权和作为生成时所需要考虑的“上下文 ”：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-59bd1aaa484e98085d206bdba7ab4b20.jpg" data-rawwidth="219" data-rawheight="50"&gt;通过为生成不同的目标单词学习不同的&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4fcc6bc925b566047f510f202c4774bc.jpg" data-rawwidth="96" data-rawheight="40"&gt;分布，使得生成不同单词时解码器可以将“注意力”集中在不同的输入词语上。注意力权值&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4fcc6bc925b566047f510f202c4774bc.jpg" data-rawwidth="96" data-rawheight="40"&gt;可以有多种不同的计算方法，一种常见的实现方法考虑编码器每个隐状态ht和解码器生成词语yi+1时的隐状态hyi+1的相近程度（内积），将权值定义为：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f0830c3bb70f325b7591b924b1e0e534.jpg" data-rawwidth="331" data-rawheight="86"&gt;&lt;b&gt;初探与进展&lt;/b&gt;&lt;p&gt;基于编码器-解码器架构和注意力机制的序列到序列学习模型最初用于神经机器翻译，但原理上可以直接照搬应用于标题生成(Lopyrev, 2015; Hu et al., 2015)。甚至不采用注意力机制的多层LSTM-RNN编码器-解码器也在一般基于词汇删除的语句压缩任务上取得了一定效果(Filippova et al., 2015)。而神经网络方法在语句简化、标题生成任务上最早的应用中，比较著名的当属Sasha Rush组的相关工作(Rush et al., 2015)。虽然同样是一种编码器-解码器神经网络，但在具体的架构设计上和基于RNN的序列到序列学习有一定差异。&lt;/p&gt;&lt;p&gt;这个工作中对&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)应用了C阶马尔科夫假设，即生成一个新的词语yi+1时只依赖于之前&lt;em&gt;C&lt;/em&gt;个已经生成的词语yc=y[i-C+1,…,i]，同时对&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)求对数将其分解为求和形式：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e2094d432cd68c1b0d02aeec789b8be7.jpg" data-rawwidth="380" data-rawheight="79"&gt;局部概率P(yi+1|yc,x;&lt;em&gt;θ&lt;/em&gt;)定义为一个前馈神经网络：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d8d31ca83c5b1baa80f286ae4a64445f.jpg" data-rawwidth="389" data-rawheight="50"&gt;其中隐状态h由上下文yc的嵌入表示y ̃c通过一层变换得到：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-72d3aee9a95460ca2c2e4d3df1e3bf21.jpg" data-rawwidth="222" data-rawheight="71"&gt;而enc是以x作为输入的编码器模块。文中尝试了三种不同的编码器，分别为词袋模型编码器enc1、卷积编码器enc2和注意力机制的编码器enc3。整个模型架构比较简单，如图3(a)所示。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3519982a1ed1ad305ed9a13168739252.jpg" data-rawwidth="348" data-rawheight="239"&gt;词袋模型编码器和卷积编码器不考虑注意力机制。词袋模型enc1定义为词向量的简单平均。记每个输入词xi为one-hot表示，左乘词向量编码矩阵F可以得到其对应的word embedding，整个词袋模型编码器可以写作：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6d8f4a9ff94e7d53642e61d86b902bad.jpg" data-rawwidth="248" data-rawheight="108"&gt;其中p∈[0,1]M是输入词语上的均匀分布，词袋模型编码器要学习的参数只有词向量编码矩阵F。这个编码器忽略输入词语序列的顺序和相邻词之间的关系。&lt;/p&gt;&lt;p&gt;卷积编码器enc2对词袋模型编码器不考虑词语之间关系的特点进行了改进，采用了一种标准的时延神经网络（time-delay neural network, TDNN），使得编码器可以利用词语之间的局部关系。这个编码器包含L层，每层主要由1维的卷积过滤器Q和max-pooling操作构成：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fffce88554d2e3a6f83b7f835996a74f.jpg" data-rawwidth="624" data-rawheight="212"&gt;而enc3将注意力机制引入到词袋模型编码器中，使得enc3对x进行编码的过程中利用到之前&lt;em&gt;C&lt;/em&gt;个已经生成的词语作为上下文yc。用G表示上下文编码矩阵，模型结构如图3(b)所示，形式上写作：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7e78c607d56cfa551d72a8e1d9ba3f39.jpg" data-rawwidth="309" data-rawheight="237"&gt;模型训练使用批量随机梯度法最小化训练数据&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-70a742fb8d1b8b29d92474e7c64ca410.jpg" data-rawwidth="155" data-rawheight="48"&gt;的负对数似然：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-62884515d483bf277d1168b9edb2677f.jpg" data-rawwidth="618" data-rawheight="88"&gt;使用动态规划（Viterbi算法）精确求解这个问题的时间复杂度为&lt;em&gt;O&lt;/em&gt;(NVC)，而词汇表大小&lt;em&gt;V&lt;/em&gt;一般较大。前文已经提到，实际应用中一般可以采用集束搜索近似求解，即在生成每一个yi的时候都只保存当前最优的&lt;em&gt;K&lt;/em&gt;个部分解，之后仅从这&lt;em&gt;K&lt;/em&gt;个部分解开始进行下一步生成。这样时间复杂度被降为&lt;em&gt;O&lt;/em&gt;(KNV)。&lt;/p&gt;&lt;p&gt;直觉上，人工语句简化时一般仍会保留一些原句中的词汇。一个好的语句压缩模型最好既能够逐个从词汇表&lt;em&gt;V&lt;/em&gt;生成目标压缩句中的词汇，又能够捕捉从原句中进行词汇抽取的过程。文中(Rush et al., 2015)给出了一个权衡“生成”和“抽取”的初步方案，称为抽取式调节（extractive tuning）。本质上就是经典统计机器翻译中的对数线性模型(Och and Ney, 2002)，通过线性加权将多个特征组合起来定义概率：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d395719aa8b4849b6f0f1d748aab50db.jpg" data-rawwidth="374" data-rawheight="78"&gt;其中α为5维权向量，对应的5维特征f包括之前模型的概率估计，以及四个和输入句有关的示性函数特征（和输入句存在一元词、二元词、三元词匹配或调序）：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b4aa0ef5d2424a3710fe7d403afc5dbf.jpg" data-rawwidth="445" data-rawheight="195"&gt;这样的分数定义在形式上仍然根据每一步&lt;em&gt;i&lt;/em&gt;来分解，所以不需要修改使用动态规划或者柱搜索进行解码的过程。而调节权向量α的过程也可以像经典统计机器翻译一样采用最小错误率训练（minimum error rate training, MERT）(Och, 2003)来完成。&lt;p&gt;这个工作完成时间相对较早，并没有使用最适合对序列数据建模的RNN结构。同研究组今年的后续工作(Chopra et al., 2016)中，将解码器由前馈神经网络替换为RNN，并改变了编码器结构：同时为输入词及其所在位置学习embedding，并用卷积计算当前位置上下文表示，作为解码过程中注意力权重计算的依据。最后得到的架构中不再需要前文所述的“抽取式调节”模块，成为更纯粹的端到端系统；在Gigaword数据集上的实验结果也取得了更优的性能。&lt;/p&gt;&lt;p&gt;基于神经网络的语句简化与标题生成后续也在不同方面取得进展。目前生成类任务训练指标主要为训练集数据的似然函数，但生成类任务的常用自动评价准则是ROUGE或BLEU，本质上大约相当于系统生成结果和参考答案之间关于n-gram（连续若干个词）的匹配程度。近期有工作尝试利用最小化风险训练（minimum risk training, MRT）思想(Och, 2003; Smith and Eisner, 2006)改进神经机器翻译，直接对BLEU值进行优化。这一策略在标题生成任务上也同样适用，只需用类似的方式去优化训练集生成结果的ROUGE值(Ayana et al., 2016)。具体而言，用∆(y',y)表示任务相关的实际损失函数，如标题生成任务中将其设为生成结果y'在参考答案y上计算的ROUGE值（这里表达为风险最小化问题，所以还需要取负）。训练目标是最小化期望风险：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-01be58118eb840a020563825d7008750.jpg" data-rawwidth="382" data-rawheight="63"&gt;最小化期望风险的一个好处在于：即使原本损失函数∆(y',y)是定义在离散结构上的离散函数，训练目标关于概率模型的参数也还是连续函数，所以仍然可以求导进行反向传播更新参数。然而，穷举所有可能产生的结果y’开销过大，并不可行。所以只在上面取一个显著抽样&lt;em&gt;S&lt;/em&gt;(x;&lt;em&gt;θ&lt;/em&gt;)来近似整个概率分布，并引入一个较小的超参数ϵ尝试让近似分布更为平滑：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-67e245db82e5cf184bd890d80a8e1eee.jpg" data-rawwidth="422" data-rawheight="59"&gt;实际上，如果固定超参数ϵ为1，这一近似计算最小化期望风险的做法就和强化学习早期工作中的REINFORCE算法(Williams, 1992)不谋而合。近期也有工作从REINFORCE算法出发，对随机初始化概率模型的做法进行改进，提出首先根据正确答案用交叉熵损失学习若干轮、得到较好的初始概率模型，然后利用退火机制逐步将训练过程转向REINFORCE算法(Ranzato et al., 2016)。实验表明，这些对训练目标的改进都可以显著改善自动评价指标所度量的性能。另一方面，原句中可能存在模型词汇表中所没有的词（out of vocabulary, OOV），尤其是很多专有名词，并不在生成词汇的范围&lt;em&gt;V&lt;/em&gt;之中。实现上为了降低解码复杂度，一般都会采用相对较小的词汇表。如果系统不能输出原句中的OOV词、仅能用&amp;lt;UNK&amp;gt;等占位符代替，显然有可能会造成关键信息损失。受指针网（pointer networks，一种输出序列中每个元素分别指向输入序列中元素的编码器-解码器网络）(Vinyals et al., 2015)启发，近期已有多个工作都不约而同地考虑了一种解决思路：在解码的过程中以一部分概率根据当前状态来生成、一部分概率直接从原句中抽取(Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016)。&lt;/p&gt;&lt;p&gt;另一方面，如何利用其它任务数据作为辅助性监督信息也是一个正在被考虑的方向。例如今年有工作在同一个多层双向RNN网络中进行语句压缩、阅读视线预测（gaze prediction）、组合范畴文法（combinatory category grammar, CCG）超标注（supertagging）的多任务学习，使得语句压缩任务的性能得到改善(Klerke et al., 2016)。这几个任务在直觉上具有一定相关性，有机会起到相互强化的效果。&lt;/p&gt;&lt;p&gt;上面所介绍的架构都属于直接对条件概率&lt;em&gt;P&lt;/em&gt;(y│x;&lt;em&gt;θ&lt;/em&gt;)建模的判别式模型范畴。近期也有利用深层产生式模型来对语句压缩任务建模的工作。常见神经网络结构中，自编码器被广泛应用于表示学习和降维，将类似思想对文本数据建模自然也可能学习到更紧凑的表示。最近就有尝试在变分自编码器（variational auto-encoder, VAE）架构下得到语句压缩模型的工作(Miao and Blunsom, 2016)。关于一般VAE模型的详细信息本文不予赘述，感兴趣的读者可参考相关教程 (Doersch 2016)。原始的VAE可以将输入数据压缩到低维表示，而这个工作类比提出将输入的长句压缩为更紧凑的短句，得到如图4所示的自编码压缩模型。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0c855811086e8412c0724c27381f2651.jpg" data-rawwidth="652" data-rawheight="326"&gt;用s和c分别记原始输入句和压缩句，整个模型包含两部分：(1) 压缩模型（图4左下部分虚线框，由编码器连接压缩器组成）为以s作输入、c作输出的推断网络qφ (c│s)，以及 (2) 重构模型（图4右上部分虚线框，由压缩器连接解码器组成）为基于压缩表示c重构原始输入句s的生成网络pθ (s│c)。为了让压缩句中仅使用原句中出现过的词，文中选用了指针网(Vinyals et al., 2015)作为压缩模型qφ (c│s)，同时将编码器设计为双向LSTM-RNN，压缩器使用带有注意机制的单向LSTM-RNN。而重构模型pθ (s│c)则直接使用经典的序列到序列结构，即带注意机制的RNN，以压缩器端的c作输入，从解码器端产生原句s。模型训练过程中需要对两组网络参数φ和θ进行更新。与最原始的VAE一样，只需要无标记数据作为输入，使用变分推断来优化数据对数似然的一个下界&lt;em&gt;L&lt;/em&gt;：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-562c3d1c7e5ad87db453a37c8f8a6573.jpg" data-rawwidth="477" data-rawheight="109"&gt;其中需要计算变分分布qφ (c│s)和一个先验语言模型p(c)的KL散度。本文讨论的任务是语句压缩，需要同时保证压缩句尽可能流畅和简洁，所以预训练了一个偏好短句的语言模型作为p(c)。&lt;/p&gt;&lt;p&gt;由于不易对从变分分布q中随机产生的值进行反向传播，原始VAE推断过程使用重参数化（reparameterization）技巧，将产生样本的随机性全部转移到一个辅助的噪声随机变量中，保持和参数直接相关的部分相对固定，从而可以通过对这些非随机部分求导进行反向传播参数更新。但自编码语句压缩模型处理对象为离散结构的文本，重参数化技巧不能直接使用。因此文中使用了前面提到的REINFORCE算法，根据一组随机采样的误差进行反向传播，近似最小化期望损失，并引入偏置项来降低梯度估计的方差。&lt;/p&gt;&lt;p&gt;VAE变分推断进行模型训练的效率十分依赖推断网络q（对应这个工作中的压缩模型部分）的梯度估计质量。为了在训练过程初期就能引导压缩模型产生较好的压缩结果，文中进一步提出另一个模型，称为强制注意力语句压缩（forced-attention sentence compression；图5），强制让注意力的学习和额外的有标记语句压缩数据更吻合。本质上是通过有监督训练来实现前面提到的一种语句简化策略：以一部分概率根据指针网直接从原句中抽词（对应图5中的α）、一部分概率根据当前状态来生成整个词汇表V中可能的词（对应图5中的β）。这样就可以引入语句简化任务的有标记平行语料，进行半监督学习。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bbde2273886f03ea643d260a28a5a1dc.jpg" data-rawwidth="528" data-rawheight="296"&gt;&lt;b&gt;局限与展望&lt;/b&gt;&lt;/p&gt;&lt;p&gt;需要指出的是，对于任何涉及自然语言生成的任务而言，像ROUGE、BLEU那样基于局部单元匹配的自动指标并不能完全取代基于语义理解的人工评价。目前基于神经网络的相关工作几乎全部缺少人工对语义完整性、流畅度等关键指标的评分（这一点在相关论文的审稿环节理应有人指出；也有可能竞标这类论文的审稿人主要来自对神经网络了解甚于自然语言生成的研究人员）。所以不同方法的实际性能差异究竟有多少，其实尚不明确。&lt;/p&gt;&lt;p&gt;细心的读者可能已经注意到，虽然本文介绍的相关文献标题中有些包含“语句摘要（sentence summarization）”甚至“文本摘要（text summarization）”这样的字眼，但我们在本文的描述中尚未开始使用“摘要”一词。因为目前提出的方法大多仅能够应用于将一两句话作为输入的情形，实际上只是语句级别的压缩或简化。&lt;/p&gt;&lt;p&gt;语句简化的最终目标仍然还是对更大范围内的信息摘要，比如用几句话去概况整篇文档中的主要内容。目前的神经网络方法大多以短文本（如句子、微博）作为输入，鲜有直接对文档结构进行编码的架构，最终解码也只能得到标题长度的信息，尚不足以作为整篇文档的内容总结。对于自动标题生成而言，是否只需要去利用每篇文档最开始一两句话中的信息，也仍有待商榷；这个问题在非新闻语料上可能更为明显。另一方面，对本身已经较短的文本再做进一步简化的实用价值，可能也无法和文档信息摘要相提并论。&lt;/p&gt;&lt;p&gt;关于文档摘要任务，现有的基于神经网络的模型仍以抽取式摘要（即从输入文档中直接抽取若干关键句作为摘要）居多，此时神经网络模型起到的作用也仅限于对文档中每个句子进行估分、排序，这和从文档到摘要进行端到端训练、直接逐词“生成”摘要的理想目标仍有距离。经典序列到序列架构在语句简化、标题生成任务可以取得不错的效果，但在文档摘要任务上还没有出现较为成功的应用。一个可能的原因在于整篇文档篇幅过长，不适合直接套用经典序列架构来编码和解码。&lt;/p&gt;&lt;p&gt;因此，对句子和词进行分级层次化编码(Li et al., 2015)可能是一种可以尝试的路线。今年提出的一种端到端神经摘要模型(Cheng and Lapata, 2016)中，将文档视为语句的序列，用各语句的编码作为编码器RNN中每个单元的输入，而语句的编码由一个CNN通过卷积和池化操作将词汇级信息汇总得到（图6）。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9f7ffac4b18dc044b454fadd469b861f.jpg" data-rawwidth="623" data-rawheight="492"&gt;这样可以直接实现句子级抽取，比如文中的做法是用一个多层感知机根据当前状态来估计是否抽取该句的概率（pt-1表示前一句应当被抽取的概率）：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e2b9b82fd646e1b8f7431647bfff3505.jpg" data-rawwidth="401" data-rawheight="102"&gt;为了进一步能够通过原文词汇重组构建和生成“非抽取式”摘要，文中提出一种层次化注意力架构，利用句子级的注意力权值作为输入来计算句子中每一个词的注意力权值。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7d20b661018a6c4f68e1367997c66de8.jpg" data-rawwidth="653" data-rawheight="293"&gt;这个工作在句子抽取上能取得一定效果，但词汇级生成摘要仍有待提高，不论在自动评价和人工评价结果上都还不够理想。&lt;/p&gt;&lt;p&gt;而另一个侧重于标题生成的工作(Nallapati et al., 2016)中也提出了一种层次化编码思想：使用两级双向RNN分别刻画词和句子的序列结构，解码过程计算每个词的注意力权值时，用所在句子的注意力权值予以加权（reweight）。但很遗憾这样的设计暂时也并没有使得生成多句摘要的任务得到性能上的提升。&lt;/p&gt;&lt;p&gt;总而言之，目前的编码器-解码器架构在短文本简化任务上取得了一定进展。现在应用于文本简化的编码器-解码器架构设计也比较多样，可以为各种不同需求下文本简化的后续研究工作提供多种可能的参考思路。然而，深度学习方法在文档摘要任务上仍存在巨大的提升空间。如果期望使用完全端到端的方式训练文档级摘要模型，可能还需要在编码器和解码器的设计上产生一些新的突破，使得模型可以更好地表示和生成结构性更明显、篇幅更长的内容。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（未附带链接文献大多可在http://aclweb.org/anthology/ 公开下载）&lt;/p&gt;&lt;p&gt;Ayana; Shen, S.; Liu, Z.; and Sun, M. 2016. Neural Headline Generation with Minimum Risk Training. http://arxiv.org/abs/1604.01904 &lt;/p&gt;&lt;p&gt;Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015. http://arxiv.org/abs/1409.0473&lt;/p&gt;&lt;p&gt;Cheng, J.; and Lapata, M. 2016. Neural Summarization by Extracting Sentences and Words. In ACL 2016.&lt;/p&gt;&lt;p&gt;Chopra, S.; Auli, M.; and Rush, A. M. 2016. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks. In NAACL 2016.&lt;/p&gt;&lt;p&gt;Doersch, C. 2016. Tutorial on Variational Autoencoders. http://arxiv.org/abs/1606.05908 Filippova, K.; Alfonseca, E.; Colmenares, C. A.; Kaiser, L.; and Vinyals, O. 2015. Sentence Compression by Deletion with LSTMs. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Gu, J.; Lu, Z.; Li, H.; and Li, V. O. K. 2016. Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In ACL 2016.&lt;/p&gt;&lt;p&gt;Gulcehre, C.; Ahn, S.; Nallapati, R.; Zhou, B.; and Bengio, Y. 2016. Pointing the Unknown Words. In ACL 2016.&lt;/p&gt;&lt;p&gt;Hu, B.; Chen, Q.; and Zhu, F. 2015. LCSTS: A Large Scale Chinese Short Text Summarization Dataset. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Klerke, S.; Goldberg, Y.; and Søgaard, A. 2016. Improving Sentence Compression by Learning to Predict Gaze. In NAACL 2016. (Best short paper winner)&lt;/p&gt;&lt;p&gt;Li, J.; Luong, M.; and Jurafsky, D. 2015. A Hierarchical Neural Autoencoder for Paragraphs and Documents. In ACL 2015.&lt;/p&gt;&lt;p&gt;Lopyrev, K. 2015. Generating News Headlines with Recurrent Neural Networks. http://arxiv.org/abs/1512.01712&lt;/p&gt;&lt;p&gt;Miao, Y.; and Blunsom, P. 2016. Language as a Latent Variable: Discrete Generative Models for Sentence Compression. To appear in EMNLP 2016. http://arxiv.org/abs/1609.07317&lt;/p&gt;&lt;p&gt;Nallapati, R.; Zhou, B.; dos Santos, C.; Gulcehre, C.; and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. In CoNLL 2016.&lt;/p&gt;&lt;p&gt;Och, F. J.; and Ney, H. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In ACL 2002.&lt;/p&gt;&lt;p&gt;Och, F. J. 2003. Minimum error rate training in statistical machine translation. In ACL 2003.Ranzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2016. Sequence Level Training with Recurrent Neural Networks. In ICLR 2016. http://arxiv.org/abs/1511.06732&lt;/p&gt;&lt;p&gt;Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In EMNLP 2015.&lt;/p&gt;&lt;p&gt;Smith, D. A.; and Eisner, J. 2006. Minimum risk annealing for training log-linear models. In COLING/ACL 2006.&lt;/p&gt;&lt;p&gt;Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In NIPS 2014. http://arxiv.org/abs/1409.3215&lt;/p&gt;&lt;p&gt;Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer Networks. In NIPS 2015. http://arxiv.org/abs/1506.03134&lt;/p&gt;&lt;p&gt;Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;b&gt; 作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文作者&lt;b&gt;谭继伟&lt;/b&gt;和&lt;b&gt;姚金戈&lt;/b&gt;均为北京大学计算机科学与技术研究所在读博士生，研究方向主要包括文本信息推荐与自动摘要。联系方式：{tanjiwei, yaojinge} @ pku.edu.cn&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325543&amp;amp;idx=1&amp;amp;sn=db467a2dd2b8c6dae017fc4833650862&amp;amp;chksm=f235a52dc5422c3b33f0b45d1e1a59ad03b49b8768f68e34c0aaae4614448588b3eb8e09fb6b&amp;amp;scene=0#wechat_redirect" class="" data-editable="true" data-title="深度学习在文本简化中的应用进展"&gt;深度学习在文本简化中的应用进展&lt;/a&gt;&lt;/b&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22956057&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Fri, 14 Oct 2016 18:34:47 GMT</pubDate></item><item><title>深度学习解决机器阅读理解任务的研究进展</title><link>https://zhuanlan.zhihu.com/p/22671467</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4e56cc333f6e8d32b383bc213465ebad_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;关于阅读理解，相信大家都不陌生，我们接受的传统语文教育中阅读理解是非常常规的考试内容，一般形式就是给你一篇文章，然后针对这些文章提出一些问题，学生回答这些问题来证明自己确实理解了文章所要传达的主旨内容，理解地越透彻，学生越能考出好的成绩。&lt;/p&gt;&lt;p&gt;如果有一天机器能够做类似于我们人类做阅读理解任务，那会发生什么呢？很明显教会机器学会阅读理解是自然语言处理（NLP）中的核心任务之一。如果哪一天机器真能具备相当高水准的阅读理解能力，那么很多应用便会体现出真正的智能。比如搜索引擎会在真正理解文章内容基础上去回答用户的问题，而不是目前这种以关键词匹配的方式去响应用户，这对于搜索引擎来说应该是个技术革命，其技术革新对产品带来的巨大变化，远非在关键词匹配之上加上链接分析这种技术进化所能比拟的。而众所周知，谷歌其实就是依赖链接分析技术起家的，所以如果机器阅读理解技术能够实用化，对搜索引擎领域带来的巨变很可能是颠覆性的。对话机器人如果换个角度看的话，其实也可以看做是一种特殊的阅读理解问题，其他很多领域也是如此，所以机器阅读理解是个非常值得关注的技术方向。&lt;/p&gt;&lt;p&gt;深度学习近年来在NLP中广泛使用，在机器阅读理解领域也是如此，深度学习技术的引入使得机器阅读理解能力在最近一年内有了大幅提高，本文对深度学习在机器阅读理解领域的技术应用及其进展进行了归纳梳理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;什么是机器阅读理解&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器阅读理解其实和人阅读理解面临的问题是类似的，不过为了降低任务难度，很多目前研究的机器阅读理解都将世界知识排除在外，采用人工构造的比较简单的数据集，以及回答一些相对简单的问题。给定需要机器理解的文章以及对应的问题，比较常见的任务形式包括人工合成问答、Cloze-style queries和选择题等方式。&lt;/p&gt;&lt;p&gt;人工合成问答是由人工构造的由若干简单事实形成的文章以及给出对应问题，要求机器阅读理解文章内容并作出一定的推理，从而得出正确答案，正确答案往往是文章中的某个关键词或者实体。比如图1展示了人工合成阅读理解任务的示例。图1示例中前四句陈述句是人工合成的文章内容，Q是问题，而A是标准答案。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9fe1cdfa6a7d0cd6350eb4d7d10c3387.jpg" data-rawwidth="627" data-rawheight="302"&gt;Cloze-style queries是类似于“完形填空”的任务，就是让计算机阅读并理解一篇文章内容后，对机器发出问题，问题往往是抽掉某个单词或者实体词的一个句子，而机器回答问题的过程就是将问题句子中被抽掉的单词或者实体词预测补全出来，一般要求这个被抽掉的单词或者实体词是在文章中出现过的。图2展示了完形填空式阅读理解任务的示例。图中表明了文章内容、问题及其对应的答案。这个例子是将真实的新闻数据中的实体词比如人名、地名等隐去，用实体标记符号替换掉实体词具体名称，问题中一般包含个占位符placeholder，这个占位符代表文章中的某个实体标记，机器阅读理解就是在文章中找出能够回答问题的某个真实答案的实体标记。目前的各种阅读理解任务中“完形填空式”任务是最常见的类型。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ab39f0597ba82b411486c111b09000dd.jpg" data-rawwidth="617" data-rawheight="393"&gt;还有一种任务类型是选择题，就是阅读完一篇文章后，给出问题，正确答案是从几个选项中选择出来的，典型的任务比如托福的听力测试，目前也有研究使用机器来回答托福的听力测试，这本质上也是一种阅读理解任务。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1abecf0cc024e02c0e4f06ba8637189d.jpg" data-rawwidth="657" data-rawheight="321"&gt;如果形式化地对阅读理解任务和数据集进行描述的话，可以将该任务看作是四元组：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-38215fa3d54c5e28907a3df24ab39076.jpg" data-rawwidth="116" data-rawheight="32"&gt;其中，代表一篇文章，代表针对文章内容提出的一个问题，是问题的正确答案候选集合而代表正确答案。对于选择题类型来说，就是明确提供的答案候选集合而是其中的正确选项。对于人工合成任务以及完形填空任务来说，一般要求：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f88cb16faf97e673ed032e116587a544.jpg" data-rawwidth="63" data-rawheight="28"&gt;&lt;p&gt;也就是说，要求候选答案是在文章中出现过的词汇或者实体词。&lt;/p&gt;&lt;p&gt;&lt;b&gt;深度学习技术进展&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本节内容对目前机器阅读理解领域中出现的技术方案进行归纳梳理，正像本文标题所述，我们只对深度学习相关的技术方案进行分析，传统技术方案不在讨论之列。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.文章和问题的表示方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用神经网络处理机器阅读理解问题，首先面临的问题就是如何表示文章和问题这两个最重要的研究对象。我们可以从现有机器阅读理解相关文献中归纳总结出常用的表示方法，当然这些表示方法不仅仅局限于阅读理解问题，也经常见于NLP其他子领域中。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-efc2b74cb321dca249401cf76b4d551b.jpg" data-rawwidth="295" data-rawheight="183"&gt;首先，对于机器阅读理解中的文章来说，有两种常见的文章内容表达方式。最常见的一种即是将一篇文章看成有序的单词流序列（参考图4的模型一，图中每个圆即代表某个单词的神经网络语义表达，图中的BiRNN代表双向RNN模型），在这个有序序列上使用RNN来对文章进行建模表达，每个单词对应RNN序列中的一个时间步t的输入，RNN的隐层状态代表融合了本身词义以及其上下文语义的语言编码。这种表示方法并不对文章整体语义进行编码，而是对每个单词及其上下文语义进行编码，在实际使用的时候是使用每个单词的RNN隐层状态来进行相关计算。至于具体的RNN模型，常见的有标准RNN、LSTM、GRU及其对应的双向版本等。对于机器阅读理解来说双向RNN是最常用的表示方法，一般每个单词的语义表示由正向RNN隐层状态和反向RNN隐层状态拼接来表示，即：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-39fccc584e72670f223c28c1ee496d39.jpg" data-rawwidth="133" data-rawheight="35"&gt;&lt;p&gt;模型一往往在机器阅读理解系统的原始输入部分对文章进行表征，因为对于很多阅读理解任务来说，本质上是从文章中推导出某个概率最大的单词作为问题的答案，所以对文章以单词的形式来表征非常自然。&lt;/p&gt;&lt;p&gt;另外一种常见的文章内容表达方式则是从每个单词的语义表达推导出文章整体的Document Embedding表达，这种形式往往是在对问题和文章进行推理的内部过程中使用的表达方式。典型的表达过程如图5所示的模型二所示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6d2afce482ff556d6bc210bcc27df28a.jpg" data-rawwidth="537" data-rawheight="370"&gt;模型二的含义是：首先类似于模型一，用双向RNN来对每个单词及其上下文进行语义表征，形成隐层状态表示，然后对于向量的每一维数值，乘以某个系数，这个系数代表了单词对于整个文章最终语义表达的重要程度，将每个单词的系数调整后的隐层状态累加即可得到文章的Word Embedding语义表达。而每个单词的权重系数通常用Attention计算机制来计算获得，也有不使用权重系数直接累加的方式，这等价于每个单词的权重系数都是1的情形，所以可以看作加权平均方法的特殊版本。以公式表达的话，文章的语义表达公式如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c1c58a7c42f5a0f7f35ebea5dd6e287f.jpg" data-rawwidth="165" data-rawheight="36"&gt;对于机器阅读理解中的问题来说，有三种常见的语义表达方式。如果将查询看作一种特殊的文章的话，很明显文章的语义表达方式同样可以用来表征问题的语义，也就是类似于文档表示方法的模型一和模型二。问题的表示方法模型一如图6所示，模型二如图7所示，其代表的含义与文章表征方式相似，所以此处不赘述。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-530479d86477acf9c19093c5926129ab.jpg" data-rawwidth="591" data-rawheight="544"&gt;问题表示方法的另外一种表示如图8所示，我们可以称之为模型三。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b3f47380d3a260b03880fd04bab31dba.jpg" data-rawwidth="310" data-rawheight="249"&gt;模型三也是在模型一的基础之上的改进模型，也是NLP任务中表达句子语义的最常见的表达方式。首先类似于模型一，使用双向RNN来表征每个单词及其上下文的语义信息。对于正向RNN来说，其尾部单词（句尾词）RNN隐层节点代表了融合了整个句子语义的信息；而反向RNN的尾部单词（句首词）则逆向融合了整个句子的语义信息，将这两个时刻RNN节点的隐层状态拼接起来则可以表征问题的整体语义：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f6de01434a1bae047ab7324a33db69a1.jpg" data-rawwidth="141" data-rawheight="43"&gt;&lt;p&gt;理论上模型三也可以用来表征文章的语义信息，但是一般不会这么用，主要原因是文章往往都比较长，RNN对于太长的内容表征能力不足，所以类似模型三的方法会存在大量的信息丢失，而“问题”一般来说都是比较短的一句话，所以用模型三表征是比较合适的。&lt;/p&gt;&lt;p&gt;以上介绍的几个模型是在机器阅读理解领域里常用的表征文章和问题的表示方法。下面我们从机器阅读理解神经网络结构的角度来进行常用模型的介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.机器阅读理解的深度学习模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前机器阅读理解研究领域出现了非常多的具体模型，如果对这些模型进行技术思路梳理的话，会发现本质上大多数模型都是论文“Teaching Machines to Read and Comprehend”提出的两个基础模型”Attentive Reader”和“Impatient Reader”的变体（参考文献1），当然很多后续模型在结构上看上去有了很大的变化，但是如果仔细推敲的话会发现根源和基础思路并未发生颠覆性的改变。&lt;/p&gt;&lt;p&gt;我们将主流模型技术思路进行归纳梳理以及某些技术点进行剥离组合，将其归类为“一维匹配模型”、“二维匹配模型”、“推理模型”等三类模型，其中“一维匹配模型”和“二维匹配模型”是基础模型，“推理模型”则是在基础模型上重点研究如何对文本内容进行推理的机制。当然，还有个别模型在结构上有其特殊性，所以最后会对这些模型做些简介。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1  一维匹配模型&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6d2fe3addd3e4f5d15b532d02a6a7d2c.jpg" data-rawwidth="581" data-rawheight="341"&gt;&lt;p&gt;目前机器阅读理解任务的解决方案中，有相当多的模型可以被归类到“一维匹配模型”这种技术范型中，这类模型本质上是“Attentive Reader”的变体。我们首先介绍这种技术思路的总体流程结构，然后说明下主流方法在这个框架下的一些区别。&lt;/p&gt;&lt;p&gt;图9所示是“一维匹配模型”的技术流程示意图：首先，对文章内容使用“文章表示方法：模型一”的方式对文章语义内容进行编码，对于问题来说，则一般会使用“问题表示方法:模型三”的方式对问题进行语义编码，即使用双向RNN的头尾部节点RNN隐层状态拼接作为问题的语义表示。然后，通过某种匹配函数来计算文章中每个单词Di（编码中包括单词语义及其上下文单词的语义）语义和问题Q整体语义的匹配程度，从含义上可以理解为F是计算某个单词Di是问题Q的答案的可能性映射函数。接下来，对每个单词的匹配函数值通过SoftMax函数进行归一化，整个过程可以理解为Attention操作，意即凸显出哪些单词是问题答案的可能性。最后，因为一篇文章中，某个单词可能在多处出现，而在不同位置出现的同一个单词都会有相应的Attention计算结果，这代表了单词在其具体上下文中是问题答案的概率，将相同单词的Attention计算出的概率值进行累加，即可作为该单词是问题Q答案的可能性，选择可能性最大的那个单词作为问题的答案输出。在最后相同单词概率值累加这一步，一般容易质疑其方式：如果这样，那么意味着这个方法隐含一个假设，即出现次数越多的单词越可能成为问题的答案，这样是否合理呢？实验数据表明，这个假设基本是成立的，所以这种累加的方式目前是非常主流的技术方案，后文所述的AS Reader和GA Reader采取了这种累加模式，而Stanford AR和Attentive Reader则采取非累加的模式。之所以将这个结构称为“一维匹配模型”，主要是其在计算问题Q和文章中单词序列的匹配过程形成了一维线性结构。&lt;/p&gt;&lt;p&gt;上述内容是“一维匹配模型”的基本思路，很多主流的模型基本都符合上述架构，模型之间的最大区别主要是匹配函数的定义不同。具体而言，“Attention Sum Reader”，（后文简称AS Reader，参考文献2）、“Stanford Attentive Reader”（后文简称 Stanford AR，参考文献3）、“Gated-Attention Reader”（后文简称GA Reader，参考文献4）、“Attentive Reader”（参考文献1）、AMRNN(参考文献5)等模型都基本遵循这个网络结构。&lt;/p&gt;&lt;p&gt;AS Reader可以看作是一维匹配结构的典型示例，其匹配函数定义为Di和Q向量的点积：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b355c6c9a56225aa8b8975ef2d8d64de.jpg" data-rawwidth="123" data-rawheight="33"&gt;Attentive Reader是最早提出的模型之一，所以在整体结构上和一维匹配结构有些差异，模型性能相对差些，不过这些结构上的差异点并非性能差异的关键，而匹配函数能够解释其和效果好的模型性能差异的主要原因，其采用了前向神经网络的形式：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d723c1e5653d5873440d76f8b699c1b6.jpg" data-rawwidth="226" data-rawheight="32"&gt;Stanford AR的匹配函数则采用了双线性（Bilinear）函数：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-89e3bc5a66f231d7368600e14ea7ee46.jpg" data-rawwidth="142" data-rawheight="34"&gt;&lt;p&gt;这里需要说明的是，Stanford AR的效果已经是目前所有机器阅读理解模型中性能最好的之一，同时其一维匹配模型相对简单，且没有采用深层的推理机制，所以这个模型是值得关注的。而其相对Attentive Reader来说，对提升性能最主要的区别就在于采用了双线性函数，而这个改变对性能提升带来了极大的帮助；相对AS Reader来说，其性能也有明显提升，很明显双线性函数在其中起了主要作用。由此可见，目前的实验结果支持双线性函数效果明显优于很多其它匹配模型的结论。&lt;/p&gt;&lt;p&gt;AMRNN是用来让机器做TOFEL听力题的阅读理解系统采用的技术方案，类似于GA Reader的整体结构，其是由一维匹配模型加深层网络组合而成的方案，同样的，深层网络是为了进行推理，如果摘除深层网络结构，其结构与AS Reader也是基本同构的。其采用的匹配函数则使用Di和Q的Cosine相似性，类似于AS Reader向量点积的思路。AMRNN解决的是选择题而非完形填空任务，所以在输出阶段不是预测文中哪个单词或实体是问题的答案，而是对几个候选答案选项进行评估，从中选择正确答案。&lt;/p&gt;&lt;p&gt;由上述模型对比可以看出，一维匹配模型是个结构简洁效果整体而言也不错的模型范式，目前相当多的具体模型可以映射到这个范式中，而其中的关键点在于匹配函数如何设计，这一点是导致具体模型性能差异的相当重要的影响因素。可以预见，后续的研究中必然会把重心放在如何改进设计出更好地匹配函数中来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2  二维匹配模型&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-aab93ca69da88cd905d459daf9450603.jpg" data-rawwidth="584" data-rawheight="295"&gt;&lt;p&gt;顾名思义，“二维匹配模型”是相对“一维匹配模型”而言的，其最初的思想其实体现在”Impatient Reader”的思路中。图10是机器阅读理解中二维匹配模型的整体流程示意图，从中可以看出，其整体结构与一维匹配模型是类似的，最主要的区别体现在如何计算文章和问题的匹配这个层面上。与一维匹配模型不同的是：二维匹配模型的问题表征方式采用“问题表示方法：模型一”，就是说不是将问题的语义表达为一个整体，而是问题中的每个单词都单独用Word Embedding向量来表示。这样，假设文档长度为||D||，问题长度为||Q||，那么在计算问题和文章匹配的步骤中，就形成了||D||*||Q||的二维矩阵，就是说文章中任意单词Di和问题中的任意单词Qj都应用匹配函数来形成矩阵的位置的值。&lt;/p&gt;&lt;p&gt;当二维矩阵的值根据匹配函数填充完毕后，就可以考虑进行Attention计算。因为是二维矩阵，所以可以有很多种不同的Attention计算机制。比如可以如图11这样按照二维矩阵的行来进行Attention计算，因为矩阵的一行代表文档中某个单词Di相对问题中每个单词Qj(1&amp;lt;j&amp;lt;||Q||)的匹配程度向量，按行计算Attention表达的是文档单词Di和问题中各个单词的语义相似程度。同样的，可以按照二维矩阵的列来进行Attention计算（如图12所示），因为矩阵的一列代表问题中某个单词Qi相对文档序列中每个单词Dj(1&amp;lt;j&amp;lt;||D||)的匹配程度向量，按列计算Attention表达的是问题单词Qi和文档序列中各个单词的语义相似程度&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0eac4c70ab22e2721024735071a037c6.jpg" data-rawwidth="449" data-rawheight="783"&gt;&lt;p&gt;Consensus Attention 模型（后文简称CA Reader，参考文献6）、Attention-over-Attention模型（后文简称AOA Reader，参考文献7）和Match-LSTM模型（参考文献8）基本都符合二维匹配结构的范式，其主要区别在于Attention计算机制的差异上。CA Reader按照列的方式进行Attention计算，然后对每一行文档单词对应的针对问题中每个单词的Attention向量，采取一些启发规则的方式比如取行向量中最大值或者平均值等方式获得文档每个单词对应的概率值。AOA Reader则对CA Reader进行了改进，同时结合了按照列和按照行的方式进行Attention计算，核心思想是把启发规则改为由按行计算的Attention值转换成的系数，然后用对按列计算出的Attention加权平均的计算方式获得文档每个单词对应的概率值。Match-LSTM模型则是按行进行Attention计算，同样地把这些Attention值转换成列的系数，不过与AOA不同的是，这些系数用来和问题中每个单词的Word Embedding相乘并对Word Embedding向量加权求和，拟合出整个问题的综合语义Word Embedding（类似于“问题表示方法：模型二”思路），并和文章中每个单词的Word Embedding进行合并，构造出另外一个LSTM结构，在这个LSTM结构基础上去预测哪个或者那些单词应该是正确答案。&lt;/p&gt;&lt;p&gt;由于二维匹配模型将问题由整体表达语义的一维结构转换成为按照问题中每个单词及其上下文的语义的二维结构，明确引入了更多细节信息，所以整体而言模型效果要稍优于一维匹配模型。&lt;/p&gt;&lt;p&gt;从上面的具体模型介绍可以看出，目前二维匹配模型相关工作还不多，而且都集中在二维结构的Attention计算机制上，由于模型的复杂性比较高，还有很多很明显的值得改进的思路可以引入。最直观的改进就是探索新的匹配函数，比如可以摸索双线性函数在二维结构下的效果等；再比如可以引入多层网络结构，这样将推理模型加入到阅读理解解决方案中等。可以预见，类似的思路很快会被探索。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3  机器阅读理解中的推理过程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;人在理解阅读文章内容的时候，推理过程几乎是无处不在的，没有推理几乎可以断定人是无法完全理解内容的，对于机器也是如此。比如对于图1中所展示的人工合成任务的例子，所提的问题是问苹果在什么地方，而文章表达内容中，刚开始苹果在厨房，Sam将其拿到了卧室，所以不做推理的话，很可能会得出“苹果在厨房”的错误结论。&lt;/p&gt;&lt;p&gt;乍一看“推理过程”是个很玄妙而且说不太清楚的过程，因为自然语言文本不像一阶逻辑那样，已经明确地定义出符号以及表达出符号之间的逻辑关系，可以在明确的符号及其关系上进行推理，自然语言表达有相当大的模糊性，所以其推理过程一直是很难处理好的问题。&lt;/p&gt;&lt;p&gt;现有的工作中，记忆网络（Memory Networks，参考文献9）、GA Reader、Iterative Alternating神经网络（后文简称IA Reader，参考文献10）以及AMRNN都直接在网络结构中体现了这种推理策略。一般而言，机器阅读理解过程网络结构中的深层网络都是为了进行文本推理而设计的，就是说，通过加深网络层数来模拟不断增加的推理步骤。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-23beacd25f42e7df1eb57adadcbf9924.jpg" data-rawwidth="577" data-rawheight="238"&gt;记忆网络是最早提出推理过程的模型，对后续其它模型有重要的影响。对于记忆网络模型来说，其第一层网络的推理过程（Layer-Wise RNN模式）如下（参考图13）：首先根据原始问题的Word Embedding表达方式以及文档的原始表达，通过f函数计算文档单词的Attention概率，然后g函数利用文章原始表达和Attention信息，计算文档新的表达方式，这里一般g函数是加权求和函数。而t函数则根据文档新的表达方式以及原始问题表达方式，推理出问题和文档最终的新表达方式，这里t函数实际上就是通过两者Word Embedding的逐位相加实现的。t函数的输出更新下一层网络问题的表达方式。这样就通过隐式地内部更新文档和显示地更新问题的表达方式实现了一次推理过程，后续每层网络推理过程就是反复重复这个过程，通过多层网络，就实现了不断通过推理更改文档和问题的表达方式。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7bd996c6617130632505a41674990f89.jpg" data-rawwidth="577" data-rawheight="234"&gt;AMRNN模型的推理过程明显受到了记忆网络的影响，图14通过摒除论文中与记忆网络不同的表面表述方式，抽象出了其推理过程，可以看出，其基本结构与记忆网络的Layer-Wise RNN模式是完全相同的，唯一的区别是：记忆网络在拟合文档或者问题表示的时候是通过单词的Word Embedding简单叠加的方式，而AMRNN则是采用了RNN结构来推导文章和问题的表示。所以AMRNN模型可以近似理解为AS Reader的基础网络结构加上记忆网络的推理过程。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dd243b9346e038255e05d668cb7dcf98.jpg" data-rawwidth="615" data-rawheight="245"&gt;GA Reader的推理过程相对简洁，其示意图如图15所示。它的第一层网络推理过程如下：其每层推理网络的问题表达都是原始问题表达方式，在推理过程中不变。而f函数结合原始问题表达和文档表达来更新文档表达到新的形式，具体而言,f函数就是上文所述的被称为Gated-Attention模型的匹配函数，其计算过程为Di和Q两个向量对应维度数值逐位相乘，这样形成新的文档表达。其它层的推理过程与此相同。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ac0097b67d5b56d6cde6ad00470565db.jpg" data-rawwidth="597" data-rawheight="289"&gt;&lt;p&gt;IA Reader的推理结构相对复杂，其不同网络层是由RNN串接起来的，图16中展示了从第i层神经网络到第i+1层神经网络的推理过程，其中虚线部分是RNN的组织结构，每一层RNN结构是由新的文档表达和问题表达作为RNN的输入数据。其推理过程如下：对于第i层网络来说，首先根据RNN输入信息，就是第i层的文档表达和问题表达，更新隐层状态信息；然后f函数根据更新后的隐层状态信息以及原始的问题表达，计算问题中词汇的新的attention信息；g函数根据新的attention信息更新原始问题的表达形式，形成第i+1层网络的新的问题表达，g函数一般采取加权求和的计算方式；在获得了第i+1层新的问题表达后，t函数根据第i层RNN隐层神经元信息以及第i+1层网络新的问题表达形式，更新原始文档表达形成第i+1层文档的新表达形式。这样，第i+1层的问题表达和文档表达都获得了更新，完成了一次推理过程。后面的推理过程都遵循如此步骤来完成多步推理。&lt;/p&gt;&lt;p&gt;从上述推理机制可以看出，尽管不同模型都有差异，但是其中也有很多共性的部分。一种常见的推理策略往往是通过多轮迭代，不断更新注意力模型的注意焦点来更新问题和文档的Document Embedding表达方式，即通过注意力的不断转换来实现所谓的“推理过程”。&lt;/p&gt;&lt;p&gt;推理过程对于有一定难度的问题来说具有很明显的帮助作用，对于简单问题则作用不明显。当然，这与数据集难度有一定关系，比如研究证明(参考文献10)，CNN数据集整体偏容易，所以正确回答问题不需要复杂的推理步骤也能做得很好。而在CBT数据集上，加上推理过程和不加推理过程进行效果对比，在评价指标上会增加2.5%到5%个绝对百分点的提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.4  其它模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上文对目前主流的技术思路进行了归纳及抽象并进行了技术归类，除了上述的三种技术思路外，还有一些比较重要的工作在模型思路上不能归于上述分类中，本节对这些模型进行简述，具体模型主要包括EpiReader（参考文献11）和动态实体表示模型（Dynamic Entity Representation，后文简称DER模型，参考文献12）。&lt;/p&gt;&lt;p&gt;EpiReader是目前机器阅读理解模型中效果最好的模型之一，其思路相当于使用AS Reader的模型先提供若干候选答案，然后再对候选答案用假设检验的验证方式再次确认来获得正确答案。假设检验采用了将候选答案替换掉问题中的PlaceHolder占位符，即假设某个候选答案就是正确答案，形成完整的问题句子，然后通过判断问题句和文章中每个句子多大程度上是语义蕴含（Entailment）的关系来做综合判断，找出经过检验最合理的候选答案作为正确答案。这从技术思路上其实是采用了多模型融合的思路，本质上和多Reader进行模型Ensemble起到了异曲同工的作用，可以将其归为多模型Ensemble的集成方案，但是其假设检验过程模型相对复杂，而效果相比模型集成来说也不占优势，实际使用中其实不如直接采取某个模型Ensemble的方案更实用。&lt;/p&gt;&lt;p&gt;DER模型在阅读理解时，首先将文章中同一实体在文章中不同的出现位置标记出来，每个位置提取这一实体及其一定窗口大小对应的上下文内容，用双向RNN对这段信息进行编码，每个位置的包含这个实体的片段都编码完成后，根据这些编码信息与问题的相似性计算这个实体不同语言片段的Attention信息，并根据Attention信息综合出整篇文章中这个实体不同上下文的总的表示，然后根据这个表示和问题的语义相近程度选出最可能是答案的那个实体。DER模型尽管看上去和一维匹配模型差异很大，其实两者并没有本质区别，一维匹配模型在最后步骤相同单词的Attention概率合并过程其实和DER的做法是类似的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 问题与展望&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用深度学习解决机器阅读理解问题探索历史时间并不长，经过最近一年的探索，应该说很多模型相对最初的技术方案来说，在性能提升方面进展明显，在很多数据集上性能都有大幅度的提高，甚至在一些数据集(bAbi,CNN,Daily Mail等)性能已经达到性能上限。但是总体而言，这距离让机器像人一样能够理解文本并回答问题还有非常遥远的距离。最后我们对这个研究领域目前面临的问题进行简述并对一些发展趋势进行展望。&lt;/p&gt;&lt;p&gt;1. 需要构建更具备难度的大规模阅读理解数据集&lt;/p&gt;&lt;p&gt;手工构建大规模的阅读理解训练数据需要花费极大成本，因此目前的不少数据集都存在一定问题。某些数据集比如MCTest经过人的精心构建，但是规模过小，很难用来有效训练复杂模型。另外一类是采用一定的启发规则自动构建的数据集，这类数据集数据规模可以做得足够大，但是很多数据集失之于太过简单。不少实验以及对应的数据分析结果证明常用的大规模数据集比如CNN和Daily Mail 相对简单，正确回答问题所需要的上下文很短（5个单词窗口大小范围）（参考文献3），只要采取相对简单的模型就可以达到较好的性能，复杂技术发挥不出优势，这从某种角度说明数据集难度偏小，这也极大限制了新技术的探索。&lt;/p&gt;&lt;p&gt;为了能够促进领域技术的进一步快速发展，需要一些大规模的人工构建的阅读理解数据集合，这样既能满足规模要求，又能具备相当难度。类似SQuAD数据集(参考文献13)这种采用众包的方式制作数据集合是个比较有前途的方向，也期待更多高质量的数据集尤其是更多中文数据集的出现（哈工大在16年7月份公布了第一份中文阅读理解数据集）。&lt;/p&gt;&lt;p&gt; 2. 神经网络模型偏单一&lt;/p&gt;&lt;p&gt;从上述相关工作介绍可以看出，目前在机器阅读理解任务中，解决方案的神经网络结构还比较单一，后续需要探索更多种多样的新型网络结构及新式模型，以促进这个研究领域的快速发展。&lt;/p&gt;&lt;p&gt;3. 二维匹配模型需要做更深入的探索&lt;/p&gt;&lt;p&gt;二维匹配模型由于引入了更多的细节信息，所以在模型性能上具备一定优势。但是目前相关的工作还不多，而且大多集中在Attention机制的改造上。这个模型需要做更多的探索，比如匹配函数的创新、多层推理机制的引入等，我相信短期内在这块会有很多新技术会被提出。&lt;/p&gt;&lt;p&gt;4. 世界知识（World Knowledge）的引入&lt;/p&gt;&lt;p&gt;对于人来说，如果需要真正理解一篇新的文章，除了文章本身提供的上下文外，往往需要结合世界知识，也就是一些常识或者文章相关的背景知识，才能真正理解内容。目前机器阅读理解任务为了降低任务难度，往往将世界知识排除在任务之外，使得阅读理解任务更单纯，这在该领域研究初期毫无疑问是正确的思路，但是随着技术发展越来越完善，会逐渐引入世界知识来增强阅读理解任务的难度，也和人类的阅读理解过程越来越相似。&lt;/p&gt;&lt;p&gt;5. 发展更为完善的推理机制&lt;/p&gt;&lt;p&gt;在阅读一篇文章后，为了能够回答复杂的问题，设计合理的推理机制是阅读理解能力进行突破的关键技术，如上文所述，目前的推理机制大多数还是采用注意力焦点转移的机制来实现，随着复杂数据集的逐渐出现，推理机制会在阅读理解中起到越来越重要的作用，而今后需要提出更加丰富的推理机制。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1]&lt;/b&gt; Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proc. of NIPS, pages 1684–1692.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, andJan Kleindienst. 2016. Text understanding with the attention sum reader network. tarXiv:1603.01547.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt; Danqi Chen, Jason Bolton, and Christopher D. Manning.2016. A thorough examination of the cnn / daily mail reading comprehension task. In Proc. of ACL.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4] &lt;/b&gt;Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and Ruslan Salakhutdinov. 2016. Gated-attention readers for text comprehension. arXiv preprint arXiv:1606.01549&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] &lt;/b&gt;Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee and Lin-Shan Lee,2016. Towards Machine Comprehension of Spoken Content:Initial TOEFL Listening Comprehension Test by Machine. arXiv preprint arXiv: 1608.06378&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6]&lt;/b&gt; Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. 2016. Consensus attention-based neural networks for chinese reading comprehension. arXiv preprint arXiv:1607.02250.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7]&lt;/b&gt; Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu and Guoping Hu.2016. Attention-over-Attention Neural Networks for Reading Comprehension. arXiv preprint arXiv: 1607.04423v3.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8]&lt;/b&gt; Shuohang Wang and Jing Jiang.2016. Machine Comprehension Using Match-LSTM and Answer Pointer. arXiv preprint arXiv: 1608.07905&lt;/p&gt;&lt;p&gt;&lt;b&gt;[9]&lt;/b&gt; Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.2015. End-to-end memory networks. In Proc. of NIPS,pages 2431–2439.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[10] &lt;/b&gt;Alessandro Sordoni, Phillip Bachman, and Yoshua Bengio. 2016. Iterative alternating neural attention for machine reading. arXiv preprint arXiv:1606.02245.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[11]&lt;/b&gt; Adam Trischler, Zheng Ye, Xingdi Yuan, and Kaheer Suleman. 2016. Natural language comprehension with theepireader. arXiv preprint arXiv:1606.02270&lt;/p&gt;&lt;p&gt;&lt;b&gt;[12] &lt;/b&gt;Sosuke Kobayashi, Ran Tian,Naoaki Okazaki, and Kentaro Inui. 2016. Dynamic entity representations with max-pooling improves machine reading. In NAACL-HLT.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[13]&lt;/b&gt; Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questionsfor machine comprehension of text. In Proceedings of the Conference on Empirical Methods inNatural Language Processing, 2016&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果."&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7253b4c0a2bc70c5b87f1beb5029bcd0.jpg" data-rawwidth="108" data-rawheight="121"&gt;&lt;p&gt;&lt;b&gt;张俊林，&lt;/b&gt;技术书籍《这就是搜索引擎：核心技术详解》（该书荣获全国第十二届输出版优秀图书奖）、《大数据日知录：架构与算法》的作者。目前在用友畅捷通负责人工智能相关业务。在此之前，张俊林曾经在阿里巴巴搜索技术中心、百度商务搜索部凤巢广告平台以及新浪微博搜索部及数据系统部担任资深技术专家， 新浪微博技术委员会成员，负责算法策略方向。他还曾是智能信息聚合网站“玩聚网”的联合创始人之一。他的研发兴趣集中在：搜索技术、推荐系统、社交挖掘、自然语言处理与大数据算法架构等方面，并在以上领域有 多年工业界实践经验。&lt;/p&gt;&lt;p&gt;他本科毕业于天津大学管理学院，1999年至2004年在中科院软件所直接攻读博士学位，研究方向是信息检索理论与自然语言处理，就学期间曾在 ACL/COLING/IJCNLP等国际顶级会议发表多篇学术论文，另外，他在此期间领导设计的搜索系统曾在美国国防部DARPA主持的TREC第二届 高精度检索系统评测中在17支国际高水平研究团队激烈竞争中胜出，并取得综合排名第一名的优异成绩。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325512&amp;amp;idx=1&amp;amp;sn=076b66898c27ef07ede4cc0feab0fce4&amp;amp;chksm=f235a502c5422c147370e8e96ddfb0178b09e1afa564f3fe905c6a688d6dd9de8a9119446458&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="深度学习解决机器阅读理解任务的研究进展"&gt;深度学习解决机器阅读理解任务的研究进展&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22671467&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 28 Sep 2016 11:39:52 GMT</pubDate></item><item><title>技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道</title><link>https://zhuanlan.zhihu.com/p/22589208</link><description>深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;近年来，随着深度学习的崛起，计算机视觉得到飞速发展。目标检测作为计算机视觉的基础算法，也搭上了深度学习的快车。基于Proposal的检测框架，从R-CNN到Faster R-CNN，算法性能越来越高，速度越来越快。另一方面，直接回归Bounding Box的框架，从YOLO到SSD，在保持速度优势的同时，性能也逐渐得到提升。“深度学习大讲堂”往期介绍过这方面的进展，在此不再赘述。&lt;/p&gt;&lt;p&gt;近期，我们在PASCAL VOC2012目标检测上提交的结果mAP性能达到87.9，刷新了世界记录，排名第一名，如下图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1bcc1e044daf779423bcd709eb1b3f4d.png" data-rawwidth="658" data-rawheight="339"&gt;&lt;p&gt;方法上，基于Faster R-CNN [1]，我们做了一系列的算法改进，使得性能相比Baseline得到显著提升。本文主要给大家分享我们做出的这些算法上的改进技巧，以及一些工程上的实践经验。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.寻找更优的网络结构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;“Features matter.” 去年MSRA凭借ResNets [2]取得了多项任务性能上的突破。以ResNet-101为基准，有没有网络可以提供更优的特征？我们验证了以下几个网络。&lt;/p&gt;&lt;p&gt;&lt;b&gt;a) &lt;/b&gt;进一步增加网络深度。在12GB显存容量的限制下，101层的网络已经是极限。然而，如果把预训练模型的BN层参数融合进前一层卷积层，然后去掉BN层，则可以容纳ResNet-152。根据我们的实验，在ImageNet DET数据集上，去掉BN层的ResNet-152比保留BN层的ResNet-101还要差约1个点。这说明BN层的作用还是比较重要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;b) &lt;/b&gt;BN层的训练策略。我们发现训练时如果更新BN层的参数，相比固定参数，性能会下降一大截。原因可能是Faster R-CNN训练时batch size只有1，batch之间的均值/方差变化太大，太不稳定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;c) &lt;/b&gt;MSRA和Facebook相继公开了自己训练的ResNets模型。后续MSRA又提出了Identity Mapping版本的ResNets [3]。我们验证发现，Identity Mapping版本的ResNet-101检测性能略优于MSRA的原始ResNet-101模型和Facebook的模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 改进RPN Proposal&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在Faster R-CNN框架里面，RPN提取Proposal和FRCN对Proposal进行分类其实是2个独立的问题。针对RPN，我们做出了以下2处改进：&lt;/p&gt;&lt;p&gt;&lt;b&gt;a) &lt;/b&gt;均衡正负Anchor比例。理想状态下，RPN 正负Anchor的比例是1:1。我们发现，在batch size比较大（256）的情况下，这个比例会非常悬殊。特别是目标数量比较少的图像，正的Anchor数量会非常少。这样训练出来的模型会偏向于背景类别，容易漏检。我们对这个比例做了限制，让负样本数量不超过正样本的1.5倍，发现Proposal的召回率可以提高5个点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;b) &lt;/b&gt;级联RPN。受CRAFT [4]的启发，我们设计了自己的级联RPN。[4]中先得到标准的RPN Proposal，然后用一个分类性能更强的FRCN分支来改进Proposal质量。我们则是用了2个标准的RPN（图 1）。第一个RPN用滑窗得到的Proposal作为Anchor，第二个RPN用第一个RPN输出的Proposal作为新的Anchor位置。相比[4]，我们的算法优势是实现简单，额外增加的计算量非常少。对于中大目标，可以明显提升Proposal位置的准确度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-de7304f90bc25c6552196fa9f020942c.png" data-rawwidth="611" data-rawheight="543"&gt;&lt;p&gt;&lt;b&gt;3. 全局上下文建模&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个Proposal对应原始图像中的一个ROI区域。对这个ROI区域进行分类时， FRCN先把ROI映射到中间特征图上，然后在中间特征图上做裁剪（RoIPooling）。裁剪出来的小特征图输入到CNN分类器中。可以看到，CNN分类只使用了ROI区域内的局部特征。实际上，ROI周围的上下文信息对于判断这个ROI类别是很有帮助的。例如对一个乒乓球分类，很容易和光源混淆。如果知道周围有乒乓球拍、乒乓球台等目标，则更容易判断这是个乒乓球。&lt;/p&gt;&lt;p&gt;全局上下文建模是从整幅图像提取特征，然后和每个Proposal的局部特征相融合，用于分类。去年MSRA [2]使用全局上下文，得到了1个点的性能提升。然而他们没有发布具体的实现细节。我们实现的全局上下文网络结构如图 2所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e278757c741316e374ee2a87958e1751.png" data-rawwidth="628" data-rawheight="616"&gt;&lt;p&gt;我们发现，对于图中的全局上下文网络分支，训练时如果采用随机初始化，性能提升非常有限。如果用预训练的参数初始化，在ImageNet DET验证集上可以得到超过3个点的性能提升。对于ResNets，RoIPooling后面的conv5有9层卷积。而Faster R-CNN finetune时一般初始学习速率又会设得比较小（0.001）。这就导致从头训练这9层卷积比较困难。因此，这里预训练显得尤为重要。另外，[2]还把全局上下文特征同时用于分类和Bounding Box回归。我们发现全局上下文特征对于Bounding Box回归没有帮助，只对分类有帮助。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. 训练技巧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;a) &lt;/b&gt;平衡采样。很多数据集存在样本不均衡的问题，有些类别特别多，有些类别特别少。训练模型时，从一个图像列表中依次读取样本训练。这样的话，小类样本参与训练的机会就比大类少。训练出来的模型会偏向于大类，即大类性能好，小类性能差。平衡采样策略就是把样本按类别分组，每个类别生成一个样本列表。训练过程中先随机选择1个或几个类别，然后从各个类别所对应的样本列表中随机选择样本。这样可以保证每个类别参与训练的机会比较均衡。在PASCAL VOC数据集上，使用平衡采样性能可以提升约0.7个点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;b) &lt;/b&gt;难例挖掘（OHEM [5]）。使用了难例挖掘后，收敛更快，训练更稳定。在ImageNet DET数据集上，性能可以提升1个多点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;c) &lt;/b&gt;多尺度训练。使用多尺度训练的话，可以让参与训练的目标大小分布更加均衡，使模型对目标大小具有一定的鲁棒性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.  预测技巧&lt;/b&gt;&lt;/p&gt;&lt;p&gt;预测阶段，我们用了多尺度预测，水平翻转，和检测框投票。这些策略的具体实现在很多文献中都有描述。这里我们可以分享一下多个检测结果的融合策略。当使用多尺度预测，水平翻转，还有多模型Ensemble时，对于同一张测试图像，我们会得到好几组结果。对于这些结果，最直观的融合方式就是把所有的检测框放在一起，然后用非极大值抑制（NMS）处理一下。但是我们发现另一种方式效果更好，就是把RPN和FRCN分开来做。先对RPN做多尺度、水平翻转、多模型的融合，得到一组固定的Proposal之后，再对FRCN进行多尺度、水平翻转、多模型的融合。RPN的融合用NMS更好，FRCN的融合用对Proposal的置信度和Bounding Box位置取平均值的方式更好。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文总结了我们做出的一些Faster R-CNN改进技巧，并分享了算法实现过程中遇到的细节问题。正如谚语所言，”The devil is in the details.” 希望我们的这些算法细节对同行以及相关的算法爱好者们提供一定的帮助和指引。我们抛砖引玉，期待同行们也可以分享自己的经验。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1] &lt;/b&gt;Ren,Shaoqing, et al. "Faster R-CNN: Towards real-time object detection withregion proposal networks." Advances in neural information processingsystems. 2015.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;He,Kaiming, et al. "Deep residual learning for image recognition." arXivpreprint arXiv:1512.03385 (2015).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3] &lt;/b&gt;He,Kaiming, et al. "Identity mappings in deep residual networks." arXivpreprint arXiv:1603.05027 (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4] &lt;/b&gt;Yang,Bin, et al. "Craft objects from images." arXiv preprint arXiv:1604.03239(2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] &lt;/b&gt;Shrivastava,Abhinav, Abhinav Gupta, and Ross Girshick. "Training region-based objectdetectors with online hard example mining." arXiv preprintarXiv:1604.03540 (2016).&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果." class="" data-editable="true"&gt;@果果是枚开心果.&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;作者简介：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-de9a7f09cf630b1cd5357e4c70e2cf65.png" data-rawwidth="99" data-rawheight="93"&gt;&lt;p&gt;&lt;b&gt;钟巧勇，&lt;/b&gt;本科毕业于南京大学，博士毕业于中科院上海生科院计算生物学研究所。2014年加入海康威视研究院，现任高级研究员。主要从事深度学习，计算机视觉方面的算法研究工作，研究方向是基于深度学习的目标检测。海康威视研究院招贤纳士，欢迎投简历至：zhongqiaoyong@hikvision.com&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325480&amp;amp;idx=1&amp;amp;sn=e39482fc9e83b89d6e72efc5ee689b92&amp;amp;chksm=f235a6e2c5422ff494d14137ba4a25a14900cc8ebc05e6f9a48cf6cb8dd9d3e95396c0b1da06&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道" class=""&gt;技术揭秘：海康威视PASCAL VOC2012目标检测权威评测夺冠之道&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22589208&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Fri, 23 Sep 2016 15:29:29 GMT</pubDate></item><item><title>基于深度学习的VQA（视觉问答）技术</title><link>https://zhuanlan.zhihu.com/p/22530291</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-28521eec06c1b49ddc650ee6e481c004_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;视觉问答导读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;视觉问答（Visual Question Answering，VQA），是一种涉及计算机视觉和自然语言处理的学习任务。这一任务的定义如下： A VQA system takes as input an image and a free-form, open-ended, natural-language question about the image and produces a natural-language answer as the output[1]。 翻译为中文：一个VQA系统以一张图片和一个关于这张图片形式自由、开放式的自然语言问题作为输入，以生成一条自然语言答案作为输出。简单来说，VQA就是给定的图片进行问答。&lt;/p&gt;&lt;p&gt;VQA系统需要将图片和问题作为输入，结合这两部分信息，产生一条人类语言作为输出。针对一张特定的图片，如果想要机器以自然语言来回答关于该图片的某一个特定问题，我们需要让机器对图片的内容、问题的含义和意图以及相关的常识有一定的理解。VQA涉及到多方面的AI技术（图1）：细粒度识别（这位女士是白种人吗？）、 物体识别（图中有几个香蕉？）、行为识别（这位女士在哭吗？）和对问题所包含文本的理解（NLP）。综上所述，VQA是一项涉及了计算机视觉（CV）和自然语言处理（NLP）两大领域的学习任务。它的主要目标就是让计算机根据输入的图片和问题输出一个符合自然语言规则且内容合理的答案。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a5f5bb39b382acce83d62ad8d7d6920f.jpeg" data-rawwidth="674" data-rawheight="556"&gt;&lt;p&gt;与VQA类似——看图说话（Image Caption）任务也同时涉及到CV和NLP两个领域，但是与VQA不同的是看图说话只需要产生对图片的一般性描述，而视觉问答根据问题的不同仅聚焦与图片中的某一部分，而且某些问题还需要一定的常识推理才能做出回答。例如图2中的第一个问题，你能在这停车吗？计算机需要读懂这张图片而且还要有那些地方可以停车，哪些地方不可以。而对于看图说话，则只需要产生一条类似“花园的左边有一辆车，后边有一个消防栓”的描述即可。因此，VQA相比看图说话在图像语义的理解方面有更高的要求，因此也具有更大的技术挑战。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-bd01bb42d53f92a51444010248725964.jpeg" data-rawwidth="682" data-rawheight="662"&gt;&lt;p&gt;&lt;b&gt;视觉问答中State-of-the-art方法介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随着深度学习的在CV和NLP中的广泛应用，深度学习强大的特征学习能力极大的推动了CV和NLP领域的研究。在CV中以CNN为代表的各种深度网络层出不穷，可以端到端的学习图像特征而不依赖手工设计的特征。CNN通过逐层的特征抽取，将图像从简单的边缘、角点等底层特征，逐层组合成为更高层次的特征。CNN在图像上强大的特征抽取能力使得它可以更加完备的抽取并压缩图像信息。而RNNs模型在NLP领域中也展现出了其强大之处，尤其是在语音识别，机器翻译，语言模型与文本生成等方面取得很大的成功。由于VQA涉及到CV和NLP两个领域的内容，那么很自然的一种VQA解决方案就是将在CV和NLP中应用非常成功的CNN和RNN结合构造组合模型。&lt;/p&gt;&lt;p&gt;由深度CNN和LSTM网络结构组合而成的VQA模型是目前来说在视觉问答中性能相对较好的模型。接下来，将介绍一些具有代表性的研究中提到的一些比较好的模型组合。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Deeper LSTM Q + norm I模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该模型由Aishwarya Agrawal等人提出，文章发表于ICCV2015[1]，其中I指的是提取后的图片特征，norm I指的是对CNN抽取的图像语义信息向量(1024维)做L2归一化处理。CNN抽取图像语义信息，LSTM抽取问题中包含的文本语义信息，将两者的信息融合，让模型学习到问题的含义/意图，最后送入一个以Softmax为输出层的多层MLP中产生答案输出，整体结构如图3所示。图3中输入图像中包含了一个室外场景中的2匹马和2个人在，这些信息由不含分类层的CNN抽取，而问题部分的信息则由一个LSTM网络结构按照问题单词的输入顺序逐个抽取问题信息，然后将两个压缩后的信息融合，将融合后的信息送入MLP中产生结果输出（当前问题是一个物体计数 (count object)问题）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-9d9a7e44fecc9311ae79c47bbde245d2.jpeg" data-rawwidth="674" data-rawheight="286"&gt;&lt;p&gt;该模型使用2层LSTM编码问题并用VGGNet去编码图像，然后将图像特征使用L2归一化。之后将图像和问题特征变换到同一个特征空间，通过点乘的方式将信息融合，融合后的信息送入一个以Softmax为分类器的三层MLP中产生答案输出。模型在训练的过程中，固定CNN，只有LSTM层和最后的分类网络参加训练。之所以CNN不参加训练的原因是对CNN的微调训练极度耗时，而且微调后的结果对模型整体的性能并没有特别大的提升。接下来的模型的CNN部分都与[1]中模型相同，因此不再赘述。&lt;/p&gt;&lt;p&gt;文中所使用的问答数据集是问答对和问答对对应的图片构成，问题的形式是开放式和多选，对应的答案由1-2个单词组成。文中为了研究图片和问题单独包含的信息量，分别设计了只提供问题(BoW  Q ,  LSTM + Q, deeper LSTM Q)、只提供图像(I)以及同时提供两者(BoW Q + I, LSTM Q + I, deep LSTM Q + norm I)的情况下回答问题的结果，实验结果如下图4所示，我们可以看到，单独的语言模型和视觉模型都有一定的精度，但是它们的精度并不高，这说明图像和问题各自独立包含了一些产生答案的过程中必要信息，单独利用某一方面都不能得到很好的效果，文中提出的模型在开放式和多选这两种问题得到的效果是最好的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-76784167280c0f8a27183c38bb651ad0.jpeg" data-rawwidth="588" data-rawheight="268"&gt;&lt;p&gt;&lt;b&gt;2. VIS+LSTM模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;此模型由Mengye Ren等人提出，文章发表于NIPS2015[2]，文中模型的基本结构是首先使用CNN抽取图片信息，完成之后接LSTM产生预测结果。Mengye Ren等人考虑到由于没有一个完好的评价答案句子精确度的标准，因此他们将注意力集中在有限的领域问题，这些问题可以用一个单词作为视觉问答的答案，这样就可以把视觉问答视为一个多分类问题，从而可以利用现有的精确度评价标准度量答案。&lt;/p&gt;&lt;p&gt;模型基本结构如图5所示，首先用在ImageNet上训练好的VGG19抽取图像特征得到一个4096维的向量（最后一个隐含层输出），然后将这个向量视为问题句子的第一个词。由于LSTM网络的输入是一个300或500维的词向量，为了使图片特征向量匹配词向量的纬度，文中[2]使用仿射或者线性变换将图像特征向量变换成一个300或500维的向量。接下来使用结合图像特征作为第一个词的LSTM网络按顺序提取问题的信息，最后将LSTM网络的最后一个输出送入Softmax作为最后的分类输出层产生答案。[2]中模型的训练与[1]略有不同，[2]中提出的模型选用的LSTM类型可以是双向的以及图像特征与问题特征融合的位置在[2]中可以同时在始末两处加入。[1]和[2]将视觉问答任务视为多分类问题，这种形式的模型一个优势是易于实现和验证。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1372e76f225a64a9def888e7a62ad7ad.jpeg" data-rawwidth="671" data-rawheight="297"&gt;&lt;p&gt;文章在此基础上产生了另外3个模型：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. &lt;/b&gt;2-VIS+BLSTM：将图像特征同时作为句子的第一个和最后一个词，使用双向的LSTM网络抽取问题信息。双向LSTM网络是一个扩展的LSTM网络，它同时考虑了序列的历史信息和未来信息，可以更好的利用句子的语境信息。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. &lt;/b&gt;IMG+BOW：BOW替换LSTM来抽取问题信息。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. &lt;/b&gt;FULL：简单的将前三种模型的输出结果求平均。&lt;/p&gt;&lt;p&gt;文章采用Accuracy、WUPS(0.9)、WUPS(0.0)来评价以上4个模型的性能。WUPS[6](Wu-Palmer Similarity)根据两个单词在一个分类树中的最长公共子序列来计算相似性。如果预测单词和标准答案单词的相似性低于设定的阈值则候选答案的评分为0。[2]在MS-COCO-QA[7]和DAQUAR[8](该数据集是一个室内场景数据集)数据集上的实验结果如图6所示。实验过程中有两点比较有趣：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.&lt;/b&gt; IMG+BOW模型与VIS+LSTM模型在两个数据集上的表现十分相似。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.&lt;/b&gt; 在DAQUAR中，仅有语言模型的结果并没有比CNN+LSTM模型差太多，而在MS-COCO-QA数据集上，仅有语言模型的结果与CNN+LSTM模型的结果差距较大。&lt;/p&gt;&lt;p&gt;作者认为第一点出现的原因可能是因为视觉问答中，尤其是简单问题，词语之间序列信息的作用可能没有其他NLP任务中那么重要。第二点在DAQUAR数据集上单纯的语言模型没有比CNN+LSTM模型在结果上相差太多，原因应该是ImageNet的图片所对应的场景与室内场景差异太大。这里需要解释一下DAQUAR数据集问题，由于本文作者使用的VGGNet是在ImageNet数据集上训练的得到，而ImageNet和DAQUAR包含的图片的类型相差较大，ImageNet包含较多的室外场景和动物类别而DAQUAR多为室内场景。因此使用ImageNet训练得到的VGGNet在抽取室内场景信息的时候并不准确，这造成提取的图像语义信息并没有提供太大的贡献。从图6中实验结果中可以看出，VIS+LSTM模型在DAQUAR数据集的精度没有COCO-QA数据集上的精度好。在两个数据集上的精度差距较大的原因，作者分析其原因有：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.&lt;/b&gt; COCO-QA的数据集与ImageNet有重叠。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. &lt;/b&gt;COCO-QA很少有室内场景的物体。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. &lt;/b&gt;COCO-QA的数据量极其庞大，可以训练出比较复杂的网络。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-9250b602c27437f897296f306b85e13a.jpeg" data-rawwidth="681" data-rawheight="312"&gt;&lt;p&gt;&lt;b&gt;3. Neural-Image-QA模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该由Mateusz Malinowski等人提出，文章发表在ICCV2015[3]。模型以CNN和LSTM为基础，以一种新的使用方式，设计了一个预测结果长度可变的模型。该模型将视觉问答任务视为结合图像信息作为辅助的sequence to sequence任务。模型结构如图7所示，首先由一个预训练好的深度CNN模型抽取出要回答的图片特征，然后将图片特征和转化为词向量的问题词一起送入LSTM网络，在每次送入一个问题词的同时将图片特征送入网络，直到所有的问题特征信息抽取完毕。接下来用同一个LSTM网络产生答案，直至产生结束符($)为止。该模型的训练过程是结合图像特征的LSTM网络的训练以及词向量的生成器的训练。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2a8c7a7a0deb39b029eb90f0876959da.jpeg" data-rawwidth="582" data-rawheight="266"&gt;模型评价方面，该模型使用了与[2]中相同的评价方法即Accuracy和WUPS[6]。作者在DAQUAR及其减缩版数据集（构成答案的单词个数仅有1，2，3，4，即有4个DAQUAR的子集）上进行了测试，DAQUAR上的测试结果如图8所示。该模型提出了一种利用同一个LSTM网络产生变长答案的模型，[3]中分析认为不管是问题还是答案他们都属于同一个语料库，因此可以使用同种编码和信息抽取方式。我们可以看到，在多词类型的答案中，本文提出的模型较Malinowski et al提出的模型在Accuracy和WUPS分别有9%和11%的提升，这个提升幅度还是比较大的。在单个词类型的答案中，Accuracy的提升是Malinowski et al的2倍。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f556d09315a7961effdeaca033b3a3cc.jpeg" data-rawwidth="573" data-rawheight="346"&gt;&lt;p&gt;&lt;b&gt;4. mQA模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该模型由Gao H等人提出，文章于NIPS2015上发表[4]。此文中对视觉问答任务的理解是：这个模型需要针对一个图片的自由形式的问题给出一个答案，而这个答案可以是一个句子，一个短语或者一个词。因此[4]中提出了一个相对之前的几个模型来说较为复杂的模型。mQA总共由4个子模块组成，第一个模块由一个LSTM网络将自然语句编码成一个稠密的词向量特征，即抽取问题中所包含的信息，下文称问题LSTM网络；第二个模块由一个深度CNN抽取图片的特征；第三个模块是另外一个不同的LSTM网络，这个网络用于编码在答案中当前词和之前的一些词的特征信息，下文称为答案LSTM网络，将其作为答案语境；最后一个模块则是用来融合前面三个模型产生的信息，来预测当前阶段要产生在答案中的下一个词。模型的结构如下图9所示。mQA模型的训练与[3]不同，它用两个LSTM分别处理问题和答案，这样在训练过程中就有两个LSTM结构和一个信息融合网络需要同时训练，该模型相对复杂且极易产生过拟合。mQA中为了避免此问题，将信息融合部分的中间层和Softmax层之间的权重矩阵与词向量生成层的权重以转置的方式共享；理由是输入层的词向量是把one-hot形式的词向量编码成到一个稠密的词特征空间，而Softmax层的输出则是将词从稠密空间解码成one-hot形式，两者是一个相反的 过程。这样一来，简化了模型的参数个数，简化了模型，在一定程度上缓解了模型的过拟合。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-657aaa652243e37cdf06ad412e7e80bb.jpeg" data-rawwidth="671" data-rawheight="295"&gt;&lt;p&gt;从模型的结构中我们可以看到，首先CNN和问题LSTM分别抽取图片特征以及问题特征，产生图片和问题的稠密词向量特征。这两部分信息准备完毕之后，答案LSTM网络首先由一个特定的答案开始符（&amp;lt;BOA&amp;gt;）作为该LSTM网络的起始输入，经过答案LSTM网络后将会产生一个当前阶段预测词的特征向量表示，然后将图片特征、问题特征、前一个词（就图中来看是&amp;lt;BOA&amp;gt;）的词向量特征、答案LSTM网络产生的预测词特征向量做线性加和，然后送入第四个模型产生当前阶段的预测词。依次按照这个步骤产生答案，直到特定的结束符（EOA）出现，停止产生答案。该模型与[3]中提出的模型主要区别：使用了两个独立的LSTM网络处理问题和答案。&lt;/p&gt;&lt;p&gt;本文作者分析认为，由于问题和答案的属性不同（例如，两者的语法格式不同），使用两个独立的LSTM网络处理更加合理，但是需要注意的是，组成问题和答案的单个词语同属一个语料库，所具有的含义应该是相同的，因此应使用同一个词向量层来编码问题和答案的每一个词，该部分则与[3]中相同。&lt;/p&gt;&lt;p&gt;文中基于MS-COCO生成了一个新的更大的数据集，FM-IQA(Freestyle Multilingual Image Question Answering Dataset)，访问地址是&lt;a href="http://idl.baidu.com/FM-IQA.html" data-editable="true" data-title="IDL研究项目&amp;amp;成果" class=""&gt;IDL研究项目&amp;amp;成果&lt;/a&gt;，该数据集的问答对非常有趣且极具多样性，答案的长度不一，答案的结果不唯一，很多情况下一个问题有很多种形式的回答都是正确的，因此前面使用的度量标准和一些用于图像标注的度量标准都无法给出合理评判（详见[4]的第5部分）。因此[4]中使用人来评判模型，即视觉图灵测试（Visual Turing Test）。除此之外，[4]中还对模型产生的句子给一个评分，评分越高越好或越高越差。这种细粒度的评分使用0，1，2三个等级，“2”表示答案完全错误或者完全正确，“1”表示答案部分正确，为模型评分的人可以与图灵测试的相同也可以不同。两部分的实验结果如下图10所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ca8b7f6aa52e571ea52ea1e8b4b6d921.jpeg" data-rawwidth="609" data-rawheight="171"&gt;&lt;p&gt;&lt;b&gt;视觉问答临的挑战及可能的改进方向&lt;/b&gt;&lt;/p&gt;&lt;p&gt;虽然目前的VQA研究取得了一些成就，但是就目前的模型达到的效果来看，整体准确率并不高，除了在回答单一答案的简单问题上有较高的准确率外，其他方面模型的准确率普遍偏低，即便是文献[4]提出的复杂模型的图灵测试通过率也仅有60%左右。当前的VQA模型结构还相对简单，答案的内容和形式比较单一，对于稍复杂的需要更多先验知识进行简单推理的问题还无法做出正确的回答。例如[4]中给出了部分模型mQA的错误结果分析，分析中发现当对图片背景的常识性推理错误、问题聚焦的物体太小、需要高层次的逻辑推理等问题出现时，模型往往无法给出正确的预测（这方面详见文献[4]的Discussion）。究其原因，除了CNN的图像信息外，LSTM在学习过程中的知识来源只有训练集中的问答对，知识结构比较简单且信息量匮乏。&lt;/p&gt;&lt;p&gt;未来的RNN模型一种可能的改进方式是：增加一个显式的先验知识库，在提取问题信息的同时融合与问题信息相关的先验知识。正如[5]中提出的一种端到端的记忆网络，它在Q与A之间增加了一系列记忆模块参与计算，记忆模块由一组给定的语句集和两个不同的词向量生成器构成，这些语句由两个编码器独立编码生成2组词向量，第一组词向量与问题融合产生意图/关注点，之后将另一组词向量与产生的意图/注意点融合产生记忆输出，记忆输出与Q对应的词向量融合最终产生输出。该记忆网络与RNN结构不同的是在中间过程中没有输出且增加了一系列记忆模块。将问题与事先设定的记忆部分映射，答案的产生不只与训练集的答案有关，还与记忆部分的记忆内容相关，这样可以产生更多样化且合理的答案。受该记忆网络的启发，对于未来的RNN结构或可在其计算过程中增加这样一些类似的记忆模块以丰富模型在训练过程中的知识源。参考类似这种记忆网络结构，可以在一定程度上缓解RNNs在学习过程由于先验知识不足而导致对于那些需要日常经验加以推断的问题回答不正确的现象，进而改善模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;视觉问答实验源代码下载 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;在研究过程中作者参考目前应用较为广泛的模型，利用mxnet深度学习框架实现了一个简单的VQA原型，代码可以从&lt;a href="https://github.com/liuzhi136/Visual-Question-Answering" data-editable="true" data-title="GitHub - liuzhi136/Visual-Question-Answering"&gt;GitHub - liuzhi136/Visual-Question-Answering&lt;/a&gt;访问，训练数据使用微软提供的MS-COCO数据集，下载地址：&lt;a href="http://visualqa.org/download.html" data-editable="true" data-title="VQA: Visual Question Answering"&gt;VQA: Visual Question Answering&lt;/a&gt;。实验得到的结果与[1]中给出的实验结果基本一致，在测试集上达到52.34%的准确度。&lt;/p&gt;&lt;p&gt;以上部分完全是作者的个人理解，欢迎大家批评指正。&lt;/p&gt;&lt;p&gt;&lt;b&gt;致谢：&lt;/b&gt;本文作者在此感谢本文匿名审稿人和中科院计算所博士生邬书哲对本文所提出的建设性修改意见。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1] &lt;/b&gt;Antol S, Agrawal A, Lu J, et al. VQA: Visual question answering[C] //Proceedings of the IEEE International Conference on Computer Vision. 2015: 2425-2433.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2]&lt;/b&gt; Ren M, Kiros R, Zemel R. Exploring models and data for image question answering[C]//Advances in Neural Information Processing Systems. 2015: 2953-2961.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt; Malinowski M, Rohrbach M, Fritz M. Ask your neurons: A neural-based approach to answering questions about images[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1-9.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[4] &lt;/b&gt;Gao H, Mao J, Zhou J, et al. Are you talking to a machine? dataset and methods for multilingual image question[C]//Advances in Neural Information Processing Systems. 2015: 2296-2304.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[5] &lt;/b&gt;Sukhbaatar S, Weston J, Fergus R. End-to-end memory networks[C]//Advances in neural information processing systems. 2015: 2440-2448.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[6] &lt;/b&gt;Malinowski M, Fritz M. A multi-world approach to question answering about real-world scenes based on uncertain input[C]//Advances in Neural Information Processing Systems. 2014: 1682-1690.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[7]&lt;/b&gt; T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, “Microsoft ´ COCO: Common Objects in Context,” in ECCV, 2014&lt;/p&gt;&lt;p&gt;&lt;b&gt;[8] &lt;/b&gt;M. Malinowski and M. Fritz, “Towards a visual Turing challenge,” in NIPS Workshop on Learning Semantics, 2014&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-title="@果果是枚开心果. " class="" data-editable="true"&gt;@果果是枚开心果. &lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-660c446c51d911bb7926ce95b946fc1a.jpeg" data-rawwidth="119" data-rawheight="118"&gt;&lt;b&gt;刘智，&lt;/b&gt;东北大学计算机科学与工程硕士研究生二年级。导师刘辉林教授。研究兴趣包括深度学习与计算机视觉，以及基于深度学习的推荐算法。个人邮箱：liuzhi7936@sina.cn。个人github: &lt;a href="https://github.com/liuzhi136" data-editable="true" data-title="liuzhi136 · GitHub"&gt;liuzhi136 · GitHub&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325472&amp;amp;idx=1&amp;amp;sn=65cdfe2a60ad4ccf7b440ddbe9fba0f8&amp;amp;chksm=f235a6eac5422ffced668a4a985934929e3e2e0ee18045aca5c4d5d4e23c3af106fbf8e7f8c7&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="基于深度学习的VQA（视觉问答）技术" class=""&gt;基于深度学习的VQA（视觉问答）技术&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a29f11daca9717751e639f2c3a3f8b93.jpeg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22530291&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 20 Sep 2016 16:01:04 GMT</pubDate></item><item><title>SeetaFace开源人脸识别引擎介绍</title><link>https://zhuanlan.zhihu.com/p/22451474</link><description>&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;区分不同的人是很多智能系统的必备能力。为实现此目的，一种可能的技术手段是通过对人脸的光学成像来感知人、识别人，即所谓的人脸识别技术。经过几十年的研发积累，特别是近年来深度学习技术的涌现，人脸识别取得了长足的进步，在安防、金融、教育、社保等领域得到了越来越多的应用，成为计算机视觉领域最为成功的分支领域之一。&lt;/p&gt;&lt;p&gt;然而，人脸识别并非完全成熟的技术，离公众期望的全面应用尚有距离，还需要学术界、工业界的共同努力。为此，整个人脸识别社区需要有基准（Baseline）系统，而且基准系统的水平显然会极大影响着该领域的发展水平。可是令人尴尬的是，这个领域迄今尚无一套包括所有技术模块的、完全开源的基准人脸识别系统！我们希望改变现状，因此开源了SeetaFace人脸识别引擎。该引擎由中科院计算所山世光研究员带领的人脸识别研究组研发。代码基于C++实现，且不依赖于任何第三方的库函数，开源协议为BSD-2，可供学术界和工业界免费使用。&lt;/p&gt;&lt;p&gt;SeetaFace人脸识别引擎包括了搭建一套全自动人脸识别系统所需的三个核心模块，即：人脸检测模块SeetaFace Detection、面部特征点定位模块SeetaFace Alignment以及人脸特征提取与比对模块 SeetaFace Identification。其中，SeetaFace Detection采用了一种结合传统人造特征与多层感知机（MLP）的级联结构，在FDDB上达到了84.4%的召回率（100个误检时），并可在单个i7 CPU上实时处理VGA分辨率的图像。面部特征点定位模块SeetaFace Alignment通过级联多个深度模型（栈式自编码网络）来回归5个关键特征点（两眼中心、鼻尖和两个嘴角）的位置，在AFLW数据库上达到state-of-the-art的精度，定位速度在单个i7 CPU上超过200fps。人脸识别模块SeetaFace Identification采用一个9层的卷积神经网络（CNN）来提取人脸特征，在LFW数据库上达到97.1%的精度（注：采用SeetaFace人脸检测和SeetaFace面部特征点定位作为前端进行全自动识别的情况下），特征提取速度为每图120ms（在单个i7 CPU上）。 &lt;/p&gt;&lt;p&gt;下面对上述三个模块的情况做简要介绍，更详细的介绍请参考我们相应的学术论文。&lt;/p&gt;&lt;p&gt;&lt;b&gt;人脸检测模块SeetaFace Detection&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该模块基于我们提出的一种结合经典级联结构和多层神经网络的人脸检测方法[1]实现，其所采用的漏斗型级联结构（Funnel-Structured Cascade，FuSt）专门针对多姿态人脸检测而设计，其中引入了由粗到精的设计理念，兼顾了速度和精度的平衡。如图1所示，FuSt级联结构在顶部由多个针对不同姿态的快速LAB级联分类器[2]构成，紧接着是若干个基于SURF特征的多层感知机（MLP）级联结构，最后由一个统一的MLP级联结构（同样基于SURF特征）来处理所有姿态的候选窗口，整体上呈现出上宽下窄的漏斗形状。从上往下，各个层次上的分类器及其所采用的特征逐步变得复杂，从而可以保留人脸窗口并排除越来越难与人脸区分的非人脸候选窗口。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0ac76ef73b66f492d45c7b0a6abc77dd.jpg" data-rawwidth="672" data-rawheight="377"&gt;与SeetaFace Detection开源代码配套开放的是一个准正面人脸检测模型（使用了约20万人脸图像训练而来），可以实现准正面人脸的准确检测（旋转角度约45度以内，但对于姿态偏转较大的人脸也具备一定的检测能力），图2给出了一些检测结果的示例（注：测试时图像金字塔下采样比例设置为0.8，滑动步长设置为4和2，最小人脸设置为20x20）。在人脸检测领域最重要的评测集FDDB上对SeetaFace Detector进行评测，在输出100个误检时（FPPI=0.035）召回率达到84.4%，输出1000个误检时召回率达到88.0%。图3则给出了SeetaFace Detector在FDDB上的离散型得分ROC曲线，并与其它已发表的学术界公开结果（从FDDB官网获得）进行了对比。不难看出，尽管SeetaFace人脸检测器并非目前精度最高的，但在学术界公开的结果中仍然具有很强的竞争力，而且可以完全满足多数人脸识别系统的需求。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ea8fe2fb2617613bfa9b4ce63beec989.jpg" data-rawwidth="668" data-rawheight="339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f21fca833f6c0976ec7eb10e4ffa46a9.jpg" data-rawwidth="558" data-rawheight="440"&gt;&lt;p&gt;此外，与其他算法相比，SeetaFace Detector在速度上有一定优势。对于640x480大小的VGA图像，检测速度的对比情况如表1所示。其中，SeetaFace的速度在单个3.40GHz的i7-3770 CPU上测得，Cascade CNN[3]在CPU上的速度在2.0GHz的CPU上测得（引自原文）。而各方法在GPU上的速度在NVIDIA Titan Black GPU上测得。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/88da0ea24a8dc9d1fc7eb7e2e0842cd3.jpg" data-rawwidth="578" data-rawheight="173"&gt;&lt;p&gt;&lt;b&gt;特征点定位模块SeetaFace Alignment&lt;/b&gt;&lt;/p&gt;&lt;p&gt;面部特征点定位（人脸对齐）在人脸识别、表情识别、人脸动画合成等诸多人脸分析任务中扮演着非常重要的角色。由于姿态、表情、光照和遮挡等因素的影响，真实场景下的人脸对齐任务是一个非常困难的问题。形式上，该问题可以看作是从人脸表观到人脸形状的复杂非线性映射。为此，SeetaFace Alignment采用的是我们提出的一种由粗到精的自编码器网络（Coarse-to-Fine Auto-encoder Networks, CFAN [8]）来求解这个复杂的非线性映射过程。如图 4所示，CFAN级联了多级栈式自编码器网络，其中的每一级都刻画从人脸表观到人脸形状的部分非线性映射。具体来说，输入一个人脸区域（由人脸检测模块得到），第一级自编码器网络直接从该人脸的低分辨率版本中快速估计大致的人脸形状S0。然后，提高输入人脸图像的分辨率，并抽取当前人脸形状S0（相应提升分辨率）各特征点位置的局部特征，输入到下一级自编码器网络来进一步优化人脸对齐结果。以此类推，通过级联多个栈式自编码器网络，在越来越高分辨率的人脸图像上逐步优化人脸对齐结果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/937aefc92439047daac5e298c2a13e62.jpg" data-rawwidth="621" data-rawheight="439"&gt;此次开源的SeetaFace Alignment基于上述CFAN方法实现了5个面部关键特征点（两眼中心，鼻尖和两个嘴角）的精确定位，训练集包括23,000余幅人脸图像（标注了5点）。需要注意的是，为加速之目的，在基本不损失精度的情况下，开源实现中将CFAN级联的数目减少到了2级，从而可在单颗Intel i7-3770 (3.4 GHz CPU)上达到每个人脸5ms的处理速度（不包括人脸检测时间）。图5给出了一些用SeetaFace Alignment开源引擎定位面部5点的效果示例，可见其对表情、姿态、肤色等均具有较好的鲁棒性。在AFLW数据集上的量化评价和对比情况如图6所示，其中平均定位误差根据两眼中心距离做了归一化。不难看出，SeetaFace Alignment取得了state-of-the-art的定位结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/656524adea16ee73a51b02faf46c7645.jpg" data-rawwidth="502" data-rawheight="541"&gt;&lt;p&gt;&lt;b&gt;人脸特征提取与比对模块SeetaFace Identification&lt;/b&gt;&lt;/p&gt;&lt;p&gt;人脸识别本质上是要计算两幅图像中人脸的相似程度，其一为注册阶段（类比人的相识过程）输入系统的，另一幅为识别阶段（即再见时的辨认过程）的输入。为此，如图7所示，一套全自动的人脸识别系统在完成前述的人脸检测与人脸对齐两个步骤之后，即进入第三个核心步骤：人脸特征提取和比对。这个阶段也是深度学习风起云涌之后进步最大的模块，目前大多数优秀的人脸识别算法均采用卷积神经网络（CNN）来学习特征提取器（即图7中的函数F）。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/bf367f7fe1faad6b8df5e19f3efa7af0.jpg" data-rawwidth="559" data-rawheight="306"&gt;SeetaFace开源的人脸特征提取模块也是基于卷积神经网络的。具体地说，其实现的是[9]中所描述的深度卷积神经网络VIPLFaceNet：一个包含7个卷积层与2个全连接层的DCNN。其直接修改自Hinton教授的学生Alex Krizhevsky等于2012年设计的AlexNet（即引爆CNN在视觉中广泛应用的网络）。如表2对比所示，与AlexNet相比，VIPLFaceNet将5x5的卷积核拆分为两层3x3的卷积核，从而增加了网络深度，而并没有增加计算量；VIPLFaceNet还减少了每个卷积层的kernel数目以及FC2层的节点数。同时，通过引入Fast Normalization Layer（FNL），加速了VIPLFaceNet的收敛速度，并在一定程度上提升了模型的泛化能力。测试表明，在相同训练集情况下，VIPLFaceNet在LFW测试集上识别错误率比AlexNet降低了40%，而训练和测试时间分别为AlexNet的20%和60%。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/514a4d4ba5246525360894abc61047c8.jpg" data-rawwidth="495" data-rawheight="348"&gt;&lt;p&gt;与开源的SeetaFace Identification代码一起发布的人脸识别模型是使用140万人脸图像训练出来的，这些训练图像来自于约1.6万人，其中既有东方人也有西方人。人脸特征直接采用VIPLFaceNet FC2层的2048个结点的输出，特征比对可简单采用Cosine计算相似度，然后进行阈值比较（验证应用）或排序（识别应用）即可。该引擎在多数人脸识别场景下均具有良好的性能，例如，在LFW standard Image-Restricted测试协议下，使用SeetaFace Detector与SeetaFace Alignment检测并对齐人脸，采用SeetaFace Identification进行特征提取和比对，可以达到97.1%的识别正确率（请注意：这是系统全自动运行的结果，对少量不能检到人脸的图像，截取中间区域输入人脸对齐模块即可）。速度方面，在单颗Intel i7-3770 CPU上，开源代码提取一张人脸之特征的时间约为120ms（不含人脸检测和特征点定位时间）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;开源网址&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前，SeetaFace开源人脸识别引擎已全部发布在Github上供国内外同行和工业界使用，项目网址为：&lt;a href="http://github.com/seetaface" data-editable="true" data-title="seetaface · GitHub" class=""&gt;seetaface · GitHub&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[1] Shuzhe Wu, Meina Kan, Zhenliang He, Shiguang Shan, and Xilin Chen. Funnel-Structured Cascade for Multi-View Face Detection with Alignment-Awareness. Neurocomputing (under review), 2016.&lt;/p&gt;&lt;p&gt;[2] Shengye Yan, Shiguang Shan, Xilin Chen, Wen Gao. Locally Assembled Binary (LAB) Feature for Fast and Accurate Face Detection. IEEE Computer Society International Conference on Computer Vision and Pattern Recognition, CVPR2008, Anchorage, Alaska, U.S.A, Jun. 2008&lt;/p&gt;&lt;p&gt;[3] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua. A convolutional neural network cascade for face detection. CVPR 2015.&lt;/p&gt;&lt;p&gt;[4] Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. From Facial Parts Responses to Face Detection: A Deep Learning Approach. ICCV 2015.&lt;/p&gt;&lt;p&gt;[5] Xuehan Xiong, Fernando De la Torre. Supervised descent method and its applications to face alignment. CVPR 2013&lt;/p&gt;&lt;p&gt;[6] Yi Sun, Xiaogang Wang, Xiaoou Tang. Deep Convolutional Network Cascade for Facial Point Detection. CVPR 2013&lt;/p&gt;&lt;p&gt;[7] Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang. Facial Landmark Detection by Deep Multi-task Learning. ECCV 2014&lt;/p&gt;&lt;p&gt;[8] Jie Zhang, Shiguang Shan, Meina Kan, Xilin Chen. Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment. ECCV 2014&lt;/p&gt;&lt;p&gt;[9] Xin Liu, Meina Kan, Wanglong Wu, Shiguang Shan, Xilin Chen. VIPLFaceNet: An Open Source Deep Face Recognition SDK. Frontier of Computer Science, Accepted&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文欢迎大家转载！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文作者： &lt;/b&gt;VIPL_Face&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325457&amp;amp;idx=1&amp;amp;sn=5fa67f028980b3f451d1e2b568d49cbf&amp;amp;chksm=f235a6dbc5422fcd7eefff058dfaccaeca2b3b0000ccee0edaf523a3db7740967c018cd25d00&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="SeetaFace开源人脸识别引擎介绍"&gt;SeetaFace开源人脸识别引擎介绍&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22451474&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 14 Sep 2016 19:01:31 GMT</pubDate></item><item><title>基于深度学习的商品检索技术</title><link>https://zhuanlan.zhihu.com/p/22451308</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dada18fd517008827e476ec6b0ec4008_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;商品检索是一门综合了物体检测、 图像分类以及特征学习的技术。 近期， 很多研究者成功地将深度学习方法应用到这个领域。 本文对这些方法进行了总结， 然后概括地提出了商品特征学习框架以及垂类数据挖掘方式， 最后介绍了商品检索技术在服装搭配中的应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/p&gt;&lt;p&gt;几年前，当人们还在感叹于网页购物的快速便捷时，各大电商巨头就“悄悄地”将它们的购物应用推广到了用户的手机里。从那一刻起，用户购买的习惯也在悄悄地发生着改变：人们不再局限于时间与地点，只要拥有一部联网的手机，就能轻松获取想要的商品。发展至今，移动设备的安全、高速等特点越来越获得人们的认可，也使得移动购物行为变得更加普遍。然而目前PC和Mobile终端中，用户基本都是通过文本关键词获取目标商品，这种单一的关键词描述有时很难获取用户的真实需求。为此，电商们也进行了很多改进。其中最有效的一些做法是构建高度结构化的后台商品数据库。其目的是能够通过分析用户的查询来推荐一些更加精细粒度、时效性好、热度高的商品品类；并提供给用户一个限定了价格、品牌、风格等等的商品候选集合。这种基于文本的由粗到精的推荐方式， 能够很好的帮助用户定位到具有精细且具体标签的商品。然而，当用户需求的商品的周边信息不明确时，很难通过抽象出有限的关键词来进行检索。这类商品包括：未知品牌的化妆品，样式新颖的家具或者时尚流行的服装等（如图1）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/54a33f139394207606465569287632b5.jpg" data-rawwidth="573" data-rawheight="181"&gt;&lt;b&gt;所见即所得&lt;/b&gt;&lt;p&gt;对于上述的问题，可以用一句话归结为：当需求物品难以用文本量化描述时， 给定它的一张图像，是否有可能推荐给用户相关的商品？ 可以想象这样的场景： 当你看到一件喜欢的物品，只通过手机拍照将其图像上传购物网站，就能获取实物购买信息。如果商品检索能做到这样的“所见即所得”， 必将会给有购物需求的用户带来很大的便捷。“所见”如何才能变成“所得”呢？ 在回答这个问题之前， 首先需要了解商品检索中的难点问题：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.) 商品细品类繁多&lt;/b&gt;&lt;/p&gt;&lt;p&gt;小到柴米油盐，大到家具电器， 都可以称为商品。而且很多商品都包括多级且细致的分类。例如，家具可分为卧室家具、客厅家具、餐厅家具、书房家具等；服装的一级品类包括女装、男装、内衣、配饰与童装童鞋等， 女装又可分为连衣裙、T恤、雪纺衫等； 母婴中的童车童床类别可分为安全座椅、婴儿推车、婴儿床、婴儿床、垫餐、椅学步车等。由此可见， 好的检索技术不仅要识别这么多的商品类别， 并且需要区分每个类别下的不同商品实例； 同时后台商品数据库应该具有很高的覆盖面。图2给出了一个电商网站对商品品类的划分。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b8f1c6d72e9f46d019465031681557f5.jpg" data-rawwidth="680" data-rawheight="358"&gt;&lt;p&gt;&lt;b&gt;2.) 同款与相似款的混淆&lt;/b&gt;&lt;/p&gt;&lt;p&gt;根据多级类目或属性进行商品划分的方式，尽管区分了大多数具有精细语义的商品，但在区分同款与相似款上的作用仍然是有限的，即无法确认两件分为一个类别的商品是相同款。 举例来说，已知两个人都穿着白色短袖圆领T恤， 因为姿态、角度、光照等影响，有可能会使得相似款更像同款，或者同款被误识别为相似款。这就是计算机视觉中经常碰到的类内差异性与类间相似性问题。 图3的例子可以说明这两个问题。 左侧(a)中的上衣是同一款衣服，但由于人体姿态、悬挂方式、手臂遮挡、光线等问题的存在，使得它的颜色以及长度等表观属性具有很大的差异性; 三款相似的黑色印花连衣裙如(b)所示，它们拥有相似的不规则的印花图案，以及黑色的底色和A字裙摆；这些特点都让他们很相似，但从袖型可看出它们非同款。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1da71098a79bae5fded082bfcc44826b.jpg" data-rawwidth="570" data-rawheight="152"&gt;&lt;p&gt;其实，计算机视觉的各个领域都在解决这样的“所见即所得”难题， 即如何让机器能够自动准确的理解图像内容。 随着深度学习的兴起， 包括人脸识别、 图像分类与物体检测在内的方向都取得了很多重要的进展， 也为深度学习在商品检索中的应用奠定了坚实的基础。 &lt;/p&gt;&lt;p&gt;概括的讲， 为达到“所见即所得”的目标， 商品检索技术的框架中需要包含以下三个部分：&lt;/p&gt;&lt;p&gt;1.商品主体检测： 用于自动定位用户感兴趣的商品，去除背景、多主体等因素的影响，也有利于抽取的语义特征的对齐。&lt;/p&gt;&lt;p&gt;2.商品品类识别：通过识别商品的主体的品类， 使得在检索时可以在商品子数据子库进行搜索，提升检索的效果与效率。&lt;/p&gt;&lt;p&gt;3.商品特征表示： 通过学习获得商品主体的判别性特征， 使得同款商品距离更近且非同款商品相距更远； 对光照、姿态、遮挡等变化有一定的鲁棒性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;服饰检索技术回顾&lt;/b&gt;&lt;/p&gt;&lt;p&gt;基于拍照的商品检索问题本质是一个跨域(cross-domain)图像检索问题: 需要根据用户输入的移动拍照图像， 从电商库中获取同款或是非常相似的商品图片列表。这些特点决定了商品检索是一项综合性的图像处理技术——它涉及图像识别、检测、特征学习等各方面的内容。&lt;/p&gt;&lt;p&gt;	其中， 服装垂类检索是商品检索中一个重要的问题。因为服装包含非常多的细品类， 而且存在非常多的视觉变化， 如光照、形变、视角、尺度、背景影响等等。解决服装检索的技术能够很好的被推广到其他垂类上。当前，很多学者和研究机构都尝试基于深度学习进行服装检索技术的探究与创新。 下文将回顾三篇基于深度学习来解决跨域服装检索问题的文章。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.) Where-to-Buy-It (WTBI)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章发表于ICCV2015，作者是来自北卡罗来纳大学教堂山分校的M. Hadi Kiapour。作者把street-to-shop的服装检索场景， 形式化为cross-domain的商品相似度学习问题， 并设计了一种用于特定类别的相似度计算的网络参数学习方式。整个学习流程如图4所示。 首先， 利用裙子、外套、上衣、裤子、裙子等五个主要的商品类别的同款标注图像， 基于cross entropy loss训练一个通用商品的同款判别模型； 然后， 对于特定细分类的商品检索模型学习问题， 采用其对应的同款训练数据进行网络参数微调， 将通用同款模型迁移成特定类别的同款模型。 在进行方法验证时， 文中还收集了40万的电商数据，以及近4万组的street-to-shop的同款商品数据。 实验表明， 通过这种“由粗到细”方式学习到的相似度量网络， 比基于ImageNet训练的深度特征有更好的检索性能。 但此文只基于离线CNN特征学习相似度， 并没有进行端到端的检索模型的探索。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e844aabe074bc8c8ed3741515aae6019.jpg" data-rawwidth="464" data-rawheight="407"&gt;&lt;p&gt;&lt;b&gt;2.) Dual Attribute-aware Ranking Network (DARN)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章发表于ICCV2015，作者是来自新加坡国立大学的 Junshi Huang。此文与WTBI方法相比的不同在于： 在处理街拍场景(street scenario)与电商场景(shopping scenario)服装图像之间的检索问题时， 提出了一种端到端的双路神经网络模型（DARN）来学习深度特征，如图5所示； 其中一路网络学习街拍场景下的服装特征；另一路网络学习电商场景下的服装特征。为了提升图像检索特征的判别能力， 作者还采用了多种标注数据来监督网络的学习过程： 多标签的属性标注与服装同款ID标注。为此，在设计网络损失时， 同时采用了基于多标签属性数据的cross-entropy loss以及服装同款ID数据的triplet loss。总的来看， 网络的输出特征同时隐含了局部语义属性的判别能力以及全局表观的区分性， 在检索效果的提升上具有很好的互补性。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/785977e799ef842250390d3958120642.jpg" data-rawwidth="675" data-rawheight="409"&gt;&lt;p&gt;&lt;b&gt;3.) DeepFashion&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章发表于CVPR2016，作者是来自香港中文大学的Ziwei Liu。为了使服饰识别相关的研究更加贴近实际应用场景， 作者收集了一个规模更大且语义标注更全面的服装数据集DeepFashion； 它在图像数目、类别与属性数目、同款对、位置标定与数据开放等方面都占据优势。其与WTBI和DARN中的数据库对比如表格1所示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8bd1f7f7fcdf74852c30f9ae5119e3ce.jpg" data-rawwidth="584" data-rawheight="205"&gt;此文还提出了一种FashionNet， 融合了大类、属性、服装ID以及关键点四种监督信息来进行服装特征学习。它的创新之处是， 设计了分别对全局表观以及局部部件进行特征学习的网络； 其中的局部网络结构利用了服装局部关键点对卷积特征响应图进行对齐，避免了关键点所在部件的变化带来的影响。整个网络结构如图6所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9708b55ab8586273978c606deb8b922d.jpg" data-rawwidth="501" data-rawheight="301"&gt;&lt;p&gt;&lt;b&gt;4.) 方法总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;复杂体系下的商品类别识别以及检索问题的解决， 不仅在于网络结构的设计，而且需要多种类型的标注数据来约束整个网络的训练； 这些数据包括商品位置、商品类别、 商品属性以及商品同款数据等； 由此， 检索结果与查询图像才能具有全局表观相似性与局部语义一致性。当然， 对于如何结合这些监督数据进行学习仍有待进一步探索； 是否端到端网络的性能一定优于分段学习网络也犹未可知。 与传统方法相比， 此类深层神经网络模型在进行商品检索特征学习时并没有脱离一般图像检索的特征学习框架： 不仅需要在前端进行语义对齐， 也需要在后端提升特征判别性。总的来说， 以上深度学习方法的探索与创新, 都将为商品检索技术趋于实用化打下扎实的基础。&lt;/p&gt;&lt;p&gt;&lt;b&gt;特征学习框架&lt;/b&gt;&lt;/p&gt;&lt;p&gt;尽管以上的论文主要在探究服装类商品的检索技术， 但这些方法在其他的商品垂类上也是适用的。如图7所示, 这些方法可概括成一套特征学习框架，。图中三个部分的意义分别是：   &lt;/p&gt;&lt;p&gt;1. 商品图像预处理。商品有刚体（如鞋子、箱包、化妆品等）与非刚体（如男装、女装、童装等）之分， 姿态、形变、尺寸等差异很大； 因此， 需要采用一定的语义对齐方式使得模型对这些变化鲁棒， 常见操作有商品检测框对齐、旋转对齐、局部关键点对齐等。&lt;/p&gt;&lt;p&gt;2. 全局表观与局部语义特征融合。将一个商品图像映射为一个特征的方法有很多； 为了使得到的特征具有很好的判别性， 多种语义监督信息被用于引导模型的学习。以服装垂类为例， 最终的特征不仅需要区分语义（如服装的袖长、 领型、 扣型等）， 也需要能衡量表观的相似性（如颜色、 纹理等）。 因此， 这类监督数据的收集也是整个特征学习框架的重要组成。&lt;/p&gt;&lt;p&gt;3. 特征降维。 特征的学习是一个精益求精的过程， 维度低且判别性好的特征才能保证检索的性能与效率。用于降维学习的数据一般是商品同款数据； 常用的降维方式有线性判别分析（LDA）、 图像分类与度量学习等。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/dada18fd517008827e476ec6b0ec4008.jpg" data-rawwidth="575" data-rawheight="295"&gt;&lt;p&gt;&lt;b&gt;垂类数据挖掘&lt;/b&gt;&lt;/p&gt;&lt;p&gt;基于这套框架，特征学习就可以依靠大量的标注数据来完成。如何来获取标注数据呢？ 简单粗暴的全量数据标注会非常耗时耗力。 这里针对同款数据与类别数据分别给出了数据挖掘的方法， 如图8所示。 &lt;/p&gt;&lt;p&gt;（a）同款数据挖掘。 基于已有的检索特征模型以及大类属性分类模型， 可以将互联网数据按照类别预测结果进行划分， 并根据子类进行单独的聚类。 对于每个cluster， 根据一些准则（如特征数目、平均距离、距离方差等）来判定噪声并进行筛选； 最后通过人工标注的方式进一步切分每一个cluster来获取同款的商品。 &lt;/p&gt;&lt;p&gt;（b） 类别数据挖掘。 首先， 通过爬虫抓取以及人工构造的方式， 可以获得大量的关键词集合； 并将它们进行多词组合的方式在图像搜索引擎获取top-K的检索结果， 放入类别图像候选集合； 之后， 基于已有的大类属性模型， 对候选集进行提纯， 去除低质量以及语义错误的图像。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/439fb608e1d1fa99f86ad3fe5b608d02.jpg" data-rawwidth="586" data-rawheight="343"&gt;&lt;p&gt;&lt;b&gt;技术应用&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本节介绍一种新商品检索技术应用方向： 服装搭配。 服装搭配是指根据用户给定的一件衣服单品， 推荐出能够与之搭配的时尚款式。它的应用场景包括时尚资讯推荐、电商导购等。由于服饰品类繁多、穿着标准各异， 如何定义并获取时尚的款式以及给用户个性化推荐搭配方案， 都面临很大的挑战。 下文将围绕这两个问题， 介绍一种基于商品检索技术的服饰搭配方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.) 定义时尚款式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;“工欲善其事必先利其器”。在服饰搭配过程中，构建时尚款式的数据库是非常必要的。然而，时尚是一种比较感性的认识， 且人们对于时尚的理解各不相同，“时尚款式”的定义是没有一个统一的量化标准的。下图给出了一些时尚图像的例子， 可以看出, 图像中服装的时尚取决于很多方面： 服装样式、发型、 鞋子、 拍照场景、 身材等等。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b13ac4a43093d8cae1d2c255352f455e.jpg" data-rawwidth="567" data-rawheight="237"&gt;&lt;p&gt;为了解决这个难题，数据来源选自多个顶级时尚网站。这些网站往往通过时尚达人编辑的方式来推荐出时尚图片，确保了服装的时尚性与新颖性； 除此之后， 从视觉上影响图像时尚程度的因素还有很多， 如背景灰暗、T台秀、非全身图、身材差、分辨率低等； 基于这类数据训练低质图片过滤模型， 就能获取最终的高质时尚库。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;2.) 服装搭配技术&lt;/b&gt;&lt;/p&gt;&lt;p&gt;简单讲，服饰搭配就是一种通过用户上衣（下装），推荐时尚下装（上衣）的技术。这里根据优化目标的不同将现有方法分为两大类： 基于上下衣度量学习的方法以及基于相似服饰检索的方法。前者的实现基于不同服装部件的度量学习： 适合搭配的上下装距离应该尽量的近，而不适合搭配的则要尽量的远。 后者假定时尚库的图像拥有优质的搭配, 将用户服装单品输入时尚服装数据库的检索引擎， 获得语义与表观相似的时尚推荐结果。目前，时尚搭配App—&lt;b&gt;FOLLOW&lt;/b&gt;， 采用的就是基于检索技术的解决方案。 图10中个给出了&lt;b&gt;FOLLOW&lt;/b&gt;搭配的效果, 欢迎扫码试用。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/cd2c3cb90a4ca68d4d0e5ffa86969bbd.jpg" data-rawwidth="571" data-rawheight="301"&gt;&lt;p&gt;http://weixin.qq.com/r/TDsyKufEzrqxraNJ925e (二维码自动识别)&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结与展望&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文回顾了基于深度学习的服装检索技术，并且基于这些方法， 概括出一套通用的商品特征学习框架。针对不同种类商品图像的采集， 给出了基于图像搜索引擎的数据挖掘方法。后续仍有待进一步探究的方向包括多品类商品检索技术、基于大规模同款数据的特征学习以及全自动数据挖掘方法等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果. "&gt;@果果是枚开心果. &lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d8a4808f5443f2acca619795f34fdfd0.jpg" data-rawwidth="122" data-rawheight="122"&gt;&lt;b&gt;严灿祥，&lt;/b&gt;硕士毕业于中科院计算所VIPL课题组； 目前就职于百度深度学习研究院。主要从事商品检索技术的研发。所在的识图策略组包括商品搜索、相似搜索、相同搜索与图像猜词等方向。 欢迎有实习意愿的同学投简历至： yancanxiang@baidu.com&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325442&amp;amp;idx=1&amp;amp;sn=999ca95c21c165b6052503ffca4497e1&amp;amp;chksm=f235a6c8c5422fde6528e853e1c1975e84bd9f2a004e204a776d087f5f44976bc2bfaf6b294e&amp;amp;scene=0#wechat_redirect" class="" data-editable="true" data-title="基于深度学习的商品检索技术"&gt;基于深度学习的商品检索技术&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22451308&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Wed, 14 Sep 2016 18:44:21 GMT</pubDate></item><item><title>深度学习在目标跟踪中的应用</title><link>https://zhuanlan.zhihu.com/p/22334661</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f1864dd1c5a52926ab6955233a5422a8_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;/p&gt;&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近年来，深度学习方法在物体跟踪领域有不少成功应用，并逐渐在性能上超越传统方法。本文对现有基于深度学习的目标跟踪算法进行了分类梳理，希望能给读者带来启发。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f1864dd1c5a52926ab6955233a5422a8.jpg" data-rawwidth="554" data-rawheight="200"&gt;&lt;p&gt;开始本文之前，我们首先看上方给出的3张图片，它们分别是同一个视频的第1，40，80帧。在第1帧给出一个跑步者的边框(bounding-box)之后，后续的第40帧，80帧，bounding-box依然准确圈出了同一个跑步者。以上展示的其实就是目标跟踪(visual object tracking)的过程。目标跟踪(特指单目标跟踪)是指：给出目标在跟踪视频第一帧中的初始状态（如位置，尺寸），自动估计目标物体在后续帧中的状态。&lt;/p&gt;&lt;p&gt;人眼可以比较轻松的在一段时间内跟住某个特定目标。但是对机器而言，这一任务并不简单，尤其是跟踪过程中会出现目标发生剧烈形变、被其他目标遮挡或出现相似物体干扰等等各种复杂的情况。过去几十年以来，目标跟踪的研究取得了长足的发展，尤其是各种机器学习算法被引入以来，目标跟踪算法呈现百花齐放的态势。2013年以来，深度学习方法开始在目标跟踪领域展露头脚，并逐渐在性能上超越传统方法，取得巨大的突破。本文首先简要介绍主流的传统目标跟踪方法，之后对基于深度学习的目标跟踪算法进行介绍，最后对深度学习在目标跟踪领域的应用进行总结和展望。&lt;/p&gt;&lt;p&gt;&lt;b&gt;经典目标跟踪方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前跟踪算法可以被分为产生式(generative model)和判别式(discriminative model)两大类别。&lt;/p&gt;&lt;p&gt;产生式方法运用生成模型描述目标的表观特征，之后通过搜索候选目标来最小化重构误差。比较有代表性的算法有稀疏编码(sparse coding)，在线密度估计(online density estimation)和主成分分析(PCA)等。产生式方法着眼于对目标本身的刻画，忽略背景信息，在目标自身变化剧烈或者被遮挡时容易产生漂移。&lt;/p&gt;&lt;p&gt;与之相对的，判别式方法通过训练分类器来区分目标和背景。这种方法也常被称为tracking-by-detection。近年来，各种机器学习算法被应用在判别式方法上，其中比较有代表性的有多示例学习方法(multiple instance learning), boosting和结构SVM(structured SVM)等。判别式方法因为显著区分背景和前景的信息，表现更为鲁棒，逐渐在目标跟踪领域占据主流地位。值得一提的是，目前大部分深度学习目标跟踪方法也归属于判别式框架。&lt;/p&gt;&lt;p&gt;近年来，基于相关滤波(correlation filter)的跟踪方法因为速度快,效果好吸引了众多研究者的目光。相关滤波器通过将输入特征回归为目标高斯分布来训练 filters。并在后续跟踪中寻找预测分布中的响应峰值来定位目标的位置。相关滤波器在运算中巧妙应用快速傅立叶变换获得了大幅度速度提升。目前基于相关滤波的拓展方法也有很多，包括核化相关滤波器(kernelized correlation filter, KCF), 加尺度估计的相关滤波器(DSST)等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;基于深度学习的目标跟踪方法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不同于检测、识别等视觉领域深度学习一统天下的趋势，深度学习在目标跟踪领域的应用并非一帆风顺。其主要问题在于训练数据的缺失：深度模型的魔力之一来自于对大量标注训练数据的有效学习，而目标跟踪仅仅提供第一帧的bounding-box作为训练数据。这种情况下，在跟踪开始针对当前目标从头训练一个深度模型困难重重。目前基于深度学习的目标跟踪算法采用了几种思路来解决这个问题，下面将依据思路的不同展开介绍，并在最后介绍目前跟踪领域出现的运用递归神经网络(recurrent neural network)解决目标跟踪问题的新思路。&lt;/p&gt;&lt;p&gt;&lt;b&gt;利用辅助图片数据预训练深度模型，在线跟踪时微调&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在目标跟踪的训练数据非常有限的情况下，使用辅助的非跟踪训练数据进行预训练，获取对物体特征的通用表示(general representation )，在实际跟踪时，通过利用当前跟踪目标的有限样本信息对预训练模型微调(fine-tune), 使模型对当前跟踪目标有更强的分类性能，这种迁移学习的思路极大的减少了对跟踪目标训练样本的需求，也提高了跟踪算法的性能。&lt;/p&gt;&lt;p&gt;这个方面代表性的作品有DLT和SO-DLT，都出自香港科技大学王乃岩博士。&lt;/p&gt;&lt;p&gt;&lt;b&gt;DLT(NIPS2013)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Learning a Deep Compact Image Representation for Visual Tracking&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/58a57702bf24a90478d8a8585b8b21bf.jpg" data-rawwidth="2398" data-rawheight="1172"&gt;&lt;p&gt;DLT是第一个把深度模型运用在单目标跟踪任务上的跟踪算法。它的主体思路如上图所示：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;  先使用栈式降噪自编码器(stacked denoising autoencoder，SDAE)在Tiny Images dataset这样的大规模自然图像数据集上进行无监督的离线预训练来获得通用的物体表征能力。预训练的网络结构如上图(b)所示，一共堆叠了4个降噪自编码器, 降噪自编码器对输入加入噪声，通过重构出无噪声的原图来获得更鲁棒的特征表达能力。SDAE1024-2560-1024-512-256这样的瓶颈式结构设计也使获得的特征更加compact。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;  之后的在线跟踪部分结构如上图(c)所示，取离线SDAE的encoding部分叠加sigmoid分类层组成了分类网络。此时的网络并没有获取对当前被跟踪物体的特定表达能力。此时利用第一帧获取正负样本，对分类网络进行fine-tune获得对当前跟踪目标和背景更有针对性的分类网络。在跟踪过程中，对当前帧采用粒子滤波(particle filter)的方式提取一批候选的patch(相当于detection中的proposal)，这些patch输入分类网络中，置信度最高的成为最终的预测目标。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt;   在目标跟踪非常重要的模型更新策略上，该论文采取限定阈值的方式，即当所有粒子中最高的confidence低于阈值时，认为目标已经发生了比较大的表观变化，当前的分类网络已经无法适应，需要进行更新。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;DLT作为第一个将深度网络运用于单目标跟踪的跟踪算法，首先提出了“离线预训练＋在线微调”的思路，很大程度的解决了跟踪中训练样本不足的问题，在CVPR2013提出的OTB50数据集上的29个跟踪器中排名第5。&lt;/p&gt;&lt;p&gt;但是DLT本身也存在一些不足：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;  离线预训练采用的数据集Tiny Images dataset只包含32*32大小的图片，分辨率明显低于主要的跟踪序列，因此SDAE很难学到足够强的特征表示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)  &lt;/b&gt; 离线阶段的训练目标为图片重构，这与在线跟踪需要区分目标和背景的目标相差甚大。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)  &lt;/b&gt; SDAE全连接的网络结构使其对目标的特征刻画能力不够优秀，虽然使用了4层的深度模型，但效果仍低于一些使用人工特征的传统跟踪方法如Struck等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;SO-DLT(arXiv2015)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Transferring Rich Feature Hierarchies for Robust Visual Tracking&lt;/b&gt;&lt;/p&gt;&lt;p&gt;SO-DLT延续了DLT利用非跟踪数据预训练加在线微调的策略，来解决跟踪过程中训练数据不足的问题，同时也对DLT存在的问题做了很大的改进。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/da42f3364fd2959784f22a574d4b2e8a.jpg" data-rawwidth="1357" data-rawheight="252"&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   使用CNN作为获取特征和分类的网络模型。如上图所示，SO-DLT使用了的类似AlexNet的网络结构，但是有几大特点：一、针对跟踪候选区域的大小将输入缩小为100*100，而不是一般分类或检测任务中的224*224。 二、网络的输出为50*50大小，值在0-1之间的概率图(probability map)，每个输出像素对应原图2*2的区域，输出值越高则该点在目标bounding-box中的概率也越高。这样的做法利用了图片本身的结构化信息，方便直接从概率图确定最终的bounding-box,避免向网络输入数以百计的proposal，这也是SO-DLT structured output得名的由来。三、在卷积层和全连接层中间采用SPP-NET中的空间金字塔采样（spatial pyramid pooling）来提高最终的定位准确度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt;   在离线训练中使用ImageNet 2014的detection数据集使CNN获得区分object和非object（背景）的能力。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1500c52a2ba599fc90a877c5849f3706.jpg" data-rawwidth="1345" data-rawheight="444"&gt;&lt;p&gt;SO-DLT在线跟踪的pipeline如上图所示:&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)  &lt;/b&gt; 处理第t帧时，首先以第t-1帧的的预测位置为中心，从小到大以不同尺度crop区域放入CNN当中，当CNN输出的probability map的总和高于一定阈值时，停止crop, 以当前尺度作为最佳的搜索区域大小。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;  选定第t帧的最佳搜索区域后，在该区域输出的probability map上采取一系列策略确定最终的bounding-box中心位置和大小。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt;   在模型更新方面，为了解决使用不准确结果fine-tune导致的drift问题,使用了long-term 和short-term两个CNN，即CNNs和CNNl。CNNs更新频繁，使其对目标的表观变化及时响应。CNNl更新较少，使其对错误结果更加鲁棒。二者结合，取最confident的结果作为输出。从而在adaptation和drift之间达到一个均衡。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;SO-DLT作为large-scale CNN网络在目标跟踪领域的一次成功应用，取得了非常优异的表现：在CVPR2013提出的OTB50数据集上OPE准确度绘图(precision plot)达到了0.819, OPE成功率绘图(success plot)达到了0.602。远超当时其它的state of the art。&lt;/p&gt;&lt;p&gt;SO-DLT有几点值得借鉴：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   针对tracking问题设计了有针对性的网络结构。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt;   应用CNNS和CNNL用ensemble的思路解决update 的敏感性，特定参数取多值做平滑，解决参数取值的敏感性。这些措施目前已成为跟踪算法提高评分的杀手锏。&lt;/p&gt;&lt;p&gt;但是SO－DLT离线预训练依然使用的是大量无关联图片，作者认为使用更贴合跟踪实质的时序关联数据是一个更好的选择。&lt;/p&gt;&lt;p&gt;&lt;b&gt;利用现有大规模分类数据集预训练的CNN分类网络提取特征&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2015年以来，在目标跟踪领域应用深度学习兴起了一股新的潮流。即直接使用ImageNet这样的大规模分类数据库上训练出的CNN网络如VGG-Net获得目标的特征表示，之后再用观测模型(observation model)进行分类获得跟踪结果。这种做法既避开了跟踪时直接训练large-scale CNN样本不足的困境，也充分利用了深度特征强大的表征能力。这样的工作在ICML15，ICCV15，CVPR16均有出现。下面介绍两篇发表于ICCV15的工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;FCNT(ICCV15)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visual Tracking with Fully Convolutional Networks&lt;/b&gt;&lt;/p&gt;&lt;p&gt;作为应用CNN特征于物体跟踪的代表作品，FCNT的亮点之一在于对ImageNet上预训练得到的CNN特征在目标跟踪任务上的性能做了深入的分析,并根据分析结果设计了后续的网络结构。&lt;/p&gt;&lt;p&gt;FCNT主要对VGG-16的Conv4-3和Conv5-3层输出的特征图谱（feature map）做了分析,并得出以下结论：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)  &lt;/b&gt; CNN 的feature map可以用来做跟踪目标的定位。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)  &lt;/b&gt; CNN 的许多feature map存在噪声或者和物体跟踪区分目标和背景的任务关联较小。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) &lt;/b&gt;  CNN不同层的特征特点不一。高层(Conv5-3)特征擅长区分不同类别的物体，对目标的形变和遮挡非常鲁棒，但是对类内物体的区分能力非常差。低层(Conv4-3)特征更关注目标的局部细节，可以用来区分背景中相似的distractor，但是对目标的剧烈形变非常不鲁棒。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e9b30b24e85c54bd027d0c7290fe5cdc.png" data-rawwidth="2366" data-rawheight="690"&gt;&lt;p&gt;依据以上分析，FCNT最终形成了如上图所示的框架结构：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   对于Conv4-3和Conv5-3特征分别构建特征选择网络sel-CNN(1层dropout加1层卷积)，选出和当前跟踪目标最相关的feature map channel。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)  &lt;/b&gt; 对筛选出的Conv5-3和Conv4-3特征分别构建捕捉类别信息的GNet和区分distractor(背景相似物体)的SNet(都是两层卷积结构)。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)  &lt;/b&gt; 在第一帧中使用给出的bounding-box生成热度图(heat map)回归训练sel-CNN, GNet和SNet。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4)  &lt;/b&gt; 对于每一帧，以上一帧预测结果为中心crop出一块区域，之后分别输入GNet和SNet，得到两个预测的heatmap,并根据是否有distractor决定使用哪个heatmap 生成最终的跟踪结果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;FCNT根据对CNN不同层特征的分析，构建特征筛选网络和两个互补的heat-map预测网络。达到有效抑制distractor防止跟踪器漂移，同时对目标本身的形变更加鲁棒的效果，也是ensemble思路的又一成功实现。在CVPR2013提出的OTB50数据集上OPE准确度绘图(precision plot)达到了0.856,OPE成功率绘图(success plot)达到了0.599，准确度绘图有较大提高。实际测试中FCNT的对遮挡的表现不是很鲁棒，现有的更新策略还有提高空间。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Hierarchical Convolutional Features for Visual Tracking(ICCV15)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇是作者在2015年度看到的最简洁有效的利用深度特征做跟踪的论文。其主要思路是提取深度特征，之后利用相关滤波器确定最终的bounding-box。&lt;/p&gt;&lt;p&gt;这篇论文简要分析了VGG-19特征( Conv3_4, Conv4_4, Conv5_4 )在目标跟踪上的特性，得出的结论和FCNT有异曲同工之处，即：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)  &lt;/b&gt; 高层特征主要反映目标的语义特性，对目标的表观变化比较鲁棒。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)  &lt;/b&gt; 低层特征保存了更多细粒度的空间特性，对跟踪目标的精确定位更有效。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/bdf1bb043ba3c8b579fe4ccebac1d1ec.png" data-rawwidth="1428" data-rawheight="806"&gt;&lt;p&gt;基于以上结论，作者给出了一个粗粒度到细粒度(coarse-to-fine)的跟踪算法即：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1) &lt;/b&gt;  第一帧时，利用Conv3_4,Conv4_4,Conv5_4特征的插值分别训练得到3个相关滤波器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt;   之后的每帧，以上一帧的预测结果为中心crop出一块区域，获取三个卷积层的特征，做插值，并通过每层的相关滤波器预测二维的confidence score。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)  &lt;/b&gt; 从Conv5_4开始算出confidence score上最大的响应点，作为预测的bounding-box的中心位置，之后以这个位置约束下一层的搜索范围，逐层向下做更细粒度的位置预测,以最低层的预测结果作为最后输出。具体公式如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d1b1c59cf3b8e80cd5989c3f5bcfd73c.jpg" data-rawwidth="256" data-rawheight="76"&gt;&lt;p&gt;&lt;b&gt;(4) &lt;/b&gt;  利用当前跟踪结果对每一层的相关滤波器做更新。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;这篇文章针对VGG-19各层特征的特点，由粗粒度到细粒度最终准确定位目标的中心点。在CVPR2013提出的OTB50数据集上OPE准确度绘图达到了0.891,OPE成功率绘图达到了0.605，相较于FCNT和SO-DLT都有提高，实际测试时性能也相当稳定，显示出深度特征结合相关滤波器的巨大优势。但是这篇文章中的相关滤波器并没有对尺度进行处理，在整个跟踪序列中都假定目标尺度不变。在一些尺度变化非常剧烈的测试序列上如CarScale上最终预测出的bounding-box尺寸大小和目标本身大小相差较大。&lt;/p&gt;&lt;p&gt;以上两篇文章均是应用预训练的CNN网络提取特征提高跟踪性能的成功案例，说明利用这种思路解决训练数据缺失和提高性能具有很高的可行性。但是分类任务预训练的CNN网络本身更关注区分类间物体，忽略类内差别。目标跟踪时只关注一个物体，重点区分该物体和背景信息，明显抑制背景中的同类物体，但是还需要对目标本身的变化鲁棒。分类任务以相似的一众物体为一类，跟踪任务以同一个物体的不同表观为一类，使得这两个任务存在很大差别，这也是两篇文章融合多层特征来做跟踪以达到较理想效果的动机所在。&lt;/p&gt;&lt;p&gt;&lt;b&gt;利用跟踪序列预训练，在线跟踪时微调&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1和2中介绍的解决训练数据不足的策略和目标跟踪的任务本身存在一定偏离。有没有更好的办法呢？VOT2015冠军MDNet给出了一个示范。该方法在OTB50上也取得了OPE准确度绘图0.942,OPE成功率绘图0.702的惊人得分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;MDNet(CVPR2016)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Learning Multi-Domain Convolutional Neural Networks for Visual Tracking&lt;/b&gt;&lt;/p&gt;&lt;p&gt;意识到图像分类任务和跟踪之间存在巨大差别，MDNet提出直接用跟踪视频预训练CNN获得general的目标表示能力的方法。但是序列训练也存在问题，即不同跟踪序列跟踪目标完全不一样，某类物体在一个序列中是跟踪目标，在另外一个序列中可能只是背景。不同序列中目标本身的表观和运动模式、环境中光照、遮挡等情形相差甚大。这种情况下，想要用同一个CNN完成所有训练序列中前景和背景区分的任务，困难重重。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d8391f490e4a9f8c9a8838495958b94a.png" data-rawwidth="1406" data-rawheight="564"&gt;&lt;p&gt;最终MDNet提出Multi-Domain的训练思路和如上图所示的Multi-Domain Network。该网络分为共享层和domain-specific层两部分。即:将每个训练序列当成一个单独的domain,每个domain都有一个针对它的二分类层(fc6)，用于区分当前序列的前景和背景，而网络之前的所有层都是序列共享的。这样共享层达到了学习跟踪序列中目标general的特征表达的目的，而domain-specific层又解决了不同训练序列分类目标不一致的问题。&lt;/p&gt;&lt;p&gt;具体训练时，MDNet的每个mini-batch只由一个特定序列的训练数据构成，只更新共享层和针对当前序列的特定fc6层。这样共享层中获得了对序列共有特征的表达能力，如对光照、形变等的鲁棒性。MDNet的训练数据也非常有意思，即测试OTB100数据集时，利用VOT2013－2015的不重合的58个序列来做预训练。测试VOT2014数据集时，利用OTB100上不重合的89个序列做预训练。这种交替利用的思路也是第一次在跟踪论文中出现。&lt;/p&gt;&lt;p&gt;在线跟踪阶段针对每个跟踪序列，MDNet主要有以下几步：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   随机初始化一个新的fc6层。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;  使用第一帧的数据来训练该序列的bounding box回归模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt;   用第一帧提取正样本和负样本，更新fc4, fc5和fc6层的权重。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4) &lt;/b&gt;  之后产生256个候选样本，并从中选择置信度最高的，之后做bounding-box regression得到最终结果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(5)  &lt;/b&gt; 当前帧最终结果置信度较高时，采样更新样本库，否则根据情况对模型做短期或者长期更新。&lt;/p&gt;&lt;p&gt;MDNet有两点值得借鉴之处：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   MDNet应用了更为贴合跟踪实质的视频数据来做训练，并提出了创新的Multi-domain训练方法和训练数据交叉运用的思路。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;  此外MDNet从检测任务中借鉴了不少行之有效的策略，如难例挖掘(hard negative mining)，bounding box回归等。尤其是难例回归通过重点关注背景中的难点样本（如相似物体等）显著减轻了跟踪器漂移的问题。这些策略也帮助MDNet在TPAMI2015 OTB100数据集上OPE准确度绘图从一开始的0.825提升到0.908, OPE成功率绘图从一开始的0.589提升到0.673。&lt;/p&gt;&lt;p&gt;但是也可以发现MDNet的总体思路和RCNN比较类似，需要前向传递上百个proposal,虽然网络结构较小，速度仍较慢。且boundingbox回归也需要单独训练，因此MDNet还有进一步提升的空间。&lt;/p&gt;&lt;p&gt;&lt;b&gt;运用递归神经网络进行目标跟踪的新思路&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近年来RNN尤其是带有门结构的LSTM，GRU等在时序任务上显示出了突出的性能。不少研究者开始探索如何应用RNN来做解决现有跟踪任务中存在的问题，以下简要介绍两篇在这方面比较有代表性的探索文章。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RTT(CVPR16)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Recurrently Target-Attending Tracking&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章的出发点比较有意思，即利用多方向递归神经网络(multi-directional recurrent neural network)来建模和挖掘对整体跟踪有用的可靠目标部分(reliable part)，实际上是二维平面上的RNN建模，最终解决预测误差累积和传播导致的跟踪漂移问题。其本身也是对part-based跟踪方法和相关滤波(correlation filter)方法的改进和探索。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6df1c33eaa0b357abec13c711d9fcddd.png" data-rawwidth="2222" data-rawheight="790"&gt;&lt;p&gt;RTT的整体框架如上图所示：&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt;   首先对每一帧的候选区域进行网状分块，对每个分块提取HOG特征，最终相连获得基于块的特征&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/936368d0e7239fb932d1bf744cdaf5bf.jpg" data-rawwidth="133" data-rawheight="32"&gt;&lt;p&gt;&lt;b&gt;(2) &lt;/b&gt;  得到分块特征以后，RTT利用前5帧训练多方向RNN来学习分块之间大范围的空间关联。&lt;/p&gt;&lt;p&gt;通过在4个方向上的前向推进，RNN计算出每个分块的置信度，最终每个块的预测值组成了整个候选区域的置信图（confidence map）。受益于RNN的recurrent结构，每个分块的输出值都受到其他关联分块的影响，相比于仅仅考虑当前块的准确度更高，避免单个方向上遮挡等的影响，增加可靠目标部分在整体置信图中的影响。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3) &lt;/b&gt;  由RNN得出置信图之后，RTT执行了另外一条pipeline。即训练相关滤波器来获得最终的跟踪结果。值得注意的是，在训练过程中RNN的置信图对不同块的filter做了加权，达到抑制背景中的相似物体，增强可靠部分的效果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4)&lt;/b&gt;   RTT提出了一个判断当前跟踪物体是否被遮挡的策略，用其判断是否更新。即计算目标区域的置信度和，并与历史置信度和的移动平均数(moving average)做一个对比，低于一定比例，则认为受到遮挡，停止模型更新，防止引入噪声。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;RTT是第一个利用RNN来建模part-based跟踪任务中复杂的大范围关联关系的跟踪算法。在CVPR2013提出的OTB50数据集上OPE准确度绘图为0.827,OPE成功率绘图达到了0.588。相比于其他基于传统特征的相关滤波器算法有较大的提升，说明RNN对关联关系的挖掘和对滤波器的约束确实有效。RTT受制于参数数目的影响，只选用了参数较少的普通RNN结构（采用HOG特征其实也是降低参数的另外一种折中策略）。结合之前介绍的解决训练数据缺失的措施，RTT可以运用更好的特征和RNN结构，效果还有提升空间。&lt;/p&gt;&lt;p&gt;&lt;b&gt;DeepTracking: Seeing Beyond Seeing Using Recurrent Neural Networks(AAAI16)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇文章的应用场景是机器人视觉，目标是将传感器获得的有遮挡的环境信息还原为真实的无遮挡的环境信息。严格来说这篇文章仅输出还原后的图片，没有明确预测目标的位置和尺寸等状态信息，和之前介绍的所有文章的做法都不一样，不妨称为一种新的跟踪任务。&lt;/p&gt;&lt;p&gt;在模型方面，不同于RTT用RNN建模二维平面关联，DeepTracking利用RNN来做序列关联的建模，并最终实现了端到端的跟踪算法。&lt;/p&gt;&lt;p&gt;传统的贝叶斯跟踪方法一般采用高斯分布(卡尔曼滤波Kalman filter)或者离散的采样点权重（粒子滤波particle filter）来近似需要求解的后验概率 &lt;i&gt;P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;|&lt;i&gt;x&lt;/i&gt;1:&lt;i&gt;t&lt;/i&gt;) (&lt;i&gt;yt &lt;/i&gt;为需要预测的机器人周围的真实场景,&lt;i&gt; xt&lt;/i&gt; 为传感器直接获得的场景信息)，其表达能力有限。DeepTracking拓展了传统的贝叶斯跟踪框架，并利用RNN强大的表征能力来建模后验概率。&lt;/p&gt;&lt;p&gt;具体而言DeepTracking引入了一个具有马尔可夫性质的隐变量 &lt;i&gt;ht&lt;/i&gt; ，认为其反映了真实环境的全部信息。最终需要预测的&lt;i&gt; yt&lt;/i&gt; 包含了 &lt;i&gt;ht&lt;/i&gt;，包含了&lt;i&gt; ht &lt;/i&gt;的部分信息，可由&lt;i&gt; ht &lt;/i&gt;得到。假设&lt;i&gt; Bt &lt;/i&gt;为关于 &lt;i&gt;ht &lt;/i&gt;的信念(belief),对应于后验概率：&lt;i&gt;Bel&lt;/i&gt;(&lt;i&gt;ht&lt;/i&gt;) = &lt;i&gt;P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;|&lt;i&gt;ht&lt;/i&gt;) 。之后经典贝叶斯跟踪框架中由 &lt;i&gt;P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;-1|&lt;i&gt;x&lt;/i&gt;1:&lt;i&gt;t&lt;/i&gt;-1) 到&lt;i&gt; P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;|&lt;i&gt;x&lt;/i&gt;1:&lt;i&gt;t&lt;/i&gt;) 到的时序更新在这里转化为：&lt;i&gt;Bt &lt;/i&gt;= &lt;i&gt;F&lt;/i&gt;(&lt;i&gt;Bt&lt;/i&gt;-1,&lt;i&gt;xt&lt;/i&gt;）和 &lt;i&gt;P&lt;/i&gt;(&lt;i&gt;ty&lt;/i&gt;|&lt;i&gt;x&lt;/i&gt;1:&lt;i&gt;t&lt;/i&gt;) = &lt;i&gt;P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;|&lt;i&gt;Bt&lt;/i&gt;)。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4208fb7f75b2c45d69ffa2a6b2354429.jpg" data-rawwidth="671" data-rawheight="455"&gt;&lt;p&gt;给出形式表达之后的关键是，如何将其对应到RNN的框架中去。DeepTracking的核心思路是用利用两个权重 &lt;i&gt;WF&lt;/i&gt; 和 &lt;i&gt;WP&lt;/i&gt; 来分别建模 &lt;i&gt;F&lt;/i&gt;(&lt;i&gt;Bt&lt;/i&gt;-1, &lt;i&gt;xt&lt;/i&gt;)和&lt;i&gt;P&lt;/i&gt;(&lt;i&gt;yt&lt;/i&gt;|&lt;i&gt;Bt&lt;/i&gt; )，将&lt;i&gt;Bt&lt;/i&gt; 定义为RNN时序之间传递的memory 信息。此时，如上图所示RNN的各个状态和推进流程就和跟踪任务完美的对接上了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7da6238431a2882ecddb1d4d43587dc1.png" data-rawwidth="1450" data-rawheight="858"&gt;实验部分，DeepTracking采用模拟的2维传感器数据和如上图所示的3层RNN的网络结构，&lt;i&gt;Bt &lt;/i&gt;对应于第三层的网络输出。通过无监督的预测 &lt;i&gt;xt&lt;/i&gt;+&lt;i&gt;n&lt;/i&gt; 的任务来使网络获得预测 &lt;i&gt;yt &lt;/i&gt;的潜在能力。&lt;p&gt;&lt;b&gt;小结：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DeepTracking作为用RNN建模跟踪时序任务的作品，其亮点主要在对RNN和贝叶斯框架融合的理论建模上。实验展示了该方法在模拟场景下的不错效果，但是模拟数据和真实场景差距很大，能否在实际应用中有比较好的表现还有待商榷。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文介绍了深度学习在目标跟踪领域应用的几种不同思路。三种解决训练数据缺失的思路各有千秋，作者认为使用序列预训练的方法更贴合跟踪任务的本质因此值得关注（近期也有应用Siamese Network和视频数据训练的跟踪算法涌现，具体参见王乃岩博士在VLASE公众号上的介绍文章《Object Tracking新思路》）。总的来说，基于RNN的目标跟踪算法还有很大提升空间。此外，目前已有的深度学习目标跟踪方法还很难满足实时性的要求，如何设计网络和跟踪流程达到速度和效果的提升，还有很大的研究空间。&lt;/p&gt;&lt;p&gt;&lt;b&gt;致谢：&lt;/b&gt;本文作者特此感谢匿名审稿人和图森科技首席科学家王乃岩博士对本文所提出的建设性意见。&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" class="" data-editable="true" data-title="@果果是枚开心果. "&gt;@果果是枚开心果. &lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c9a077045fbf8071680a106b6ed07e08.jpg" data-rawwidth="116" data-rawheight="120"&gt;徐霞清，&lt;/b&gt;中国科学院计算技术研究所VIPL组硕士生，导师常虹副研究员。研究方向为深度学习与计算机视觉（目标跟踪等），个人邮箱：xiaqing.xu@vipl.ict.ac.cn&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325425&amp;amp;idx=1&amp;amp;sn=3f3a9ce9aef82af63b28286081c93386#rd" data-editable="true" data-title="深度学习在目标跟踪中的应用" class=""&gt;深度学习在目标跟踪中的应用&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22334661&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Tue, 06 Sep 2016 17:31:52 GMT</pubDate></item><item><title>美国人文与科学院Poggio院士谈神经科学与人工智能</title><link>https://zhuanlan.zhihu.com/p/22282572</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/bc3055c293d2920aecdd51a79ddae84e_r.png"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;CCAI2016大会期间，美国人文与科学院Poggio院士介绍了从脑启发的视觉认知模型H-max到深度学习的变迁，从宏观维度上阐述了神经科学与深度学习研究的紧密联系，并通过介绍CBMM的组织结构与研究目标，展望了神经科学与智能科学研究相互融合的未来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;导读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Tomaso A. Poggio教授任职于MIT麦戈文脑、计算机科学与人工智能实验室，同时他也是美国人文与科学院院士。Poggio院士长期从事脑认知科学方面的研究，他所带领的实验室认为不论从自然角度还是人工角度，学习一直都是解决智能问题的核心。学习是了解大脑运转规律的途径，进而才可能制造智能的机器。故此，该实验室采用多学科融合的方式研究大脑学习的相关问题。目前该实验室的研究重点为大脑运作机制以及如何把该原理结合统计学运用于数学和计算机科学。&lt;/p&gt;&lt;p&gt;本次CCAI2016大会，Poggio院士带来的报告题目是《The Science and The Engineering of Intelligence》，系统阐释了智能科学与智能工程的概念，并提出智能工程的发展依赖于智能科学基础研究的进步。同时他也介绍了MIT CBMM（the MIT Center of Brain, Minds and Machines）的研究目标——促进神经科学、计算机科学、机器学习多领域协同发展。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/7de3f9ec9afab6eb5468beabecd9ed10.jpg" data-rawwidth="300" data-rawheight="444"&gt;&lt;p&gt;图1. Poggio教授主页上的照片&lt;/p&gt;&lt;p&gt;&lt;b&gt;背景回顾：Hubel &amp;amp; Wiesel的发现&lt;/b&gt;&lt;/p&gt;&lt;p&gt;回顾历史，今天大红大紫的深度学习模型的神经科学鼻祖可以追溯到 Hubel和Wiesel 于1959年通过研究猫的视觉皮层感受野提出的视觉神经系统的层级结构模型 ，即从简单细胞到复杂细胞、超复杂细胞的层级信息处理结构。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/88be9a86b8d92fa0cb2fc5835472d5ab.jpg" data-rawwidth="510" data-rawheight="169"&gt;Hubel和Wiesel的研究成果在1981年获得诺贝尔生理学或医学奖，获奖理由是“their discoveries concerning information processing in the visual system”, 即他们关于觉系统信息处理机制的发现。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4b629927d28053a47c1176b37c6f98ac.jpg" data-rawwidth="748" data-rawheight="667"&gt;&lt;p&gt;图3. Hubel和Wiesel在1981年获得诺贝尔奖之后进行庆祝&lt;/p&gt;&lt;p&gt;&lt;b&gt;2 H-MAX与大脑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1999年，在LeCun发明LeNet之后的一年，人工神经网络的研究进入了第二次凛冽的寒冬。Poggio另辟蹊径，提出了大脑皮层中物体识别的层级模型H-max。简单来说，H-max是一个生物启发的计算机视觉模型，一定程度上受到了Hubel &amp;amp; Wiesel模型的启发。 H-max模型的一种实现是将简单细胞S1实现为Gabor滤波器，复杂细胞C1实现为Max-Pooling，组合特征细胞S2实现为加权和，复杂组合特征细胞C2实现为Max-Pooling。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6d26ef45c3b668010dd2afe06dc943c0.png" data-rawwidth="642" data-rawheight="496"&gt;&lt;p&gt;图4. H-max模型示意图，摘自&lt;a href="http://maxlab.neuro.georgetown.edu/hmax.html"&gt;http://maxlab.neuro.georgetown.edu/hmax.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;实际上，H-max模型的层级结构与大脑中的视觉通路具有一定的相似性。在大脑中存在一个从视网膜到LGN（侧膝体）再到初级视觉皮层最后到高级功能区的一个视觉通路，如下图所示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/51a8728f86ab9fa862211e2726481026.png" data-rawwidth="1202" data-rawheight="668"&gt;&lt;p&gt;图5. 大脑视觉通路图解，摘自北京大学黄铁军教授讲座PPT&lt;/p&gt;&lt;p&gt;在Poggio院士的讲座中，给出了一张利用大脑中的ventral stream（腹通路，被认为和视觉通路紧密相关）来解释H-max有效性的图示，见下图。需要指出的是，虽然从结构和功能上H-max都和大脑的视觉通路有很多相似性，但是简单的将H-max理解为模拟了大脑的视觉机制并不准确。大脑的结构和机制更加复杂，比如注意力机制、反馈机制、更复杂的神经元连接等，这些都是H-max模型未能建模的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/0e728ac3399f3d47f5822891be582ecd.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;图6. 借助大脑的Ventral Stream机制解释层级的前向视觉模型的有效性&lt;/p&gt;&lt;p&gt;&lt;b&gt;从Hubel-Wiesel的发现到深度学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Poggio院士在讲座中总结了从Hubel-Wiesel的发现到深度学习（主要是卷积网络）的发展历史。领域前辈的智慧在这一页PPT中熠熠发光。这里面代表性的工作有福岛邦彦的神经认知机Neocognitron，有LeCun的LeNet，Poggio的H-max，以及2011年Freeman和Simoncelli发表在Nature上的工作Metamers of the ventral stream等。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/584e54d24e81fef0ae3bb143e8962348.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;图7. 从Hubel-Wiesel的发现到卷积网络&lt;/p&gt;&lt;p&gt;万变不离其宗，Poggio梳理出了视觉皮层、视觉模型与深度学习网络之间的共性：层级结构。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d63994b194d95422ecc72a3fd7eb6852.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;图8. 层级结构是H-max和深度学习的共同信仰&lt;/p&gt;&lt;p&gt;&lt;b&gt;CBMM的终极理想&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Poggio院士近年来的工作重心放在了CBMM （Center of Brain, Mind and Machine）研究中心。CBMM位于MIT，拥有堪称豪华的研究阵容，研究人员来自Caltech、NYU、MIT、Mobileye和Deepmind等知名研究机构。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6e70b909c0d51c854c5e7773d7a1491c.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;图9. 堪称豪华的CBMM研究团队&lt;/p&gt;&lt;p&gt;CBMM的主要使命是推动对智能的理解——理解大脑如何产生心智、大脑如何工作和如何构建智能机器。CBMM的主要目标是在理解智能的科学上取得进步从而更好的进行智能的工程化。Poggio院士的理想和信仰，坐落在神经科学与人工智能融合的王冠之上。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5ad6f5721e6bd746b2667b644b481c27.jpg" data-rawwidth="1280" data-rawheight="960"&gt;&lt;p&gt;图10. CBMM打通神经科学与人工智能的终极理想&lt;/p&gt;&lt;p&gt;&lt;b&gt;致谢：&lt;/b&gt;本文作者在此感谢CSDN的赠票和优秀的会议组织，以及深度学习大讲堂的特邀记者邀约。此外，感谢中科院计算所博士生邬书哲对本文技术内容的修订。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考资料：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[1] &lt;/b&gt;&lt;a href="http://cbcl.mit.edu/people/poggio/poggio-new.htm" class=""&gt;http://cbcl.mit.edu/people/poggio/poggio-new.htm&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[2] &lt;/b&gt;Riesenhuber, M. &amp;amp; Poggio, T. (1999). Hierarchical Models of Object Recognition in Cortex. Nature Neuroscience 2: 1019-1025.&lt;/p&gt;&lt;p&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;a href="http://cbmm.mit.edu/" class=""&gt;http://cbmm.mit.edu/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果. " class=""&gt;@果果是枚开心果. &lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d48e3f42e09143f4aa45dabf5e65c761.jpg" data-rawwidth="123" data-rawheight="122"&gt;&lt;p&gt;&lt;b&gt;尚静，&lt;/b&gt;毕业于复旦大学金融工程专业，现任紫牛基金投资经理。90后野生动物一只，认知神经科学与人工智能学习者与爱好者，欢迎高智商理工生一起交流，请注明自身特质和兴趣点，微信号jingzi12300。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325367&amp;amp;idx=1&amp;amp;sn=5995e48a6913112c61a823a75e1bed22&amp;amp;scene=0#wechat_redirect" data-editable="true" data-title="美国人文与科学院Poggio院士谈神经科学与人工智能"&gt;美国人文与科学院Poggio院士谈神经科学与人工智能&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22282572&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Fri, 02 Sep 2016 15:00:05 GMT</pubDate></item><item><title>近期GAN的模型和理论发展</title><link>https://zhuanlan.zhihu.com/p/22265724</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/a07e88c3e83f8389f66a8dd9c5e85825_r.jpg"&gt;&lt;/p&gt;深度学习大讲堂致力于推送人工智能，深度学习方面的最新技术，产品以及活动。请关注我们的知乎专栏！&lt;p&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在过去一两年中，生成式模型 Generative Adversarial Networks（GAN）的新兴为生成式任务带来了不小的进展。尽管 GAN 在被提出时存在训练不稳定等诸多问题，但后来的研究者们分别从模型、训练技巧和理论等方面对它做了改进。本文旨在梳理这些相关工作。&lt;/p&gt;&lt;p&gt;尽管大部分时候，有监督学习比无监督的能获得更好的训练效果。但真实世界中，有监督学习需要的数据标注（label）是相对少的。所以研究者们从未放弃去探索更好的无监督学习策略，希望能从海量的无标注数据中学到对于这个真实世界的表示（representation）甚至知识，从而去更好地理解我们的真实世界。&lt;/p&gt;&lt;p&gt;评价无监督学习好坏的方式有很多，其中生成任务就是最直接的一个。只有当我们能生成/创造我们的真实世界，才能说明我们是完完全全理解了它。然而，生成任务所依赖的生成式模型（generative models）往往会遇到两大困难。首先是我们需要大量的先验知识去对真实世界进行建模，其中包括选择什么样的先验、什么样的分布等等。而建模的好坏直接影响着我们的生成模型的表现。另一个困难是，真实世界的数据往往很复杂，我们要用来拟合模型的计算量往往非常庞大，甚至难以承受。&lt;/p&gt;&lt;p&gt;而在过去一两年中，有一个让人兴奋的新模型，则很好地避开了这两大困难。这个模型叫做 Generative Adversarial Networks（GAN），由 [1] 提出。在原始的 GAN paper [1] 中，作者是用博弈论来阐释了 GAN 框架背后的思想。每一个 GAN 框架，都包含着一对模型 —— 一个生成模型（G）和一个判别模型（D）。因为 D 的存在，才使得 GAN 中的 G 不再需要对于真实数据的先验知识和复杂建模，也能学习去逼近真实数据，最终让其生成的数据达到以假乱真的地步 —— D 也无法分别 —— 从而 G 和 D 达到了某种纳什均衡。[1] 的作者曾在他们的 slides 中，给出过一个比喻：在 GAN 中，生成模型（G）和判别模型（D）是小偷与警察的关系。G 生成的数据，目标是要骗过身为警察的判别模型（D）。也就是说，G 作为小偷，要尽可能地提高自己的偷窃手段，而 D 作为警察也要尽可能地提高自己的业务水平防止被欺骗。所以，GAN 框架下的学习过程就变成了一种生成模型 （G） 和判别模型 （D） 之间的竞争过程 —— 随机从真实样本和由生成模型 （G） 生成出的 “假样本” 中取一个，让判别模型 （D） 去判断是否为真。所以，体现在公式上，就是下面这样一个 minmax 的形式。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1018e170944090c4e00a4e619b4460df.jpg" data-rawwidth="643" data-rawheight="53"&gt;&lt;p&gt;然而，GAN 虽然不再需要预先建模，但这个优点同时也带来了一些麻烦。那就是尽管它用一个 noise z 作为先验，但生成模型如何利用这个 z，是无法控制的。也就是说，GAN 的学习模式太过于自由了，使得 GAN 的训练过程和训练结果很多时候都不太可控。为了稳定 GAN ，后来的研究者们分别从 heuristic 、 模型改进和理论分析的角度上提出了许多训练技巧和改进方法。&lt;/p&gt;&lt;p&gt;比如在原始 GAN 论文 [1] 中，每次学习参数的更新过程，被设为 D 更新 k 回， G 才更新 1 回，就是出于减少 G 的 “自由度” 的考虑。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/f86fa2f7d52336cb1ebc95ce4c2bb888.jpg" data-rawwidth="663" data-rawheight="382"&gt;&lt;p&gt;另一篇重量级的关于 GAN 训练技巧的研究的工作便是 Deep Convolutional Generative Adversarial Networks（DCGAN）[6] 。[6] 中总结了许多对于 GAN 这的网络结构设计和针对 CNN 这种网络的训练经验。比如，他们用 strided convolutional networks 替代传统 CNN 中的 pooling 层，从而将 GAN 中的生成模型 （G）变成了 fully differentiable 的，结果使得 GAN 的训练更加稳定和可控。&lt;/p&gt;&lt;p&gt;为了提高训练的稳定性，另一个很自然的角度就是改变学习方法。把纯无监督的 GAN 变成半监督或者有监督的。这便可以为 GAN 的训练加上一点点束缚，或者说加上一点点目标。[2] 中提出的 Conditional Generative Adversarial Nets （CGAN）便是十分直接的模型改变，在生成模型（G）和判别模型（D）的建模中均引入 conditional variable y，这个 y 就是数据的一种 label。也因此，CGAN 可以看做把无监督的 GAN 变成有监督的模型的一种改进。这个简单直接的改进被证明非常有效，并广泛用于后续的相关工作中。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/3b6c35178f34a004368770cda3ea41cd.jpg" data-rawwidth="696" data-rawheight="363"&gt;第三种改进 GAN 过于自由的思路，和第一种会比较相似。既然太难控制 GAN 的学习，不如我们就拆解一下，不要让 GAN 一次学完全部的数据，而是让 GAN 一步步完成这个学习过程。具体到图片生成来说就是，不要让 GAN 中的生成模型（G）每次都直接生成一整张图片，而是让它生成图片的一部分。这个思想可以认为是 DeepMind 也很有名的工作 DRAW 的一种变形。DRAW 的论文 [3] 开篇就说，我们人类在绘制一张图片时，很少是一笔完成的。既然我们人类都不是这样，为什么我们要寄希望于机器可以做到呢？论文 [4] 中提出的 LAPGAN 就是基于这个思想，将 GAN 的学习过程变成了 sequential “序列式” 的。 具体上，LAPGAN 采用了 Laplacian Pyramid 实现了 “序列化” ，也因此起名做 LAPGAN 。值得一提的是，这个 LAPGAN 中也有 “残差” 学习的思想（与后来大火的 ResNet 也算是有一点关联）。在学习序列中，LAPGAN 不断地进行 downsample 和 upsample 操作，然后在每一个 Pyramid level 中，只将残差传递给判别模型（D）进行判断。这样的 sequential + 残差结合的方式，能有效减少 GAN 需要学习的内容和难度，从而达到了 “辅助” GAN 学习的目的。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a07e88c3e83f8389f66a8dd9c5e85825.jpg" data-rawwidth="653" data-rawheight="234"&gt;另一个基于 sequential 思想去改进 GAN 的工作来自于 [5] 中的 GRAN。与 LAPGAN [4] 每一个 sequential step（Pyramid level）都是独立训练的不同的是，GRAN 把 GAN 和 LSTM 结合，让 sequence 中的每一步学习和生成能充分利用上一步的结果。具体上来看，GRAN 的每一步都有一个像 LSTM 中的 cell，C_t，它决定了每一步生成的内容和结果；GRAN 中的 h_{c,t} 也如 LSTM 一样，代表着 hidden states 。既然是结合 LSTM 和 GAN，那么说完了 LSTM 方面的引入，便是 GAN 方面的了。GRAN 将 GAN 中生成模型（G）的先验也进行了建模，变成了 hidden of prior h_z；然后将 h_z 和 h_{c,t} 拼接（concatenate）之后传递给每一步的 C_t。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0fa9a42dfc752b18e5ca801523d3e8d8.jpg" data-rawwidth="698" data-rawheight="250"&gt;最后一种改进 GAN 的训练稳定性的方式则更加贴近本质，也是最新的研究成果。这便是号称 openAI 近期五大突破之一的 infoGAN [7] 。InfoGAN [7] 的出发点是，既然 GAN 的自由度是由于仅有一个 noise z，而无法控制 GAN 如何利用这个 z。那么我们就尽量去想办法在 “如何利用 z” 上做文章。于是，[7] 中将 z 做了拆解，认为 GAN 中生成模型（G）应该包含的 “先验” 分成两种： （1）不能再做压缩的 noise z；（2）和可解释地、有隐含意义的一组隐变量 c_1, c_2, …, c_L，简写为 c 。这里面的思想主要是，当我们学习生成图像时，图像有许多可控的有含义的维度，比如笔划的粗细、图片的光照方向等等，这些便是 c ；而剩下的不知道怎么描述的便是 z 。这样一来，[7] 实际上是希望通过拆解先验的方式，让 GAN 能学出更加 disentangled 的数据表示（representation），从而既能控制 GAN 的学习过程，又能使得学出来的结果更加具备可解释性。为了引入这个 c ，[7] 利用了互信息的建模方式，即 c 应该和生成模型 （G）基于 z 和 c 生成的图片，即 G ( z,c )，高度相关 —— 互信息大。利用这种更加细致的隐变量建模控制，infoGAN 可以说将 GAN 的发展又推动了一步。首先，它们证明了 infoGAN 中的 c 对于 GAN 的训练是有确实的帮助的，即能使得生成模型（G）学出更符合真实数据的结果。其次，他们利用 c 的天然特性，控制 c 的维度，使得 infoGAN 能控制生成的图片在某一个特定语义维度的变化。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ee7f92800471ba9ef9a6507a36897bfe.jpg" data-rawwidth="636" data-rawheight="743"&gt;&lt;p&gt;然而实际上， infoGAN 并不是第一个将信息论的角度引入 GAN 框架的工作。这是因为，在 infoGAN 之前，还有一个叫做 f-GAN [8] 的工作。并且，GAN 本身也可以从信息论角度去解释。如本文开篇所说，在原始 GAN 论文 [1] 中，作者是通过博弈论的角度解释了 GAN 的思想。然而，GAN 的生成模型（G）产生的数据和真实数据就可以看做一颗硬币的两面。当抛硬币抛到正面时，我们就将一个真实数据样本展示给判别模型（D）；反之，则展示由生成模型 （G）生成的“假”样本。而 GAN 的理想状态是，判别模型（D）对于硬币的判断几乎等同于随机，也就是生成模型（G）产生的数据完全符合真实数据。那么这时候，GAN 的训练过程实际在做的就是最小化这颗硬币和真实数据之间的互信息。互信息越小，判别模型（D）能从观察中获得的信息越少，也就越只能像 “随机” 一样猜结果。既然有了这样一个从互信息角度的对于 GAN 的理解，那么是否能对 GAN 进行更进一步的改造呢？其实是可以的。比如可以把针对互信息的建模更进一步地泛化为基于 divergence 的优化目标。这方面的讨论和改进可以见论文 [8]，f-GAN 。&lt;/p&gt;&lt;p&gt;上面这些对于 GAN 的改进工作都几乎是在短短一年半时间内完成的，尤其是近半年。这里面最大的原因就在于 GAN 相较于以前的 generative models，巧妙地将 “真假” 样本转换为一种隐性的 label，从而实现了一种 “无监督” 的生成式模型训练框架。这种思想也可以从某种程度上看做 word2vec 中 Skip-Gram 的一种变形。未来，不仅仅是 GAN 的更多改进值得被期待，无监督学习和生成式模型的发展也同样值得关注。&lt;/p&gt;&lt;p&gt;&lt;b&gt;References:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1.《Generative Adversarial Nets》&lt;/p&gt;&lt;p&gt;2.《Conditional Generative Adversarial Nets》&lt;/p&gt;&lt;p&gt;3.《DRAW: A Recurrent Neural Network For Image Generation》&lt;/p&gt;&lt;p&gt;4.《Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks》&lt;/p&gt;&lt;p&gt;5.《Generating Images with Recurrent Adversarial Networks》&lt;/p&gt;&lt;p&gt;6.《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》&lt;/p&gt;&lt;p&gt;7.《InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets》&lt;/p&gt;&lt;p&gt;8.《f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization》&lt;/p&gt;&lt;p&gt;&lt;b&gt;该文章属于“深度学习大讲堂”原创，如需要转载，请联系&lt;a href="https://www.zhihu.com/people/guo-dan-qing" data-editable="true" data-title="@果果是枚开心果. "&gt;@果果是枚开心果. &lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者简介：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0dd2f070b05fccf451b2f649d3f98c0d.jpg" data-rawwidth="115" data-rawheight="122"&gt;&lt;b&gt;李嫣然，&lt;/b&gt;香港理工大学在读博士生，研究方向为自然语言理解与对话生成。微信公众号“程序媛的日常”主要发起人之一（小S）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;amp;mid=2650325352&amp;amp;idx=1&amp;amp;sn=90fb15cee44fa7175a804418259d352e&amp;amp;scene=0#wechat_redirect" class="" data-editable="true" data-title="近期GAN的模型和理论发展"&gt;近期GAN的模型和理论发展&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家关注我们的微信公众号，搜索微信名称：深度学习大讲堂&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a29f11daca9717751e639f2c3a3f8b93.jpg" data-rawwidth="346" data-rawheight="67"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22265724&amp;pixel&amp;useReferer"/&gt;</description><author>程程</author><pubDate>Thu, 01 Sep 2016 11:29:19 GMT</pubDate></item></channel></rss>