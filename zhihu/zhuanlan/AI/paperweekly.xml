<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PaperWeekly - 知乎专栏</title><link>https://zhuanlan.zhihu.com/paperweekly</link><description>每周会分享N篇nlp领域好玩的paper，旨在用最少的话说明白paper的贡献。同时也运营一个公众号，PaperWeekly，欢迎大家关注。</description><lastBuildDate>Sat, 01 Oct 2016 13:16:34 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>cs.CL weekly 2016.09.26-2016.09.30</title><link>https://zhuanlan.zhihu.com/p/22723620</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;h1&gt;一周值得读（偏学术）&lt;/h1&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#HyperNetworks" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.09106v1.pdf" data-editable="true" data-title="HyperNetworks"&gt;HyperNetworks&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger network. 工作来自Google Brain。介绍HyperNetworks的博客：&lt;a href="http://blog.otoro.net/2016/09/28/hyper-networks/" data-editable="true" data-title="otoro.net 的页面"&gt;http://blog.otoro.net/2016/09/28/hyper-networks/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Incorporating-Relation-Paths-in-Neural-Relation-Extraction" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.07479v1.pdf" data-editable="true" data-title="Incorporating Relation Paths in Neural Relation Extraction"&gt;Incorporating Relation Paths in Neural Relation Extraction&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为实体关系抽取，传统方法往往只利用同时包含两个目标实体的句子，而忽略包含单目标实体的句子，本文针对这一问题，在俩目标实体之间构建了一个用于推理的中间实体，并提出一种基于路径的关系抽取模型，实验结果表明该模型很好地利用了包含单目标实体的句子信息。本工作来自于刘知远老师组里。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Language-as-a-Latent-Variable-Discrete-Generative-Models-for-Sentence-Compression" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.07317v1.pdf" data-editable="true" data-title="Language as a Latent Variable: Discrete Generative Models for Sentence Compression"&gt;Language as a Latent Variable: Discrete Generative Models for Sentence Compression&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为句子压缩，作者提出了一种VAE模型，先根据背景语言模型生成一个latent摘要句子，然后根据latent句子生成目标句子。实验中用到了抽取式和摘要式两种监督方法，并在最后探索出半监督方法的效果可能会好于监督学习的方法。句子压缩任务可以看做是sentence-level的文本摘要任务，本文的方法同样可以启发文本摘要任务的研究。本文工作来自deepmind，并且是EMNLP 2016 Accepted。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Annotating-Derivations-A-New-Evaluation-Strategy-and-Dataset-for-Algebra-Word-Problems" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.07197v1.pdf" data-editable="true" data-title="Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"&gt;Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容很有意思，是algebra word problems，是自动求解代数问题的基础，这个问题可以等同为一个semantic parsing的问题，模型通过读入一段文本，理解其意思，然后构造出一个方程，最后给出方程的解。作者还给出了一个新的dataset和评价标准，本文工作来自伊大香槟分校和微软研究院。这个task本身非常有意思，也很有难度。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Online-Segment-to-Segment-Neural-Transduction" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.08194v1.pdf" data-editable="true" data-title="Online Segment to Segment Neural Transduction"&gt;Online Segment to Segment Neural Transduction&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文针对之前encoder-decoder模型面临的一个瓶颈，即将输入全部读入并保存为一个固定大小的hidden states，作者提出了一种新的attention机制，将attention权重作为一种隐变量，在句子摘要上证明了效果，本文工作来自deepmind。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://rsarxiv.github.io/#%E4%B8%80%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB%EF%BC%88%E5%81%8F%E5%BA%94%E7%94%A8%EF%BC%89" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;一周值得读（偏应用）&lt;/h1&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Google%E2%80%99s-Neural-Machine-Translation-System-Bridging-the-Gap-between-Human-and-Machine-Translation" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.08144.pdf" data-editable="true" data-title="Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本周最受关注，也备受争议的一篇paper，Google放出了他们最新一代的机器翻译系统，一种神经网络翻译系统。指标上的提升，说明了效果确实有提升，但不代表具体到每一句话都能令人满意。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#UbuntuWorld-1-0-LTS-A-Platform-for-Automated-Problem-Solving-amp-Troubleshooting-in-the-Ubuntu-OS" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.08524v1.pdf" data-editable="true" data-title="UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving &amp;amp; Troubleshooting in the Ubuntu OS"&gt;UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving &amp;amp; Troubleshooting in the Ubuntu OS&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文给出了一个Ubuntu系统问题咨询和错误排查的bot，可以在bash terminal中运行，通过增强学习进行训练，可以回答一些基本的问题和错误排查。demo bot被封装成一个python package，即插即用。回答问题的数据来自于Ask Ubuntu。测试了DQN在特定领域bot中的效果，定义了几组简单的命令作为action，open/close，install/remove等等，technical support是客户服务中难度非常大的一类，本文尝试了用一种完全端到端+增强学习的方案来探索解决此类问题。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Character-Sequence-Models-for-ColorfulWords" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.08777v1.pdf" data-editable="true" data-title="Character Sequence Models for ColorfulWords"&gt;Character Sequence Models for ColorfulWords&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容非常有意思，输入一个word，输出这个word对应的color并着色。作者构建了一组大型的color-name对数据集，来做一个color图灵测试。该系统的demo地址：&lt;a href="http://colorlab.us./" data-editable="true" data-title="colorlab.us. 的页面"&gt;http://colorlab.us./&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Equation-Parsing-Mapping-Sentences-to-Grounded-Equations" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.08824v1.pdf" data-editable="true" data-title="Equation Parsing: Mapping Sentences to Grounded Equations"&gt;Equation Parsing: Mapping Sentences to Grounded Equations&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容非常有趣也很有实际意义，即从文本中抽取出数学关系，作者将该任务定义如下：给定一句话，抽取出其中的变量和数学关系，并用方程表示。这个研究可以被应用在新闻机器人上，财经、体育等。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#Inducing-Multilingual-Text-Analysis-Tools-Using-Bidirectional-Recurrent-Neural-Networks" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1609.09382v1.pdf" data-editable="true" data-title="Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks"&gt;Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;资源稀缺语言的标注问题是一个经典的问题，一般的做法是将资源丰富的语音对齐映射过去进行标注，自动词对齐的错误会影响最终的效果。本文针对这个问题，提出了一种BiRNN模型，并且融合外部信息解决问题。该模型具有以下特点：1、不需要词对齐信息；2、不限定语言，可用于多种资源少的语言；3、提供一种真正的多语言tagger。&lt;/p&gt;&lt;h1&gt;一周资源&lt;/h1&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#THULAC" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://github.com/thunlp/THULAC.so" data-editable="true" data-title="THULAC"&gt;THULAC&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;THULAC.so：一个高效的中文词法分析工具包，为了满足Python下分词对速度的要求，发布了一个产生.so文件的THULAC版本，并且提供Python调用的示例代码。这样THULAC在Python下的分词速度得到大幅度提高。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://rsarxiv.github.io/#tinyflow" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;&lt;a href="https://github.com/tqchen/tinyflow" data-editable="true" data-title="tinyflow"&gt;tinyflow&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;DMLC陈天奇开放了一个两千行代码的样例项目，教你如何从头开始打造一个和TensorFlow一样API的深度学习系统。其中涉及到一个非常重要的开源库NNVM，地址： &lt;a href="https://github.com/dmlc/nnvm" data-editable="true" data-title="GitHub - dmlc/nnvm: Intermediate Computational Graph Representation  for Deep Learning Systems"&gt;GitHub - dmlc/nnvm: Intermediate Computational Graph Representation  for Deep Learning Systems&lt;/a&gt; 。博客介绍：&lt;a href="http://dmlc.ml/2016/09/30/build-your-own-tensorflow-with-nnvm-and-torch.html" data-editable="true" data-title="Build your own TensorFlow with NNVM and Torch"&gt;Build your own TensorFlow with NNVM and Torch&lt;/a&gt; ，中文版：&lt;a href="http://weibo.com/ttarticle/p/show?id=2309404025388832575825#_0" data-editable="true" data-title="NNVM打造模块化深度学习系统"&gt;NNVM打造模块化深度学习系统&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;&lt;a href="http://rsarxiv.github.io/#%E5%B9%BF%E5%91%8A%E6%97%B6%E9%97%B4" class="" data-editable="true" data-title="PaperWeekly"&gt;PaperWeekly&lt;/a&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/paperweekly" data-editable="true" data-title="weibo.com 的页面"&gt;http://weibo.com/u/paperweekly&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22723620&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Sat, 01 Oct 2016 10:58:39 GMT</pubDate></item><item><title>PaperWeekly 第七期 -- 基于Char-level的NMT OOV解决方案</title><link>https://zhuanlan.zhihu.com/p/22700538</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;h1&gt;引言&lt;/h1&gt;&lt;p&gt;神经网络机器翻译(NMT)是seq2seq模型的典型应用，从2014年提出开始，其性能就接近于传统的基于词组的机器翻译方法，随后，研究人员不断改进seq2seq模型，包括引入注意力模型、使用外部记忆机制、使用半监督学习和修改训练准则等方法，在短短2年时间内使得NMT的性能超过了传统的基于词组的机器翻译方法。在27号谷歌宣布推出谷歌神经网络机器翻译系统，实现了NMT的首个商业化部署，使得NMT真正从高校实验室走向了实际应用。本期Paperweekly的主题是神经网络机器翻译下的字符级方法，主要用来解决NMT中的out-of-vocabulary词问题，分别是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation，2016&lt;/li&gt;&lt;li&gt;Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models，2016&lt;/li&gt;&lt;li&gt;Character-based Neural Machine Translation，Costa-Jussa, 2016&lt;/li&gt;&lt;li&gt;Character-based Neural Machine Translation，Ling, 2016&lt;/li&gt;&lt;li&gt;Neural Machine Translation of Rare Words with Subword Units，2016&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/abs/1603.06147" data-editable="true" data-title="A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation" class=""&gt;A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Junyoung Chung, Kyunghyun Cho, Yoshua Bengio&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Universite de Montreal&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Segmentation, Character-level, Bi-scale recurrent network&lt;/p&gt;&lt;h2&gt;文章来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;能否在不需要分词的前提下直接在字符级进行神经机器翻译。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;在讲模型之前，本文花了大量篇幅论证为何需要在不分词的前提下进行字符级翻译，首先作者总结了词级翻译的缺点。&lt;/p&gt;&lt;p&gt;词级翻译的缺点包括：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;任何一个语言都没有完美的分词算法，完美的分词算法应该能够将任意句子划分为lexemes和morphemes组成的序列&lt;/li&gt;&lt;li&gt;导致的问题就是在词典中经常充斥着许多共享一个lexeme但有着不同morphology的词，比如run,runs,ran,running可能都存在于词典中，每个词都对应一个词向量，但是它们明显共享相同的lexeme——run&lt;/li&gt;&lt;li&gt;存在unknown word问题和rare word问题，rare word问题是指某些词典中词在训练集中出现次数过少，导致无法训练得到很好的词向量；unknown word问题是指不在词典中的词被标记为UNK（OOV词）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;接着作者指出使用字符集翻译可以解决上述问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;使用LSTM或GRU可以解决长时依赖问题&lt;/li&gt;&lt;li&gt;使用字符级建模可以避免许多词态变形词出现在词典中&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;然而上述字符级方法依然需要进行分词，然后对每个词的字符序列进行编码，因此引出了本文的motivation，即是否能直接在不分词的字符序列上进行翻译。&lt;/p&gt;&lt;p&gt;本文使用的模型同样是经典的seq2seq模型，其创新点主要在decoder端，引入了一种新的网络结构biscale RNN，来捕获字符和词两个timescale上的信息。具体来说，主要分为faster层和slower层，faster层的gated激活值取决于上一步的faster和slower层的激活值，faster层要想影响slower层，则必须要是faster层处理完当前数据，并且进行重置。换句话说，slower层无法接受faster层输入，直到faster层处理完其数据，因此比faster层要慢，而这样的层次结构也对应字符和词在timescale上的关系。下图为网络结构示意图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bfe8f04b9da6af7bcb2d69c4ea4772c9.png" data-rawwidth="1224" data-rawheight="742"&gt;&lt;/p&gt;&lt;p&gt;在4种语言翻译任务上的实验显示完全可以在不分词的情况下进行字符级翻译，性能优于state-of-the-art的非神经翻译系统&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;Sennrich ACL2016提出使用BPE算法对subword建模。Kim AAAI2016中提出直接对字符进行encode，Costa-jussa ICLR2016中将该模型用在了NMT任务中。Ling ICLR2016的工作中使用Bi-RNN来编码字符序列。以上工作基于字符级展开，但它们都依赖于知道如何将字符分为词，即分词。本文研究能否在不分词的情况下进行字符级翻译。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文是Bengio组工作，Bi-scale RNN受启发于该组之前提出的GF-RNN，本文创新点主要是提出了一种新的RNN结构，可以在字符和词两个timescales上进行处理，输出字符序列不需要进行分词。不足是未考虑encoder端是否也可以直接使用未分词的字符序列，而是仅仅使用了分词后的BPE序列。&lt;/p&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/pdf/1604.00788v2.pdf" data-editable="true" data-title="Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models" class=""&gt;Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Minh-Thang Luong and Christopher D. Manning&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Stanford University&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;OOV, hybrid word-character models, NMT&lt;/p&gt;&lt;h2&gt;文章来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;机器翻译里面的OOV问题, 如何处理UNK&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;提出了一种混合word-character的NMT模型.在训练难度和复杂度不是很高的情况下,同时解决源语言和目标语言的OOV问题.&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c6266b642821032437d60e65989dee7a.png" data-rawwidth="372" data-rawheight="414"&gt;&lt;/p&gt;&lt;p&gt;这个图表达了模型的整体思路. 大多数情况下,模型在word-level进行translation. 当出现unk的时候,则会启用character-level的模型. 对source unk, 由character-level模型来得到它的representation; 对target unk, 用character-level模型来产生word.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;整体上采用他们组以前提出的基于global attention的encoder-decoder模型. RNN采用的是deep LSTM.&lt;/li&gt;&lt;li&gt;源语言端和目标语言端的character-level模型都是基于character的deep LSTM. 对源语言端来说, 它的character-level模型是context independent的. 隐层状态全部初始化为0, 因此在训练时可以预先计算mini-batch里的每一个rare word的representation. 而对于目标语言端来说, 它的character-level模型是context dependent的.它的第一层的hidden state要根据当前context来初始化, 其它部分都初始化为0.训练时, 在目标语言的decoder阶段, 首先用word-level的decoder产生句子, 这时句子里包含了一些unk. 接着对这些unk, 用character-level模型以batch mode来产生rare word.&lt;/li&gt;&lt;li&gt;对于目标语言端character-level模型的初始化问题, 作者提出了两种方法来表示当前的context. 一种叫做same-path, 用预测的softmax层之前的ht来表达. 但是因为ht是用来预测的, 所以所有ht的值都会比较相似,这样很难用来产生不同的目标rare word. 因此作者提出了第二种表达叫做separate-path, 用ht’来表达context. ht’不用预测unk, 是专门作为context在character-level的输入的. 它的计算方法和ht’相同,只是用了一个不一样的矩阵.&lt;/li&gt;&lt;li&gt;模型训练的目标函数是cross-entropy loss, 同时考虑了word level和character level的loss.&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;NMT的模型分为word-level和character-level的. 对于word-level模型,要解决OOV问题, 之前的工作提出了unk replacement (Luong et al. 2015b), 使用大字典并在softmax时进行采样(Jean et al. 2015), 对unk进行Huffman编码(Chitnis et al. 2015)等方法. 而对于character-level的模型, 本身可以处理OOV词, 但是训练难度和复杂度会增加.&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文的创新之处在于提出了混合word-character model的NMT模型. 这个混合模型结合了二者的优点, 在保证模型复杂度较低的同时,实现了很好的效果.因为加入了character, 特别适合单词有丰富变形的语言.&lt;/p&gt;&lt;h1&gt;&lt;a href="http://arxiv.org/abs/1511.04586" data-editable="true" data-title="Character-based Neural Machine Translation" class=""&gt;Character-based Neural Machine Translation&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Marta R. Costa-jussa and Jose A. R. Fonollosa&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;TALP Research CenterUniversitat Politecnica de Catalunya, Barcelona&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;NMT，character-based word embeddings，CNN&lt;/p&gt;&lt;h2&gt;文章来源&lt;/h2&gt;&lt;p&gt;ICLR2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;本文提出使用character-based word embeddings的NMT，可以在一定程度上克服机器翻译中OOV问题。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4de6daed90818f0e6905912ded052c25.png" data-rawwidth="484" data-rawheight="656"&gt;&lt;p&gt;如上图所示，这篇论文使用的基本模型架构是一个带attention机制的seq2seq的encoder-decoder的架构，使用的神经网络单元是GRU。encoder把源句子转化成一个向量（双向），使用attention的机制来捕获context信息，decoder把context解码成目标句子。网络的输入仍然使用word embedding，但是作者在获取word embedding的时候使用的方法不同。本文是基于词中的character来生成word embedding的，具体方法如下图所示。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-709e2b33e1f6486d4a5ce1786431f223.png" data-rawwidth="968" data-rawheight="886"&gt;&lt;/p&gt;&lt;p&gt;上图中，最底层是一个character-based embedding组成的序列，对应的是每个词中的字母。然后这个序列被送入一个由不同长度的一维卷积过滤器组成的集合中进行处理，不同的长度对应单词中不同数量的字母（从1到7）。对于每个卷积过滤器，只取最大的值作为输出。然后把每个卷积过滤器输出的最大值连接起来组成一个向量。最后这个向量再通过两层Highway layer的处理作为最终的word embeddings。这个方法的详细信息可以参考Kim的论文&lt;a href="http://arxiv.org/abs/1508.06615" data-editable="true" data-title="Character-Aware Neural Language Models"&gt;Character-Aware Neural Language Models&lt;/a&gt;(2016)。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;本文数据集[German-English WMT data] (&lt;a href="http://www.statmt.org/wmt15/translation-task.html" data-editable="true" data-title="Translation Task"&gt;Translation Task&lt;/a&gt;) &lt;/li&gt;&lt;li&gt;建立对比模型使用的软件包&lt;a href="http://dl4mt.computing.dcu.ie/" data-editable="true" data-title="DL4MT"&gt;DL4MT&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;（1）2003年，基于短语的统计机器翻译模型。Statistical Phrase-Based Translation （2）2013年，基于神经网络的机器翻译模型。Recurrent continuous translation models （3）2014年，seq2seq的神经网络模型用于机器翻译。Sequence to sequence learning with neural networks&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文作者将基于character来产生word embedding的方法应用于机器翻译，可以在一定程度上克服OOV的问题。同时，由于利用了单词内部的信息，这篇论文提出的方法对于词形变化丰富的语言的翻译也产生了更好的效果。但是，作者只是在source side使用了上述方法，对于target side，仍然面临词典大小的限制。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://arxiv.org/abs/1511.04586" data-editable="true" data-title="CHARACTER-BASED NEURAL MACHINE TRANSLATION" class=""&gt;CHARACTER-BASED NEURAL MACHINE TRANSLATION&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Wang Ling, Isabel Trancoso, Chris Dyer, Alan W Black&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;LF Spoken Systems Lab,Instituto Superior Tecnico Lisbon, Portugal&lt;/li&gt;&lt;li&gt;Language Technologies Institute, Carnegie Mellon University Pittsburga, PA 15213, USA&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;NMT, Character-Based&lt;/p&gt;&lt;h2&gt;文章来源&lt;/h2&gt;&lt;p&gt;ICLR 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;尝试在字符级别上应用神经机器学习方法&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;在带注意力机制的神经机器学习模型的前后端增加字符到词（C2W)和词向量到字符（V2C）的模块。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-5dc8bd622f6638c5da5250dedbdbb68e.png" data-rawwidth="918" data-rawheight="463"&gt;&lt;/p&gt;&lt;p&gt;图中，小矩形是一个双向LSTM，双向LSTM的前向和后向的最终状态以及bias之和为词的向量表示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7f72e5c3da4a5795865c07ecf0404c60.png" data-rawwidth="901" data-rawheight="395"&gt;&lt;p&gt;这个模块主要由三个步骤组成：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;将字符转换为向量表示。&lt;/li&gt;&lt;li&gt;将字符向量和之前模型产生注意力向量的a和目标词在前向LSTM中产生的向量表示做拼接并输入到LSTM。&lt;/li&gt;&lt;li&gt;将得到的向量输入到softmax层得到结果。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Neural machine translation by jointly learning to align and translate.&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;这篇文章在基于注意力机制的机器翻译模型上增加了两个模块。由于是基于字符集别的模型，该模型自然可以学得一些语言中的前后缀在翻译中的关系。此外，基于字符级别的模型在翻译未知词时有灵活性。可是，文中也提到，该模型为能够准确的翻译未知词。并且该文也没有明确表明该模型和其他模型相比具有哪些明显的优势。从实际上来说，该模型在V2C部分的训练速度慢是一个很大的弱点，因此若仅根据文章的表述，该模型的实际应用价值应该有限。&lt;/p&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/abs/1508.07909" data-editable="true" data-title="Neural Machine Translation of Rare Words with Subword Units"&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Rico Sennrich and Barry Haddow and Alexandra Birch&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;School of Informatics, University of Edinburgh&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;NMT;Rare Words;Subword Units;BPE&lt;/p&gt;&lt;h2&gt;文章来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;NMT中的OOV（集外词）和罕见词（Rare Words）问题通常用back-off 词典的方式来解决，本文尝试用一种更简单有效的方式（Subword Units）来表示开放词表。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;本文从命名实体、同根词、外来语、组合词（罕见词有相当大比例是上述几种）的翻译策略中得到启发，认为把这些罕见词拆分为“子词单元”(subword units)的组合，可以有效的缓解NMT的OOV和罕见词翻译的问题。子词单元的拆分策略，则是借鉴了一种数据压缩算法：Byte Pair Encoding(BPE)(Gage,1994)算法。该算法的操作过程和示例如Figure1所示。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-062443ab5a0364552b0deba26fdee66c.jpg" data-rawwidth="371" data-rawheight="559"&gt;&lt;/p&gt;&lt;p&gt;不同于(Chitnis and DeNero,2015)提出的霍夫曼编码，这里的压缩算法不是针对于词做变长编码，而是对于子词来操作。这样，即使是训练语料里未见过的新词，也可以通过子词的拼接来生成翻译。本文还探讨了BPE的两种编码方式：一种是源语言词汇和目标语言词汇分别编码，另一种是双语词汇联合编码。前者的优势是让词表和文本的表示更紧凑，后者则可以尽可能保证原文和译文的子词切分方式统一。从实验结果来看，在音译或简单复制较多的情形下（比如英德）翻译，联合编码的效果更佳。实验结果分别在WMT15英德和英俄的任务上得到1.1和1.3个BLEU值的提升。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;本文提出的子词拆分算法代码在 &lt;a href="https://github.com/rsennrich/subword-nmt" data-editable="true" data-title="GitHub - rsennrich/subword-nmt: Subword Neural Machine Translation"&gt;GitHub - rsennrich/subword-nmt: Subword Neural Machine Translation&lt;/a&gt;实验所用的NMT系统为Groundhog: github.com/sebastien-j/LV_groundhog实验数据来自WMT 2015&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;OOV的处理一直是机器翻译研究的重点。基于字符的翻译在短语SMT模型中就已被提出，并在紧密相关的语种对上验证是成功的(Vilar et al., 2007; Tiedemann,2009; Neubig et al., 2012)。 此外还有各种形态素切分方法应用于短语模型，(Nießen and Ney,2000; Koehn and Knight, 2003; Virpioja et al.,2007; Stallard et al., 2012)。对于NMT，也有很多基于字符或形态素的方法用于生成定长连续词向量(Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015)。与本文类似的一项工作 (Ling et al., 2015b)发现在基于词的方法上没有明显提升。其与本文的一个区别在于，attention机制仍然在词层级进行操作，而本文在子词层级上。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;这篇文章的创新点在于提出了一种介乎字符和单词之间，也不同于字符n-gram的文本表示单元，并借鉴BPE压缩算法，在词表大小和文本长度两个方面取得一个较为平衡的状态。应用在非同源/近源的语言对（如英汉）是否可以有类似的效果，尚待研究。在NMT模型的优化上，也还有探讨的空间。本文的实验评价方法值得学习，单看BLEU值并不觉得有惊艳之处，但加上CHR F3和(对所有词、罕见词和集外词分别统计的)unigram F1这两个评价指标，尤其是Figure2和3画出来的效果，还是让人比较信服的。&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;OOV词对于翻译性能和实用性的影响非常巨大，如何处理OOV词并达到open vocabulary一直是NMT的主要研究方向。传统方法基于单词级别来处理该问题，比如使用UNK替换、扩大词典规模等方法，往往治标不治本。因此最近一些研究者提出基于字符的NMT模型，取得了不错的成绩，字符级方法的主要优势包括不受语言的形态变化、能预测出词典中未出现的单词并降低词典大小等。值得一提的是，基于字符的模型不仅局限于NMT上，任何生成模型都面临OOV词问题，因此是否能够将字符级方法用在其他NLP任务，比如阅读理解或文本摘要上，让我们拭目以待。&lt;/p&gt;&lt;p&gt;以上为本期Paperweekly的主要内容，感谢EdwardHux、Mygod9、Jaylee1992、Susie和AllenCai五位同学的整理。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d.jpg" data-rawwidth="430" data-rawheight="430"&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博" class=""&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22700538&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Fri, 30 Sep 2016 11:39:50 GMT</pubDate></item><item><title>cs.CL weekly 2016.09.19-2016.09.23</title><link>https://zhuanlan.zhihu.com/p/22602959</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;h1&gt;一周值得读&lt;/h1&gt;&lt;h2&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1609.04904v1.pdf" data-editable="true" data-title="Long-Term Trends in the Public Perception of Artificial Intelligence"&gt;Long-Term Trends in the Public Perception of Artificial Intelligence&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究了30年来纽约时报对AI的报道，研究了人们这30年来对AI的兴趣、关注度和各种各样的讨论。是一篇很有意思的文章，是一种长时间段内的舆情监测和分析。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.77/arxiv.org/pdf/1609.04873v1.pdf" data-editable="true" data-title="Distant Supervision for Relation Extraction beyond the Sentence Boundary"&gt;Distant Supervision for Relation Extraction beyond the Sentence Boundary&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的问题是非结构化文本中的关系抽取问题，针对传统方法在抽取关系时仅限于单个句子，本文提出了一种新的方法，从多个句子中进行关系抽取。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.04938v1.pdf" data-editable="true" data-title="What You Get Is What You See: A Visual Markup Decompiler"&gt;What You Get Is What You See: A Visual Markup Decompiler&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;【转发较多】本文研究的问题是如何从web页面中生成html代码，以及如何从公式图片中生成latex代码，为此作者构造了两个相关的大型数据集，用了完全数据驱动的端到端训练方法得到了不错的效果。本文工作来自Harvard。&lt;/p&gt;&lt;p&gt;Demo|Dataset|Code: &lt;a href="http://lstm.seas.harvard.edu/latex/"&gt;http://lstm.seas.harvard.edu/latex/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1609.05244v1.pdf" data-editable="true" data-title="Select-Additive Learning: Improving Cross-individual Generalization in Multimodal Sentiment Analysis"&gt;Select-Additive Learning: Improving Cross-individual Generalization in Multimodal Sentiment Analysis&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容是多模态情感分析，针对当前相关高质量数据集规模太小造成的情感依赖于个体特征的问题，提出了一种Select-Additive学习方法提高通用性。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.05234v1.pdf" data-editable="true" data-title="Interactive Spoken Content Retrieval by Deep Reinforcement Learning"&gt;Interactive Spoken Content Retrieval by Deep Reinforcement Learning&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容是DQN算法来做语音内容检索，通过人机交互来完成内容检索。DQN相比传统的RL模型明显的优势在于不依赖hand-crafted features。本文被Interspeech 2016录用。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://arxiv.org/pdf/1609.05600v1.pdf" data-editable="true" data-title="Graph-Structured Representations for Visual Question Answering"&gt;Graph-Structured Representations for Visual Question Answering&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为VQA，VQA的主要挑战在于对visual和text两个领域都需要理解。传统的模型中常常忽略场景中的结构和问题中的语言结构，本文针对这两个问题提出了一种图模型，取得了不错的效果。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://arxiv.org/pdf/1609.05787v1.pdf" data-editable="true" data-title="Context-aware Sequential Recommendation"&gt;Context-aware Sequential Recommendation&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;用户行为建模是推荐系统中的一个关键部件，行为数据是序列数据，天然适合用RNN来建模。但实际应用中context信息(time,location,weahter)也很重要，本文针对这个问题提出了一种CA-RNN模型将context考虑在内，取得了不错效果。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.05284v1.pdf" data-editable="true" data-title="ReasoNet: Learning to Stop Reading in Machine Comprehension"&gt;ReasoNet: Learning to Stop Reading in Machine Comprehension&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为机器阅读理解，之前效果不错的方法大多数停留在有限的几轮reasoning，本文用增强学习来动态地决定是否继续读下去或者停下来进行答案选择。本文工作来自微软研究院。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1609.06038v1.pdf" data-editable="true" data-title="Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference"&gt;Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为自然语言推理，作者认为LSTM类的模型潜力并没有被充分挖掘，基于此，本文在传统LSTM模型的基础上增加了syntactic parse信息，得到了更好的效果。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1609.06127v1.pdf" data-editable="true" data-title="A framework for mining process models from emails logs"&gt;A framework for mining process models from emails logs&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容是邮件日志的挖掘，作者提出了一种无监督的挖掘方法，并且提出了一种半自动化的邮件标注方法。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.06686v1.pdf" data-editable="true" data-title="Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution"&gt;Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容为authorship attribution，是一个典型的多分类任务。作者利用字符级别的多通道CNN模型对大规模dataset进行了建模，取得了不错的结果。作者之一来自aylien.com 公司，一家非常出色的NLP SaaS 公司。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1609.06649v1.pdf" data-editable="true" data-title="Minimally Supervised Written-to-Spoken Text Normalization"&gt;Minimally Supervised Written-to-Spoken Text Normalization&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究的内容是特定语言领域知识在构建text normalization system的时候应该如何做trade-off，本文作者来自Google。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1609.06380v1.pdf" data-editable="true" data-title="Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention"&gt;Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文研究内容是如何识别隐式的discourse关系，作者提出了一种多层注意力模型，联合注意力机制和外部memory来做关系识别。本文是EMNLP2016的长文。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.06693v2.pdf" data-editable="true" data-title="SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks"&gt;SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;【转发较多】本文提出了一种新的正则化方法，通过在训练过程中调整label来实现，达到了和Dropout接近的效果。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.77/arxiv.org/pdf/1609.06657v1.pdf" data-editable="true" data-title="The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)"&gt;The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文提出了一个1 million的Visual Question Answer Dataset，数据地址：&lt;a href="http://www.mi.t.u-tokyo.ac.jp/static/projects/fsvqa/" data-editable="true" data-title="Projects - Full-Sentence Visual Question Answering"&gt;Projects - Full-Sentence Visual Question Answering&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="http://arxiv.org/pdf/1609.07075v1.pdf" data-editable="true" data-title="Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs"&gt;Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;【转发较多】当前知识表示存在两个挑战：1、如何更好地利用entity的context；2、如何发现与entity相关的句子；针对这两个问题，本文提出了一种从多个句子中学习表示的模型。给定每个entity的参考句子，首先用带池化的RNN或LSTM来encode与该entity相关的句子，然后用attention模型来衡量每个句子的信息量，最后得到entity的表示。模型在triple classification和link prediction两个任务上都取得了满意的结果。本文工作来自@刘知远THU组。&lt;/p&gt;&lt;p&gt;刘知远：我觉得这个工作的最有意思的地方是，能够为实体找到最有信息量的句子，这些句子往往是该实体的定义或描述。这样，在构建知识图谱时，我们就可以自动为新增的实体构建对应的文本描述信息了。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1609.07053v1.pdf" data-editable="true" data-title="Semantic Tagging with Deep Residual Networks"&gt;Semantic Tagging with Deep Residual Networks&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;本文提出一种多语言智能tagger，模型采用了char-level和word-level的深度残差网络，在词性标注任务中取得了不错的效果，本文COLING 2016在审。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://arxiv.org/pdf/1609.07028v1.pdf" data-editable="true" data-title="Image-embodied Knowledge Representation Learning"&gt;Image-embodied Knowledge Representation Learning&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;【转发较多】entity图像中包含丰富的信息，大多数传统方法并没有利用这一点，本文提出了一种知识表示模型，利用了triples和image信息，并在知识图谱补全和triple分类两个任务中取得了不错的效果。本文是一篇典型的多信息融合的文章，非常值得思考！工作同样来自@刘知远THU老师组。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1609.06791v1.pdf" data-editable="true" data-title="Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling"&gt;Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;推特上的推对于topic建模有以下缺点：1、短；2、非结构化；3、口语化；也有优点：1、作者；2、hashtags；3、粉丝网络。本文结合推特信息的优点提出了一种新模型。topic model是个老话题了，多源信息的融合是突破研究瓶颈一个不错的方向，本文的方法同样可借鉴于微博和其他社交网络。&lt;/p&gt;&lt;h2&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.06773v1.pdf" data-editable="true" data-title="Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning"&gt;Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;【转发较多】Attention类模型在端到端语音识别领域取得了不错的效果，但当输入噪声非常大的的时候，识别长句子效果不是很好。CTC是另外一种不错的端到端模型，本文结合两者的优势构建模型。构建了联合模型之后，克服了之前的问题。大家都在用Attention，都说Attention好，但终究还是有些情境下attention并不能如人意。那么问题来了，到底哪些场景下attention表现不好，原因是什么？想清楚这个到底之后，改进的方法大概也就在路上了。#Attention Model的缺点#&lt;/p&gt;&lt;h1&gt;资源分享&lt;/h1&gt;&lt;h2&gt;&lt;a href="https://www.producthunt.com/topics/bots" data-editable="true" data-title="Bots - Product Hunt"&gt;Bots - Product Hunt&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;一个分享和点评各种好玩product的站点，其中一个栏目有各种各样的bot。&lt;/p&gt;&lt;h2&gt;&lt;a href="https://github.com/chewxy/gorgonia" data-editable="true" data-title="Gorgonia is a library that helps facilitate machine learning in Go"&gt;Gorgonia is a library that helps facilitate machine learning in Go&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;用Go写的机器学习开源框架。&lt;/p&gt;&lt;h2&gt;&lt;a href="https://github.com/danqi/rc-cnn-dailymail" data-editable="true" data-title="A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"&gt;A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;这篇paper的代码放出来了，同时包括CNN和Daily Mail的数据集。来自斯坦福Danqi Chen的工作。&lt;/p&gt;&lt;h1&gt;业界新闻&lt;/h1&gt;&lt;h2&gt;&lt;a href="https://api.ai/blog/2016/09/19/api-ai-joining-google/" data-editable="true" data-title="API.AI is joining Google!"&gt;API.AI is joining Google!&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;chatbot构建平台api.ai被Google收购了&lt;/p&gt;&lt;h2&gt;&lt;a href="https://techcrunch.com/2016/09/20/angel-ai-a-company-that-builds-chat-bots-acqui-hired-by-amazon/" data-editable="true" data-title="Angel.ai, a company that builds chat bots, acqui-hired by Amazon | TechCrunch"&gt;Angel.ai, a company that builds chat bots, acqui-hired by Amazon | TechCrunch&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;TechCrunch报道称，继api.ai被google收购之后，一家做自然语言理解的公司angel.ai也几乎被Amazon收购。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d.jpg" data-rawwidth="430" data-rawheight="430"&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博" class=""&gt;PaperWeekly的微博&lt;/a&gt; ）每天都会分享当天arXiv cs.CL板块刷新的高质量paper&lt;/p&gt;&lt;p&gt;知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22602959&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Sat, 24 Sep 2016 11:51:43 GMT</pubDate></item><item><title>PaperWeekly 第六期------机器阅读理解</title><link>https://zhuanlan.zhihu.com/p/22577648</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;h1&gt;引&lt;/h1&gt;&lt;p&gt;本期paperweekly的主题是Question Answering Models，解决这一类问题可以很好地展现AI理解人类自然语言的能力，通过解决此类dataset可以给AI理解人类语言很好的insights。问题的定义大致是，给定较长一段话的context和一个较短的问题，以及一些candidate answers，训练一些可以准确预测正确答案的模型。&lt;/p&gt;&lt;p&gt;此问题也存在一些变种，例如context可以是非常大块的knowledge base，可以不提供candidate answers而是在所有的vocabulary中搜索答案，或者是在context中提取答案。&lt;/p&gt;&lt;p&gt;基于(Recurrent) Neural Network的一些模型在这一类问题上给出了state of the art models，本期paperweekly就带领大家欣赏这一领域有趣的工作。&lt;/p&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/abs/1607.04423" data-editable="true" data-title="Attention-over-Attention Neural Networks for Reading Comprehension" class=""&gt;Attention-over-Attention Neural Networks for Reading Comprehension&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu and Guoping Hu&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;iFLYTEK Research, ChinaResearch Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Question Answering, Attentive Readers&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv, 201608&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;本文优化了attention机制，同时apply question-to-document and document-to-question attention，提升了已有模型在Cloze-Style Question Answering Task上的准确率。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;本文解决的是Cloze-style question answering的问题，给定一个Document和一个Query，以及一个list的candidate answers，模型需要给出一个正确答案。&lt;/p&gt;&lt;p&gt;已有的模型大都通过比较每一个Query + candidate answer和context document的相似性来找出正确答案，这种相似性measure大都通过把query 投射到context document每个单词及所在context的相似性来获得。本文的不同之处在于模型还计算了context投射到每个query单词的相似度，进一步丰富了context和query相似度的计算。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-af08e8e9fb96b8ae0927bdc4f6b2ea8c.png" data-rawwidth="694" data-rawheight="482"&gt;&lt;/p&gt;&lt;p&gt;首先，document和query都会被model成biGRU。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-854674cf407253946e97d634e72803a4.png" data-rawwidth="468" data-rawheight="153"&gt;&lt;/p&gt;&lt;p&gt;然后使用document biGRU和query biGRU的每一个position做inner product计算，可以得到一个similarity matrix。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-293c440a77d4023981ba9375ac3d7d28.png" data-rawwidth="356" data-rawheight="58"&gt;&lt;/p&gt;&lt;p&gt;对这个matrix做一个column-wise softmax，可以得到每个query单词在每个document单词上的similarity。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1e3f420c54d62e837a00253fe5a892d2.png" data-rawwidth="450" data-rawheight="97"&gt;&lt;/p&gt;&lt;p&gt;similarly，对这个matrix做一个row-wise softmax，可以得到每个document单词在每个query单词上的similarity。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-59b0db8160a066f9049c8da726f94fc5.png" data-rawwidth="508" data-rawheight="78"&gt;&lt;/p&gt;&lt;p&gt;取个平均就得到了每个query单词在整个context document上的similarity。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-256ea139cfabd3b6d4ca824916a352c2.png" data-rawwidth="187" data-rawheight="89"&gt;&lt;/p&gt;&lt;p&gt;然后把alpha和beta做个inner product就得到了每个context document word的probability。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1f2f0e78e26abe31b627ed55a78b5fef.png" data-rawwidth="206" data-rawheight="62"&gt;&lt;/p&gt;&lt;p&gt;每个candidate answer的probability就是它出现在上述s中的probability之和。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0b0e8959a461212fd2ccde576250d151.png" data-rawwidth="285" data-rawheight="79"&gt;&lt;/p&gt;&lt;p&gt;Loss Function可以定义为正确答案的log probability之和。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6794ef52d8e7d4bb716707baacddb1a1.png" data-rawwidth="225" data-rawheight="57"&gt;&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/deepmind/rc-data" data-editable="true" data-title="cnn和daily mail datasets"&gt;cnn和daily mail datasets&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://research.facebook.com/research/babi/" data-editable="true" data-title="Children’s book test"&gt;Children’s book test&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;利用attentive readers解决question answering问题最早出自deep mind: teaching machines to read and comprehend。后来又有Bhuwan Dhingra: Gated-Attention Readers for Text Comprehension和Danqi Chen: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task，以及其他相关工作，在此不一一赘述。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文很好地完善了attentive reader的工作，同时考虑了query to document and document to query attentions，在几个data set上都取得了state of the art效果，思路非常清晰，在question answering问题上很有参考价值。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1608.07905v1.pdf" data-editable="true" data-title="MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER"&gt;MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Shuohang Wang, Jing Jiang&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Singapore Management University&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Machine comprehension, Match-LSTM, Pointer Net&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv，201608&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;提出一种结合match-LSTM和Pointer Net的端到端神经网络结构，来解决SQuAD数据集这类没有候选项且答案可能是多个词的machine comprehension问题。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;本文提出的模型结合了match-LSTM(mLSTM)和Pointer Net(Ptr-Net)两种网络结构。&lt;/p&gt;&lt;p&gt;1、match-LSTM&lt;/p&gt;&lt;p&gt;mLSTM是由Wang和Jiang提出的一种解决文本蕴含识别（RTE）问题的一种神经网络结构。模型结构见下图，该模型首先将premise和hypothesis两句话分别输入到两个LSTM中，用对应LSTM的隐层输出作为premise和hypothesis中每个位置对应上下文信息的一种表示（分别对应图中的Hs和Ht）。对于hypothesis中的某个词的表示ht_i，与premise中的每个词的表示Hs计算得到一个权重向量，然后再对premise中的词表示进行加权求和，得到hti对应的上下文向量a_i（attention过程）。最后把hypothesis中该词的表示ht_i和其对应的context向量a_i拼接在一起，输入到一个新的LSTM中。该模型将两个句子的文本蕴含任务拆分成词和短语级别的蕴含识别，因此可以更好地识别词之间的匹配关系。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-721234392c72ee01ab06f67edf1b6318.png" data-rawwidth="1156" data-rawheight="520"&gt;&lt;/p&gt;&lt;p&gt;2、 Pointer networks&lt;/p&gt;&lt;p&gt;该模型与基于attention的生成模型类似。区别之处在于，pointer networks生成的结果都在输入序列中，因此pointer networks可以直接将attention得到的align向量中的每个权重直接作为预测下一个词对应的概率值。&lt;/p&gt;&lt;p&gt;3、 Sequence Model &amp;amp; Boundary Model&lt;/p&gt;&lt;p&gt;本文提出的模型结构见下图，具体到本文的神经网络结构，可以简单分为下面两部分：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-dad87a89d1400f1b400bbc2361c780fb.png" data-rawwidth="1513" data-rawheight="840"&gt;（1）Match-LSTM层：该部分将machine comprehension任务中的question作为premise，而passage作为hypothesis。直接套用上述的mLSTM模型得到关于passage每个位置的一种表示。为了将前后方向的上下文信息全部编码进来，还用相同的方法得到一个反向mLSTM表示，将两个正反方向的表示拼接在一起作为最终passage的表示。&lt;/p&gt;&lt;p&gt;（2）生成答案序列部分，论文中提出了两种生成方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Sequence方法与Pointer Net相同，即根据每一个时刻attention的align向量生成一个词位置，直到生成终止符为止。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Boundary方法则是利用SQuAD数据集的答案均是出现在passage中连续的序列这一特点，该方法仅生成首尾两个位置，依据起始位置和终止位置来截取passage的一部分作为最终的答案。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文在SQuAD数据集上进行实验，两种方法实验结果较之传统LR方法均有大幅度提升。其中Boundary方法比Sequence方法效果更好。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" data-editable="true" data-title="SQuAD"&gt;SQuAD&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;数据集相关论文SQuAD: 100,000+ Questions for Machine Comprehension of Text模型相关论文Learning Natural Language Inference with LSTMPointer networks&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本篇论文提出的模型是第一个在SQuAD语料上应用端到端神经网络的模型，该模型将Match-LSTM和Pointer Networks结合在一起，利用了文本之间的蕴含关系更好地预测答案。本文提出了两种方法来生成答案，其中Boundary方法巧妙地利用SQuAD数据集的答案均是文本中出现过的连续序列这一特点，只生成答案的起始和终止位置，有效地提升了模型的效果。&lt;/p&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/pdf/1607.06275v2.pdf" data-editable="true" data-title="Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering" class=""&gt;Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Baidu IDL&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Question Answering, Sequence Labeling, CRF&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv, 201609&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;作者给出了一个新的中文的QA数据集, 并且提出了一个非常有意思的baseline model.&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;1、WebQA Dataset&lt;/p&gt;&lt;p&gt;作者来自百度IDL, 他们利用百度知道和一些其他的资源, 构建了这个中文的QA数据集. 这个数据集里所有的问题都是factoid类型的问题, 并且问题的答案都只包含一个entity (但是一个entity可能会包含多个单词). 对于每个问题, 数据集提供了若干个’evidence’, 这些evidence是利用搜索引擎在网络中检索的.&lt;/p&gt;&lt;p&gt;2、Recurrent Sequence Labeling Model&lt;/p&gt;&lt;p&gt;作者把QA类型的问题看做sequence labeling问题, 给出的模型大概分三部分:&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4382b4020263b01609342f3b296e46ad.png" data-rawwidth="741" data-rawheight="385"&gt;&lt;/p&gt;&lt;p&gt;（1）Question LSTM这部分很简单, 就是普通的单向LSTM, 对整个Question sequence进行encoding, 之后计算self-attention, 并用attention对question encoding求加权平均作为问题的representation.&lt;/p&gt;&lt;p&gt;（2）Evidence LSTMs这部分比较有意思, 首先, 作者从数据中提取出两种feature: 每个词是否在question和evidence中共同出现, 以及每个词是否同时在多个evidence中出现. 之后, 模型用一个三层的单向LSTM对evidence/quesiton/feature进行编码.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一层: 将evidence/question representation/feature进行连接, 放进一个正向LSTM.&lt;/li&gt;&lt;li&gt;第二层: 将第一层的结果放入一个反向LSTM.&lt;/li&gt;&lt;li&gt;第三层: 将第一层和第二层的结果进行连接, 放进一个正向LSTM.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;（3）CRF经过evidence LSTMs, question和evidence的representation已经揉在一起, 所以并不需要其他QA模型(主要是Attention Sum Reader)广泛用的, 用question representation和story representation进行dot product, 求cosine similarity. 这时候只需要对evidence representation的每一个time step进行分类就可以了, 这也是为什么作者将数据标注成IOB tagging的格式, 我们可以直接用一个CRF层对数据进行预测. 在一些实验中, 作者将答案之前的词用O1, 答案之后的词用O2进行标注, 这又给了模型关于非答案词的位置信息(正确答案是在这个词的前面还是后面).&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://idl.baidu.com/webqa.html" data-editable="true" data-title="WebQA dataset"&gt;WebQA dataset&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/baidu/Paddle" data-editable="true" data-title="Baidu Paddle"&gt;Baidu Paddle&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;关于CRF进行序列标注的问题, 可以参考这篇文章.Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. arXiv:1508.01991v1.&lt;/li&gt;&lt;li&gt;关于multi-word答案选择在SQuAD dataset上的模型, 可以参考这篇.Shuohang Wang, Jing Jiang. 2016. Machine Comprehension Using Match_LSTM and Answer Pointer. arXiv: 1608.07905v1.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;首先对所有release数据集的人表示感谢.关于dataset部分, 百度利用了自己庞大的资源收集数据. 第一, 百度知道里的问题都是人类问的问题, 这一点相比于今年前半年比较流行的CNN/CBT等等cloze style的问题, 要强很多. 第二, 数据集中包含了很多由多个词组成的答案, 这也使数据集的难度大于CNN/CBT这种单个词作为答案的数据. 第三, 对于每个问题, 并没有给出备选答案, 这使得对于答案的搜索空间变大(可以把整个evidence看做是备选答案). 第四, 对于每一个问题, dataset中可能有多个supporting evidence, 这也迎合了最近multi-supporting story的趋势, 因为对于有些问题, 答案并不只在某一个单一的文章中(对于百度来说, 如果搜索一个问题, 那么答案并不一定在单一的搜索结果网页中), 那么一个好的model需要在有限的时间内对尽可能多的搜索结果进行检索.&lt;/p&gt;&lt;p&gt;关于model部分, 本文尝试将QA问题看做是序列标注问题, 某种意义上解决了multiword answer的难点. 熟悉前半年QA paper的人都会对Attention Sum Reader以及延伸出来的诸多模型比较熟悉, 由于用了类似Pointer Network的机制, 一般的模型只能从文中选择story和question的cosine similarity最高的词作为答案, 这使得multiple word answer很难处理, 尤其是当multiple answer word不连续的时候, 更难处理. 而CRF是大家都熟知的简单高效的序列标注工具, 把它做成可训练的, 并且放在end to end模型中, 看起来是非常实用的. 在Evidence LSTM的部分, 加入的两个feature据作者说非常有帮助, 看起来在deep learning 模型中加入一些精心设计的feature, 或者IR的要素, 有可能能够对模型的performance给予一定的提升. 在entropy的角度, 虽然不一定是entropy reduction, 因为这些信息其实本来已经包含在question/evidence中了, 但是有可能因为你提供给模型这些信息, 它就可以把更多精力用在一些其他的特征上?&lt;/p&gt;&lt;p&gt;另外值得一提的是, 最近Singapore Management University的Wang and Jiang也有所突破, 在SQuAD dataset(也是multiple word answer)上一度取得了state of the art的结果, 他们用的mLSTM模型也十分有趣.&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;这一类model都大量使用了Recurrent Neural Network(LSTM或者GRU)对text进行encoding，得到一个sequence的hidden state vector。然后通过inner product或者bilinear term比较不同位置hidden state vector之间的similarity来计算它们是正确答案的可能性。可见Recurrent Neural Network以及对于Similarity的定义依旧是解决此类问题的关键所在，更好地改良这一类模型也是提升准确率的主流方法。笔者认为，similarity的计算给了模型从原文中搜索答案的能力，然而模型非常缺乏的是推理和思考的能力（其实也有相关工作&lt;a href="http://arxiv.org/abs/1508.05508" data-editable="true" data-title="Towards Neural Network-based Reasoning"&gt;Towards Neural Network-based Reasoning&lt;/a&gt;），如果模型能够配备逻辑思考能力，那么解决问题的能力会大大增强。非常期待有新的思路能够出现在这一领域中，令AI能够更好地理解人类语言。&lt;/p&gt;&lt;p&gt;以上为本期PaperWeekly的主要内容，感谢&lt;strong&gt;eric yuan&lt;/strong&gt;、&lt;strong&gt;destinwang&lt;/strong&gt;、&lt;strong&gt;zewei chu&lt;/strong&gt;、&lt;strong&gt;韩晓伟&lt;/strong&gt;四位同学的整理。 &lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ea9f4f4503d2ed9e15a56fe56e26e98d.jpg" data-rawwidth="430" data-rawheight="430"&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22577648&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Fri, 23 Sep 2016 09:30:09 GMT</pubDate></item><item><title>cs.CL weekly 2016.09.12-2016.09.16</title><link>https://zhuanlan.zhihu.com/p/22471808</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本周（2016.09.12-2016.09.16）质量较高的arXiv cs.CL的paper如下：（点击标题可看原文）&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.75/arxiv.org/pdf/1609.02846v1.pdf" data-editable="true" data-title="Dialogue manager domain adaptation using Gaussian process reinforcement learning" class=""&gt;Dialogue manager domain adaptation using Gaussian process reinforcement learning&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文是Steve Young组的一篇大作，文中详细介绍了Gaussian process reinforcement learning框架的思路和优势，并且在多个对话领域中进行了实验并得到更好的结果。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.02745v1.pdf" data-editable="true" data-title="A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis" class=""&gt;A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出用分层双向LSTM模型对网站评论数据进行观点挖掘，发表在EMNLP 2016。该作者今天在arxiv上提交了三篇同类问题不同解决方案的paper，对评论观点和情感挖掘的童鞋可作参考。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.03286v1.pdf" data-editable="true" data-title="Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks" class=""&gt;Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出了用先验知识+attention network的模型，用来解决了自然语言理解存在问题：通过从少量训练数据中捕获重要子结构，来缓解测试集中的unseen data问题，同时提高理解能力。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.03193v2.pdf" data-editable="true" data-title="Wav2Letter: an End-to-End ConvNet-based Speech Recognition System"&gt;Wav2Letter: an End-to-End ConvNet-based Speech Recognition System&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出了一种语音识别的端到端模型，基于CNN和graph decoding，在不依赖因素对齐的前提下，输出letters。本文工作来自Facebook AI。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.03976v1.pdf" data-editable="true" data-title="Multimodal Attention for Neural Machine Translation"&gt;Multimodal Attention for Neural Machine Translation&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文通过利用image caption的多模态、多语言数据构建了一个NMT模型，模型的输入不仅是source language，还有所描述的图像，输出是target language。通过输入更多的信息，得到了更好的效果。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.03632v1.pdf" data-editable="true" data-title="Joint Extraction of Events and Entities within a Document Context"&gt;Joint Extraction of Events and Entities within a Document Context&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文针对传统信息抽取方法将event和entity分开考虑的问题，提出了在docuemnt-level context下考虑event和entity之间关系进行信息抽取的新方法，取得了非常好的结果。本文发表在NAACL2016.&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.75/arxiv.org/pdf/1609.03777v1.pdf" data-editable="true" data-title="Character-Level Language Modeling with Hierarchical Recurrent Neural Networks" class=""&gt;Character-Level Language Modeling with Hierarchical Recurrent Neural Networks&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;语言模型问题上，char-level可以很好地解决OOV的问题，但效果不如word-level，本文针对该问题提出了一种分层模型，同时兼顾word-level和char-level的优势。本文发表在nips2016。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.04186v1.pdf" data-editable="true" data-title="Neural Machine Translation with Supervised Attention" class=""&gt;Neural Machine Translation with Supervised Attention&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;attention机制可以动态地对齐source和target words，但准确率不如传统方法。本文提出了用传统方法作为teacher，来“教”model学习alignment，模型称为supervised attention。本文已投稿COLING2016，在审。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1609.04309v1.pdf" data-editable="true" data-title="Efficient softmax approximation for GPUs" class=""&gt;Efficient softmax approximation for GPUs&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出了一种高效的softmax近似方法，并且可以方便地进行并行计算。本文称之为adaptive softmax，根据词分布进行聚类，极大地提高了计算效率并保证了不错的准确率。本文工作来自Facebook AI Research。&lt;/p&gt;&lt;p&gt;在自然语言生成任务中常常面临word vocabulary size太大的困境，softmax的效率非常低，本文给出了一种快速计算的方法。Tomas Mikolov之前也提到过类似的思路。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.04779v1.pdf" data-editable="true" data-title="Characterizing the Language of Online Communities and its Relation to Community Reception"&gt;Characterizing the Language of Online Communities and its Relation to Community Reception&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文研究了在线社区语言的style和topic哪个更具代表性，这里style用复合语言模型来表示，topic用LDA来表示，通过Reddit Forum实验得到style比topic更有代表性。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.04621v1.pdf" data-editable="true" data-title="Factored Neural Machine Translation"&gt;Factored Neural Machine Translation&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;针对机器翻译领域中两个常见的问题：1、目标语言词汇表过大；2、OOV问题；利用了单词的词形和语法分解，提出了一种新的NMT模型，并取得了满意的效果。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.04628v1.pdf" data-editable="true" data-title="Context Aware Nonnegative Matrix Factorization Clustering"&gt;Context Aware Nonnegative Matrix Factorization Clustering&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;大多数paper都在研究NMF在聚类中的初始化和优化部分，而本文关注的点在于最后的聚类分配上。本文被 ICPR 2016全文收录。&lt;/p&gt;&lt;p&gt;以下内容为arXiv外的优质内容：&lt;/p&gt;&lt;h1&gt;&lt;a href="http://www.sigdial.org/workshops/conference17/proceedings/SIGDIAL-2016.pdf" data-editable="true" data-title="SIGDIAL 2016 Accepted Paper"&gt;SIGDIAL 2016 Accepted Paper&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;SIGdial是ACL下面的一个关于对话系统地特别兴趣小组，每年开一次会。今年的会议最近正在开，会议录用的所有paper都已经放出。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://speech.sv.cmu.edu/software.html" data-editable="true" data-title="CMU SPEECH Team Homepage" class=""&gt;CMU SPEECH Team Homepage&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;CMU SPEECH Team的主页，包括他们的开源软件Yoda和publication及其开源实现。&lt;/p&gt;&lt;h1&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/?st=ISZ6YT6D&amp;amp;sh=02bd0722" data-editable="true" data-title="Machine Learning - WAYR (What Are You Reading)" class=""&gt;Machine Learning - WAYR (What Are You Reading)&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;reddit上的这个帖子很有意思，和paperweekly想做的一个事情非常像，就是可以让读类似或者同一篇paper的童鞋得到充分交流。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）每天都会分享当天arXiv cs.CL板块刷新的高质量paper知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22471808&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Sat, 17 Sep 2016 08:28:05 GMT</pubDate></item><item><title>PaperWeekly 第五期------从Word2Vec到FastText</title><link>https://zhuanlan.zhihu.com/p/22466665</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ded3556dca46e452ac525a5b7beecf3c_r.png"&gt;&lt;/p&gt;&lt;h1&gt;引&lt;/h1&gt;&lt;p&gt;Word2Vec从提出至今，已经成为了深度学习在自然语言处理中的基础部件，大大小小、形形色色的DL模型在表示词、短语、句子、段落等文本要素时都需要用word2vec来做word-level的embedding。Word2Vec的作者Tomas Mikolov是一位产出多篇高质量paper的学者，从RNNLM、Word2Vec再到最近流行的FastText都与他息息相关。一个人对同一个问题的研究可能会持续很多年，而每一年的研究成果都可能会给同行带来新的启发，本期的PaperWeekly将会分享其中三篇代表作，分别是：&lt;/p&gt;&lt;p&gt;1、Efficient Estimation of Word Representation in Vector Space, 20132、Distributed Representations of Sentences and Documents, 20143、Enriching Word Vectors with Subword Information, 2016&lt;/p&gt;&lt;h1&gt;&lt;a href="https://arxiv.org/pdf/1301.3781.pdf" data-editable="true" data-title="Efficient Estimation of Word Representation in Vector Space" class=""&gt;Efficient Estimation of Word Representation in Vector Space&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Google Inc., Mountain View, CA&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Word Representation, Word Embedding, Neural Network, Syntactic Similarity, and Semantic Similarity&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv, 201309&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何在一个大型数据集上快速、准确地学习出词表示？&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;传统的NNLM模型包含四层，即输入层、映射层、隐含层和输出层，计算复杂度很大程度上依赖于映射层到隐含层之间的计算，而且需要指定上下文的长度。RNNLM模型被提出用来改进NNLM模型，去掉了映射层，只有输入层、隐含层和输出层，计算复杂度来源于上一层的隐含层到下一层隐含层之间的计算。&lt;/p&gt;&lt;p&gt;本文提出的两个模型CBOW (Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)结合了上面两个模型的特点，都是只有三层，即输入层、映射层和输出层。CBOW模型与NNLM模型类似，用上下文的词向量作为输入，映射层在所有的词间共享，输出层为一个分类器，目标是使当前词的概率最大。Skip-gram模型与CBOW的输入跟输出恰好相反，输入层为当前词向量，输出层是使得上下文的预测概率最大，如下图所示。训练采用SGD。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/3b94982780b22466c499eaff3e04df65.jpg" data-rawwidth="362" data-rawheight="223"&gt;&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;Code: &lt;a href="https://code.google.com/archive/p/word2vec/" data-editable="true" data-title="C++代码"&gt;C++代码&lt;/a&gt;Dataset: &lt;a href="https://sites.google.com/site/semeval2012task2/" data-editable="true" data-title="SemEval-2012"&gt;SemEval-2012&lt;/a&gt;,用来评估语义相关性。&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;Bengio[1]在2003年就提出了language model的思路，同样是三层（输入层，隐含层和输出层）用上下文的词向量来预测中间词，但是计算复杂度较高，对于较大的数据集运行效率低；实验中也发现将上下文的n-gram出现的频率结合进去会提高性能，这个优点体现在CBOW和Skip-gram模型的输出层中，用hierarchical softmax（with huffman trees）来计算词概率。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文的实验结果显示CBOW比NNLM在syntactic和semantic上的预测都要好，而Skip-gram在semantic上的性能要优于CBOW，但是其计算速度要低于CBOW。结果显示用较大的数据集和较少的epoch，可以取得较好的效果，并且在速度上有所提升。与LSI和LDA相比，word2vec利用了词的上下文，语义信息更加丰富。基于word2vec，出现了phrase2vec, sentence2vec和doc2vec，仿佛一下子进入了embedding的世界。NLP的这些思想也在用于recommendation等方面，并且与image结合，将image跟text之间进行转换。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1405.4053v2.pdf" data-editable="true" data-title="Distributed Representations of Sentences and Documents" class=""&gt;Distributed Representations of Sentences and Documents&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Quoc V. Le, Tomas Mikolov&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Google Inc, Mountain View, CA&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;sentence representation&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ICML 2014&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;基于word2vec的思路，如何表示sentence和document？&lt;/p&gt;&lt;h2&gt;模型&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c6dac075c8ed2bb00718673967777bb1.jpg" data-rawwidth="418" data-rawheight="245"&gt;&lt;/h2&gt;&lt;p&gt;利用one-hot的表示方法作为网络的输入，乘以词矩阵W，然后将得到的每个向量通过平均或者拼接的方法得到整个句子的表示，最后根据任务要求做一分类，而这过程中得到的W就是词向量矩阵，基本上还是word2vec的思路。&lt;/p&gt;&lt;p&gt;接下来是段落的向量表示方法：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ef7668051ab7f574ab1e41a0f8d3f0eb.jpg" data-rawwidth="400" data-rawheight="225"&gt;依旧是相同的方法，只是在这里加上了一个段落矩阵，用以表示每个段落，当这些词输入第i个段落时，通过段落id就可以从这个矩阵中得到相对应的段落表示方法。需要说明的是，在相同的段落中，段落的表示是相同的。文中这样表示的动机就是段落矩阵D可以作为一个memory记住在词的context中遗失的东西，相当于增加了一个额外的信息。这样经过训练之后，我们的就得到了段落表示D，当然这个段落就可以是一段或者一篇文章。&lt;/p&gt;&lt;p&gt;最后一种就是没有词序的段落向量表示方法：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a9781307fe0c25097e3674bc72d6d372.jpg" data-rawwidth="400" data-rawheight="261"&gt;从图中就可以感觉到这个方法明显和skip-gram非常相似，这里只是把重点放在了段落的表示中，通过段落的表示，来预测相应的context 词的表示。最后我们依然可以得到段落矩阵D，这样就可以对段落进行向量化表示了。但是输入起码是句子级别的表示，而输出则是词的向量表示，因此个人比较怀疑这种方法的合理性。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;这篇文章是word2vec的方法提出一年后提出的方法，因此本文并没有使用目前非常流行的word2vec的训练方法来训练词向量，而是利用word2vec的思路，提出了一种更加简单的网络结构来训练任意长度的文本表示方法。这样一方面好训练，另一方面减少了参数，避免模型过拟合。优点就是在训练paragraph vector的时候加入了一个paragraph matrix，这样在训练过程中保留了一部分段落或者文档信息。这点在目前看来也是有一定优势的。但是目前深度学习发展迅速，可以处理非常大的计算量，同时word2vec以及其变种被应用得非常普遍，因此该文章提出的方法思路大于模型，思路我们可以借鉴，模型就不具有优势了。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.80/arxiv.org/pdf/1607.04606v1.pdf" data-editable="true" data-title="Enriching Word Vectors with Subword Information"&gt;Enriching Word Vectors with Subword Information&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;Facebook AI Research&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Word embedding, morphological, character n-gram&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv, 201607&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何解决word2vec方法中罕见词效果不佳的问题，以及如何提升词形态丰富语言的性能？&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。&lt;/p&gt;&lt;p&gt;方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。&lt;/p&gt;&lt;p&gt;实验分为三个部分，分别是（1）计算两个词之间的语义相似度，与人类标注的相似度进行相关性比较；（2）与word2vec一样的词类比实验；（3）与其他考虑morphology的方法比较。结果是本文方法在语言形态丰富的语言（土耳其语，法语等）及小数据集上表现优异，与预期一致。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;源码公布在Facebook的fastText项目中：&lt;a href="https://github.com/facebookresearch/fastText" data-editable="true" data-title="GitHub - facebookresearch/fastText: Library for fast text representation and classification."&gt;GitHub - facebookresearch/fastText: Library for fast text representation and classification.&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;利用语言形态学来改进nlp的研究源远流长，本文提及的许多关于character-level和morphology的有趣工作值得参考。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;文章中提出的思路对于morphologically rich languages（例如土耳其语，词缀的使用极为普遍而有趣）来说十分有意义。词缀作为字母与单词之间的中层单位，本身具有一定的语义信息。通过充分利用这种中层语义来表征罕见词汇，直观上讲思路十分合理，也是应用了compositionality的思想。&lt;/p&gt;&lt;p&gt;利用形态学改进word embedding的工作十分丰富，但中文NLP似乎很难利用这一思路。其实个人感觉中文中也有类似于词缀的单位，比如偏旁部首等等，只不过不像使用字母系统的语言那样容易处理。期待今后也有闪光的工作出现在中文环境中。&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;从Word2Vec到FastText，从word representation到sentence classification，Tomas Mikolov的工作影响了很多人。虽然有个别模型和实验结果曾遭受质疑，但终究瑕不掩瑜。word2vec对NLP的研究起到了极大地推动作用，其实不仅仅是在NLP领域中，在其他很多领域中都可以看到word2vec的思想和作用，也正是从word2vec开始，这个世界变得都被vector化了，person2vec，sentence2vec，paragraph2vec，anything2vec，world2vec。&lt;/p&gt;&lt;p&gt;以上为本期Paperweekly的主要内容，感谢memray、zhkun、gcyydxf、jell四位同学的整理。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;p&gt;http://weixin.qq.com/r/OUW0rA3ExVC6rUnP9xAr (二维码自动识别)&lt;/p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22466665&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Fri, 16 Sep 2016 12:49:32 GMT</pubDate></item><item><title>cs.CL weekly 2016.09.05-2016.09.09</title><link>https://zhuanlan.zhihu.com/p/22390774</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ea9f4f4503d2ed9e15a56fe56e26e98d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本周（2016.09.05-2016.09.09）质量较高的arXiv cs.CL的paper如下：（点击标题可看原文）&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.00718v1.pdf" data-editable="true" data-title="Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level" class=""&gt;Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;张潼老师的文章，通过实验对比了shallow word-level CNN（本文工作）和deep char-level CNN模型在而文本分类任务上的表现，结论是本文工作又快又准。&lt;/p&gt;&lt;p&gt;（这篇文章对于选择char-level还是word-level做文本分类非常有指导意义）&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.00565v1.pdf" data-editable="true" data-title="Skipping Word: A Character-Sequential Representation based Framework for Question Answering" class=""&gt;Skipping Word: A Character-Sequential Representation based Framework for Question Answering&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文用char-level CNN模型来做句子表示，然后进行question和answer之间的相关匹配学习，CIKM2016 short paper accepted。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.78/arxiv.org/pdf/1609.00777v1.pdf" data-editable="true" data-title="End-to-End Reinforcement Learning of Dialogue Agents for Information Access"&gt;End-to-End Reinforcement Learning of Dialogue Agents for Information Access&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文是微软研究软邓力老师的文章，构建了一种从知识图谱中形成response的聊天机器人KB-InfoBot，并且提出了一种端到端的增强学习训练方案。&lt;/p&gt;&lt;p&gt;（本文对于构建一个端到端的KB + task-oriented chatbot非常有启发和指导意义）&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.01462v1.pdf" data-editable="true" data-title="Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks"&gt;Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出一种模型，将intent detection、slot filling和language modeling融合在一起进行学习，用于解决对话系统中的SLU task。本文是SIGDIAL 2016 paper。&lt;/p&gt;&lt;p&gt;用到的数据集在Dropbox有一份&lt;a href="http://t.cn/Rcbcpfl" data-editable="true" data-title="copy"&gt;copy&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.01454v1.pdf" data-editable="true" data-title="Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling"&gt;Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;和上一篇paper是同一个作者，解决的是同一个问题。将RNN换成了attention-based RNN，被另外一个会议录取。(有点灌水的意思)&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.77/arxiv.org/pdf/1609.02116v1.pdf" data-editable="true" data-title="Ask the GRU: Multi-task Learning for Deep Text Recommendations" class=""&gt;Ask the GRU: Multi-task Learning for Deep Text Recommendations&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文提出了用端到端的解决方案来做paper的推荐任务，用GRU将文本序列（标题、摘要等）encode到一个latent vector中。并且通过多任务学习来完成内容推荐和条目预测两个task，取得了不错的效果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以下内容为arXiv外的优质内容：&lt;/b&gt;&lt;/p&gt;&lt;h1&gt;&lt;a href="http://www.matthen.com/research/papers/Discriminative_Methods_for_Statistical_Spoken_Dialogue_Systems_Matthew_Henderson_PhD_Thesis.pdf" data-editable="true" data-title="Discriminative Methods for Statistical Spoken Dialogue Systems" class=""&gt;Discriminative Methods for Statistical Spoken Dialogue Systems&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;剑桥大学Spoken Dialogue System组毕业的Matthew Henderson博士，师从于Steve Young教授，研究领域是对话系统中的Dialogue State Tracking，主要特色是用transfer learning来解决discriminative model的扩展性和通用性。&lt;/p&gt;&lt;p&gt;如果你对chatbot感兴趣，强烈建议好好研读一下这篇博士论文。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/main.pdf" data-editable="true" data-title="CONNECTING IMAGES AND NATURAL LANGUAGE" class=""&gt;CONNECTING IMAGES AND NATURAL LANGUAGE&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;斯坦福大学Feifei Li的博士生Andrej Karpathy的PhD thesis，Karpathy维护着几个非常流行的开源代码库，并且有着一个影响力非常大的博客。名师出高徒，这篇博士博士论文值得一看！&lt;/p&gt;&lt;p&gt;最近，他更新了一篇博客，谈论了一些自己对读博的思考和建议。 &lt;a href="http://karpathy.github.io/2016/09/07/phd/" data-editable="true" data-title="A Survival Guide to a PhD"&gt;A Survival Guide to a PhD&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;&lt;a href="https://pan.baidu.com/share/link?shareid=317480&amp;amp;uk=1594817379" data-editable="true" data-title="Mendeley Docs"&gt;Mendeley Docs&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;paper越看越多，一个优秀的paper管理工具就变得非常必要了，Mendeley是其中最优秀的代表之一。&lt;/p&gt;&lt;p&gt;Easily organize your papers, read &amp;amp; annotate your PDFs, collaborate in private or open groups, and securely access your research from everywhere.&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;p&gt;http://weixin.qq.com/r/OUW0rA3ExVC6rUnP9xAr (二维码自动识别)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22390774&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Sat, 10 Sep 2016 13:06:12 GMT</pubDate></item><item><title>PaperWeekly第四期------基于强化学习的文本生成技术</title><link>https://zhuanlan.zhihu.com/p/22385421</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e230b2395dd90e3491d83701bc3ec152_r.jpg"&gt;&lt;/p&gt;&lt;h1&gt;引&lt;/h1&gt;&lt;p&gt;2013年以来Deep mind团队相继在NIPS和Natures上发表了用深度增强（强化）学习玩Atari游戏，并取得良好的效果，随后Alpha go与李世乭的一战更使得深度增强学习家喻户晓。在游戏上取得了不错的成果后，深度增强学习也逐渐被引入NLP领域。本期介绍目前NLP领域较为热点的研究方向，基于强化学习的文本生成技术（NLG），共选择了三篇文章，分别为：&lt;/p&gt;&lt;p&gt;(1)《Generating Text with Deep Reinforcement Learning》应用Deep Q-Network作为生成模型用于改善seq2seq模型&lt;/p&gt;&lt;p&gt;(2) 《Deep Reinforcement Learning for Dialogue Generation》应用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型&lt;/p&gt;&lt;p&gt;(3)《Hierarchical Reinforcement Learning for Adaptive Text Generation_lshowway》以任务为导向的户内导航对话系统用分层强化学习进行文本生成&lt;/p&gt;&lt;p&gt;以下为三篇文章的主要信息：&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1510.09202v1.pdf" data-editable="true" data-title="Generating Text with Deep Reinforcement Learning"&gt;Generating Text with Deep Reinforcement Learning&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Hongyu Guo&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;National Research Council Canada&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Reinforcement Learning、Seq2Seq、Text Generation&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;NIPS2015 Workshop (2015.10.30)&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;本文提出将Deep Q-Network作为生成模型用于改善seq2seq模型，将decoding修改为迭代式的过程，实验表明本模型具有更好的泛化性。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;对seq2seq模型改进的论文层出不穷，本文率先引入深度强化学习的思想，将DQN用于文本生成。对DQN还不了解的同学可以先阅读DeepMind的论文Playing Atari with Deep Reinforcement Learning。本文的模型如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/eeb5b40e85d79a6db4d937aa37924be0.jpg" data-rawwidth="445" data-rawheight="251"&gt;&lt;/p&gt;&lt;p&gt;如同一般的神经网络，我们也可以把DQN当做一个黑盒来使用。只需要准备好DQN需要的四个元素s(i),a(i),r(i),s(i+1)，分别代表i时刻下state,action,reword和i+1时刻的state。&lt;/p&gt;&lt;p&gt;对照上图我们把算法解剖分为4个步骤：&lt;/p&gt;&lt;p&gt;Step 1: 先是传统的seq2seq模型。通过LSTM先把输入序列encode为一个定长向量EnSen(i)，然后作为decode阶段的初始状态依次生成新的序列DeSen(i)（decoding search使用beam search算法来 expand next words）。经过第一步我们得到初始state：(EnSen(i), DeSen(i))和action集合：每个位置的hypotheses。&lt;/p&gt;&lt;p&gt;Step 2: 接下来从hypotheses（actions）中选择一个可以获得最大reward的单词（action）作为该位置新生成的词，用新单词来代替之前的旧词，于是生成新的state：(EnSen(i), DeSen(i+1))。&lt;/p&gt;&lt;p&gt;Step 3: 接着就是标准的DQN的部分，计算Loss函数并对其应用梯度下降。&lt;/p&gt;&lt;p&gt;Step 4: 回到Step 2，对得到的state继续迭代，每一次迭代都只生成一个新词来代替旧词，直到迭代次数达到设好的值（作者将次数定为句子长度的两倍，同学们可以思考一下理由）。&lt;/p&gt;&lt;p&gt;总结DQN所需的四个元素对应如下：(1) i时刻下的state：(EnSen(i), DeSen(i))；(2) i时刻下的action：beam search得到的每个位置的hypotheses；(3) i时刻下的reword：target sentence和DeSen(i+1)的相似度（BLEU score）；(4) i+1时刻下的state：(EnSen(i), DeSen(i+1))；&lt;/p&gt;&lt;p&gt;为了更好的提取句子的特征，作者在decode阶段使用了双向LSTM。同时还在reinforcement learning中加入attention机制，可以达到先decode比较简单的部分再处理困难部分的效果。最后在生成相似句子的实验中得到了比只用LSTM decoder效果更好的结论：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/5d92477453549f38a825cf8a09fd27c3.jpg" data-rawwidth="591" data-rawheight="78"&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1533d7282033cfba05652bf1d4f6541e.jpg" data-rawwidth="529" data-rawheight="139"&gt;&lt;/h2&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文的思想其实非常符合写作的一种情况，就像贾岛推敲的故事，回想小时候刚学习写句子时，也不能一次写好，总会不断对一些词语进行修改。Google DeepMind的文章《DRAW：A Recurrent Neural Network For Image》也和本文异曲同工：画画也不是一次画好，也要不断的完善。不同之处在于本文率先引入DQN做文本生成。在机器学习各个分支下，强化学习和人类与环境的交互方式非常相似，在许多领域开始初露头角，期待看到更多将强化学习结合语言模型的应用。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1606.01541v3.pdf" data-editable="true" data-title="Deep Reinforcement Learning for Dialogue Generation" class=""&gt;Deep Reinforcement Learning for Dialogue Generation&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;(1) Stanford University, Stanford, CA, USA(2) Microsoft Research, Redmond, WA, USA(3) Ohio State University, OH, USA&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Reinforcement Learning、Seq2Seq、Text Generation&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;arXiv.org(2016.06.25)&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;强化学习中的reward&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/41a37779955dd98c73489e7ae6462786.jpg" data-rawwidth="418" data-rawheight="67"&gt;&lt;/p&gt;&lt;p&gt;易被响应（Ease of answering），不容易出现对话僵局，其中 S 是无意义回答合集，s是某一时刻的响应&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8a4cd62898e6529a1081adf455ec209b.jpg" data-rawwidth="498" data-rawheight="74"&gt;&lt;/p&gt;&lt;p&gt;信息流，若开辟新的话题，有利于对话的继续发展，隐层表示 hpi 和 hpi+1 的夹角余弦&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d4cb3c510958387b995d38389603a7ab.jpg" data-rawwidth="470" data-rawheight="60"&gt;&lt;/p&gt;&lt;p&gt;语义连贯性，减少与对话无关问题的影响，其中，pseq2seq(a|pi,qi) 是由上一轮状态得到响应的概率，后一项是由当前产生响应通过网络生成之前的 qi 的概率。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/796302276156e6dab686864c3cdde67c.jpg" data-rawwidth="471" data-rawheight="55"&gt;&lt;/p&gt;&lt;p&gt;最终的reward是对三者加权求和，系数分别为：0.25、0.25、0.5.&lt;/p&gt;&lt;p&gt;对比试验：(1) 对话初始状态为一个SEQ2SEQ加attention的模型作为强化学习的初始状态。&lt;/p&gt;&lt;p&gt;(2) 在前面的基础上将最大互信息加入其中作为reward，对于一个给定的输入[pi,qi]，可以根据模型生成一个候选回答集合A。对于A中的每一个回答a,从预训练模型中得到的概率分布上可以计算出互信息的值 m(a,[pi,qi])。&lt;/p&gt;&lt;p&gt;(3) 将互信息训练过的模型作为初始模型，用策略梯度更新参数并加入课程学习策略，最终最多限定五轮对话。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e230b2395dd90e3491d83701bc3ec152.jpg" data-rawwidth="537" data-rawheight="273"&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/fbf89747e22b3dde78fded6cb406f50e.jpg" data-rawwidth="618" data-rawheight="199"&gt;&lt;/h2&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文作者提出了一个强化学习框架，模拟两个agent让其自动对话训练神经网络SEQ2SEQ模型，将Encoder-Decoder模型和强化学习整合，从而能保证使对话轮数增加。文中使用的模型非常简洁，reward函数定义清晰，评价指标也较为科学，可以生成信息更为丰富、易于响应的对话系统。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://www.aclweb.org/anthology/W10-4204" data-editable="true" data-title="Hierarchical Reinforcement Learning for Adaptive Text Generation"&gt;Hierarchical Reinforcement Learning for Adaptive Text Generation&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Nina Dethlefs, Heriberto Cuay´ahuitl&lt;/p&gt;&lt;h2&gt;单位&lt;/h2&gt;&lt;p&gt;University of Bremen, Germany&lt;/p&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;NLG, 分层强化学习, 文本生成, wayfinding&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;国际自然语言生成会议INLG(2010)&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;在wayfinding（户内导航对话系统）领域利用分层强化学习进行文本生成。该方法的目标是对wayfinding的NLG任务整合进行优化，并在模拟系统中验证该方法的有效性。&lt;/p&gt;&lt;h2&gt;模型&lt;/h2&gt;&lt;p&gt;本文任务在wayfinding中的NLG任务有多个，且各个任务之间并非独立。从而提出应该根据用户类型，导航距离， 环境条件等作出不同的导航策略，介绍了分层强化学习。&lt;/p&gt;&lt;p&gt;文章将户内导航对话系统的文本生成问题分为四块：&lt;/p&gt;&lt;p&gt;(1) Content Selection：给不熟悉环境的用户的导航要比熟悉环境的用户的导航更细致(2) Text Structure：根据导航距离以及用户熟悉环境程度给予不同类型的导航，如大白话的，以fisrt， second…表达或者示意性的。(3) Referring Expression Generation：一间房间可以叫“A203”，也可以叫“办公室”或者“小白楼”(4) Surface Realisation：往前走可以用“go”也可以用“walk”等。&lt;/p&gt;&lt;p&gt;强化学习示意图如下，分层强化学习的思想与强化学习类似，但在强化学习的基础上加上层次，不同层次的模型处理不同层次的问题。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/84608ba0f6e25f6920b55839fdf3e5ec.jpg" data-rawwidth="275" data-rawheight="187"&gt;&lt;/p&gt;&lt;p&gt;agent根据当前状态，执行动作a与环境交互，之后环境产生一个新的状态s并返回给agent一个奖赏r（可正可负），强化学习的目标函数便是使agent获得奖赏r最大。&lt;/p&gt;&lt;p&gt;分层增强学习包含L个层，每层N个模型，如Figure 1是有15个agents的hierarchy，其中不同的agent负责不同的层次。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5a8fb2547d568b1efa67b691486a5e87.jpg" data-rawwidth="491" data-rawheight="201"&gt;&lt;/p&gt;&lt;p&gt;每个agent定义为半马尔科夫决策过程，可以表示成一个四元组&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7d2f305f56718569b40425f4ebea7db7.jpg" data-rawwidth="135" data-rawheight="30"&gt;&lt;/p&gt;&lt;p&gt;分别为状态集，动作集，转换函数，奖励函数。&lt;/p&gt;&lt;p&gt;奖励函数表示agent在时间t状态s是执行动作a转换到新的状态s’所获得的奖励。半马尔科夫的目标是找到policy π*，&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e7fdd4b0397777e5d2ca885814ed9031.jpg" data-rawwidth="269" data-rawheight="54"&gt;&lt;/p&gt;&lt;p&gt;使得在从当前状态转换到新的状态获得的累计奖励最多。&lt;/p&gt;&lt;p&gt;本文使用两种奖励函数，一种着重在 interaction length， 另一种着重在alignment and variation之间的平衡（具体公式可见论文）。&lt;/p&gt;&lt;p&gt;本文是在模拟环境中进行试验，其中模拟环境包括user type（熟悉环境，不熟悉环境）， information need（高，低），length of the current route（短，中长，长），next action to perform（转，直走），current focus of attention（继续走，关注标识）。baseline为为部分agent随机选择action，即不考虑用户类型，导航距离等因素。经与baseline比较，效果较好。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;词性标注工具：&lt;a href="http://nlp.stanford.edu/software/tagger.shtml" data-editable="true" data-title="The Stanford Natural Language Processing Group" class=""&gt;The Stanford Natural Language Processing Group&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;将来的工作：将分层强化学习应用于其他NLG任务不足之处：实验是在模拟环境下进行的，未来应该在真实环境进行评估。&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;这三篇文章皆是强化学习在NLP领域的应用，第一篇主要侧重点在于应用DQN进行文本生成，并用BLUE指标进行评价，对比传统的LSTM-decoder和加入DQN之后的结果；第二篇文章侧重点在于虚拟两个Agent，在传统Seq2Seq的基础上加入强化学习从而使得聊天能够持续下去；第三篇文章侧重点在于任务驱动的对话系统应用分层强化学习，针对不同情况进行分层处理。&lt;/p&gt;&lt;p&gt;以上为本期Paperweekly的主要内容，感谢lshowway、美好时光海苔、Tonya三位同学的整理。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;p&gt;http://weixin.qq.com/r/OUW0rA3ExVC6rUnP9xAr (二维码自动识别)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22385421&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Fri, 09 Sep 2016 21:34:21 GMT</pubDate></item><item><title>cs.CL weekly 2016.08.29-2016.09.02</title><link>https://zhuanlan.zhihu.com/p/22293954</link><description>&lt;p&gt;本周（2016.08.29-2016.09.02）质量较高的arXiv cs.CL的paper如下：（点击标题可看原文）&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.75/arxiv.org/pdf/1602.06023v5.pdf" data-editable="true" data-title="Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond" class=""&gt;Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;一篇老文的update，seq2seq+attention的机制来解决abstractive text summarization，针对文本摘要的关键问题在基础模型中增加了对关键词、词句层次性和低频词的处理。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.76/arxiv.org/pdf/1608.07905v1.pdf" data-editable="true" data-title="Machine Comprehension Using Match-LSTM and Answer Pointer" class=""&gt;Machine Comprehension Using Match-LSTM and Answer Pointer&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文基于Match-LSTM和Answer Pointer两个模型在Stanford Question Answering Dataset (SQuAD)上得到了state-of-the-art的结果。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.75/arxiv.org/pdf/1608.08716v1.pdf" data-editable="true" data-title="Measuring Machine Intelligence Through Visual Question Answering"&gt;Measuring Machine Intelligence Through Visual Question Answering&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;本文指出了image caption作为评测AI效果的任务存在的缺陷，同时提出用visual QA作为评测任务更加有效，并且给出了一个大型Visual QA的数据集。数据集地址：&lt;a href="http://www.visualqa.org" data-editable="true" data-title="VQA: Visual Question Answering"&gt;VQA: Visual Question Answering&lt;/a&gt;.&lt;/p&gt;&lt;h1&gt;&lt;a href="http://120.52.73.79/arxiv.org/pdf/1609.00070v1.pdf" data-editable="true" data-title="How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions" class=""&gt;How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;文章提出了一个好玩的任务，以一个统计数字作为上下文来生成一段简短的描述，描述的内容是一种带有这个数字的观点。整个过程分为两步：公式的构建和观点的生成。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;p&gt;http://weixin.qq.com/r/OUW0rA3ExVC6rUnP9xAr (二维码自动识别)&lt;/p&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ），每天会发布arXiv cs.CL高质量paper和简评。知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22293954&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Sat, 03 Sep 2016 11:49:16 GMT</pubDate></item><item><title>PaperWeekly 第三期</title><link>https://zhuanlan.zhihu.com/p/22279528</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d89931bc502f2f7326b1aba605d4184e_r.png"&gt;&lt;/p&gt;&lt;h1&gt;引&lt;/h1&gt;&lt;p&gt;历经半个月时间终于发布了新一期PaperWeekly，大家久等了。在这个半个月里，PaperWeekly发生了一些明显的变化。维护和运营从我一个人变成了一个十人左右的团队来一起做，小伙伴们来自全球各地，颠倒着黑夜和白天进行沟通。团队中的每个人都有一颗热爱知识和分享知识的心，都认为分享是一种美德，是一种付出，更是一种回报。可能我们不完美，但我们相信我们正在追求完美的路上坚定地走着。&lt;/p&gt;&lt;p&gt;有了更多的同学加入，PaperWeekly会更加多元化，不再受限于我个人感兴趣的方向和阅读、写作习惯。PaperWeekly会坚持每周发布一期文章，每一期的文章尽量围绕同一个topic展开，在微信公众号、官方微博和知乎专栏会同步更新，除了这一篇文章，我们还会坚持在微博上提供一个新的服务，cs.CL daily，帮助大家过滤掉arXiv cs.CL上比较水的paper，留下质量高的paper，并且用简评的方式分享在微博上，每周末会更新一篇cs.CL weekly出来，将一周值得读的cs.CL paper汇总发布。&lt;/p&gt;&lt;p&gt;PaperWeekly组织了一个高质量的NLP讨论群，只要有你相关的问题，群里的高手会第一时间站出来解答或者讨论你的问题，有的时候会给出一些开源code和相关的paper，提问者、讨论者和潜水者都会有很大的收获。分享paper导读的意义在于讨论，大家一起来讨论，才能更加充分地吸收paper里的营养，这也是我为什么组织一个讨论群的原因。&lt;/p&gt;&lt;p&gt;寒暄的话就说到这里，本期分享的topic是ACL 2016，一共10篇文章，涉及的内容包括：Logic Form、NMT、Summarization、QA、Chatbot等。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1073.pdf" data-editable="true" data-title="Sentence Rewriting for Semantic Parsing" class=""&gt;Sentence Rewriting for Semantic Parsing&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Semantic Parsing、Sentence Rewriting&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;语义分析的表现形式是将自然语言（natural language）转化成逻辑形式（logic form）。因语言表达多样性的问题导致两者间存在mismatch problem。&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;先给出一个语义分析的例子：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/72e0e629eadcca4a8bb9a987b066057d.jpg" data-rawwidth="416" data-rawheight="224"&gt;&lt;/p&gt;&lt;p&gt;给句子换个表达（How many people live in Berlin?），对应的逻辑形式就变得复杂很多(count(λx.person(x)∧live(x,Berlin)))。&lt;/p&gt;&lt;p&gt;作者认为，原句子和逻辑形式之间存在的结构不匹配导致了语义分析的困难，而结构不匹配的核心是词汇的不匹配。作者率先提出先把句子重写再转成目标逻辑形式的语义分析方案，如下图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e164c5bb6c5fce0fcb9250105e6f3ab4.jpg" data-rawwidth="429" data-rawheight="152"&gt;&lt;/p&gt;&lt;p&gt;针对词汇不匹配问题的两种情况分别给出基于字典和基于模板两种方法。&lt;/p&gt;&lt;p&gt;1）问题一：1-N mismatch是指一个单词（word）对应一个复合的逻辑形式（compound formula）。&lt;/p&gt;&lt;p&gt;例如daughter对应 child ∩ female。但在开放域的知识体系下，制定这些规则十分困难。于是作者提出将句子中的常用名词替换为字典（Wiktionary）中的解释，比如先把刚才的daughter转换为female child，接着再转换为逻辑形式child ∩ female就十分自然了。&lt;/p&gt;&lt;p&gt;2）问题二：N-1 mismatch是指将复杂的自然语言表达对应为单个逻辑表达。&lt;/p&gt;&lt;p&gt;例如将How many people live in Berlin?转化为λx.population(Berlin,x)的分析过程中，How many people live in被对应为逻辑式常量population。如同问题一，这样的规则实在过多，作者的思路是将复杂的表达式转化为简单的形式。&lt;/p&gt;&lt;p&gt;沿用之前的句子来了解算法流程。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6d59c63aedf120ee27d0b61d309bb890.jpg" data-rawwidth="432" data-rawheight="98"&gt;&lt;/p&gt;&lt;p&gt;Step 1 替换实体生成候选template，例如得到模板how many people live in #y。Step 2 检索template pairs来替换模板，例如找到(a：how many people live in #y, b：what is the population of #y)的模板对，于是将b作为新模板，Step 3 把实体替换回去得到容易生成逻辑形式的what is the population of Berlin。&lt;/p&gt;&lt;h2&gt;相关工作&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b2e89a59bcd76d62bdf840dd3c6370b8.jpg" data-rawwidth="419" data-rawheight="184"&gt;&lt;/h2&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;如今深度学习在自然语言处理领域大红大紫，也给语义分析的方法带来更多的思考。比如ACL2016另外一篇文章Language to Logical Form with Neural Attention，就把语义分析转换为seq2seq问题，进而使用深度学习的方法来解决。如果我们把词向量这样的表示形式比喻为粗糙的连结主义，那么逻辑表达就好比精细的形式主义。两者各有优势，希望以后会有更多结合两种思想的工作出现。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1004.pdf" data-editable="true" data-title="Language to Logical Form with Neural Attention" class=""&gt;Language to Logical Form with Neural Attention&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Logical Forms, Sequence to Sequence&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何把自然语言转化成Structured Logical Forms？&lt;/p&gt;&lt;h2&gt;文章思路&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2263e1dd44c4bf05b226d3c3b8c69205.jpg" data-rawwidth="353" data-rawheight="213"&gt;&lt;/h2&gt;&lt;p&gt;模型总体是一个encoder-decoder架构，input sequence首先通过LSTM encoder转化成一个vector，然后这个vector通过LSTM decoder被转化成Logical Forms。在decode过程中用到了一个attention layer去获取context信息。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2bf99675c901b85453aac1c833562a75.jpg" data-rawwidth="306" data-rawheight="301"&gt;&lt;/p&gt;&lt;p&gt;和encoder-decoder模型类似，作者提出了一种hierarchical decoder。与普通的decoder不同，首先，decode之后的sequence中存在一个特殊字符代表nonterminal。在nonterminal的基础上，decoder可以继续进行下一个layer的decoding。每一次decoding的输入不仅包含current hidden state,还包含这一个parent nonterminal的hidden state。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d125a2f36aa2cb05f32ddc3ab0bb6f84.jpg" data-rawwidth="279" data-rawheight="146"&gt;&lt;/p&gt;&lt;p&gt;作者还使用了一种attention机制，在构建current hidden state的时候将hidden state与所有encoder中的hidden state进行对比，给每一个encoder hidden state一个weight。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;代码：&lt;a href="https://github.com/donglixp/lang2logic" data-editable="true" data-title="GitHub - donglixp/lang2logic"&gt;GitHub - donglixp/lang2logic&lt;/a&gt;Jobs和GEO数据集：&lt;a href="http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf"&gt;http://www.cs.columbia.edu/~mcollins/papers/uai05.pdf&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;之前的大部分工作都采用一些parsing models，string-to-tree transformation rules，文中没有提到之前有人采用seq2seq/deep learning的方法。本文中使用的seq2seq方法主要来自Kalchbrenner, Blunsom, Cho, Sutskever 在machine translation中提出的模型。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文解决的是一个非常有趣的问题，将自然语言转换成结构化的Logical Forms。试想如果此模型能够很好的解决这个问题，那么将来的各种query language甚至programming languages都可以由自然语言转换而成。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1046.pdf" data-editable="true" data-title="Neural Summarization by Extracting Sentences and Words" class=""&gt;Neural Summarization by Extracting Sentences and Words&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Summarization、Hierarchical Document Encoder、Attention-based Extractor&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何使用数据驱动的方法来做提取式摘要？&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;本文针对的任务分为sentence和word两个level的summarization。sentence level是一个序列标签问题，每个句子有0或1两个标签，为1表示需要提取该句作为总结。而word level则是一个限定词典规模下的生成问题，词典规模限定为原文档中所有出现的词。&lt;/p&gt;&lt;p&gt;使用的模型也比较有特点，首先在encoder端将document分为word和sentence来encode，word使用CNN encode得到句子表示，接着将句子表示输入RNN得到encoder端隐藏层状态。从word到sentence的encode体现了本文的hierarchical document encoder的概念。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1dbf5b848521e79a7f0973c1c38095a6.jpg" data-rawwidth="252" data-rawheight="272"&gt;&lt;/p&gt;&lt;p&gt;在decoder端根据任务的不同使用不同网络结构，sentence任务就是一个简单的有监督下二分类问题，使用RNN网络结构更新decoder端隐藏层状态， decoder端隐藏层状态串联encoder端隐藏层状态后接入一个MLP层再接sigmoid激活函数得到句子是否被extract的概率。&lt;/p&gt;&lt;p&gt;word任务则是使用传统的attention-based的方法来计算每个词的概率。但要注意本文的计算的attention不是word-level attention，而是encoder端sentence-level attention。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4ebfdd69dd0959c7efce04bf69faeb15.jpg" data-rawwidth="294" data-rawheight="199"&gt;&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;数据集：&lt;a href="http://homepages.inf.ed.ac.uk/s1537177/resources.html" data-editable="true" data-title="Jianpeng Cheng" class=""&gt;Jianpeng Cheng&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;之前大多数extractive methods都基于human-engineered特征来给句子建模，通常会对每个句子计算一个分数，然后再使用诸如binary classifiers，hidden Markov模型，graph-based算法或integer linear programming等方法来选择句子构成总结。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;之前基于data-driven的seq2seq模型在abstractive summarization任务上大放异彩，本文提出了使用类似的模型来解决extractive summarization任务。不过针对的依旧是single-document summarization任务，未来需要将工作拓展至multi-document summarization任务上。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-2008.pdf" data-editable="true" data-title="Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings" class=""&gt;Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Sequence to Sequence、Natural Language Generation、Chatbot&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何通过小规模、未对齐语料生成对话语句？&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;作者介绍了两个模型:&lt;/p&gt;&lt;p&gt;1、通过DA(diglogue acts)生成句法依赖树，再利用external surface realizer，生成语句。（如下图）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c4f596e42d0cbe2564aaa449d0318f2c.jpg" data-rawwidth="300" data-rawheight="208"&gt;&lt;/p&gt;&lt;p&gt;2、将两部分结合起来，直接生成语句。步骤如下：&lt;/p&gt;&lt;p&gt;Step 1 将DA(dialogue acts)中的每个slot(表示特定信息)表示成三元组(DA type,slot,value)并结合(下图左)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d740d2ced18c5f2f9817074bf7fcb472.jpg" data-rawwidth="506" data-rawheight="105"&gt;&lt;/p&gt;&lt;p&gt;Step 2 基于seq2seq generation technique生出语句或句法依赖树。Step 3 结合beam search和n-best列表重排序（list reranker）以减少输出中的不相关信息。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;代码: &lt;a href="https://github.com/UFAL-DSG/tgen" data-editable="true" data-title="GitHub - UFAL-DSG/tgen: Statistical NLG for spoken dialogue systems" class=""&gt;GitHub - UFAL-DSG/tgen: Statistical NLG for spoken dialogue systems&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f46341b6203fae0cf1e8e493b2ea9b16.jpg" data-rawwidth="415" data-rawheight="189"&gt;&lt;/h2&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;该方法基于广泛使用的seq2seq模型，可以用未对齐的MR对(pair of meaning representation)和句子进行训练，且只要小规模的语料就可以有很好的效果。生成器可以从数据中学会slot的对齐和值，生成流利的domain style)语句，虽然语义错误还是很频繁，但还是取得了不错的成绩。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1230.pdf" data-editable="true" data-title="On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems"&gt;On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Dialogue System、Reinforcement Learning、Online Active Reward Learning&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。&lt;/p&gt;&lt;h2&gt;文章思路&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/fe153e783f983fa26d47572c287cf66a.jpg" data-rawwidth="415" data-rawheight="215"&gt;&lt;/h2&gt;&lt;p&gt;框架分为三部分：对话策略、对话嵌入函数、用户反馈主动奖励模型。&lt;/p&gt;&lt;p&gt;无监督学习输入为双向LSTM，通过Encoder-Decoder模型表征用户意图，将对话的成功与否看做高斯过程的一个二元分类问题，当模型对当前结果不能评判时，主动学习，通过reward模型决定是否询问用户反馈，当模型不确定时，生成增强信号来训练策略。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;数据集：&lt;a href="http://camdial.org/~mh521/dstc/" class=""&gt;http://camdial.org/~mh521/dstc/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;1、之前的工作有用任务完成度和对话持续情况做Reward，但任务完成度不好衡量2、用协同过滤表征用户偏好3、用逆强化学习从行为中推出reward&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;用lSTM Encoder-Decoder表征用户意图，无需大规模标注语料和构建用户模拟器来进行训练，在较小的训练语料中取得了不错的效果，率先实现了在真实场景中的应用。但Reward函数只关心对话任务是否成功，模型过于简单。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1100.pdf" data-editable="true" data-title="Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models" class=""&gt;Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Neural Machine Translation、UNK Words&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何解决机器翻译中的未登录词问题？&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;文章提出了一个混合（层次）模型。该模型由两部分组成，分别为：a. 传统的基于词（word level）的seq2seq模型；b. 基于字母级别（character level）的LSTM模型，由一个将字母encode成单词的encoder和一个根据状态生成低频词的decoder组成。其中a部分负责进行翻译，b部分负责处理低频词（unk）。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/eed844a1612a8d83285452ee2b8d4576.jpg" data-rawwidth="230" data-rawheight="336"&gt;&lt;/p&gt;&lt;p&gt;具体地，a部分的encoder遇到unk时，会使用character level对该低频词进行encode，并使用encode出的representation作为输入。而decoder遇到unk时，会利用attention机制将当前上下文和LSTM状态初始化character level decoder。此处的初始化采用的是文章提出的separate path模式，即利用一个MLP作为character level decoder的初始化网络。值得注意的是此处word level decoder仍会选择用作为下一步的输入。&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;Unk问题属于NMT中长期存在问题。目前多是采取后处理的方法。今年ACL有两篇paper，分别是李航老师实验室的copynet和Bengio实验室的pointing the unknown words，但对机器翻译任务参考意义有限。另外一种思路则是加大词典，比较知名工作有On Using Very Large Target Vocabulary for Neural Machine Translation。此外该工作还借鉴了Jiwei Li的hierarchical auto encoder。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;文章思路新颖且简单明了。因为NMT中存在unk的问题，作者直接利用character level RNN来生成一个词替代unk。该工作对拼音文字有一定意义，对中日韩文的参考意义有限。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1014.pdf" data-editable="true" data-title="Pointing the Unknown Words" class=""&gt;Pointing the Unknown Words&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Neural Machine Translation、UNK Words&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何解决机器翻译中的未登录词问题？&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;作者在有注意力的机器翻译模型上增加了一个开关来判断和是否复制原文。&lt;/p&gt;&lt;p&gt;1、Attention-based机器翻译模型&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6836206b755db6d279c8b0766360a2ce.jpg" data-rawwidth="333" data-rawheight="175"&gt;经典的attention model这里不再赘述。&lt;/p&gt;&lt;p&gt;2、Pointer Softmax模型&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cef9064f7a3972c554d344a782836b4d.jpg" data-rawwidth="380" data-rawheight="203"&gt;&lt;/p&gt;&lt;p&gt;两个问题有待解决解决：a. 是否进行copy？b. copy的位置在哪？&lt;/p&gt;&lt;p&gt;先说第二个问题，作者先引入shortlist softmax和location softmax。前者来确定要从shortlist中选取哪一个单词作为输出，后者确定在哪个位置要进行copy操作。&lt;/p&gt;&lt;p&gt;再看第一个问题，作者引入一个二值变量（可以想象为一个开关）来选择使用shortlist softmax还是location softmax。当值为1的时候不进行copy操作，使用shortlist softmax来从shortlist中选一个词作为输出。当值为0的时候进行copy操作，使用location softmax，将原文的词直接copy到指定位置。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;代码：&lt;a href="https://github.com/caglar/pointer_softmax" data-editable="true" data-title="GitHub - caglar/pointer_softmax"&gt;GitHub - caglar/pointer_softmax&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;本文的想法很有趣，直接从原文照抄罕见词和未知词很符合日常生活中人类的处理方法。从文中实验结果来看，该模型有一定的提升效果。注意力模型的提出与对人类行为的观察密不可分，而copy机制也是从生活中提炼出来的一种有效模型，我们可以借鉴的是从人类解决问题的具体方式中进行总结和归纳不失为一种有效的解决方案。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1228.pdf" data-editable="true" data-title="Harnessing Deep Neural Networks with Logic Rules" class=""&gt;Harnessing Deep Neural Networks with Logic Rules&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;CNN、RNN、First-order Logic, Iterative Distillation Method&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;如何将深度学习与逻辑规则结合使用？&lt;/p&gt;&lt;h2&gt;文章思路&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ca52f6aa241d9fa7725c356f313d102a.png" data-rawwidth="447" data-rawheight="227"&gt;&lt;/h2&gt;&lt;p&gt;系统在构建正常神经网络(student)的同时，构建了一个基于逻辑规则的训练网络(teacher)。整个网络的目标还是优化神经网络的参数变量 θ，因为新的目标损失函数结合了二者的损失，通过这种方式，教师网络的逻辑信息就能够被转移到神经网络的θ上，从而加强神经网络的性能。 在这种结构里逻辑规则是用于辅助的可选项，通过调整权重，系统可以偏向某个网络。这种模型可以将监督学习扩展到无监督学习，比如图示中，无标记的数据通过教师子网之后提取有用信息，也可以用来训练监督学习的神经网络。&lt;/p&gt;&lt;p&gt;1、训练过程&lt;/p&gt;&lt;p&gt;假设输入数据为x, y。student神经网络的参数变量是θ, 输出层是softmax，对输入xn，输出预测概率分布σ(xn)。对teacher网络，在第ｔ次迭代中基于逻辑规则的预测结果表示为sn(t)，那么新的优化目标变成了&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1ada851124622357827bcee6b400e724.png" data-rawwidth="421" data-rawheight="101"&gt;&lt;/p&gt;&lt;p&gt;可以看出来自教师网络的反馈作为regularization加到了目标函数里，通过这种方式两个网络的信息就结合在了一起。注意教师网络在每次训练迭代中都要构建，因此整个过程被称之为iterative knowledge distillation.&lt;/p&gt;&lt;p&gt;2、教师网络&lt;/p&gt;&lt;p&gt;教师网络使用软逻辑(soft logic)来编码first-order logic的信息。soft logic在[0,1]之间的连续取值，而不是二元值{0, 1}。逻辑运算也用max, min, sum代替原来的与或非。&lt;/p&gt;&lt;p&gt;神经网络数学模型为pθ(y|x) 教师网络数学模型假设为q(y|x)。我们实际上是用基于逻辑规则的教师网络来模拟神经网络输出，因此我们希望能找到一个最优的q，使得输入尽可能满足逻辑规则的要求，同时q要尽可能接近pθ。详细推导可以参见原文，最后的优化结果就是&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7f6f7b9551453db16adcb9c02a13fa90.png" data-rawwidth="457" data-rawheight="97"&gt;&lt;/p&gt;&lt;p&gt;λl 是每个规则的自信度(confidence)，而rl, gl 是某个规则应用于某一输入时的逻辑结果，介于0,1之间。可以看到自信度比较高的规则可以使输入更容易通过规则。&lt;/p&gt;&lt;p&gt;3、应用&lt;/p&gt;&lt;p&gt;a. 基于CNN的情感分析b. 基于BLSTM-CNN的NER任务&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;1、Neural-symbolic systems (Garcez et al., 2012) 从给定的规则构建推理网络2、(Collobert 2011)， 利用领域知识domain knowledge提取额外特征，增强原始数据3、Knowledge distillation (Hinton et al., 2015) (Bucilu et al. 2006)4、Posterior regularization (PR) method (Ganchev et al., 2010)&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;创新点在于将逻辑规则与神经网络结合，可以利用人已知的知识去引导机器学习。当数据量不足的，或者对数据进行补充时，可以将人类的知识用逻辑语言表达出来，然后通过本文提出的框架进行增强训练。本文的两个例子中都提到只用了少量规则，优化的结果虽然显示要比当前其他模型好，但是没有大幅度的提高。需要进一步验证如果使用更多的规则，能不能大幅度提高准确率。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1043.pdf" data-editable="true" data-title="Easy Questions First? A Case Study on Curriculum Learning for Question Answering" class=""&gt;Easy Questions First? A Case Study on Curriculum Learning for Question Answering&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Curriculum Learning、Self-paced Learning、Question Answering&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;文章讨论了Curriculum Learning在NLP领域, 尤其是在QA task里应用的可行性。&lt;/p&gt;&lt;h2&gt;文章思路&lt;/h2&gt;&lt;p&gt;文章首先对QA类型的task给出了比较general的定义: 我们可以把QA问题看做是一个经验风险最小化(ERM)问题, 我们需要最小化:&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/27e5903f5012534c885cb908b17220b3.jpg" data-rawwidth="204" data-rawheight="49"&gt;&lt;/p&gt;&lt;p&gt;其中是a正确答案, f是给定背景知识以及问题, 模型选择出的最佳答案,Ω是regularizer.&lt;/p&gt;&lt;p&gt;之后, 作者对于Curriculum Learning, 尤其是Self-paced Learning做了介绍, 并且将其引入QA task, 进而将之前的ERM问题变为:&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0810353ef81edd235cb8915fb6274bee.jpg" data-rawwidth="277" data-rawheight="58"&gt;&lt;/p&gt;&lt;p&gt;其中v是对问题进行采样时候的权值, g是self-paced regularizer, 其中λ代表’age’, 或者说’pace’. 训练初期, 模型趋向于对简单的问题进行训练, 而随着’age’的增加, 模型越来越多地加入更复杂的问题一起训练。&lt;/p&gt;&lt;p&gt;文章给出并分析了四种流行的self-paced regularizer如Table 1:&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/b72b5a9134355f25b4b6ba48acc9903c.jpg" data-rawwidth="350" data-rawheight="168"&gt;&lt;/p&gt;&lt;p&gt;之后提出了7种新的heuristics:&lt;/p&gt;&lt;p&gt;1) Greedy Optimal (GO): 将已有的Q和一系列新的Q一起训练, 选回答正确并且loss最低的。2) Change in Objective (CiO): 将已有的Q和一系列新的Q一起训练, 选择令loss改变最小的。3) Mini-max (M2 ): 当某个新的Q与其loss最大的一个candidate answer配对时, loss最小的. (通俗地讲, 就是最差情况都没有那么糟糕的一个)。4) Expected Change in Objective (ECiO): 只拿新的Q训练, 和之前的loss改变最小的. (相比于第二种的将已有的Q和新Q一起训练)。5) Change in Objective-Expected Change in Objective (CiO - ECiO): 2)和4)的值最接近的, 按照作者的意思, 这个值反应了model见到某个新Q时surprise的程度。6) Correctly Answered (CA): 将一系列新Q在当前model上测试, 选择用最小的loss正确回答的。7) Farthest from Decision Boundary (FfDB): 只用在latent structural SVMs上, 选择答案与decision boundary最远的一个新Q。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;MCTest: &lt;a href="http://research.microsoft.com/en-us/um/redmond/projects/mctest/" data-editable="true" data-title="Machine Comprehension Test (MCTest)" class=""&gt;Machine Comprehension Test (MCTest)&lt;/a&gt;Science Textbook: &lt;a href="http://http//www.ck12.org/" data-editable="true" data-title="http://http://www.ck12.org/"&gt;http://http://www.ck12.org/&lt;/a&gt;Science question answering: &lt;a href="http://aristo-public-data.s3.amazonaws.com/AI2-Elementary-NDMC-Feb2016.zip" data-editable="true" data-title="amazonaws.com 的页面"&gt;http://aristo-public-data.s3.amazonaws.com/AI2-Elementary-NDMC-Feb2016.zip&lt;/a&gt;Simple English Wikipedia: &lt;a href="https://dumps.wikimedia.org/simplewiki/20151102/" data-editable="true" data-title="wikimedia.org 的页面"&gt;https://dumps.wikimedia.org/simplewiki/20151102/&lt;/a&gt;QANTA: &lt;a href="https://cs.umd.edu/~miyyer/qblearn/" data-editable="true" data-title="QANTA: A Deep Question Answering Model"&gt;QANTA: A Deep Question Answering Model&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关工作&lt;/h2&gt;&lt;p&gt;1、Curriculum Learning:&lt;/p&gt;&lt;p&gt;早在1958年[1], 就有认知科学的相关学者意识到, 对于人类学习过程, 相对于提供随机的知识,由浅及深的地给予有计划的训练样本, 可以得到更好的效果. 之后这一Curriculum Learning的想法也被引入到机器学习中[2], 其中Self-paced learning (SPL)[3][4][5]是比较常用的方法。&lt;/p&gt;&lt;p&gt;2、QA:&lt;/p&gt;&lt;p&gt;Jurafsky和Martin[6]对于QA系列问题有一个非常好的叙述, 而这篇文章突出讨论Curriculum Learning在non-convex的QA模型上的应用, 着重介绍了基于配对的模型[7][8][9]和基于深度学习的模型[10][11].基于配对的模型将每一个问题和问题附带的多个备选答案组成若干个QA对, 我们称之为假设, 然后在给定相关文章的情况下, 寻找有最可能是正确的一个假设作为答案. 基于深度学习的模型可以使用依赖关系树结构的递归神经网络, 对句子level的QA模型的结果取平均[10]; 也可以用RNN构建”长期”存储器, 通过学习对存储器进行读/写操作, 模拟一个动态的知识构建过程[11]。&lt;/p&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;在QA task中引入Curriculum Learning旨在在训练过程中, 启发式地对于提供给模型的数据出现的顺序进行一些调整, 从而让模型从简单的, 易于学习的样本开始, 随着模型对数据的表述愈加成熟, 逐渐加入更复杂的样本. 理想状况下这会指导模型从得到一个普通的local minima, 变成得到一个”更”好的local minima, 进而利用全部数据得到一个”更更”好的local minima。&lt;/p&gt;&lt;p&gt;通常来说, 我们给予模型的heuristic并不一定能够真正帮助模型, 因为通常我们都在猜测数据以及模型的latent representation是什么, 但是这篇文章通过了一系列的实验验证, 本文阐述的heuristic确实可以帮助QA model获得更好的准确率. 这证明了引导模型由浅及深的这种思路是可行的, 我们也许可以思考一些更复杂的heuristic, 或者将其应用到其他的一些NLP tasks。&lt;/p&gt;&lt;p&gt;然而本文给出的大部分heuristic在新问题的选择上都需要比较大的时间复杂度, 对于类似MCTest这种总共只有660个文章的小型数据集来说还算比较现实, 但是对于更大更长的数据集(比如CNN数据集, 38万个文章, 很多文章都超过了一千五百个单词, 而且备选答案数量也远超MCTest的四个)时, 就显得不那么轻松了. 最简单的Attention Sum Reader[1] 在CNN数据集上, 每个epoch都需要10个多小时, 就更别说其他基于AS Reader的模型了。&lt;/p&gt;&lt;p&gt;总体来说, 相对于实用性, 这篇文章更多在于提供了一种新的思路, 也就是把Curriculum Learning相关的概念应用到QA乃至于其他NLP task中, 非常值得思考, 因此是一篇非常值得阅读的文章。&lt;/p&gt;&lt;h1&gt;&lt;a href="http://aclweb.org/anthology/P/P16/P16-1144.pdf" data-editable="true" data-title="The LAMBADA dataset:Word prediction requiring a broad discourse context" class=""&gt;The LAMBADA dataset:Word prediction requiring a broad discourse context&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt;关键词&lt;/h2&gt;&lt;p&gt;Machine Reading Comprehension、Dataset&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;p&gt;ACL 2016&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;构建了一个难度更大的机器阅读理解数据集。&lt;/p&gt;&lt;h2&gt;构建思路&lt;/h2&gt;&lt;p&gt;以Book Corpus的小说作为数据源，构建了10222个passages，每个passage包括平均4.6句话的context和相邻着的一句target，定义的任务是通过理解context来预测target中最后一个词，平均每个passage包括约75个tokens。其中，超过80%的passage context中包括了target中需要预测的词，48%的target words是专有名词（proper nouns），37%的词是一般名词（common nouns），约7.7%的是动词。这里，专有名词和一般名词是最难猜出来的，动词有一定的概率可以不需要context，而直接从target sentence利用语言模型猜出来。&lt;/p&gt;&lt;p&gt;在处理原始数据时，作者做了一层过滤，将容易从target sentence中直接猜出target word的passages统统丢掉，将剩下的部分放在众包网站上进行人工筛选，筛选的过程比较长，目的是让留在数据集中的数据有下面的效果：通过分析passage的context可以给出正确的target word，而如果只是给定target sentence的话，是猜不出正确的target word。&lt;/p&gt;&lt;h2&gt;资源&lt;/h2&gt;&lt;p&gt;本文数据集Lambada dataset: &lt;a href="http://clic.cimec.unitn.it/lambada/" data-editable="true" data-title="cimec.unitn.it 的页面"&gt;http://clic.cimec.unitn.it/lambada/&lt;/a&gt;众包网站Crowdflower: &lt;a href="http://www.crowdflower.com/" data-editable="true" data-title="Make your data useful"&gt;Make your data useful&lt;/a&gt;原始数据集Book Corpus: &lt;a href="http://www.cs.toronto.edu/~mbweb/"&gt;http://www.cs.toronto.edu/~mbweb/&lt;/a&gt;CNN/Daily Mail dataset: &lt;a href="https://github.com/deepmind/rc-data" data-editable="true" data-title='GitHub - deepmind/rc-data: Question answering dataset featured in "Teaching Machines to Read and Comprehend'&gt;GitHub - deepmind/rc-data: Question answering dataset featured in "Teaching Machines to Read and Comprehend&lt;/a&gt;CBT dataset: &lt;a href="http://fb.ai/babi/"&gt;http://fb.ai/babi/&lt;/a&gt;MSRCC dataset: &lt;a href="https://www.microsoft.com/en-us/research/publication/the-microsoft-research-sentence-completion-challenge/" data-editable="true" data-title="The Microsoft Research Sentence Completion Challenge"&gt;The Microsoft Research Sentence Completion Challenge&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;相关数据集&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5b0d691a539f4cf8001773af649bc8be.png" data-rawwidth="1086" data-rawheight="620"&gt;&lt;/h2&gt;&lt;h2&gt;简评&lt;/h2&gt;&lt;p&gt;大型数据集是深度学习技术发展的重要基础，数据集的质量和难度也直接关系着模型的质量和实用性。机器阅读理解的数据集有很多，包括中文和英文的数据集，每一个的构建都会带来模型的创新，随着难度不断增加，对模型也提出了更高的要求。本文在构建数据集过程中为了保证任务的难度所采取的方法是值得借鉴的。&lt;/p&gt;&lt;h1&gt;致谢&lt;/h1&gt;&lt;p&gt;本期的10篇文章由以下同学完成：&lt;/p&gt;&lt;p&gt;苏辉、Xiaoyu、胡小明、赵越、周青宇、韩晓伟、Eric Yuan、Zewei Chu、tonya、张俊。&lt;/p&gt;&lt;p&gt;感谢大家地辛勤付出。&lt;/p&gt;&lt;h1&gt;广告时间&lt;/h1&gt;&lt;p&gt;PaperWeekly是一个分享知识和交流学问的民间组织，关注的领域是NLP的各个方向。如果你也经常读paper，也喜欢分享知识，也喜欢和大家一起讨论和学习的话，请速速来加入我们吧。&lt;/p&gt;&lt;p&gt;微信公众号：PaperWeekly&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8e2b86789f8cddf7e423e1c07d39ce0a.jpg" data-rawwidth="430" data-rawheight="430"&gt;&lt;/p&gt;&lt;p&gt;微博账号：PaperWeekly（&lt;a href="http://weibo.com/u/2678093863" data-editable="true" data-title="PaperWeekly的微博"&gt;PaperWeekly的微博&lt;/a&gt; ）知乎专栏：PaperWeekly（&lt;a href="https://zhuanlan.zhihu.com/paperweekly" data-editable="true" data-title="PaperWeekly - 知乎专栏"&gt;PaperWeekly - 知乎专栏&lt;/a&gt; ）微信交流群：微信+ zhangjun168305（请备注：加群 or 加入paperweekly）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22279528&amp;pixel&amp;useReferer"/&gt;</description><author>张俊</author><pubDate>Fri, 02 Sep 2016 11:27:40 GMT</pubDate></item></channel></rss>