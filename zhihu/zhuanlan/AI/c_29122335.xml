<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>混沌巡洋舰 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/c_29122335</link><description>跨界思考内容供应商，与微信公众号混沌巡洋舰同属于巡洋舰科技公司。</description><lastBuildDate>Thu, 27 Oct 2016 13:17:17 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>从chimera说起，说到思维跳跃带来的问题</title><link>https://zhuanlan.zhihu.com/p/23227797</link><description>&lt;p&gt;不要怕，chimera这个词说的是这货，拥有羊身、狮头和蛇尾，会喷火的怪物。同时这还是一个生物学用语，指来自不同个体的生物分子、细胞或组织被结合在了一起成为一个生物体，例如双人嵌合体-世上最大迷之绿帽的故事。不过我们今天想说的是从这引申出的一种比喻，来说明罗辑思维兴起的原因和其带来的问题。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7f80eb50719dde4d48d485f5dc1d74ca.jpg" data-rawwidth="408" data-rawheight="500"&gt;&lt;p&gt;移动互联网时代，知识的获取变得更容易更大量了，然而代价是知识的碎片化。微信上受欢迎的不是长文，而是几句看似有道理的话配上美图。然而无论是职场心得，科技资讯，历史感悟，都需要被放到相应的背景中才能体现出更好的价值。正是在这样的需求下，罗辑思维才应运而生。罗辑思维的套路是讲一个有趣故事，之后讲述一个不在故事范围之内，却又和故事相关的有用的道理。这相当于在两个相隔着很远的领域间建立起联系。这与罗辑思维的口号”死磕自己，愉悦大家“是对应的。类似的逻辑，很多知乎上的高分答案，也是同时兼具有趣和有用，或者有趣且有理这两种属性。这是罗辑思维之所以能火起来的原因。&lt;/p&gt;&lt;p&gt;然而让你兴盛的点也可能是让你跌落的那个点。知识的海洋里，充满了相似的概念，这些相似点需要深厚的学养，系统的训练才能看穿。举几个栗子，同样是引力波，民科嘴里的和物理学家说出的是不同的；遗传算法中的基因和生物上的基因是不同的；深度学习中的神经元和大脑中的神经元也不同。如果没有潜入到具体的叙事逻辑和历史渊源中去，这些相似的概率可能会被误用，从而造成知识间的错误组装。这里的案例是金融中对物理模型的过度依赖，物理中的无摩擦不等同与金融中的完美市场。知识间的错误连接会形成开头提到的chimera结构。这也是罗辑思维最被内行人诟病的一点，即其中的有些结论论证不够严密，对概念有误用和过度解读的嫌疑。&lt;/p&gt;&lt;p&gt;如果你的知识食谱里包含越多Chimera式的知识，你就越有可能变成纸上谈兵之人。这种人脑洞很大，谈起什么来都头头是道，听起来让人感觉很高深，然而实际上却无法掐中要害。就如同马谡和赵括。所以说，&lt;strong&gt;决定你能走多远的不止是你的眼界，更重要的是你的知识根基是否结实。&lt;/strong&gt;那该怎么做，才能避免过多的chimera类的知识了？&lt;/p&gt;&lt;p&gt;Chimera类的知识，从本身来看，问题并不大。罗辑思维的节目，也不是全无是处，其中的类比，有不少结论是合情合理的，也有不少是发人深省的。然而，结论正确不代表逻辑正确，没有证伪的证据，可能是因为结论本身就不可证伪，也有可能是有证据说明这个结论本身是对的，这两者是有区别的。如果你习惯了罗辑思维的思维方式，那么你无法保证你做出的概念间的跳跃是逻辑一致的。所以对那些吹捧罗胖的人，首先要说的是你们需要学好逻辑，明白各种逻辑谬误是怎么一回事，这里可以参考科普工具文--请对照这二十四条逻辑谬误自行打脸&lt;/p&gt;&lt;p&gt;然而单单逻辑不足以保证你不被那些跨界的理论所迷惑。要区分出跨界知识中不准确的一小部分，你需要的是更长的阅读。相似的概念，总会出现不同点，你如果做的不只是盲人摸象式的吞咽二手知识，而是从一手文献和经典中去了解全局的信息，你就能够清晰的分辨出哪些跨界的观点是真知灼见，哪些是似是而非的chimera。&lt;/p&gt;&lt;p&gt;最后说说阅读知乎贴的价值。知乎上的大牛，多半是在一个学术领域内有深厚见地的人，他们将他们所学用简短的文字概括出来，贴上不同的标签，从而降低了读者阅读经典，组装自己知识库的难度，这是知乎之所以能火起来的原因。但这仍然是零敲碎打的片段，是一个个孤岛，你要想只通过看知乎的帖子就全面的了解一个领域，你首先需要广泛的阅读，从你看到的帖子中找出重复的部分，然而再在脑中建起帖子与帖子间的关系，在这个过程中，你需要剔除那些不靠谱的帖子，从而渐渐的将知识编织成一个连贯的体系。&lt;/p&gt;&lt;p&gt;这么做是需要花费很多时间的。所以说，知乎，罗辑思维这类的平台，虽然降低了专业知识的门槛，但也使很多人无意中有了博学的错觉。&lt;strong&gt;很多人一直只看微信知乎上的短文，妄图成为某领域的专家，是以战术的勤奋来掩盖战略的懒惰。&lt;/strong&gt;完整的看书，看综述型的论文，哪怕看wikipedia都能提供更长更深的阅读体验，从而帮我们分辨出知识中的chimera。&lt;/p&gt;&lt;p&gt;一个AI的吐槽，都说温故而知新，就是把新的知识点和之前掌握的重复的部分叠在一起，从而延伸到未知的下游，只是有时你的笔记中有了跳跃，就会跳到十万八千里之外，所以说，上课就要好好记笔记，看帖就要多想想。尽信书不如无书，何况是二手的知识。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23227797&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Wed, 26 Oct 2016 21:12:21 GMT</pubDate></item><item><title>别听牛人吹苦逼，好好学习人家怎么解决问题</title><link>https://zhuanlan.zhihu.com/p/23131802</link><description>&lt;p&gt;作者 徐馨&lt;/p&gt;&lt;p&gt;话说近年Elon Musk有点火，然后有大号推送了干货满满的消息，只是标题说“坎坷暗黑的过去”，我只能叹气“标题党”。&lt;/p&gt;&lt;p&gt;对于那些创造举世瞩目成就的人好像不提点苦逼不足以表现其牛逼。写陈景润要渲染其惨，写屠呦呦要强调其实验失败多少次，好不容易出个少年班的天才说感觉很正常就在众多少年班相关的新闻中鹤立鸡群。&lt;/p&gt;&lt;p&gt;对此，我只想说，如果没找到解决问题的办法，苦逼的依旧苦逼。不信，你去精神病院里看看，比惨大赛你都没法定胜负。&lt;/p&gt;&lt;p&gt;看了Elon Musk相关的信息，我只想说，你们这些凡人啊，你们以为的不可能的事，人家有一系列的解决方案来搞定。&lt;/p&gt;&lt;p&gt;将兴趣发展为精通的技能从幼年就开始了，至于积累技能，争取机会，获得支持这些东西并不是上了大学才开始的，当然和Bill Gates一样，天才的人生少不了和志同道合的伙伴一起赚钱积累原始资本。&lt;/p&gt;&lt;p&gt;诚然，他小时候家里貌似并不富裕，但是还是在9岁那年拿到一个类似小霸王学习机的Commodor VIC-20的游戏机。然后！9岁的Musk编了个游戏卖钱！我们不讨论你小时候有没有玩过魂斗罗超级玛丽飞机大战，我就说能在那么小的年龄编游戏卖钱的有几人！有几人！所以这货的大脑绝对是开了挂的。&lt;/p&gt;&lt;p&gt;当然开挂不是本文主题，我只是想问，你能够通过自学获得多少技能换钱？&lt;/p&gt;&lt;p&gt;现在，我们都知道Elon Musk作为民间资本把火箭从上了天，还是特斯拉电动跑车和全美最大的太阳能电池板安装公司的缔造者。但是，你知不知，他在读沃顿商学院的时候就写了关于超级电容（电动跑车的核心技术）和太阳能的极其完善的商业计划！&lt;/p&gt;&lt;p&gt;这个商业计划有多完善？各位读者，你试试把你现在所做的课题项目所在的行业从现状到前景梳理一遍，并且把其中涉及的关键技术学到足够熟悉理解甚至运用的程度，对其中涉及的环境规则法律风险及可能的问题进行分析，看看做这个需要查阅多少信息资源耗费多少时间精力。更何况，Elon Musk针对的都是当时相当前沿的领域。&lt;/p&gt;&lt;p&gt;然而，这只是Elon Musk的商学院作业而已。你在进行一个项目时，做足功课了么？&lt;/p&gt;&lt;p&gt;除了特斯拉太阳能和火箭之外的Elon Musk公司我就不说了，其起承转合和我国的互联网大佬创业的始末还是蛮相似的，这里推荐一本老书，几位大佬中长得最帅的李彦宏写的《硅谷商战》。语言有点模仿章回体，蛮好玩。&lt;/p&gt;&lt;p&gt;就说说普通人认为痴人说梦的火箭飞天是怎么做到的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-c7426ca5cabcc2cda4aec800ef04b5a5.jpg" data-rawwidth="640" data-rawheight="363"&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;揣着大笔从互联网公司赚到的钱，到达美国航天重镇洛杉矶。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;给聚集火箭相关技术人才的兴趣群体捐钱，进而结识NASA科学家和大导演卡梅隆，通过完成一些貌似玩票的项目，寻觅可行的商业计划以及未来的合作伙伴。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在进行1和2的同时通过各种途径积累火箭以及飞行器方面的知识（看书、与专业人士交谈等）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;拉到人后，成立公司。逐步解决“如何制造廉价的火箭”所遇到的问题。如发明缩短阀门检验时间的自动检查装置，从而减少时间成本；如选择非航天行业内又可制造自己需要的部件的企业，打破垄断，减少配件成本。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当出现进展困难时，通过办火箭展览会等媒体手段给员工打气同时给投资人信心。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;火箭发射失败后邀请之前批评自己的专业人士进行详细的事故调查。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当出现资金困难时，想尽各种办法进行融资，必要时抱大佬（NASA）的大腿。（现金流困难几乎是所有公司都会遇到的情况，有时候就是几天的时间差，就足以决定一个公司的生死。历史记载解决办法确实很多，但有些办法，不是每个人都能用。）&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其实特斯拉的故事也有点类似，找到伙伴开公司。虽然 Musk都认真学习了相关的技术知识，但是不管是火箭还是特斯拉他都有专攻技术的合作伙伴，掌握更多的技术知识主要在于让他与技术伙伴交流更顺畅，同时对公司有更好的掌控力。&lt;/p&gt;&lt;p&gt;不过与火箭不同，特斯拉除了技术的问题要解决，还有市场的问题。如何卖出这种新型车？首先定位产品定位高端大气上档次，专卖有钱人（研发成本这么高，要不怎么回本哦）。其次，充分运用社交人脉媒体手段造势。这个真不是一般人做得到的。君不见那些明星电影上映前各种秀恩爱炒新闻跑巡回宣传跳广场舞，都是为了票房宣发啊。&lt;/p&gt;&lt;p&gt;另外，要提一下的是Musk在成本控制方面的量化思维方法——对物料数据追踪绘制曲线图（看到这里各位科研狗有没有一丝亲切感？）。&lt;/p&gt;&lt;p&gt;至于太阳能公司，亲，Musk不生产太阳能电池板，只做太阳能电池板的搬运工！他做了个软件系统会帮助客户分析太阳能电池版安装相关事宜，然后得出租更划算的结论。然后客户表示，听起来好像很有道理的样子，那么你顺便帮我安装了吧（是不是有点像聚划算之类的，亲，打折咯比价咯超划算哦，在我们家买吧）。&lt;/p&gt;&lt;p&gt;总而言之，Elon Musk是天才！（废话）就算他有公司数次囧境的苦逼，就算他曾经在出任CEO迎娶白富美的人生巅峰差点得病挂了，但是，每一次遇到困难，他总能找到解决的办法！所以他才成功了！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;虽然我们没有那么出众的智商，但是他面对问题不屈不饶寻求各种解决问题的办法的精神，以及那些前人解决问题的思路，是我们可以学习的！&lt;/strong&gt;（有没有一种小学生作文结尾的错觉？特别是“不屈不饶”这种用词。）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;微信公众号 &lt;strong&gt;董老师在硅谷&lt;/strong&gt; 的 &lt;strong&gt;《Elon Musk， 一个活在未来的人，和他背后坎坷暗黑的过去》&lt;/strong&gt;（内容确实是干货）&lt;/p&gt;&lt;p&gt;豆瓣读书  &lt;strong&gt;Invictus &lt;/strong&gt;对 Ashlee Vance所著的《Elon Musk：Tesla, SpaceX, and the Quest for a Fantastic Future》的读书笔记及书评&lt;strong&gt;《现世最激进大胆的企业家没有之一（ 全书内容整理）》&lt;/strong&gt;网页链接&lt;a href="https://book.douban.com/review/7718445/" data-editable="true" data-title="现世最激进大胆的企业家没有之一（ 全书内容整理）（Elon Musk）书评"&gt;现世最激进大胆的企业家没有之一（ 全书内容整理）（Elon Musk）书评&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;铁哥说两句&lt;/strong&gt;：Elon Musk 和我一样是物理学出身，他同时是一个极为成功的商人。 聪明人从来都是很早就发现了这个世界的秘密， 那就是， 一种事物要想得到蓬勃的发展， 就需要与商业结合。比如当下最强大的AI-阿尔法狗是google deep mind  做出来的。 火箭这个事情吧， 商业从不介入是因为人们之前认为没有商机， 而Elon Musk 最聪明的是他能够从没有商机中转化出商机， 让贵族开始想到玩火箭， 倘若未来人类真的开发火星， Musk将功不可没。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23131802&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sat, 22 Oct 2016 15:20:29 GMT</pubDate></item><item><title>交叉验证，你用的是正确的版本吗？</title><link>https://zhuanlan.zhihu.com/p/23108759</link><description>稍微了解一点机器学习的都会知道cross-validation，作为一个验证算法是否靠谱（具有可推广到新数据的能力）的主要方法。然而，魔鬼往往藏在细节中，这篇文章调查了之前学者做的cross validation 是那个版本的，为后来的研究者指出了一些方法论上需要注意的地方。&lt;p&gt;BioRivx是生物界的论文预印本网站，目前生物界的还没有把文章放到preprint上的习惯，不过很多和计算相关的文章，都可以在这里找到。这篇文章的标题是“&lt;b&gt;Voodoo Machine Learning for Clinical Predictions”。&lt;/b&gt;Voodoo 就是巫毒教的意思，这个题目够逗吧。&lt;/p&gt;&lt;p&gt;开篇介绍背景，智能手机和可穿戴设备的普及使得研究者积累了人类行为的大量数据，从而使得使用机器学习的方法来预测精神类疾病成为了可能。当越来越多的算法被使用，如何量化的评估这些方法的好坏变成了一个重要的问题。&lt;/p&gt;&lt;p&gt;接着作者介绍了这篇文章的核心概念，record-wise vs subject-wise cross-validation，作者发现record-wise cross-validation often massively overestimates the prediction accuracy of the algorithms，同时this erroneous method is used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smart phones to predict clinical outcomes. 因此，为了让之后的研究成果更加靠谱，作者提倡我们都要用正确的交叉验证方法。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b2d42ce32692da02de0234ef3a1f27cc.png" data-rawwidth="1428" data-rawheight="582"&gt;这幅图看懂了就明白了这两种方法的区别了，subject wise的不会让test集合中的数据进入训练集，而record wise则会对于每一个data，使用其他所有的数据作为训练集。The problem is, that if the algorithm can (implicitly) detect the identity of the person based on the features, it can automatically also “diagnose” the disease.这句说的是如果算法能够看出test set是对应的哪一个样本，那么其就可以判断出这个样本是否患病。&lt;/p&gt;&lt;p&gt;接下来举一个简单的例子。Imagine we recruit 4 subjects, 2 healthy and 2 affected by Parkinson’s disease (PD). We have a machine learning algorithm that estimates if a person has PD based on their walking speed. Let our two healthy subjects have constant walking speeds of 1 meter per second (m/s) and 0.4 m/s, and our two PD patients 0.6 m/s and 0.2 m/s. If we do subject-wise CV, we will be unable to predict the performance of the slow healthy subject as well as the fast PD subject, resulting in a prediction accuracy of roughly 50%.&lt;/p&gt;&lt;p&gt;然而如果是Leave-One-Subject-Out的方式，那么这个问题就简化了，在这种情况下，算法给出的预测准确度会是100%，然而这样的结果是不能泛化的。&lt;/p&gt;&lt;p&gt;接着看看作者做到对已有文献的调查分析时用的流程图吧&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f841cded4f2b111884356c65a65e700b.png" data-rawwidth="868" data-rawheight="720"&gt;&lt;p&gt;那这些文章的结论质量如何了，下图给出了总结，左边的是不同类项文章的预测错误的箱图，右侧是这些文章的引用次数。我们可以看出，使用了subject wise的文章预测错误率更高。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-610b9d642df741e4dc48a57c749ec01b.png" data-rawwidth="1377" data-rawheight="710"&gt;&lt;p&gt;说了这么多，用作者的句子总结一下这篇小文。The use of machine learning for clinical predictions is growing in popularity. Yet, such erroneously positive results threaten the progress in the field. Such results might contribute to the problem of irreproducibility of research findings and thereby undermine the trust in both medicine and data science. Only with meaningful validation procedures can the transition into machine learning driven, data-rich medicine succeed.&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23108759&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Fri, 21 Oct 2016 15:16:09 GMT</pubDate></item><item><title>知识过程与人生轨迹——读《宽客人生》</title><link>https://zhuanlan.zhihu.com/p/23099806</link><description>作者 &lt;em&gt;代毓成&lt;/em&gt;&lt;p&gt;&lt;strong&gt;当你研究&lt;/strong&gt;&lt;strong&gt;物理&lt;/strong&gt;&lt;strong&gt;学&lt;/strong&gt;&lt;strong&gt;的时候，你的对手是上帝；当你研究金融学&lt;/strong&gt;&lt;strong&gt;时&lt;/strong&gt;&lt;strong&gt;，你的对手是上帝创造的人类。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;——&lt;/strong&gt;&lt;strong&gt;伊曼纽尔.德曼&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;经济学（或金融学）研究中，有一个很大的麻烦，就是许多问题都很主观，外人难以获知。比如机会成本这一概念，本质上它是每个人内心对某一行为背后代价的主观估量；再比如费雪意义上的“收入”，它是每个人对某一行为所获满足感的主观体验，不仅是物质，还有精神满足，旁人也难以估量。&lt;/p&gt;&lt;p&gt;个人知识也是一样，它与不同的人生轨迹相结合，知识过程与人生轨迹沿着时间的维度相互交织和影响，从而产生每个人的异质性。在&lt;a href="https://zhuanlan.zhihu.com/p/22394270"&gt;阴谋论为何总是错的?&lt;/a&gt;一文中介绍过，因为每个人对于知识的获取，必须建立在个人经验（常识）的真实性之上，也必须反复出现才是可信的（近似于“&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381623&amp;amp;idx=1&amp;amp;sn=079563ccdd30186de04e4826b5b37d18&amp;amp;scene=21#wechat_redirect" data-editable="true" data-title="贝叶斯后验推断" class=""&gt;贝叶斯后验推断&lt;/a&gt;”）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-46391eab71418093d0b044676abe571c.jpg" data-rawwidth="640" data-rawheight="640"&gt;&lt;p&gt;《宽客人生》一书，作为一本第一人称的人物传记，就讲述了一个从物理学者到数量金融大师的人生轨迹和知识过程相交织的真实故事。从开普敦到美国，从物理到金融，从学界到业界再到学界，一次次告别、迎接、沉积、纠结之间，主人公德曼（Emanuel Derman）几十年的经历看似是“断裂”的，而实则是在时势之下不断探求和反思答案和生活、物理与金融的“连续性”过程。&lt;/p&gt;&lt;p&gt;“一定要在物理学界成功”，一个单纯的愿望激励20岁的德曼从开普敦来到了哥伦比亚。和许多物理学界的朋友们一样，他对于这个研究物质、空间和时间的学问有一种近乎宗教般虔诚的热情，梦想着像爱因斯坦一样，将热血洒在这些领域的研究使他不枉此生，甚至有一种高于那些奔波于世俗之人的自傲。同时，在经历之初，像很多充满激情的学者一样，德曼也是一位更偏好于存粹理论研究的还原论者，相信可以把世间复杂的事物简化到基本的构成要素。&lt;/p&gt;&lt;p&gt;但现实还是给理想上了一课，“16岁或17岁时，我曾经想成为另一个爱因斯坦；21岁时，如果能成为第二个费曼就能让我很兴奋了；到了24岁，能做到李政道那样就可以让我很知足；等到1976年，我在牛津和其他博士后研究员共用一个办公室时，我发现我居然开始嫉妒坐在我身边的一位博士后，就因为他曾经应邀去法国出席过一次研讨会。”&lt;/p&gt;&lt;p&gt;此外，有悖于还原论者对纯粹规律问题研究的“高傲”，德曼的物理之路也多是作为一名“现象学家”：阐释理论，创造出实验性的近似，将理论转化为实用的工具，再通过预期结果来验证或证伪一个理论。与“初心”的背离，虽有些许无奈和失措，但也为未来进入业界参与物理和金融的实用性研究埋下了伏笔。而后，德曼经历了十几年博士和博士后的“僧侣生活”，他逐渐学会了在起起伏伏的人生轨迹中，享受不确定性所带来的慰藉。在生活的压力下，不搞物理的“羞耻感”也逐渐变轻。“物理学界是一个残酷的赢者通吃的天下，大多数的功劳都集中于顶尖的一小部分传奇人物”，他得出结论，要么必须找到一个有机会得到终身教职的助理教授职位，要么干脆离开物理学界。&lt;/p&gt;&lt;p&gt;作为未来进入华尔街的跳板，德曼来到了贝尔实验室工作。在这里，他放弃了最初“纯物理学研究”的自命不凡的信仰，开始像多数人一样，走入了现实世界，为了钱而做上司安排的工作。而收获是，他发现误解了以往对非学术世界中工作的性质，物理学家总认为自己很聪明，一旦自降身段从事了外面世界的工作，他们的聪明才智还能使其超过其他同事。但是在许多非学术工作中，总有一些人，从事特定工作并不是一种妥协，而是一种投入和激情。往往是他们，为卓越确立了真正的标准。再者，尽管德曼几乎没有接触到什么商业或金融知识，但坚实的软件工程技巧和身心的打磨，为接下来进入高盛做足了准备。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-59370b4ec651b9fcd1c30cd1b0818547.jpg" data-rawwidth="550" data-rawheight="377"&gt;&lt;p&gt;在20世纪70年代末80年代初，石油危机与大宗商品的价格上涨，推高了利率，拉低了国债和企业债的价值，倒逼投资银行掀起了一场金融产品创新的浪潮——国债期货、债券期权、掉期期权等等，而这些产品需要越来越复杂的数学和运算能力（编程），因此为物理学家走入金融领域打开了大门。正在此时，德曼接到了猎头的电话，在薪金的激励下，他开始了自己的金融之旅。&lt;/p&gt;&lt;p&gt;从起初的软件编程人员到后来对“布莱克-托伊”模型的改进，在德曼第一段高盛的经历中，他逐渐开始享受宽客每天的生活：与其他宽客讨论，阅读理论，与客户和交易员交流，软件编程，并学习向其他人讲解复杂的概念。特别是期间他合作发表的论文，其中提到的模型，后来被广为接受。究其原因，除了受益于物理学的逻辑训练，也离不开在业界对实务工作的经历和反思：“为什么我们的模型被如此的广为接受？第一，我们是实践者，非常清楚交易部门需要什么样的模型；第二，我们的文章使用非常务实的风格，如二叉树结构，是华尔街任何人都能掌握的描述方法……”。&lt;/p&gt;&lt;p&gt;金融从业者的大幅涨薪很多都是通过跳槽，德曼也是如此。在出于薪资和其他因素的考虑下，德曼跳槽到了所罗门兄弟公司，也开始了一年的“苦难之旅”，德曼在书中吐槽了很多所罗门兄弟“赶尽杀绝”的文化形式。而这或许是由于上市公司和私人合伙制公司之间组织建构的不同，那是的高盛是私有制，合伙人在全公司范围内选拔，没有随时可套现的股票，所以更着眼于长期利益，而所罗门兄弟则显得更加让人透不过气。&lt;/p&gt;&lt;p&gt;当然，这段时间里，尽管德曼由于自己在某些业务上的不足而受尽“羞辱”和压制，但也在这段不同的文化中收获了新的知识和体验。比如，他发现，就使用模型作为销售工具背后的逻辑而言，事实是，世界上有如此多证券和股票，任何人在其中选择最合适的证券是非常困难的，而模型作为一个理念基础，可以将选择变得简单且可度量，同时，开发成功的金融模型不仅是一场寻找事实真相的战争，也是一场争夺使用者信任和体验的战争。当正确的模型、正确的概念能够让人更容易地思考价值的时候，这些模型和概念才能更加被市场所接受。&lt;/p&gt;&lt;p&gt;在1989年末，出于市场环境的压力，所罗门兄弟开始裁员，德曼也顺势离开了这个让他身心俱疲的公司，而回到了高盛，并在接下来开始领导量化策略小组，金融工程也开始成为一个真正的行业。而随着量化金融的逐步“规范”和发展，越来越多的数学家也开始进入华尔街，以往量化生活的自由随意，变成了一门学科和职业。群雄逐鹿间，更加普适金融模型开始成为了许多人焦点，包括德曼。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6e0e967c0e6a07d374611a54fb93ea89.png" data-rawwidth="395" data-rawheight="202"&gt;&lt;p&gt;在尝试了超越“布莱克-斯科尔斯”的期权局部波动率模型的竞赛中，德曼仿佛回到了当初物理领域的自己。但他逐渐发现，不同的市场都有自己不同的“波动率微笑”特征，比如在在货币市场上，由于投资者惧怕超过某一固定范围的大涨或大跌；在利率市场上，债券投资者惧怕高利率会使他们持有的资产贬值，等等。每一种恐惧都是基于痛苦的经验之上，对应了一种不同的模式，所以，不存在普适的“波动率微笑模型”。而在这市场和理论之间，遇到的矛盾越多，往往也愈加证明关于金融和人类世界的模型的局限性。&lt;/p&gt;&lt;p&gt;身处商业世界，头脑却畅游在学术的海洋里，德曼享受其中，也印证了他知识过程和人生体验相交织的路径连续性。而在后来经历了互联网泡沫破裂和“911”之后，德曼的思考从风险来到了不确定性：“模型就是模型而已，是对理想化世界进行玩具一样的描述。简单模型构想一个简单的未来；复杂模型构想一组更加复杂的未来，他们可能更接近真实的市场情况，但任何模型都不能捕捉到人类心理的负责精密”。&lt;/p&gt;&lt;p&gt;同时，借由市场的颓势，德曼“逆流勇退”，选择离开高盛，在休息一年写下这本书后，回到了大学，试图弥补学校里缺乏工作经验的所授知识，与工作中所需知识的差异：“在金融学领域，你不能简单地凭借观测就证明模型是正确的。数据是稀缺的，更重要的是，市场是行为以及对行为作出反应的地方。人们从过去的错误中学习，但继续犯下新的错误。在某一阶段中正确的东西到下个阶段就可能变成错误的……当然，如果没有希望，人在生命中也就不会乞求太多（或谓寻找意义是生命的意义）。”&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23099806&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Fri, 21 Oct 2016 08:38:07 GMT</pubDate></item><item><title>升级论 -读《少有人走的路》</title><link>https://zhuanlan.zhihu.com/p/23065732</link><description>&lt;p&gt; 最近读了"少有人走的路"一书。这本书是一个心理医生多年的从业日志。 反复阅读，颇有领悟。 &lt;/p&gt;&lt;img rel="noreferrer" data-rawheight="460" data-rawwidth="320" src="v2-b2df83bc9b24af14bc83b855d033709f.jpg"&gt;&lt;p&gt;而我却想到了一个新名词-  叫升级论。 即人有很多的心理能量水平， &lt;strong&gt;只有使自己的心理能级不停提升，才是通往幸福之路。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;心理能级是什么，它就是人们常说的正能量，负能量。  它和物理里的能量类似， 具有动能（现有的状态）和势能（对未来状态的预估）， 取决于自身和外界。 &lt;/p&gt;&lt;p&gt;心理能级首先取决于外界。举几个例子，当一个人在人群中处于领袖的位置，被别人赞扬， 被很多人左拥右抱，她都会觉得舒坦， 自己很牛的样子， 这就是由外界条件所寄予人的心理位势。 从根本上说， 他都和人的需要能够满足， 并且具有行动上的选择权，而不是被选择有关。 比如， 你是boss，你可以炒别人而不是被炒。 你追求者一大堆， 所以你晚上可以pick anybody 去约会。   一大堆人围着你转，你不在意某个人怎么想你。 总之， 你的自由意志可以实现的可能越大，选项越多， 就越处在心理能级高的位置。 &lt;/p&gt;&lt;p&gt;当然心理能级也和自身有关， 自己的预期和真实的状况差的越小，越有条件实现， 内部的心理能级越高。 压抑和无法实现的愿望则降低心理能量。 &lt;/p&gt;&lt;p&gt;那心里能级低呢？  心理能级低的装填往往是比较负面的，把上述那些都反过来， 变成没有可选择性，就是低了。  &lt;/p&gt;&lt;p&gt;心理能量的水平， 很大程度决定人的行为模式。&lt;/p&gt;&lt;p&gt;人的本性是追求快乐。  无论是处在高位点或者低位点的人，都会以追求快乐为己任，但是形式却殊异。  同样一件事情，高能级的人往往会挑战新鲜的，是探险挑战者的姿态。&lt;strong&gt;低能势的人则是对现有的东西产生依赖，是瘾君子的姿态。&lt;/strong&gt;比如高能级的人认为只有发自内深处的相互爱慕才可以结婚， 如果现有的人不再满足他，他会继续其爱情探险。 而低能级的人则可能为了寻找陪伴或泄欲找个对象。&lt;strong&gt;低能级的人最典型的状态是沉溺于自己的嗜好，比如酒鬼， 赌徒， 或者每天沉迷于收藏一些玩具的人。 也许他们也有一份体面的工作，但他们几乎一定是对现有的生活并不满意却又害怕失去现有的东西而不想改变。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;或者说低能级的人是宠物狗的心理，没有选项， 处处被动挨打， 通过对某些东西的成瘾，来逃避问题，过着每天都一样的生活。 而高能级的人， 是草原苍鹰的心理，他们的选项多，是生活中的主动者， 越级挑战者， 希望每日都有新鲜的挑战，实现不同的自己。 &lt;/p&gt;&lt;p&gt;理解“少有人走的路”的核心，在于理解这种能级跃迁。&lt;/p&gt;&lt;p&gt;人的性格，过往的经历或者暂时的境遇受挫， 都可以导致心里能量偏低。 而持续处在心理能量低的人，更加容易受挫，因而在低能级陷的更深，这就是一个互为因果的过程。&lt;/p&gt;&lt;p&gt;但是有没有一条路帮助人实现这种心理能级的跃迁？  有的， 这就是少有人走的路一书所说的路。 &lt;/p&gt;&lt;p&gt;书的核心： 自律和自由。&lt;/p&gt;&lt;p&gt;何为自由？  这是我之前多次讨论的一个问题。 自由意味着选择权，你手里达到快乐可以执行的选项越多，选择权就越多。   &lt;/p&gt;&lt;p&gt;&lt;strong&gt;自由实现的条件是摆脱依赖，依赖越多的人选择权就越少。&lt;/strong&gt; 为什么？  你依赖父母供给你生活，那你必然反过来要让父母决定你的事情，毕竟钱是他们出的。你实现财务独立，你才有不依靠父母意志的选择。  最典型的依赖而不自由的例子是宠物了 ， 虽然它锦衣玉食， 它却生活在牢笼里，甚至身体都被做了绝育手术。 反观身边很多30岁还被父母供着的小孩子也就是如此。还有那些被供养起来的全职太太，看似没有压力，实则精神生活十分悲惨。 &lt;/p&gt;&lt;p&gt;自由这么好，有些人却逃避自由，这就是低心理能级的人的状态，他们先是缺乏自由，继而爱上枷锁。这也是中国统治者常用的伎俩，把人困在低能级。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;对更高自由度的追求，是实现能级跃迁的基础。&lt;/strong&gt; 因为被束缚的东西，将永远处于低能级。 屌丝这个词非常生动，给人以挂在天线上的一根丝的感觉， 没有自由，只有那一亩三分地， 屌丝热爱枷锁，用一些无聊的消遣方式把自己维系在现有水平而不是甩掉枷锁。  这种心理就是不去通过对自我的约束提高自己的心理能级， 而是通过对某种东西的成瘾，而让自己在奥特曼的单曲循环里卡死往复。&lt;/p&gt;&lt;p&gt;对于一个人， 最重要的是摆脱“历史” 在其身上造成的心理势井。 这是一个人的独立战争， 因为越独立，越自由。  &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第一： 正视不确定性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;摆脱依赖的第一个要素就是正视不确定性。因为对不确定性的厌恶是所有心理低能者的共同特征。 因为枷锁和笼子反过来也是一种生活保证，提供食物和隔离更强大的猎食者。 和宠物狗对立的那些野猫野狗， 那是相当自由的，但是他们有一个问题， 就是有上顿不接下顿以及更强大动物的威胁。也正是因为这种可能性，让他们不停的狂野奔跑，唤醒了他们的生命力。  因此， 直面和接受这种不确定性，走出精神避风港，就是提升心里能级的第一要素。  你要在心里默念，接受不确定性欣赏生活的狂风巨浪正是生命意义所在，也许你明天会被吃掉，却比在笼子里终老强的多。 &lt;/p&gt;&lt;img rel="noreferrer" data-rawheight="220" data-rawwidth="351" src="v2-10bd257287d43b22e4f1723e529d71e4.jpg"&gt;&lt;p&gt;&lt;strong&gt;第二， 直视死亡&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;两千多年前 , 古罗马哲塞内加第十点，内观冥想说过:“ 人生不断学习生存 , 人生也不断学习死亡。”  要摆脱依赖，得到独立，必须有能够容忍某些自己之前生活的一部分，甚至十分心爱的一部分在自己生活中消失。因为某部分生命的死亡， 是引领人如蛇蜕皮一般走向更新更成熟阶段的必经之路。 那些更新阶段代表着更强大的心理能势。 那些害怕失去而躲在过去里的人，即使到了老年也难以脱离心理的幼年阶段。  &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第三点， 认知地图 &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;被困在枷锁里的人一般都有一张模糊的世界地图， 他看不太清自己的圈子外面的东西是什么，因为不清而恐惧，因恐惧而诋毁， 沿用自己老掉牙的地图，还要教导别人。 正确的决策需要正确的认知， 而没有人在开始有正确的认知。人都是一开始用一些假设来描述真实的世界，然后出去行动犯了错把那些假设改掉。 圣经里几个核心人物，如约拿， 都是因为悔改而重新赢得了上帝的重用。都说明内省和修改认知的重要性。 那些每次错误后抱怨客观的人，终究是被上帝遗弃的人。 而把每个错误看出修正认知地图机会的人，终将拿到越来越正确的地图。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第四点，自律和边界 &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;没有边界的自由是消极的，具有破坏性的。我们在自然的状态下有一种天然的向下滑落的趋势， 做那些容易上瘾的事情，去任由自己堕落。这就是确立边界和自律的作用。 通过确立边界而不任由自己向下掉，而有自发向上的趋势。 确立边界，删除一些垃圾的咨询文章，一些无聊的朋友， 一些耗时间的无聊癖好， 即使不去做任何更好的规划， 你都会向好。 &lt;/p&gt;&lt;img rel="noreferrer" data-rawheight="220" data-rawwidth="355" src="v2-f92e878b27d9f5819d968f93a32ee700.jpg"&gt;&lt;p&gt;然后说自律，这恐怕是西方文明自我修养的核心。  人在一定程度是不变的，一定程度又是可以改变的。  不变是因为人的行为惯性巨大， 如果没有巨大的外力， 你几乎会保持一样的状态。 因此所有的鸡汤立志都必然失败。 没有人能够单凭看了几本书许了几个愿就改变。 但是有一点， 却可以逐步改变一个人， 就是用微小的行动改变他的习惯。  所谓日积月累，日拱一卒，敌营可破。  那么这一点一滴的坚持就是靠自律。 &lt;/p&gt;&lt;p&gt;很多人常说的一句话是， 及时行乐因为未来不确定。  但是及时行乐的含义其实就是干他每天干的那些事，及通过自己的各种瘾或者嗜好找乐子。这样看似能够乐一阵子，却始终去掉了太多一生可能有的机会。自律的作用就是减掉这些消极的解决方式。  &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第五点， 行动，通过干掉小的任务增长能量  &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;有一句叫行动对抗虚无。 当你任何事情都想不到， 不知该做什么， 就应该做一件离你最近的积极的事情，当你行动起来哪怕看到最些微的收获， 你的心理能级都将提升，可以进一步做更复杂的事情。   &lt;/p&gt;&lt;p&gt;一个人要完成一个非常长期的任务， 在中期遇到困难的时候必然气馁，进入心理能量的低谷。 相比唯一的办法是把长期的核心利益切分成一连串小的任务，从最简单的逐步复杂。 每完成一个就得到足够的反馈增大心理能量。  每干掉一个小的任务， 就是增大一点选择权，就更接近高能级。 而如果一开始就跟比较复杂的任务对抗，则可能很快把心理能量用完。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第六点，  善用地点和外界力量&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;地点是世界最重要的构成力量。 因为地点即选择权。 当你很难改变自己， 就用腿来改变一切。 一个有一大群高素质朋友的地方，一个充满了变化和可能的地方，让你不胜而胜。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第七点，  合理的释放方式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;人一天的行为是有一个能量金子塔的。处于塔顶的最困难的核心任务， 然后是周边的次级任务， 用于维持生命的任务等，最后则是释放和消遣。 因为为了那些高级任务而被压抑的欲望必须得到发泄。 &lt;/p&gt;&lt;p&gt;而最危险的也是消遣和发泄， 很多消遣都是一些容易上瘾的消极游戏。 沉迷其中的时候， 它们会反客为主吞噬生活的主业。  &lt;/p&gt;&lt;p&gt;我的想法是用一些积极的手段释放， 例如进行体育锻炼而不是去DOTA里去虐菜鸟，又比如和朋友诉说失败的前因后果而不是自己独自喝闷酒。&lt;/p&gt;&lt;p&gt;生活是围绕欲望展开的， 把自己的需求分解，让每一种需求都对应一个能量向上的通道而不是向下， 就是最好的提升心理能量的方式。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第八点， 扩大自我的边界&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;扩大自我， 就是抬升自己的眼界，站在上帝的位置看世界的格局，再回去看我。 这样看， 我们不仅能看到自己， 还能看到很多相关的人，平时认为无关的人，并且主动开始增大关心的对象。 因为， 你会看到，你们本是命运相连，同时连接在一个风雨飘摇的船上，在无常世界里搏击。 这个时候，你的心理能级会因为和世界的紧密相连而得到提升。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第九点， 爱 &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们都因追寻爱而活，都渴望被接纳，被关注，最终被爱。  在自我边界扩大后， 我们会深刻的体会到其他人的需求， 而不是被局限在自己的荣辱得失里。 在这种对别人需求的深刻体会下的行动， 才是爱的基础。  而爱， 是提升心理能级最有力的武器。  &lt;/p&gt;&lt;p&gt;&lt;strong&gt;第十点，内观冥想&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们的理性在帮助我们做出正确决策的同时，也占据了本不属于其的情感与审美的“频道”，从而影响了我们感受幸福的能力，而我们若是不幸福，那么我们的心理能级也不会高。冥想可以帮助我们破除“知识障“，让我们不会时时刻刻都不自觉的动员理性与逻辑，增加我们的同情心同理心。从而能扩展自己认知的边界，扩展爱的边界，扩展”科学的真，道德的善，艺术的美这三者统一之处“的边界。&lt;/p&gt;&lt;img rel="noreferrer" data-rawheight="220" data-rawwidth="322" src="v2-2c1ab0ff3c44d4122dc0d1b803c9c251.jpg"&gt;&lt;p&gt;&lt;strong&gt;第十一点，接受不完美&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对完美的追求会成为一种偏执，而偏执都是类似的，都是自怨自怜，从而降低你的心理能级。我们不要如鲁迅所说，患上”十景病“，什么都要凑个整数，点菜非要八荤八素。接受残缺是现实的一种，明白大成若缺的道理，是活的真实的前提&lt;strong&gt;。我们要改变世间的不如人意，可目标不应是十全十美，而应是在自律的牵引下，一步步走向认知地图告诉我们的成功的边界。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;提升自己的”能级“，是可以在外界环境不利时进行，最典型的比如东坡，只有去了黄州，才能写出”也无风雨也无晴“以及“一点浩然气，千里快哉风。”&lt;strong&gt;神经可塑性决定了我们在任何时刻都能够战胜自己过去的阴影以及环境的束缚，我们一直都拥有提升自己心理能级的条件，你其实没有那么多借口&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;更多阅读&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22668097"&gt;能级积累——从国家到个人&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22587512"&gt;生命的直角坐标系&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22984393"&gt;人生的二八定律&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23065732&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Wed, 19 Oct 2016 19:08:04 GMT</pubDate></item><item><title>人生的二八定律</title><link>https://zhuanlan.zhihu.com/p/22984393</link><description>本文作者： 许铁&lt;p&gt;社会和生物界最重要的定律叫做二八定律，说的是20%的原因决定80%的结果。比如财富的分配是不均等的。20%的人掌握有80%的财富，一个事情的成功是在20%的努力里，一个公司80%的利润来自20%的客户，而你真正在乎的人，只是你众多亲友中的两三个。&lt;/p&gt;&lt;p&gt;也许绝对的2-8比例近乎迷信，但是它的确在点名我们所在世界的基本结构，它叫我们认识生活的悲剧，因为大部分人从来都淹没在8里而见不到2。&lt;/p&gt;&lt;p&gt;我想的一个典型的例子是摄影。一张好照片往往有一个清晰的焦点和虚幻的背景，焦点往往只占据画面极小的一部分，摄影师的功底也体现在这个焦点上。 一个全部清晰的照片往往是没重点的废片。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7983e33ebe2774fd5e016bf7a30ee1d1.jpg" data-rawwidth="535" data-rawheight="540"&gt;&lt;p&gt;说起来简单，大多数人明白。但是轮到个体却还是无能为力，因为人生来是情绪的奴隶，我们往往是被情绪牵着行动，我看不清二在那里，我们拖着我们的身体与周围的世界无休止的摩擦，年龄不代表智慧，有多少人在幼稚和肤浅中终老。&lt;/p&gt;&lt;p&gt;萨特说：他人是地狱。因为周遭人的行为和话语，往往对我们有出人预料的影响力，我们不得不投入无限心力其中，而二八定律所强调的理性从来被忘记，为什么，什么使我们无法对生活有清晰的焦点？&lt;/p&gt;&lt;p&gt;走出门看看我门外的生活。邻居正牵着女友的手在厅里边做饭边亲吻，手机里的朋友们发着微信，每天的爱好变成看每个人的微信，同龄人的生活渐渐稳定，每个人似乎都很忙，都很世俗，都很幸福。&lt;/p&gt;&lt;p&gt;于是我想逃跑，逃到烟花繁华的世界，开个小饭馆，找个老婆过幸福的日子。 日子匆匆的过，似乎我很快会老了。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4e064ccea4c36a8ed211f09a362cb942.jpg" data-rawwidth="640" data-rawheight="320"&gt;&lt;p&gt;这样做我会幸福吗？&lt;/p&gt;&lt;p&gt;也许第一个年第二年第三年每天早上忙忙碌碌，晚上吃吃喝喝，岁月静好中我转眼50岁了， 恍然回头的时候， 发现日复一日，终只一样，一切好像发生过，却又都没发生。&lt;/p&gt;&lt;p&gt;反过来回头我会明白， 我的存在只是作为别人信息的垃圾桶，我曾经沉迷的五花八门的微信文章，今天叫我享受当下，明天告诉我要到世界看看，后天就是要炒股创业搞代购，被吹的东倒西歪实则原地踏步。 &lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们这个时代，所谓大数据，往往是大噪音，大脑天生的难以抵抗那些有诱惑力的信息， 如同一个垃圾桶一样被倒进各种各样的垃圾，那些垃圾制造者通过这些在你脑力装上无数他们可以利用的插件。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;周围人的信息越多，不一定是福祉，对于不够坚定的心，往往被此淹没，焦点就此失去。我们没完没了的接受信息，我们不自主的把什么都抓住，就是我们难以分出生活的二八的原因。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e28de9c920ce035fb875dfef26852797.png" data-rawwidth="630" data-rawheight="325"&gt;&lt;p&gt;另外一个难以分出二八的原因是我们的欲望太多。 其实学会忽略比学会抓住重点还重要，就像对一个摄影师学会虚化比学会对焦还重要。因为我们想要拥有的太多，我们就什么都得不到。我们来到这个世界走一遭，我们总以为我们是生活的主人，但是其实注定我们以为拥有的一切都不是我们的。  即使有的时候你真的具有一种拥有的幻觉，就像一个女生在和你做爱后说“我是你的了” ， 你也会最终明白那只是一瞬。&lt;/p&gt;&lt;p&gt;那么有什么办法呢？ 谁都不是先天的能抓住那个二， 尤其在这样一个充满随机性的世界。首先我要说的是即使你觉得还不够强， 你也要有用自己的心去判断世界的勇气，并因此去拒绝你最不喜欢的东西。 所谓哥今天也要一意孤行一把。&lt;/p&gt;&lt;p&gt;再此之上，就是你要懂得一个事实，那就是我们的世界是一个有层级的世界（Hierarchy)。 层级越高， 信息就越多越有效，噪声就越少。 从这点上看，到世界看看， 不是一张机票就可以搞定的，因为同层级的你只能看同级别的世界，无论是在北京还是新德里。 而你要打的是一个升级游戏，你要磨练的是一双火眼金睛， 站在更高的角度看世界。你可以不关注张国荣的死，但是你要关注约翰纳什，乔布斯的死。 因为通过这些人的眼镜，世界变得更清晰。&lt;strong&gt; 见贤思齐，见不贤而内自省，这是最起码的向高层级奔的方法。站在巨人的肩膀上你总会更接近二，而不是八。&lt;/strong&gt;  当你的水平和成就都在上升， 你周围的人也会跟着变化，改变你所接受的信息的本质。&lt;/p&gt;&lt;p&gt;最终你抓不抓的住那个二，是取决于你的个人修为，是你能否和自己和平相处。 地狱不是他人，而是自我。纠缠着各种欲望的自我给世上的各种东西赋予价值，赋予重量，当你得到你所欲求的快感，你高兴，要不，你痛苦。 因此我们感到生命的不能承受之重。你希望来到这个世界上，别人要有的你也有。&lt;/p&gt;&lt;p&gt;但是你若看到，你不过是海边的一颗流沙，你来了，你走了，很快你被忘的很彻底。你曾经纠结很久，彷徨很久，奋斗痛苦，在一个又一个地方挪步又挪步，而一切终归无分别于自然界的风吹过流沙的响声。&lt;/p&gt;&lt;p&gt;于是你内心平淡而宁静。你不在拘泥于置身人群还是孤独一人， 你不会在害怕别人甩你或你甩别人。看到门外签收亲吻的男女你只当做风景欣赏，说不定分他们个蛋糕吃。&lt;/p&gt;&lt;p&gt;然后你带着嘴角淡然的微笑走进办公室，拿起一本很久想读的书。你不去过多拘泥于自身的成功，你变成了一个观察者和欣赏者。 忽然某一刻神开始给你灵感，告诉你你想要完成什么，就是你会在70岁时候想起来嘴角含笑的事情。于是你宁静淡泊的投入其中，繁华落尽，你始终如同葡萄园里快乐的农夫，酿出自己喜爱的酒来。&lt;/p&gt;&lt;p&gt;一颗和平的心，使你节省很多与世界人事相磨耗费的能量，你置身闹市却游刃有余的做着自己的事，你自然而然的抓住了二，虚化了八。&lt;/p&gt;&lt;p&gt;放下自己，你内心的窗打开，于是你听到今夜又有多少热闹的事情，悲欢离合，革命战斗，成功失败。异彩纷呈的生命之流里，你就是那么一个小小的观察者。这些信息的综合让你更加看清自己，寻到你和你的终点的联系，你和宇宙的联系。&lt;/p&gt;&lt;p&gt;你只是自然能量转化的通道，就像耶稣的十字架，一横一竖划出的十字路口，你是世人的通道，把自己放在合适的地方，叫这个世界运转的更顺畅，我感到生命的不能承受之轻。  &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22984393&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sun, 16 Oct 2016 11:50:03 GMT</pubDate></item><item><title>从不拒绝自命不凡，说到柏林墙的终结</title><link>https://zhuanlan.zhihu.com/p/22965591</link><description>&lt;p&gt;老人们常会对那些有些自以为是的年轻人说：“地球离了谁都会接着转的。”。而心高气傲的年轻人则会不屑一顾的笑笑。但老人的话往往是睿智的。科学上有一个和奥斯卡姆的剪刀一样通用，却不那么广为人知的principle，即 Copernican principle，说的是：“人，作为观测者，没有道理处在一个最特殊的位置上。”更专业的说法是：&lt;/p&gt;&lt;p&gt;If an item is drawn at random from one of several sets or categories, 
it's likelier to come from the most numerous category than from any one 
of the less numerous categories.&lt;/p&gt;&lt;p&gt;提起哥白尼，我们想到的是他的日心说，日心说相比与地心说，将人作为观察者，从宇宙的中心这个特殊的位置移开。类似的，进化论也让人类不在是那么特殊的一种观察者。而认知科学家在近些年里，更是一次次的重写“人是唯一一种可以×××的动物”，×××的内容从语言到使用工具，人类作为观察者，正变得越来越没有那么特殊。&lt;/p&gt;&lt;p&gt;哥白尼法则也预示着我们处在中间的位置，不论是从那个角度去看。从微观到宏观的物理尺度来看，人类的位置如下图所示&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-9835a88061d361b82fd83f1c9dedae9e.jpg" data-rawwidth="640" data-rawheight="492"&gt;&lt;p&gt;从可预测性上来看，我们感兴趣的大部分系统也处在中间位置，比如金融系统及人际关系网络，这些系统即不像单摆那样单调，又不是完全的混沌，从而在理论上就不可预测。&lt;/p&gt;&lt;p&gt;而将哥白尼法则应用到时间上，则意味着我们观察一件事的时候既不是这件事开始的时候，也不是其将要结束的时候，而应该是在这件事正在进行的时候。这时就要讲一讲哥白尼法则最成功的运用了，即1969年，美国普林斯顿大学教授J. Richard Gott偶然参观了柏林墙说：“这座墙最多还能存在24年。我现在并不清楚它为什么会倒塌。我只预测它的寿命。”&lt;/p&gt;&lt;p&gt;这个预测背后的逻辑可以用下面两幅图来说明，这里感谢公众号”乘桴“的图片&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-07e1f5bc1f99d8f94070cc860d4b5185.jpg" data-rawwidth="564" data-rawheight="261"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3e39b775ac1c99bda20706fcc7576641.jpg" data-rawwidth="564" data-rawheight="264"&gt;&lt;p&gt;第一幅图是基于高特教授参观柏林墙的这件事是偶然的，从而其也不会处在一个特殊的时间点上，从而更可能是在柏林墙已存在的时间。第二幅图是假设我们只关注柏林墙存在的上限。&lt;/p&gt;&lt;p&gt;这样的故事会让读者很容易记住哥白尼法则，而忘记哥白尼法则的基本假设，从而造成对哥白尼法则的滥用。比如你看到一个朋友在朋友圈秀恩爱，你一问他们一天前表白成功的，然后你基于哥白尼法则预测他们有很大的可能将在一天后分手。这时你多半会犯错的，因为你观测到他们秀恩爱这个行为不是随机的，而是由于他们刚刚认识才会发生的，这时你就不该用哥白尼法则。&lt;/p&gt;&lt;p&gt;为了运用哥白尼法则，我们需要了解置信度的概念，根据高特教授的推测，在80%准确度的条件下，在特殊观测位置之外，某件事物未来存在时间最少不小于过去存在时间的1/9，最大不大于过去存在时间的9倍。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6f2fb579adbb334063f8565b0e611235.jpg" data-rawwidth="562" data-rawheight="304"&gt;&lt;p&gt;这意味着我们需要足够多的观察，来确保自己的预测不犯错误。比如马斯克的超级高铁建成了，如果我为了保证其是安全的，不会在其首航乘坐，这时任何关于这项新技术的安全性的估计，根据哥白尼法则，都不够准确。而换一个视角来看，制药商要论证一个药品是安全的，则需要提供之前多次实验的数据，而不是只是检测一次。&lt;/p&gt;&lt;p&gt;将哥白尼法则运用到日常的生活中，最典型的例子就是不要总跳槽。如果你看到一个每三个月跳槽一个的求职者，你最稳妥的估计是他三个月后还会从你的公司跳出去。如果你讲述一个人的感情经历时说你有过10个前任，那么你有很大的概率成为她或者他的前任。&lt;/p&gt;&lt;p&gt;将哥白尼法则推广出来，就会是机器学习中运用的最大熵原理，即在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，从而让我们处在特殊的观察者的地位上。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-8941e68da7b4c2853e21addef60e65b8.png" data-rawwidth="300" data-rawheight="300"&gt;&lt;p&gt;熵是用来度量不确定性的，在上图中，我们知道水中加入了墨水，我们给出的预测会是右边而不是左边，不止是由于热力学第二定理。当我们假设我们观察加入墨水后的时候不那么特殊，那么最有可能的就是右边的状态。接着想象如果加入的不是一滴墨水，而是一则信息进入了股票市场，最大熵原理告诉我们在做预测的时候，也应该做出尽可能少的假设。&lt;/p&gt;&lt;p&gt;最大熵模型的应用广泛，自然语言处理，天体物理，医学，金融等，而这个模型的可扩展性，正是来源于哥白尼原理。投资时讲不要把所有鸡蛋放到一个篮子中，其背后的道理也是假设我们观察到的投资机会不是那么特殊的。&lt;/p&gt;&lt;p&gt;关于哥白尼原理，要说的还有很多。最大熵模型又是机器学习中一个比较难训练的模型，这里只是将一些观念连接起来，最后在文章的结尾开一个脑洞，老子云”多言数穷，不若守于中。”这里的道理其实也是说多言，也就会不自觉的让我们作为观察者变得特殊起来，守乎中，就是保持没那么有把握的状态，假设自己是在中间的位置上，这不也符合哥白尼原理吗？如果你接受了这一句背后的逻辑，那么道德经中这一句之前的句子你应该不会误解了。&lt;/p&gt;&lt;p&gt;道德经中的“天地不仁，以万物为刍狗：圣人不仁，以百姓为刍狗。”常常被人们误解，但若是理解到这里说的是要我们不要把自己当成是特殊的观察者，那么对这一句的道德批判就会少一些。&lt;/p&gt;&lt;p&gt;本文首发于微信公众号混沌巡洋舰（chaoscruiser）。&lt;/p&gt;&lt;p&gt;商业转载请联系作者，非商业转载请注明出处。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22965591&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sat, 15 Oct 2016 10:22:26 GMT</pubDate></item><item><title>循环神经网络RNN打开手册</title><link>https://zhuanlan.zhihu.com/p/22930328</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-63ac60c5e0fc60017e45360516c95682_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;最近朋友前小伙伴都已经传播疯了的谷歌翻译，实现了令人惊艳的性能。这里的技术核心， 就是RNN- 我们常说的传说中的循环神经网络。 &lt;b&gt;RNN可以称得上是深度学习未来最有前景的工具之一&lt;/b&gt;。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功， 你想了解它的威力的根源吗？ 你想知道一些最新的RNN应用?请看下文。&lt;/p&gt;&lt;p&gt;为什么RNN会有如此强大的效力？ 让我们从基础学起。首先， 要看RNN和对于图像等静态类变量处理立下神功的卷积网络CNN的结构区别来看，  “循环”两个字，已经点出了RNN的核心特征， 即系统的输出会保留在网络里， 和系统下一刻的输入一起共同决定下一刻的输出。&lt;b&gt;这就把动力学的本质体现了出来， 循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖。另一个角度看也符合著名的图灵机原理。&lt;/b&gt; 即此刻的状态包含上一刻的历史，又是下一刻变化的依据。 这其实包含了可编程神经网络的核心概念，即， 当你有一个未知的过程，但你可以测量到输入和输出， 你假设当这个过程通过RNN的时候，它是可以自己学会这样的输入输出规律的， 而且因此具有预测能力。 在这点上说， RNN是图灵完备的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6522f0e0cd9740f45e1ee46591898081.png" data-rawwidth="827" data-rawheight="262"&gt;&lt;p&gt;图： 图1即CNN的架构， 图2到5是RNN的几种基本玩法。图2是把单一输入转化为序列输出，例如把图像转化成一行文字。 图三是把序列输入转化为单个输出， 比如情感测试，测量一段话正面或负面的情绪。   图四是把序列转化为序列， 最典型的是机器翻译，
注意输入和输出的“时差”。 图5是无时差的序列到序列转化， 比如给一个录像中的每一帧贴标签。  图片来源 The unreasonable
effective RNN。 &lt;/p&gt;&lt;p&gt;我们用一段小巧的python代码让你重新理解下上述的原理：&lt;/p&gt;&lt;p&gt;&lt;b&gt;class&lt;/b&gt;&lt;b&gt;RNN&lt;/b&gt;:&lt;/p&gt;&lt;p&gt;&lt;i&gt;# ...&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;def &lt;/b&gt;&lt;b&gt;step&lt;/b&gt;(self, x):&lt;/p&gt;&lt;p&gt;&lt;i&gt; # update the hidden state&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;     self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;# compute the output vector&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;    y = np.dot(self.W_hy, self.h)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;    return&lt;/b&gt; y &lt;/p&gt;&lt;p&gt;这里的h就是hidden variable 隐变量，即整个网络每个神经元的状态，x是输入， y是输出， 注意着三者都是高维向量。&lt;b&gt;隐变量h，就是通常说的神经网络本体，也正是循环得以实现的基础， 因为它如同一个可以储存无穷历史信息(理论上)的水库&lt;/b&gt;，一方面会通过输入矩阵W_xh吸收输入序列x的当下值，一方面通过网络连接W_hh进行内部神经元间的相互作用（网络效应，信息传递），因为其网络的状态和输入的整个过去历史有关，  最终的输出又是两部分加在一起共同通过非线性函数tanh。&lt;b&gt; 整个过程就是一个循环神经网络“循环”的过程。  W_hh理论上可以可以刻画输入的整个历史对于最终输出的任何反馈形式&lt;/b&gt;，从而刻画序列内部，或序列之间的时间关联， 这是RNN强大的关键。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f1798a007e657d4a1e65afdd9bc8a241.png" data-rawwidth="920" data-rawheight="757"&gt;&lt;p&gt;那么CNN似乎也有类似的功能？ 那么CNN是不是也可以当做RNN来用呢? 答案是否定的，R&lt;b&gt;NN的重要特性是可以处理不定长的输入，得到一定的输出&lt;/b&gt;。当你的输入可长可短， 比如训练翻译模型的时候， 你的句子长度都不固定，你是无法像一个训练固定像素的图像那样用CNN搞定的。而利用RNN的循环特性可以轻松搞定。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-225be444ffc0da0bd0cbfb79b6b0ad80.png" data-rawwidth="1045" data-rawheight="273"&gt;图， CNN（左）和RNN（右）的结构区别，  注意右图中输出和隐变量网络间的双向箭头不一定存在，往往只有隐变量到输出的箭头。  &lt;/p&gt;&lt;p&gt;RNN的本质是一个数据推断（inference）机器，  只要数据足够多，就可以得到从x（t）到y（t）的概率分布函数， 寻找到两个时间序列之间的关联，从而达到推断和预测的目的。 这里我们无疑回想到另一个做时间序列推断的神器- HMM， 隐马尔科夫模型，
在这个模型里， 也有一个输入x和输出y，和一个隐变量h， 而这的h和刚刚的RNN里的h区别是迭代法则，
隐马通过跃迁矩阵把此刻的h和下一刻的h联系在一起。跃迁矩阵随时间变化， 而RNN中没有跃迁矩阵的概念，取而代之的是神经元之间的连接矩阵。 HMM本质是一个贝叶斯网络， 因此每个节点都是有实际含义的，而RNN中的神经元只是信息流动的枢纽而已，并无实际对应含义。两者还是存在千丝万缕的联系， 首先隐马能干的活RNN几乎也是可以做的，比如语言模型，但是就是RNN的维度会更高。在这些任务上RNN事实上是用它的网络表达了隐马的跃迁矩阵。在训练方法上， 隐马可以通过类似EM来自最大后验概率的算法得出隐变量和跃迁矩阵最可能的值。 而RNN可以通过一般的梯度回传算法训练。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-0dc175e304f9584ba4fe5dbaa054cfd7.png" data-rawwidth="711" data-rawheight="545"&gt;&lt;p&gt;那么我们看一些RNN处理任务的具体案例吧：&lt;/p&gt;&lt;p&gt;这段代码来自&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="github.com 的页面"&gt;https://gist.github.com/karpathy/d4dee566867f8291f086&lt;/a&gt;， 大家有兴趣的可以下载去训练一训练。 &lt;/p&gt;&lt;p&gt;比如说， 学说话！ 如何叫计算机说出一段类似人话的东西呢？ &lt;/p&gt;&lt;p&gt;&lt;b&gt;此处我们从一个非常具体的程序讲起， 看你如何一步步的设计一个程序做最简单的语言生成任务，这个任务的目标类似是让神经网络做一个接龙， 给它一个字母，让它猜后面的， 比如给它Hell， 它就跟着街上o。  示意图如下：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;data = open('input.txt', 'rw').read() # should be simple plain text file&lt;/p&gt;&lt;p&gt;chars = list(set(data))  #  vocabulary&lt;/p&gt;&lt;p&gt;data_size, vocab_size = len(data), len(chars)&lt;/p&gt;&lt;p&gt;print 'data has %d characters, %d unique.' % (data_size, vocab_size)&lt;/p&gt;&lt;p&gt;char_to_ix = { ch:i for i,ch in enumerate(chars) }    # vocabulary&lt;/p&gt;&lt;p&gt;ix_to_char = { i:ch for i,ch in enumerate(chars) }    # index&lt;/p&gt;&lt;p&gt;&lt;b&gt;首先我们把字母表达成向量，用到一个叫enumerate的函数， 这如同在构建语言的数字化词典（vocabulary）， 在这一步之后， 语言信息就变成了数字化的时间序列&lt;/b&gt;&lt;/p&gt;&lt;p&gt;hidden_size = 100 # size of hidden layer of neurons&lt;/p&gt;&lt;p&gt;seq_length = 25 # number of steps to unroll the RNN for&lt;/p&gt;&lt;p&gt;learning_rate = 1e-1&lt;/p&gt;&lt;p&gt;# model parameters&lt;/p&gt;&lt;p&gt;Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden&lt;/p&gt;&lt;p&gt;Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden&lt;/p&gt;&lt;p&gt;Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output&lt;/p&gt;&lt;p&gt;bh = np.zeros((hidden_size, 1)) # hidden bias&lt;/p&gt;&lt;p&gt;by = np.zeros((vocab_size, 1)) # output bias&lt;/p&gt;&lt;p&gt;&lt;b&gt;下一步我们要初始化三个矩阵，即W_xh， W_hh，W_hy  分别表示输入和隐层，
隐层和隐层， 隐层和输出之间的连接，以及隐层和输出层的激活函数中的bias（ bh和by）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Loss=[ ]&lt;/p&gt;&lt;p&gt;Out=[ ]&lt;/p&gt;&lt;p&gt;&lt;b&gt;while True:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # prepare inputs (we're sweeping
from left to right in steps seq_length long)&lt;/p&gt;&lt;p&gt;  if p+seq_length+1 &amp;gt;=
len(data) or n == 0:&lt;/p&gt;&lt;p&gt;   hprev =
np.zeros((hidden_size,1)) # reset RNN memory&lt;/p&gt;&lt;p&gt;    p = 0 # go from start of data&lt;/p&gt;&lt;p&gt;  inputs = [char_to_ix[ch] for ch
in data[p:p+seq_length]]&lt;/p&gt;&lt;p&gt;  targets = [char_to_ix[ch] for ch
in data[p+1:p+seq_length+1]]&lt;/p&gt;&lt;p&gt;&lt;b&gt;下一步是正是开始程序， 首先准备输入：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # sample from the model now and
then&lt;/p&gt;&lt;p&gt;&lt;b&gt;  if n % 100 == 0:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    sample_ix = sample(hprev,
inputs[0], 200)&lt;/p&gt;&lt;p&gt;    txt = ' '.join(ix_to_char[ix]
for ix in sample_ix)&lt;/p&gt;&lt;p&gt;print '----\n %s \n----' % (txt, )&lt;/p&gt;&lt;p&gt;&lt;b&gt;这一步要做的是每训练一百步看看效果， 看RNN生成的句子是否更像人话。 Sample的含义就是给他一个首字母，然后神经网络会输出下一个字母，然后这两个字母一起作为再下一个字母的输入，依次类推，最后会给出这个函数的定义：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # forward seq_length characters
through the net and fetch gradient&lt;/p&gt;&lt;p&gt;  loss, dWxh, dWhh, dWhy, dbh,
dby, hprev,y = lossFun(inputs, targets, hprev)&lt;/p&gt;&lt;p&gt;  smooth_loss = smooth_loss *
0.999 + loss * 0.001&lt;/p&gt;&lt;p&gt;&lt;b&gt;  if n % 100 == 0: &lt;/b&gt;&lt;/p&gt;&lt;p&gt;print 'iter %d,
loss: %f' % (n, smooth_loss) # print progress&lt;/p&gt;&lt;p&gt;&lt;b&gt;这一步是寻找梯度， loss  function即计算梯度  ，  loss function的具体内容&lt;/b&gt;&lt;b&gt;关键即测量回传的信息以供学习。函数内容再最后放出&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后一步是根据梯度调整参数的值，即学习的过程。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;  # perform parameter update with
Adagrad&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  for param, dparam, mem in
zip([Wxh, Whh, Why, bh, by],&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  [dWxh, dWhh, dWhy, dbh, dby],&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  [mWxh, mWhh, mWhy, mbh, mby]):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;        mem += dparam * dparam&lt;/p&gt;&lt;p&gt;        param += -learning_rate *
dparam / np.sqrt(mem + 1e-8) # adagrad update&lt;/p&gt;&lt;p&gt;  p += seq_length # move data
pointer&lt;/p&gt;&lt;p&gt; n += 1 # iteration counter&lt;/p&gt;&lt;p&gt; Loss.append(loss)  &lt;/p&gt;&lt;p&gt; Out.append(txt)&lt;/p&gt;&lt;p&gt;&lt;b&gt;这就是主程序，没错， 就是这么简单， 刚刚省略的loss function 如下，这个函数的输出就是错误的梯度： &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;def lossFun(inputs, targets, hprev):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;"""&lt;/p&gt;&lt;p&gt; xs, hs, ys, ps = {}, {}, {}, {}&lt;/p&gt;&lt;p&gt; hs[-1] = np.copy(hprev)&lt;/p&gt;&lt;p&gt; loss = 0&lt;/p&gt;&lt;p&gt;# forward pass&lt;/p&gt;&lt;p&gt;&lt;b&gt; for t in xrange(len(inputs)):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation&lt;/p&gt;&lt;p&gt;    xs[t][inputs[t]] = 1&lt;/p&gt;&lt;p&gt;    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state&lt;/p&gt;&lt;p&gt;&lt;i&gt;#Whh*hs--&amp;gt;Whh*y_syn*hs; y_syn[t+1]=MishaModel(y_syn[t],tau,U,hs) xe*xg(t)&lt;/i&gt;&lt;/p&gt;&lt;p&gt;    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars&lt;/p&gt;&lt;p&gt;    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars&lt;/p&gt;&lt;p&gt;    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)&lt;/p&gt;&lt;p&gt;&lt;i&gt;# backward pass: compute gradients going backwards&lt;/i&gt;&lt;/p&gt;&lt;p&gt;  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)&lt;/p&gt;&lt;p&gt;  dbh, dby = np.zeros_like(bh), np.zeros_like(by)&lt;/p&gt;&lt;p&gt;  dhnext = np.zeros_like(hs[0])&lt;/p&gt;&lt;p&gt;&lt;b&gt;  for t in reversed(xrange(len(inputs))):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    dy = np.copy(ps[t])&lt;/p&gt;&lt;p&gt;    dy[targets[t]] -= 1 &lt;/p&gt;&lt;p&gt;&lt;i&gt;# backprop into y. see &lt;a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-editable="true" data-title="CS231n Convolutional Neural Networks for Visual Recognition"&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt; if confused here&lt;/i&gt;&lt;/p&gt;&lt;p&gt;   dWhy += np.dot(dy, hs[t].T)&lt;/p&gt;&lt;p&gt;   dby += dy&lt;/p&gt;&lt;p&gt;   dh = np.dot(Why.T, dy) + dhnext # backprop into h&lt;/p&gt;&lt;p&gt;   dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity&lt;/p&gt;&lt;p&gt;   dbh += dhraw&lt;/p&gt;&lt;p&gt;   dWxh += np.dot(dhraw, xs[t].T)&lt;/p&gt;&lt;p&gt;   dWhh += np.dot(dhraw, hs[t-1].T)&lt;/p&gt;&lt;p&gt;   dhnext = np.dot(Whh.T, dhraw)&lt;/p&gt;&lt;p&gt;&lt;b&gt; for dparam in [dWxh, dWhh, dWhy, dbh, dby]:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients&lt;/p&gt;&lt;p&gt; return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1],ys&lt;/p&gt;还有刚刚生成sample的函数，这个东西让整个程序根据第一个首字母，采样生成下一个字母， 再迭代推测第三字母， 直到指定字数，形式上看，是得到RNN的输出，然后这个由输出得出字母采样的概率分布： &lt;p&gt;&lt;b&gt;def sample(h, seed_ix, n):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  """&lt;/p&gt;&lt;p&gt;  sample a sequence of integers from the model&lt;/p&gt;&lt;p&gt;  h is memory state, seed_ix is seed letter for first time step&lt;/p&gt;&lt;p&gt;  """&lt;/p&gt;&lt;p&gt;  x = np.zeros((vocab_size, 1))&lt;/p&gt;&lt;p&gt;  x[seed_ix] = 1&lt;/p&gt;&lt;p&gt;  ixes = []&lt;/p&gt;&lt;p&gt;  for t in xrange(n):&lt;/p&gt;&lt;p&gt;    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)&lt;/p&gt;&lt;p&gt;    y = np.dot(Why, h) + by&lt;/p&gt;&lt;p&gt;    p = np.exp(y) / np.sum(np.exp(y))&lt;/p&gt;&lt;p&gt;    ix = np.random.choice(range(vocab_size), p=p.ravel())&lt;/p&gt;&lt;p&gt;    x = np.zeros((vocab_size, 1))&lt;/p&gt;&lt;p&gt;    x[ix] = 1&lt;/p&gt;&lt;p&gt;    ixes.append(ix)&lt;/p&gt;&lt;p&gt;  return ixes&lt;/p&gt;&lt;p&gt;&lt;b&gt;让我们看看RNN得到的一些训练结果，训练素材是网上随便找的一小段莎剧评论文章： &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;期初一些乱码：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;T. TpsshbokKbpWWcTxnsOAoTn:og?eu
l0op,vHH4tag4,y.ciuf?w4SApx?
eh:dfokdrlKvKnaTd?bdvabr.0rSuxaurobkbTf,mb,Htl0uma4HHpeas
n4ub::wslmpscsWmtm?xbH us:HOug4nvdWS4nil hTkbH Smeu
wo0tocvTAfyuvme0vihkpviiHT0:&lt;/p&gt;&lt;p&gt;&lt;b&gt;过一会开始有一些单词模样的东西出来， 甚至有Shakespear：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;am Shakespeare brovid thiais on an 4iwpes cis
oets, primarar Sorld soenth and hathiare orthispeathames ses, An ss porkssork.
utles thake be ynlises hed and porith thes, proy ditsor thake provf provrde&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后已经像是人话了，那真的是人模狗样的句子啊，以至于让我猜测它是不是开始思考了，也就是训练了半小时样子：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;of specific events in his life and provide little
on the person who experis somewhat a mystery. There are two primary sources
that provide historians with a basic outline of his life…&lt;/p&gt;&lt;p&gt;&lt;b&gt;语言结构通过神经网络可以从一堆乱码中涌现出来， 这正是目前机器翻译的state of  the art  SMT（统计机器翻译）的基础， 下面让我们了解一下大明明鼎鼎的google翻译又是用了哪些炫技。&lt;/b&gt; 首先google翻译的基础正是这个游戏般容易， 却思想内容极为深刻的RNN。 但是这里却做了若干步变化。 这里要提到RNN的一个变种LSTM。 &lt;/p&gt;&lt;p&gt;LSTM（Long short term memory）顾名思义， 是增加了记忆功能的RNN， 首先为什么要给RNN增加记忆呢？ 这里就要提到一个有趣的概念叫梯度消失（Vanishing Gradient），刚刚说RNN训练的关键是梯度回传，梯度信息在时间上传播是会衰减的， 那么回传的效果好坏， 取决于这种衰减的快慢， 理论上RNN可以处理很长的信息， 但是由于衰减， 往往事与愿违， 如果要信息不衰减，
我们就要给神经网络加记忆，这就是LSTM的原理了。 这里我们首先再增加一个隐变量作为记忆单元，然后把之前一层的神经网络再增加三层， 分别是输入门，输出门，遗忘门， 这三层门就如同信息的闸门， 控制多少先前网络内的信息被保留， 多少新的信息进入，而且门的形式都是可微分的sigmoid函数，确保可以通过训练得到最佳参数。 &lt;/p&gt;&lt;p&gt;信息闸门的原理另一个巧妙的理解是某种“惯性” 机制，隐变量的状态更新不是马上达到指定的值，而是缓慢达到这个值， 如同让过去的信息多了一层缓冲，而要多少缓冲则是由一个叫做遗忘门的东西决定的。 如此我们发现其实这几个新增加的东西最核心的就是信息的闸门遗忘门。 根据这一原理，我们可以抓住本质简化lstm，如GRU或极小GRU。 其实我们只需要理解这个模型就够了，而且它们甚至比lstm更快更好。 &lt;/p&gt;&lt;p&gt;我们看一下最小GRU的结构：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8c8875bf1f9ba2463f21da75f705af09.png" data-rawwidth="580" data-rawheight="244"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-66b5ef0a9cc9373499abbae00ae73a20.png" data-rawwidth="530" data-rawheight="121"&gt;&lt;p&gt;摘自论文：  Minimal Gated Unit for
Recurrent Neural Networks &lt;/p&gt;&lt;p&gt;第一个方程f即遗忘门， 第二方程如果你对比先前的RNN会发现它是一样的结构， 只是让遗忘门f来控制每个神经元放多少之前信息出去（改变其它神经元的状态）， 第三个方程描述“惯性” ，即最终每个神经元保持多少之前的值，更新多少。 &lt;/p&gt;&lt;p&gt;这个结构你理解了就理解了记忆体RNN的精髓。 &lt;/p&gt;&lt;p&gt;好了是时候看一下google 翻译大法是怎么玩的， 首先，翻译是沟通两个不同的语言， 而你要这个沟通的本质是因为它们所表达的事物是相同的， 我们自己的大脑做翻译的时候，也是根据它们所表达的概念相同比如苹果-vs-apple来沟通两个语言的。如果汉语是输入，英语是输出，神经网络事实上做的是这样一件事：&lt;/p&gt;&lt;p&gt;Encoding： 用一个LSTM把汉语变成神经代码&lt;/p&gt;&lt;p&gt;Decoding：用另一个LSTM把神经代码转化为英文。  &lt;/p&gt;&lt;p&gt;第一个lstm的输出是第二个lstm的输入， 两个网络用大量语料训练好即可。 Google这一次2016寄出的大法， 是在其中加入了attention机制 ，这样google的翻译系统就更接近人脑。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-942aca912c133da5ec8fe74beebb6ec7.png" data-rawwidth="1272" data-rawheight="655"&gt;&lt;p&gt;运用记忆神经网络翻译的核心优势是我们可以灵活的结合语境，实现句子到句子，段落到段落的过度， 因为记忆特性使得网络可以结合不同时间尺度的信息而并非只抓住个别单词， 这就好像你能够抓住语境而非只是望文生义。也是因为这个RNN有着无穷无尽的应用想象力， 我们将在下一篇继续讲解google翻译以及rnn的各种应用。&lt;/p&gt;&lt;p&gt;参考文献 ： &lt;/p&gt;&lt;p&gt;The unreasonable effective RNN&lt;/p&gt;&lt;p&gt;Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation &lt;/p&gt;&lt;p&gt;Minimal Gated Unit for Recurrent Neural
Networks &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22930328&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Thu, 13 Oct 2016 17:18:03 GMT</pubDate></item><item><title>Algorithm to live by读书笔记 我们该不该自动化我们的决策</title><link>https://zhuanlan.zhihu.com/p/22911801</link><description>&lt;p&gt;作者郭瑞东&lt;/p&gt;&lt;p&gt;Algorithm to live by是一本涉猎很广的书，其中涉及的计算机领域的诸多经典算法，从其目录中就可以看出，第一章optimal stopping讲该何时停止搜索，无论你找的是车位还是梦中情人，而这个问题的本质，即探索和利用之间的权衡，是第二章的主题，我们该花多少资源去注视星空，还是该花精力去产业化已有的技术，是个动态的取舍。接下来作者谈论了排序算法，着重与不同的排序在日常生活中是如何降低争端，促进人类的协作的。之后的两章，谈论了缓存和资源调度这两种操作系统中会用到的算法，以上的几章，谈论的是如何将计算机算法中的思想应用到日常的目标管理和资源调度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-9e78c62396d00b7f60f2e98cb07b9492.jpg" data-rawwidth="294" data-rawheight="450"&gt;&lt;p&gt;从第6章开始，作者谈论了贝叶斯法则，过拟合，relaxation（放松条件），随机化，讲述这四种来自与机器学习的思路，如何指导读者更好的将新信息融合到之前的框架中，如何简化问题，如果说之前讲的是未来的自己和现在自己的权衡，那么这四章讲述的是未知的自己和已知的自己的讲和。&lt;/p&gt;&lt;p&gt;之后的两章都很长，讲述网络算法和博弈算法（例如囚徒悖论的算法实现），这两章讲的是如何自己眼中的自己和社会中呈现的自己的关系和互动。之后的conclusion，提到了computional kindness，即通过自动化的决策和权衡，让算法把我们生活的世界变成一个更美好的地方。&lt;/p&gt;&lt;p&gt;举个例子吧。比如前几天新闻报道的大学新生被骗8000元钱后，悲伤过度猝死的事情。这固然可以说是阶级固化造成了恶果，但更有建设性的方案是想办法避免类似的事情发生，可行的方法是选出那些容易上当受骗的人，给他们发短信提醒，能精准的预测出那些处于社会底层的，骗子最容易得手的人，就能避免类似的事情发生，这就是所谓的computional kindness。&lt;/p&gt;&lt;p&gt;类似的例子还出现在医疗中，如果能通过曾经患有，或者更容易患精神疾病的人的电话语音记录，那么医生可以及时在出病人发病的早期给出干预，那么这些人的生活质量也会相应提高。无论是数字化医疗还是数字化教育，其产品都需要做到computional kindly，即让你的产品通过收集数据来帮助用户做决策，就像一个好的管家那样，不侵入用户的日常生活，却能够以最少的认知成本让用户做出权衡，或者将各种选项的成本和收益给用户列出来，帮助用户做选择，而不是越俎代庖，专制的替用户选择。这也就是所谓的好的产品要回到初心。&lt;/p&gt;&lt;p&gt;读完这本书，熟悉斯坦诺维奇的《超越智商》这本书的读者会想起算法心智这个概念，对于不熟悉的读者，自主心智，算法心智，反省心智类似《思考，快与慢》这本书中讲到的系统一和系统二，见下图&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3ea23033b4e2d8d4dadfe3aba012d7ee.jpg" data-rawwidth="640" data-rawheight="440"&gt;&lt;p&gt;而这本书所讲的，正是如何将一些本来所需要用到反省心智的流程过度到使用算法心智上，这样做的目的不是让我们变得更加像机器，而是将解放出的认知资源用在更加体现人类创造力的领域，我们将用在迷茫该怎么做最好的时间，通过跨界借用已经在计算机上被证实有用的方法来解决，从而让我们能够有更多的时间精力投入到诗歌，艺术等需要理性与感性交融的地方。&lt;/p&gt;&lt;p&gt;阅读这本书，和看任何算法一样，都体现出现代社会的复杂性下所必须的权衡与弹性，最初的问题太难，放松问题的约束条件，或者去找近似解，无法证明一个问题的解一定正确，可以通过随机的验证保证其在可接受的置信度上是正确的，这些都体现了变通。问题的解决不意味着问题的终结，而是意味着暂时这个问题在可接受的时间范围内有了一个还可以接受的解。如同我们在黑暗中探索，算法带给我们生活的启发永远是gentle的，是不那么极端的。&lt;/p&gt;&lt;p&gt;然而，正如星际迷航中的那首诗中说的，Do Not Go Gentle Into That good Night，人最重要的是有反省心智，是有感情，是不服输的那口气，Old age should burn and rave at close of day; Rage, rage against the dying of the light. 讲算法带来的启发，就一定要提到算法不能够，也不应该觉得我们人生的那部分。那是一片寂寥，也是一双脚链；有时是夏日聒噪的蝉，有时秋日冲天的鹤，所以这里最好的结尾是一句诗，选择诗，是因为《Algorithm to live by》的作者既是一个程序员，也曾受过正规的古典诗歌的训练，正是这样的背景，他才能写出这样具有人文关怀的跨界神作。&lt;/p&gt;&lt;p&gt;Though wise men at their end know dark is right, Because their words had forked no lightning they Do not go gentle into that good night.&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22911801&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Wed, 12 Oct 2016 21:46:17 GMT</pubDate></item><item><title>别让我们的谈话进入到马尔科夫链的状态</title><link>https://zhuanlan.zhihu.com/p/22885220</link><description>作者 郭瑞东&lt;p&gt;武林外传中有一段经典的台词，可谓是情侣吵架的典型，今天的道理就从这段台词说起：&lt;/p&gt;&lt;p&gt;&lt;em&gt;佟湘玉：你无情，你无耻，你无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：那你就不无情，不无耻，不无理取闹吗？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：我哪里无情，哪里无耻，哪里无理取闹？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：你哪里不无情，哪里不无耻，哪里不无理取闹？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：我就算再怎么无情，再怎么无耻，再怎么无理取闹。也不会比你更无情，更无耻，更无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;      白展堂：我会比你无情！？比你残酷！？比你无理取闹！？你才是我见过最无情最残酷最无理取闹的人！&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：哼，我绝对没你无情没你残酷没你无理取闹！ &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：好既然你说我无情我残酷我无理取闹那我就给你无情无耻无理取闹看看。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：哼，还说你不无情不残酷不无理取闹终于展现自己无情无耻无理取闹了吧。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：我即使无情即使无耻即使无理取闹，也是被你无情无耻无理取闹给逼出来的！&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;    佟湘玉：就你无情无耻无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;注意到了吗,这段台词的每一句，都是对上一句的直接回应，参与对话的两个人，好像只有5秒钟的记忆，只记得住对方说的上一句话，这样的对话，是Stateless的，是符合马尔科夫链的。&lt;/p&gt;&lt;p&gt; 马尔科夫链的确在自然语言处理中有广泛的应用，但我们人类的语言，真的要陷入到这样没有背景，没有状态的简化情况吗。人间的事有太多复杂之处，高效的交流需要参与者共同用其记忆和修养构建一个平台，若是对话只是如上面的例子那样，那么我们就不可能做到对事不对人，对事需要我们明白一件事的前因后果，对人却只需要给对方扣上个帽子。&lt;/p&gt;&lt;p&gt;网上的很多对话，正是由于其匿名性，人们无法观察到对话的人平常的语气，说话的风格，容易退化成为人参公鸡，而所有人身攻击的对话，都有着类似马尔科夫链的性质。这里不想具体举例子，不是不能，是不愿意。举例子就会被看成是对人，而非对事。要想做到对事，那就要说的足够有概括性，这里举红楼梦中宝玉论述文死谏武死战的句子，在36回，宝玉说道：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“那武将不过仗血气之勇，疏谋少略，他自己无能，送了性命，这难道也是不得已！那文官更不可比武官了，他念两句书横在心里，若朝廷少有疵瑕，他就胡谈乱劝，只顾他邀忠烈之名，浊气一涌，即时拚死，这难道也是不得已”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;这段话中想批判的人，在我们现代生活中并不少见，只是当代这些键盘侠们不会面临生死的考验了。要知道一句“不得已”，遮盖了多少思考的缺失，多少人其实在给出机器人一般的机械化的反应，如上文中说的文官，没有考虑到现实的复杂性，只是教条的套用书中的句子。又有多少人，在做事的时候只是凭着热情，没有想清楚一步步该怎么做，做事情中遇到的不确定性该怎么解决。出了问题，拿一句尽力了推脱，这种短视的行为，也值得我们反省。&lt;/p&gt;&lt;p&gt;美国的大选今年吸引了太多人的关注，你应该有自己的观点，但遇到和你观点相反的人，你也不能为此红脸。为什么在不熟的人之间不能谈政治宗教之类的话题，是因为这类话题要想产生有效率有意义的谈话，需要交谈者知根知底，需要彼此互相尊重，否则很容易就退化成人身攻击。明白了很低效率的多谈话其实是一个马尔科夫链，你就应该跳出来，拒绝参加这些低效率的谈话。比如你支持川普，别人说你怎么会喜欢这个疯子，你也要笑一笑，而不去争辩，这时的言语只会带来误解。&lt;/p&gt;&lt;p&gt;不止是男女朋友之间的争吵，父母教育孩子的时候，也会不自觉的陷入马尔科夫链式的对话。这样的对话，只会带来无用的重复和孩子的逆反心理。那么该怎么办了？我们先回到一开篇武林外传的那个例子，看看这段对话是怎么结束的。&lt;/p&gt;&lt;p&gt;&lt;em&gt; （字幕，一夜过去了）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　【后院，昼】&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　（两人趴在磨上嗓子沙哑还在继续吵）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　白展堂：你才无情，你才无耻，你才无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　佟湘玉：你再这么无情，再这么无耻，再这么无理取闹（鸡叫了，才意识到天都亮了）——也该洗洗睡了。你好好干活，不许偷懒啊。我给你煎两个鸡蛋去。吵了一个晚上，你不饿我还饿呢！（欲进厨房）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;这里之所以这段持续了一晚上的对话能够结束，是因为参与者记起了其本来的身份。这暗示我们要想让我们的对话不只是无用的重复，就要时常提醒一下参与者是谁。比如情侣吵架后，和解往往是由于想起了最初为什么要在一起。&lt;/p&gt;&lt;p&gt;总结一下，这篇小文想说我们应该怎样高效的对话，方法是识别出那些低效的重复的交谈模式，即有着马尔科夫链性质的stateless的交谈，从而有意识的去避免和点头之交发生这类谈话，以及通过唤起背景信息，来让熟人之间的交流跳出这种交流模式。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22885220&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Tue, 11 Oct 2016 19:30:50 GMT</pubDate></item></channel></rss>