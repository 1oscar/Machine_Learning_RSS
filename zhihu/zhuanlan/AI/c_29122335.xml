<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>混沌巡洋舰 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/c_29122335</link><description>跨界思考内容供应商，与微信公众号混沌巡洋舰同属于巡洋舰科技公司。</description><lastBuildDate>Sun, 16 Oct 2016 10:17:12 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>从不拒绝自命不凡，说到柏林墙的终结</title><link>https://zhuanlan.zhihu.com/p/22965591</link><description>&lt;p&gt;老人们常会对那些有些自以为是的年轻人说：“地球离了谁都会接着转的。”。而心高气傲的年轻人则会不屑一顾的笑笑。但老人的话往往是睿智的。科学上有一个和奥斯卡姆的剪刀一样通用，却不那么广为人知的principle，即 Copernican principle，说的是：“人，作为观测者，没有道理处在一个最特殊的位置上。”更专业的说法是：&lt;/p&gt;&lt;p&gt;If an item is drawn at random from one of several sets or categories, 
it's likelier to come from the most numerous category than from any one 
of the less numerous categories.&lt;/p&gt;&lt;p&gt;提起哥白尼，我们想到的是他的日心说，日心说相比与地心说，将人作为观察者，从宇宙的中心这个特殊的位置移开。类似的，进化论也让人类不在是那么特殊的一种观察者。而认知科学家在近些年里，更是一次次的重写“人是唯一一种可以×××的动物”，×××的内容从语言到使用工具，人类作为观察者，正变得越来越没有那么特殊。&lt;/p&gt;&lt;p&gt;哥白尼法则也预示着我们处在中间的位置，不论是从那个角度去看。从微观到宏观的物理尺度来看，人类的位置如下图所示&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9835a88061d361b82fd83f1c9dedae9e.jpg" data-rawwidth="640" data-rawheight="492"&gt;&lt;p&gt;从可预测性上来看，我们感兴趣的大部分系统也处在中间位置，比如金融系统及人际关系网络，这些系统即不像单摆那样单调，又不是完全的混沌，从而在理论上就不可预测。&lt;/p&gt;&lt;p&gt;而将哥白尼法则应用到时间上，则意味着我们观察一件事的时候既不是这件事开始的时候，也不是其将要结束的时候，而应该是在这件事正在进行的时候。这时就要讲一讲哥白尼法则最成功的运用了，即1969年，美国普林斯顿大学教授J. Richard Gott偶然参观了柏林墙说：“这座墙最多还能存在24年。我现在并不清楚它为什么会倒塌。我只预测它的寿命。”&lt;/p&gt;&lt;p&gt;这个预测背后的逻辑可以用下面两幅图来说明，这里感谢公众号”乘桴“的图片&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-07e1f5bc1f99d8f94070cc860d4b5185.jpg" data-rawwidth="564" data-rawheight="261"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3e39b775ac1c99bda20706fcc7576641.jpg" data-rawwidth="564" data-rawheight="264"&gt;&lt;p&gt;第一幅图是基于高特教授参观柏林墙的这件事是偶然的，从而其也不会处在一个特殊的时间点上，从而更可能是在柏林墙已存在的时间。第二幅图是假设我们只关注柏林墙存在的上限。&lt;/p&gt;&lt;p&gt;这样的故事会让读者很容易记住哥白尼法则，而忘记哥白尼法则的基本假设，从而造成对哥白尼法则的滥用。比如你看到一个朋友在朋友圈秀恩爱，你一问他们一天前表白成功的，然后你基于哥白尼法则预测他们有很大的可能将在一天后分手。这时你多半会犯错的，因为你观测到他们秀恩爱这个行为不是随机的，而是由于他们刚刚认识才会发生的，这时你就不该用哥白尼法则。&lt;/p&gt;&lt;p&gt;为了运用哥白尼法则，我们需要了解置信度的概念，根据高特教授的推测，在80%准确度的条件下，在特殊观测位置之外，某件事物未来存在时间最少不小于过去存在时间的1/9，最大不大于过去存在时间的9倍。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6f2fb579adbb334063f8565b0e611235.jpg" data-rawwidth="562" data-rawheight="304"&gt;&lt;p&gt;这意味着我们需要足够多的观察，来确保自己的预测不犯错误。比如马斯克的超级高铁建成了，如果我为了保证其是安全的，不会在其首航乘坐，这时任何关于这项新技术的安全性的估计，根据哥白尼法则，都不够准确。而换一个视角来看，制药商要论证一个药品是安全的，则需要提供之前多次实验的数据，而不是只是检测一次。&lt;/p&gt;&lt;p&gt;将哥白尼法则运用到日常的生活中，最典型的例子就是不要总跳槽。如果你看到一个每三个月跳槽一个的求职者，你最稳妥的估计是他三个月后还会从你的公司跳出去。如果你讲述一个人的感情经历时说你有过10个前任，那么你有很大的概率成为她或者他的前任。&lt;/p&gt;&lt;p&gt;将哥白尼法则推广出来，就会是机器学习中运用的最大熵原理，即在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，从而让我们处在特殊的观察者的地位上。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-8941e68da7b4c2853e21addef60e65b8.png" data-rawwidth="300" data-rawheight="300"&gt;&lt;p&gt;熵是用来度量不确定性的，在上图中，我们知道水中加入了墨水，我们给出的预测会是右边而不是左边，不止是由于热力学第二定理。当我们假设我们观察加入墨水后的时候不那么特殊，那么最有可能的就是右边的状态。接着想象如果加入的不是一滴墨水，而是一则信息进入了股票市场，最大熵原理告诉我们在做预测的时候，也应该做出尽可能少的假设。&lt;/p&gt;&lt;p&gt;最大熵模型的应用广泛，自然语言处理，天体物理，医学，金融等，而这个模型的可扩展性，正是来源于哥白尼原理。投资时讲不要把所有鸡蛋放到一个篮子中，其背后的道理也是假设我们观察到的投资机会不是那么特殊的。&lt;/p&gt;&lt;p&gt;关于哥白尼原理，要说的还有很多。最大熵模型又是机器学习中一个比较难训练的模型，这里只是将一些观念连接起来，最后在文章的结尾开一个脑洞，老子云”多言数穷，不若守于中。”这里的道理其实也是说多言，也就会不自觉的让我们作为观察者变得特殊起来，守乎中，就是保持没那么有把握的状态，假设自己是在中间的位置上，这不也符合哥白尼原理吗？如果你接受了这一句背后的逻辑，那么道德经中这一句之前的句子你应该不会误解了。&lt;/p&gt;&lt;p&gt;道德经中的“天地不仁，以万物为刍狗：圣人不仁，以百姓为刍狗。”常常被人们误解，但若是理解到这里说的是要我们不要把自己当成是特殊的观察者，那么对这一句的道德批判就会少一些。&lt;/p&gt;&lt;p&gt;本文首发于微信公众号混沌巡洋舰（chaoscruiser）。&lt;/p&gt;&lt;p&gt;商业转载请联系作者，非商业转载请注明出处。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22965591&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sat, 15 Oct 2016 10:22:26 GMT</pubDate></item><item><title>循环神经网络RNN打开手册</title><link>https://zhuanlan.zhihu.com/p/22930328</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-63ac60c5e0fc60017e45360516c95682_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;最近朋友前小伙伴都已经传播疯了的谷歌翻译，实现了令人惊艳的性能。这里的技术核心， 就是RNN- 我们常说的传说中的循环神经网络。 &lt;b&gt;RNN可以称得上是深度学习未来最有前景的工具之一&lt;/b&gt;。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功， 你想了解它的威力的根源吗？ 你想知道一些最新的RNN应用?请看下文。&lt;/p&gt;&lt;p&gt;为什么RNN会有如此强大的效力？ 让我们从基础学起。首先， 要看RNN和对于图像等静态类变量处理立下神功的卷积网络CNN的结构区别来看，  “循环”两个字，已经点出了RNN的核心特征， 即系统的输出会保留在网络里， 和系统下一刻的输入一起共同决定下一刻的输出。&lt;b&gt;这就把动力学的本质体现了出来， 循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖。另一个角度看也符合著名的图灵机原理。&lt;/b&gt; 即此刻的状态包含上一刻的历史，又是下一刻变化的依据。 这其实包含了可编程神经网络的核心概念，即， 当你有一个未知的过程，但你可以测量到输入和输出， 你假设当这个过程通过RNN的时候，它是可以自己学会这样的输入输出规律的， 而且因此具有预测能力。 在这点上说， RNN是图灵完备的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6522f0e0cd9740f45e1ee46591898081.png" data-rawwidth="827" data-rawheight="262"&gt;&lt;p&gt;图： 图1即CNN的架构， 图2到5是RNN的几种基本玩法。图2是把单一输入转化为序列输出，例如把图像转化成一行文字。 图三是把序列输入转化为单个输出， 比如情感测试，测量一段话正面或负面的情绪。   图四是把序列转化为序列， 最典型的是机器翻译，
注意输入和输出的“时差”。 图5是无时差的序列到序列转化， 比如给一个录像中的每一帧贴标签。  图片来源 The unreasonable
effective RNN。 &lt;/p&gt;&lt;p&gt;我们用一段小巧的python代码让你重新理解下上述的原理：&lt;/p&gt;&lt;p&gt;&lt;b&gt;class&lt;/b&gt;&lt;b&gt;RNN&lt;/b&gt;:&lt;/p&gt;&lt;p&gt;&lt;i&gt;# ...&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;def &lt;/b&gt;&lt;b&gt;step&lt;/b&gt;(self, x):&lt;/p&gt;&lt;p&gt;&lt;i&gt; # update the hidden state&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;     self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;# compute the output vector&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;    y = np.dot(self.W_hy, self.h)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;    return&lt;/b&gt; y &lt;/p&gt;&lt;p&gt;这里的h就是hidden variable 隐变量，即整个网络每个神经元的状态，x是输入， y是输出， 注意着三者都是高维向量。&lt;b&gt;隐变量h，就是通常说的神经网络本体，也正是循环得以实现的基础， 因为它如同一个可以储存无穷历史信息(理论上)的水库&lt;/b&gt;，一方面会通过输入矩阵W_xh吸收输入序列x的当下值，一方面通过网络连接W_hh进行内部神经元间的相互作用（网络效应，信息传递），因为其网络的状态和输入的整个过去历史有关，  最终的输出又是两部分加在一起共同通过非线性函数tanh。&lt;b&gt; 整个过程就是一个循环神经网络“循环”的过程。  W_hh理论上可以可以刻画输入的整个历史对于最终输出的任何反馈形式&lt;/b&gt;，从而刻画序列内部，或序列之间的时间关联， 这是RNN强大的关键。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f1798a007e657d4a1e65afdd9bc8a241.png" data-rawwidth="920" data-rawheight="757"&gt;&lt;p&gt;那么CNN似乎也有类似的功能？ 那么CNN是不是也可以当做RNN来用呢? 答案是否定的，R&lt;b&gt;NN的重要特性是可以处理不定长的输入，得到一定的输出&lt;/b&gt;。当你的输入可长可短， 比如训练翻译模型的时候， 你的句子长度都不固定，你是无法像一个训练固定像素的图像那样用CNN搞定的。而利用RNN的循环特性可以轻松搞定。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-225be444ffc0da0bd0cbfb79b6b0ad80.png" data-rawwidth="1045" data-rawheight="273"&gt;图， CNN（左）和RNN（右）的结构区别，  注意右图中输出和隐变量网络间的双向箭头不一定存在，往往只有隐变量到输出的箭头。  &lt;/p&gt;&lt;p&gt;RNN的本质是一个数据推断（inference）机器，  只要数据足够多，就可以得到从x（t）到y（t）的概率分布函数， 寻找到两个时间序列之间的关联，从而达到推断和预测的目的。 这里我们无疑回想到另一个做时间序列推断的神器- HMM， 隐马尔科夫模型，
在这个模型里， 也有一个输入x和输出y，和一个隐变量h， 而这的h和刚刚的RNN里的h区别是迭代法则，
隐马通过跃迁矩阵把此刻的h和下一刻的h联系在一起。跃迁矩阵随时间变化， 而RNN中没有跃迁矩阵的概念，取而代之的是神经元之间的连接矩阵。 HMM本质是一个贝叶斯网络， 因此每个节点都是有实际含义的，而RNN中的神经元只是信息流动的枢纽而已，并无实际对应含义。两者还是存在千丝万缕的联系， 首先隐马能干的活RNN几乎也是可以做的，比如语言模型，但是就是RNN的维度会更高。在这些任务上RNN事实上是用它的网络表达了隐马的跃迁矩阵。在训练方法上， 隐马可以通过类似EM来自最大后验概率的算法得出隐变量和跃迁矩阵最可能的值。 而RNN可以通过一般的梯度回传算法训练。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0dc175e304f9584ba4fe5dbaa054cfd7.png" data-rawwidth="711" data-rawheight="545"&gt;&lt;p&gt;那么我们看一些RNN处理任务的具体案例吧：&lt;/p&gt;&lt;p&gt;这段代码来自&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="github.com 的页面"&gt;https://gist.github.com/karpathy/d4dee566867f8291f086&lt;/a&gt;， 大家有兴趣的可以下载去训练一训练。 &lt;/p&gt;&lt;p&gt;比如说， 学说话！ 如何叫计算机说出一段类似人话的东西呢？ &lt;/p&gt;&lt;p&gt;&lt;b&gt;此处我们从一个非常具体的程序讲起， 看你如何一步步的设计一个程序做最简单的语言生成任务，这个任务的目标类似是让神经网络做一个接龙， 给它一个字母，让它猜后面的， 比如给它Hell， 它就跟着街上o。  示意图如下：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;data = open('input.txt', 'rw').read() # should be simple plain text file&lt;/p&gt;&lt;p&gt;chars = list(set(data))  #  vocabulary&lt;/p&gt;&lt;p&gt;data_size, vocab_size = len(data), len(chars)&lt;/p&gt;&lt;p&gt;print 'data has %d characters, %d unique.' % (data_size, vocab_size)&lt;/p&gt;&lt;p&gt;char_to_ix = { ch:i for i,ch in enumerate(chars) }    # vocabulary&lt;/p&gt;&lt;p&gt;ix_to_char = { i:ch for i,ch in enumerate(chars) }    # index&lt;/p&gt;&lt;p&gt;&lt;b&gt;首先我们把字母表达成向量，用到一个叫enumerate的函数， 这如同在构建语言的数字化词典（vocabulary）， 在这一步之后， 语言信息就变成了数字化的时间序列&lt;/b&gt;&lt;/p&gt;&lt;p&gt;hidden_size = 100 # size of hidden layer of neurons&lt;/p&gt;&lt;p&gt;seq_length = 25 # number of steps to unroll the RNN for&lt;/p&gt;&lt;p&gt;learning_rate = 1e-1&lt;/p&gt;&lt;p&gt;# model parameters&lt;/p&gt;&lt;p&gt;Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden&lt;/p&gt;&lt;p&gt;Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden&lt;/p&gt;&lt;p&gt;Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output&lt;/p&gt;&lt;p&gt;bh = np.zeros((hidden_size, 1)) # hidden bias&lt;/p&gt;&lt;p&gt;by = np.zeros((vocab_size, 1)) # output bias&lt;/p&gt;&lt;p&gt;&lt;b&gt;下一步我们要初始化三个矩阵，即W_xh， W_hh，W_hy  分别表示输入和隐层，
隐层和隐层， 隐层和输出之间的连接，以及隐层和输出层的激活函数中的bias（ bh和by）：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Loss=[ ]&lt;/p&gt;&lt;p&gt;Out=[ ]&lt;/p&gt;&lt;p&gt;&lt;b&gt;while True:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # prepare inputs (we're sweeping
from left to right in steps seq_length long)&lt;/p&gt;&lt;p&gt;  if p+seq_length+1 &amp;gt;=
len(data) or n == 0:&lt;/p&gt;&lt;p&gt;   hprev =
np.zeros((hidden_size,1)) # reset RNN memory&lt;/p&gt;&lt;p&gt;    p = 0 # go from start of data&lt;/p&gt;&lt;p&gt;  inputs = [char_to_ix[ch] for ch
in data[p:p+seq_length]]&lt;/p&gt;&lt;p&gt;  targets = [char_to_ix[ch] for ch
in data[p+1:p+seq_length+1]]&lt;/p&gt;&lt;p&gt;&lt;b&gt;下一步是正是开始程序， 首先准备输入：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # sample from the model now and
then&lt;/p&gt;&lt;p&gt;&lt;b&gt;  if n % 100 == 0:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    sample_ix = sample(hprev,
inputs[0], 200)&lt;/p&gt;&lt;p&gt;    txt = ' '.join(ix_to_char[ix]
for ix in sample_ix)&lt;/p&gt;&lt;p&gt;print '----\n %s \n----' % (txt, )&lt;/p&gt;&lt;p&gt;&lt;b&gt;这一步要做的是每训练一百步看看效果， 看RNN生成的句子是否更像人话。 Sample的含义就是给他一个首字母，然后神经网络会输出下一个字母，然后这两个字母一起作为再下一个字母的输入，依次类推，最后会给出这个函数的定义：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  # forward seq_length characters
through the net and fetch gradient&lt;/p&gt;&lt;p&gt;  loss, dWxh, dWhh, dWhy, dbh,
dby, hprev,y = lossFun(inputs, targets, hprev)&lt;/p&gt;&lt;p&gt;  smooth_loss = smooth_loss *
0.999 + loss * 0.001&lt;/p&gt;&lt;p&gt;&lt;b&gt;  if n % 100 == 0: &lt;/b&gt;&lt;/p&gt;&lt;p&gt;print 'iter %d,
loss: %f' % (n, smooth_loss) # print progress&lt;/p&gt;&lt;p&gt;&lt;b&gt;这一步是寻找梯度， loss  function即计算梯度  ，  loss function的具体内容&lt;/b&gt;&lt;b&gt;关键即测量回传的信息以供学习。函数内容再最后放出&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后一步是根据梯度调整参数的值，即学习的过程。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;  # perform parameter update with
Adagrad&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  for param, dparam, mem in
zip([Wxh, Whh, Why, bh, by],&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  [dWxh, dWhh, dWhy, dbh, dby],&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;  [mWxh, mWhh, mWhy, mbh, mby]):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;        mem += dparam * dparam&lt;/p&gt;&lt;p&gt;        param += -learning_rate *
dparam / np.sqrt(mem + 1e-8) # adagrad update&lt;/p&gt;&lt;p&gt;  p += seq_length # move data
pointer&lt;/p&gt;&lt;p&gt; n += 1 # iteration counter&lt;/p&gt;&lt;p&gt; Loss.append(loss)  &lt;/p&gt;&lt;p&gt; Out.append(txt)&lt;/p&gt;&lt;p&gt;&lt;b&gt;这就是主程序，没错， 就是这么简单， 刚刚省略的loss function 如下，这个函数的输出就是错误的梯度： &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;def lossFun(inputs, targets, hprev):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;"""&lt;/p&gt;&lt;p&gt; xs, hs, ys, ps = {}, {}, {}, {}&lt;/p&gt;&lt;p&gt; hs[-1] = np.copy(hprev)&lt;/p&gt;&lt;p&gt; loss = 0&lt;/p&gt;&lt;p&gt;# forward pass&lt;/p&gt;&lt;p&gt;&lt;b&gt; for t in xrange(len(inputs)):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation&lt;/p&gt;&lt;p&gt;    xs[t][inputs[t]] = 1&lt;/p&gt;&lt;p&gt;    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state&lt;/p&gt;&lt;p&gt;&lt;i&gt;#Whh*hs--&amp;gt;Whh*y_syn*hs; y_syn[t+1]=MishaModel(y_syn[t],tau,U,hs) xe*xg(t)&lt;/i&gt;&lt;/p&gt;&lt;p&gt;    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars&lt;/p&gt;&lt;p&gt;    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars&lt;/p&gt;&lt;p&gt;    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)&lt;/p&gt;&lt;p&gt;&lt;i&gt;# backward pass: compute gradients going backwards&lt;/i&gt;&lt;/p&gt;&lt;p&gt;  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)&lt;/p&gt;&lt;p&gt;  dbh, dby = np.zeros_like(bh), np.zeros_like(by)&lt;/p&gt;&lt;p&gt;  dhnext = np.zeros_like(hs[0])&lt;/p&gt;&lt;p&gt;&lt;b&gt;  for t in reversed(xrange(len(inputs))):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    dy = np.copy(ps[t])&lt;/p&gt;&lt;p&gt;    dy[targets[t]] -= 1 &lt;/p&gt;&lt;p&gt;&lt;i&gt;# backprop into y. see &lt;a href="http://cs231n.github.io/neural-networks-case-study/#grad" data-editable="true" data-title="CS231n Convolutional Neural Networks for Visual Recognition"&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt; if confused here&lt;/i&gt;&lt;/p&gt;&lt;p&gt;   dWhy += np.dot(dy, hs[t].T)&lt;/p&gt;&lt;p&gt;   dby += dy&lt;/p&gt;&lt;p&gt;   dh = np.dot(Why.T, dy) + dhnext # backprop into h&lt;/p&gt;&lt;p&gt;   dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity&lt;/p&gt;&lt;p&gt;   dbh += dhraw&lt;/p&gt;&lt;p&gt;   dWxh += np.dot(dhraw, xs[t].T)&lt;/p&gt;&lt;p&gt;   dWhh += np.dot(dhraw, hs[t-1].T)&lt;/p&gt;&lt;p&gt;   dhnext = np.dot(Whh.T, dhraw)&lt;/p&gt;&lt;p&gt;&lt;b&gt; for dparam in [dWxh, dWhh, dWhy, dbh, dby]:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients&lt;/p&gt;&lt;p&gt; return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1],ys&lt;/p&gt;还有刚刚生成sample的函数，这个东西让整个程序根据第一个首字母，采样生成下一个字母， 再迭代推测第三字母， 直到指定字数，形式上看，是得到RNN的输出，然后这个由输出得出字母采样的概率分布： &lt;p&gt;&lt;b&gt;def sample(h, seed_ix, n):&lt;/b&gt;&lt;/p&gt;&lt;p&gt;  """&lt;/p&gt;&lt;p&gt;  sample a sequence of integers from the model&lt;/p&gt;&lt;p&gt;  h is memory state, seed_ix is seed letter for first time step&lt;/p&gt;&lt;p&gt;  """&lt;/p&gt;&lt;p&gt;  x = np.zeros((vocab_size, 1))&lt;/p&gt;&lt;p&gt;  x[seed_ix] = 1&lt;/p&gt;&lt;p&gt;  ixes = []&lt;/p&gt;&lt;p&gt;  for t in xrange(n):&lt;/p&gt;&lt;p&gt;    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)&lt;/p&gt;&lt;p&gt;    y = np.dot(Why, h) + by&lt;/p&gt;&lt;p&gt;    p = np.exp(y) / np.sum(np.exp(y))&lt;/p&gt;&lt;p&gt;    ix = np.random.choice(range(vocab_size), p=p.ravel())&lt;/p&gt;&lt;p&gt;    x = np.zeros((vocab_size, 1))&lt;/p&gt;&lt;p&gt;    x[ix] = 1&lt;/p&gt;&lt;p&gt;    ixes.append(ix)&lt;/p&gt;&lt;p&gt;  return ixes&lt;/p&gt;&lt;p&gt;&lt;b&gt;让我们看看RNN得到的一些训练结果，训练素材是网上随便找的一小段莎剧评论文章： &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;期初一些乱码：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;T. TpsshbokKbpWWcTxnsOAoTn:og?eu
l0op,vHH4tag4,y.ciuf?w4SApx?
eh:dfokdrlKvKnaTd?bdvabr.0rSuxaurobkbTf,mb,Htl0uma4HHpeas
n4ub::wslmpscsWmtm?xbH us:HOug4nvdWS4nil hTkbH Smeu
wo0tocvTAfyuvme0vihkpviiHT0:&lt;/p&gt;&lt;p&gt;&lt;b&gt;过一会开始有一些单词模样的东西出来， 甚至有Shakespear：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;am Shakespeare brovid thiais on an 4iwpes cis
oets, primarar Sorld soenth and hathiare orthispeathames ses, An ss porkssork.
utles thake be ynlises hed and porith thes, proy ditsor thake provf provrde&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后已经像是人话了，那真的是人模狗样的句子啊，以至于让我猜测它是不是开始思考了，也就是训练了半小时样子：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;of specific events in his life and provide little
on the person who experis somewhat a mystery. There are two primary sources
that provide historians with a basic outline of his life…&lt;/p&gt;&lt;p&gt;&lt;b&gt;语言结构通过神经网络可以从一堆乱码中涌现出来， 这正是目前机器翻译的state of  the art  SMT（统计机器翻译）的基础， 下面让我们了解一下大明明鼎鼎的google翻译又是用了哪些炫技。&lt;/b&gt; 首先google翻译的基础正是这个游戏般容易， 却思想内容极为深刻的RNN。 但是这里却做了若干步变化。 这里要提到RNN的一个变种LSTM。 &lt;/p&gt;&lt;p&gt;LSTM（Long short term memory）顾名思义， 是增加了记忆功能的RNN， 首先为什么要给RNN增加记忆呢？ 这里就要提到一个有趣的概念叫梯度消失（Vanishing Gradient），刚刚说RNN训练的关键是梯度回传，梯度信息在时间上传播是会衰减的， 那么回传的效果好坏， 取决于这种衰减的快慢， 理论上RNN可以处理很长的信息， 但是由于衰减， 往往事与愿违， 如果要信息不衰减，
我们就要给神经网络加记忆，这就是LSTM的原理了。 这里我们首先再增加一个隐变量作为记忆单元，然后把之前一层的神经网络再增加三层， 分别是输入门，输出门，遗忘门， 这三层门就如同信息的闸门， 控制多少先前网络内的信息被保留， 多少新的信息进入，而且门的形式都是可微分的sigmoid函数，确保可以通过训练得到最佳参数。 &lt;/p&gt;&lt;p&gt;信息闸门的原理另一个巧妙的理解是某种“惯性” 机制，隐变量的状态更新不是马上达到指定的值，而是缓慢达到这个值， 如同让过去的信息多了一层缓冲，而要多少缓冲则是由一个叫做遗忘门的东西决定的。 如此我们发现其实这几个新增加的东西最核心的就是信息的闸门遗忘门。 根据这一原理，我们可以抓住本质简化lstm，如GRU或极小GRU。 其实我们只需要理解这个模型就够了，而且它们甚至比lstm更快更好。 &lt;/p&gt;&lt;p&gt;我们看一下最小GRU的结构：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8c8875bf1f9ba2463f21da75f705af09.png" data-rawwidth="580" data-rawheight="244"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-66b5ef0a9cc9373499abbae00ae73a20.png" data-rawwidth="530" data-rawheight="121"&gt;&lt;p&gt;摘自论文：  Minimal Gated Unit for
Recurrent Neural Networks &lt;/p&gt;&lt;p&gt;第一个方程f即遗忘门， 第二方程如果你对比先前的RNN会发现它是一样的结构， 只是让遗忘门f来控制每个神经元放多少之前信息出去（改变其它神经元的状态）， 第三个方程描述“惯性” ，即最终每个神经元保持多少之前的值，更新多少。 &lt;/p&gt;&lt;p&gt;这个结构你理解了就理解了记忆体RNN的精髓。 &lt;/p&gt;&lt;p&gt;好了是时候看一下google 翻译大法是怎么玩的， 首先，翻译是沟通两个不同的语言， 而你要这个沟通的本质是因为它们所表达的事物是相同的， 我们自己的大脑做翻译的时候，也是根据它们所表达的概念相同比如苹果-vs-apple来沟通两个语言的。如果汉语是输入，英语是输出，神经网络事实上做的是这样一件事：&lt;/p&gt;&lt;p&gt;Encoding： 用一个LSTM把汉语变成神经代码&lt;/p&gt;&lt;p&gt;Decoding：用另一个LSTM把神经代码转化为英文。  &lt;/p&gt;&lt;p&gt;第一个lstm的输出是第二个lstm的输入， 两个网络用大量语料训练好即可。 Google这一次2016寄出的大法， 是在其中加入了attention机制 ，这样google的翻译系统就更接近人脑。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-942aca912c133da5ec8fe74beebb6ec7.png" data-rawwidth="1272" data-rawheight="655"&gt;&lt;p&gt;运用记忆神经网络翻译的核心优势是我们可以灵活的结合语境，实现句子到句子，段落到段落的过度， 因为记忆特性使得网络可以结合不同时间尺度的信息而并非只抓住个别单词， 这就好像你能够抓住语境而非只是望文生义。也是因为这个RNN有着无穷无尽的应用想象力， 我们将在下一篇继续讲解google翻译以及rnn的各种应用。&lt;/p&gt;&lt;p&gt;参考文献 ： &lt;/p&gt;&lt;p&gt;The unreasonable effective RNN&lt;/p&gt;&lt;p&gt;Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation &lt;/p&gt;&lt;p&gt;Minimal Gated Unit for Recurrent Neural
Networks &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22930328&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Thu, 13 Oct 2016 17:18:03 GMT</pubDate></item><item><title>Algorithm to live by读书笔记 我们该不该自动化我们的决策</title><link>https://zhuanlan.zhihu.com/p/22911801</link><description>&lt;p&gt;作者郭瑞东&lt;/p&gt;&lt;p&gt;Algorithm to live by是一本涉猎很广的书，其中涉及的计算机领域的诸多经典算法，从其目录中就可以看出，第一章optimal stopping讲该何时停止搜索，无论你找的是车位还是梦中情人，而这个问题的本质，即探索和利用之间的权衡，是第二章的主题，我们该花多少资源去注视星空，还是该花精力去产业化已有的技术，是个动态的取舍。接下来作者谈论了排序算法，着重与不同的排序在日常生活中是如何降低争端，促进人类的协作的。之后的两章，谈论了缓存和资源调度这两种操作系统中会用到的算法，以上的几章，谈论的是如何将计算机算法中的思想应用到日常的目标管理和资源调度。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9e78c62396d00b7f60f2e98cb07b9492.jpg" data-rawwidth="294" data-rawheight="450"&gt;&lt;p&gt;从第6章开始，作者谈论了贝叶斯法则，过拟合，relaxation（放松条件），随机化，讲述这四种来自与机器学习的思路，如何指导读者更好的将新信息融合到之前的框架中，如何简化问题，如果说之前讲的是未来的自己和现在自己的权衡，那么这四章讲述的是未知的自己和已知的自己的讲和。&lt;/p&gt;&lt;p&gt;之后的两章都很长，讲述网络算法和博弈算法（例如囚徒悖论的算法实现），这两章讲的是如何自己眼中的自己和社会中呈现的自己的关系和互动。之后的conclusion，提到了computional kindness，即通过自动化的决策和权衡，让算法把我们生活的世界变成一个更美好的地方。&lt;/p&gt;&lt;p&gt;举个例子吧。比如前几天新闻报道的大学新生被骗8000元钱后，悲伤过度猝死的事情。这固然可以说是阶级固化造成了恶果，但更有建设性的方案是想办法避免类似的事情发生，可行的方法是选出那些容易上当受骗的人，给他们发短信提醒，能精准的预测出那些处于社会底层的，骗子最容易得手的人，就能避免类似的事情发生，这就是所谓的computional kindness。&lt;/p&gt;&lt;p&gt;类似的例子还出现在医疗中，如果能通过曾经患有，或者更容易患精神疾病的人的电话语音记录，那么医生可以及时在出病人发病的早期给出干预，那么这些人的生活质量也会相应提高。无论是数字化医疗还是数字化教育，其产品都需要做到computional kindly，即让你的产品通过收集数据来帮助用户做决策，就像一个好的管家那样，不侵入用户的日常生活，却能够以最少的认知成本让用户做出权衡，或者将各种选项的成本和收益给用户列出来，帮助用户做选择，而不是越俎代庖，专制的替用户选择。这也就是所谓的好的产品要回到初心。&lt;/p&gt;&lt;p&gt;读完这本书，熟悉斯坦诺维奇的《超越智商》这本书的读者会想起算法心智这个概念，对于不熟悉的读者，自主心智，算法心智，反省心智类似《思考，快与慢》这本书中讲到的系统一和系统二，见下图&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3ea23033b4e2d8d4dadfe3aba012d7ee.jpg" data-rawwidth="640" data-rawheight="440"&gt;&lt;p&gt;而这本书所讲的，正是如何将一些本来所需要用到反省心智的流程过度到使用算法心智上，这样做的目的不是让我们变得更加像机器，而是将解放出的认知资源用在更加体现人类创造力的领域，我们将用在迷茫该怎么做最好的时间，通过跨界借用已经在计算机上被证实有用的方法来解决，从而让我们能够有更多的时间精力投入到诗歌，艺术等需要理性与感性交融的地方。&lt;/p&gt;&lt;p&gt;阅读这本书，和看任何算法一样，都体现出现代社会的复杂性下所必须的权衡与弹性，最初的问题太难，放松问题的约束条件，或者去找近似解，无法证明一个问题的解一定正确，可以通过随机的验证保证其在可接受的置信度上是正确的，这些都体现了变通。问题的解决不意味着问题的终结，而是意味着暂时这个问题在可接受的时间范围内有了一个还可以接受的解。如同我们在黑暗中探索，算法带给我们生活的启发永远是gentle的，是不那么极端的。&lt;/p&gt;&lt;p&gt;然而，正如星际迷航中的那首诗中说的，Do Not Go Gentle Into That good Night，人最重要的是有反省心智，是有感情，是不服输的那口气，Old age should burn and rave at close of day; Rage, rage against the dying of the light. 讲算法带来的启发，就一定要提到算法不能够，也不应该觉得我们人生的那部分。那是一片寂寥，也是一双脚链；有时是夏日聒噪的蝉，有时秋日冲天的鹤，所以这里最好的结尾是一句诗，选择诗，是因为《Algorithm to live by》的作者既是一个程序员，也曾受过正规的古典诗歌的训练，正是这样的背景，他才能写出这样具有人文关怀的跨界神作。&lt;/p&gt;&lt;p&gt;Though wise men at their end know dark is right, Because their words had forked no lightning they Do not go gentle into that good night.&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22911801&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Wed, 12 Oct 2016 21:46:17 GMT</pubDate></item><item><title>别让我们的谈话进入到马尔科夫链的状态</title><link>https://zhuanlan.zhihu.com/p/22885220</link><description>作者 郭瑞东&lt;p&gt;武林外传中有一段经典的台词，可谓是情侣吵架的典型，今天的道理就从这段台词说起：&lt;/p&gt;&lt;p&gt;&lt;em&gt;佟湘玉：你无情，你无耻，你无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：那你就不无情，不无耻，不无理取闹吗？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：我哪里无情，哪里无耻，哪里无理取闹？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：你哪里不无情，哪里不无耻，哪里不无理取闹？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：我就算再怎么无情，再怎么无耻，再怎么无理取闹。也不会比你更无情，更无耻，更无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;      白展堂：我会比你无情！？比你残酷！？比你无理取闹！？你才是我见过最无情最残酷最无理取闹的人！&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：哼，我绝对没你无情没你残酷没你无理取闹！ &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：好既然你说我无情我残酷我无理取闹那我就给你无情无耻无理取闹看看。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     佟湘玉：哼，还说你不无情不残酷不无理取闹终于展现自己无情无耻无理取闹了吧。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;     白展堂：我即使无情即使无耻即使无理取闹，也是被你无情无耻无理取闹给逼出来的！&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;    佟湘玉：就你无情无耻无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;注意到了吗,这段台词的每一句，都是对上一句的直接回应，参与对话的两个人，好像只有5秒钟的记忆，只记得住对方说的上一句话，这样的对话，是Stateless的，是符合马尔科夫链的。&lt;/p&gt;&lt;p&gt; 马尔科夫链的确在自然语言处理中有广泛的应用，但我们人类的语言，真的要陷入到这样没有背景，没有状态的简化情况吗。人间的事有太多复杂之处，高效的交流需要参与者共同用其记忆和修养构建一个平台，若是对话只是如上面的例子那样，那么我们就不可能做到对事不对人，对事需要我们明白一件事的前因后果，对人却只需要给对方扣上个帽子。&lt;/p&gt;&lt;p&gt;网上的很多对话，正是由于其匿名性，人们无法观察到对话的人平常的语气，说话的风格，容易退化成为人参公鸡，而所有人身攻击的对话，都有着类似马尔科夫链的性质。这里不想具体举例子，不是不能，是不愿意。举例子就会被看成是对人，而非对事。要想做到对事，那就要说的足够有概括性，这里举红楼梦中宝玉论述文死谏武死战的句子，在36回，宝玉说道：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“那武将不过仗血气之勇，疏谋少略，他自己无能，送了性命，这难道也是不得已！那文官更不可比武官了，他念两句书横在心里，若朝廷少有疵瑕，他就胡谈乱劝，只顾他邀忠烈之名，浊气一涌，即时拚死，这难道也是不得已”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;这段话中想批判的人，在我们现代生活中并不少见，只是当代这些键盘侠们不会面临生死的考验了。要知道一句“不得已”，遮盖了多少思考的缺失，多少人其实在给出机器人一般的机械化的反应，如上文中说的文官，没有考虑到现实的复杂性，只是教条的套用书中的句子。又有多少人，在做事的时候只是凭着热情，没有想清楚一步步该怎么做，做事情中遇到的不确定性该怎么解决。出了问题，拿一句尽力了推脱，这种短视的行为，也值得我们反省。&lt;/p&gt;&lt;p&gt;美国的大选今年吸引了太多人的关注，你应该有自己的观点，但遇到和你观点相反的人，你也不能为此红脸。为什么在不熟的人之间不能谈政治宗教之类的话题，是因为这类话题要想产生有效率有意义的谈话，需要交谈者知根知底，需要彼此互相尊重，否则很容易就退化成人身攻击。明白了很低效率的多谈话其实是一个马尔科夫链，你就应该跳出来，拒绝参加这些低效率的谈话。比如你支持川普，别人说你怎么会喜欢这个疯子，你也要笑一笑，而不去争辩，这时的言语只会带来误解。&lt;/p&gt;&lt;p&gt;不止是男女朋友之间的争吵，父母教育孩子的时候，也会不自觉的陷入马尔科夫链式的对话。这样的对话，只会带来无用的重复和孩子的逆反心理。那么该怎么办了？我们先回到一开篇武林外传的那个例子，看看这段对话是怎么结束的。&lt;/p&gt;&lt;p&gt;&lt;em&gt; （字幕，一夜过去了）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　【后院，昼】&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　（两人趴在磨上嗓子沙哑还在继续吵）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　白展堂：你才无情，你才无耻，你才无理取闹。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;　　佟湘玉：你再这么无情，再这么无耻，再这么无理取闹（鸡叫了，才意识到天都亮了）——也该洗洗睡了。你好好干活，不许偷懒啊。我给你煎两个鸡蛋去。吵了一个晚上，你不饿我还饿呢！（欲进厨房）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;这里之所以这段持续了一晚上的对话能够结束，是因为参与者记起了其本来的身份。这暗示我们要想让我们的对话不只是无用的重复，就要时常提醒一下参与者是谁。比如情侣吵架后，和解往往是由于想起了最初为什么要在一起。&lt;/p&gt;&lt;p&gt;总结一下，这篇小文想说我们应该怎样高效的对话，方法是识别出那些低效的重复的交谈模式，即有着马尔科夫链性质的stateless的交谈，从而有意识的去避免和点头之交发生这类谈话，以及通过唤起背景信息，来让熟人之间的交流跳出这种交流模式。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22885220&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Tue, 11 Oct 2016 19:30:50 GMT</pubDate></item><item><title>还不了解复杂性科学--看过来，这里的书值得你拥有</title><link>https://zhuanlan.zhihu.com/p/22862988</link><description>&lt;p&gt;爱因斯坦说过：“物理学家的无上考验在于达到那些普适的基本规律，再从它演绎出宇宙。”这是还原论的宣言“但是在每一层次中总有新的而且激动人心的有效普遍原则，却并不能由更加基础的理论自然而然地推导而来。”（Kadanoff）这是复杂科学对还原论的批判。&lt;/p&gt;&lt;p&gt; 总的来说，当今物理学的前沿主要是三个方向。一个是宏观方向，如宇宙学、广义相对论，认识宇宙范围内物质的演化和发展。一个是微观方向，从原子物理学向下，研究基本粒子，揭示微观世界中物质的结构和相互作用。而这两者其实又是相互贯通的，其核心是还原和统一。第三个方向则是为了理解物质怎样组织成愈来愈复杂的高级运动形态。&lt;/p&gt;&lt;p&gt;而这第三个方向又可以归到复杂性科学，它以数学和物理学中发展处的严谨方法为手段，以广泛的学科交叉探究一系列贯穿于各个学科的基本问题，例如，部分是如何构成具有涌现性质的整体，有序的组织又是如何从无序的自由个体中产生的。其思想可以体现在统计力学和热力学的联系，可以延伸到神经元到脑、个人到社会等等。&lt;/p&gt;&lt;p&gt;引一段凝固物理学大师P.W.Anderson（1972）对还原论的看法，物理学家，特别是理论物理学家，习惯于所谓还原论的思维方法：将复杂还原为简单，然后再从简单重构复杂...“将万事万物还原成简单的基本规律的能力，并不蕴含着从这些规律重建宇宙的能力……当面对尺度与复杂性的双重困难时重建论的假设就崩溃了。&lt;/p&gt;&lt;p&gt;其结果是，大量基本粒子的复杂聚集体的行为并不能依据少数粒子的性质作简单外推就能得到理解。取而代之的是，在每一复杂性的发展层次中呈现了全新的性质，从而我认为要理解这些新行为所需要作的研究，就其基础性而言，与其他相比也还不逊色。”如此，每一不同的聚集层次，都会展现全新的性质，这些性质已超出组成粒子的物理学的领域，可以被称作层展性质(emergence properties)。&lt;/p&gt;&lt;p&gt;所以，第三个方向则是为了理解物质怎样组织成愈来愈复杂的高级运动形态，以凝聚态物理学为代表。而这第三个方向在某种程度上又可以归到复杂性科学。它以数学和物理学中发展处的严谨方法为手段，以广泛的学科交叉探究一系列贯穿于各个学科的基本问题。&lt;/p&gt;&lt;p&gt;例如，部分是如何构成具有涌现性质的整体，有序的组织又是如何从无序的自由个体中产生的。复杂性科学的关键词如：非线性动力学(non-linear dynamics)、自组织(self-organization)、临界性( criticality)、非平衡态统计力学(nonequilibrium)、元胞自动机(cellular automaton)、网络(networks)、混沌(chaotic behaviour)、分形(fractal)、幂律(power law)……其整体大于部分之和的非线性思想，可以体现在统计力学和热力学的联系，可以延伸到神经元到脑、个人到社会等等。&lt;/p&gt;&lt;p&gt; 另一方面，尽管现在对自组织临界性，复杂系统中标度关系的产生和复杂网络等有了一定了解，但还没有一个统一的关于复杂现象的理论，有些地方还缺乏严格定义的概念和合适的数学语言。“一些科学家将寻求并发展新的合作；这些群体会从各个科学领域吸收成员，并有高效的电脑的协助；在接下来的这半个世纪中，这种新的工作方式将会大大促进在处理生物和社会的复杂性问题上所能取得的成就。”（Weaver，1948）可60多年过去了，我们仍为着同样的理由而充满信心。复杂性理论面对的是大自然和人类社会从低级到高级的深刻奥秘，其中充满着机遇和挑战，但我相信随着人们探索的深入，这种奥秘一定会被我们揭示出来。&lt;/p&gt;&lt;p&gt;这里推荐几本比较有代表性的综合性的科普书作为入门读物，抛砖引玉。当然，科普书的作用有限，更重要的还是能够帮助我们拓宽视野，了解复杂性科学可以研究的问题和丰富的可能性，或许能给大家在某些方面带来一点启发。形成一种复杂性思维去看待问题的视角，在面对这个充满着复杂性的世界的时候带有一种敬畏之情，从而能够在看待一些事情的时候保持审慎的态度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;《复杂》梅拉妮·&lt;/b&gt;&lt;b&gt;米歇尔&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果你之前对复杂性科学还没有太多了解，那这本书可以成为你复杂性科学的第一本书。&lt;/p&gt;&lt;p&gt;蚁群在没有中央控制的情况下为何会表现出如此精密的复杂行为？&lt;/p&gt;&lt;p&gt;数以亿计的神经元是如何产生出像意识这样极度复杂的事物？&lt;/p&gt;&lt;p&gt;是什么在引导免疫系统、互联网、全球经济和人类基因组等自组织结构？&lt;/p&gt;&lt;p&gt;理解复杂系统需要有全新的方法，需要超越传统的科学还原论，并重新划定学科的疆域。&lt;/p&gt;&lt;p&gt;借助于圣塔菲研究所的工作经历和交叉学科方法，复杂系统的前沿科学家米歇尔以清晰的思路介绍了复杂系统的研究，横跨生物、技术和社会学等领域，并探寻复杂系统的普遍规律，探讨了复杂性与进化、人工智能、计算、遗传、信息处理、代谢比例、网络科学等领域的关系。&lt;/p&gt;&lt;p&gt;本书选题广泛，兼具学术著作的严谨与科普作品的浅近。结构清晰，围绕着各类复杂系统的组成，运行机制，进化方向组织内容。本书的翻译也属于上乘之作，读来感觉译者基本功扎实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;《Complexity_A Very Short Introduction》&lt;/b&gt;&lt;/p&gt;&lt;p&gt;可能很多人已经看过第一本了，那么可以看14年9月最新出版的牛津通识读本。该系列图书一般是请该领域的大牛编著，权威可靠，已经出了几百本不同主题的了。&lt;/p&gt;&lt;p&gt;本书作者John Holland是复杂理论和非线性科学的先驱，遗传算法之父。本书介绍了复杂性科学的一些基本概念和核心架构，如complex physical system(CPS)、complex adaptive system(CAS)，描述了复杂系统的特征如涌现性质、自组织行为、混沌行为、胖尾分布、适应性行为。&lt;/p&gt;&lt;p&gt;正如作者在写完这本书后意识到，将一些概念以最本质的简单的形式表现出来后，会发现一些原先分开的话题间被忽视的联系，希望你读完这本书后也有这样的感觉。而且，这本书真的是Very Short。&lt;/p&gt;&lt;p&gt;&lt;b&gt;复杂性思维&lt;/b&gt;&lt;/p&gt;&lt;p&gt;再推荐一本可能更少人看过的，适合兴趣广泛、有哲学情怀的同学。本书是德国慕尼黑工业大学教授迈因策尔的代表作，14年出版的原书07年第五版的中译本。迈因策尔教授的研究范围遍及数学、物理学、科学哲学，尤其在复杂系统、非线性动力学等领域多有建树。&lt;/p&gt;&lt;p&gt;本书从哲学的高度（在这里窃以为哲学不能指导科学，但可以为科学澄清意义），从科学前沿探索与人类心智探险史的结合中，广泛涉猎物理学、生命科学、认知科学、计算机科学、经济学、社会学等诸多方面。&lt;/p&gt;&lt;p&gt;从物理世界的进化到生命世界的进化，从意识的起源到认知科学的兴起，从社会政治系统到社会经济系统的运行，从哲学史到哲学前沿的反思，揭示了不同学科体现出的共同的复杂性特征，阐释了对复杂性的探索将如何引起人们思维方式的深刻变化，引起的世人对共同未来的关怀。&lt;/p&gt;&lt;p&gt;&lt;b&gt;《复杂_诞生于秩序与混沌边缘的科学》&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后推荐一本轻松的读物。这是一本老书了，“类似于纪实小说，介绍了复杂性科学的研究中心圣塔菲研究所建立、发展的情况。&lt;/p&gt;&lt;p&gt; 你会看到那些不同领域的人是怎样由于共同的志趣走到了一起，以及又是如何涌现出诸如遗传算法、人工生命、细胞自动机、正反馈经济系统、动态博弈系统等等新思想的。&lt;/p&gt;&lt;p&gt; 这本书以小说一样的手法介绍了研究所里面个个人物的动人故事，以及他们研究的那些激动人心的成果。这本书的出版可以说给中国的学术界打开了一扇窗子，让我们真正的了解了国外的复杂性科学。有人称这本书是复杂性科学的圣经是不为过的。”&lt;/p&gt;&lt;p&gt;再次重复一遍，读入门的科普书只是为了激发兴趣。所以如果对复杂性科学感兴趣，最关键的是好好学专业知识，才可能真正有所建树。比如可以看看&lt;a href="http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=204154939&amp;amp;idx=1&amp;amp;sn=6cbb8d561919f275a5df499748d542c1&amp;amp;scene=21#wechat_redirect" data-editable="true" data-title="公开课" class=""&gt;公开课&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;这里推荐由国际权威复杂系统研究机构--&lt;strong&gt;圣塔菲研究所(Santa Fe Institute---SFI)&lt;/strong&gt;带来的MOOC&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Introduction to Complexity。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这门课只需要高中数学基础，全英文授课，但英语不难，六级水平就能够应对，课程由complexity： a guided tour 这本书的作者主讲，小编在这里也顺便推荐下这本书，也算是复杂系统领域靠谱的入门书了。&lt;/p&gt;&lt;p&gt;你只需登录网站&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.complexityexplorer.org/online-courses/19-introduction-to-complexity-fall-2014" data-editable="true" data-title="Complexity Explorer"&gt;Complexity Explorer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;无需注册，无需翻墙。就可以下载带字幕的课程视频，视频每个长度约为10分钟，适合你利用碎片时间学习，课程后有对应的习题，可以测试你对课程内容的了解，你还可以下载课程讲义及相关论文做进一步学习。&lt;/p&gt;&lt;p&gt;整个课程分为11个单元，涵盖了&lt;strong&gt;复杂习题，混沌，分形，细胞自动机，遗传算法，自组织模型，流网络&lt;/strong&gt;的基本概念与简单应用。课程中不仅包含了理论的介绍，也包含了使用模拟程序NetLogo所做的实时模型展示，是一门很实用的复杂系统的入门课程。通过学习课程，你能够全面的知晓复杂科学的框架，为之后的学习和探索打下基础，还能学会使用Netlogo这种对于复杂系统很适用的建模工具。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22862988&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Mon, 10 Oct 2016 20:49:51 GMT</pubDate></item><item><title>复杂系统入门学习资源</title><link>https://zhuanlan.zhihu.com/p/22839875</link><description>&lt;em&gt;Peter&lt;/em&gt;混沌巡洋舰&lt;p&gt;课程时间： 连续讲座98 分钟&lt;/p&gt;&lt;p&gt;课程难度： 本科课程，中英文字幕翻译&lt;/p&gt;&lt;p&gt;课程讲师： Robert Sapolsk 美国科学家、作家，现任斯坦福大学生物学、神经学和神经外科教授&lt;/p&gt;&lt;p&gt;课程介绍： 本课程是 系列课程“从生物学看人类行为中”的一讲。该节课程涉及遗传学，神经科学，细胞学，数学等领域。该课程从还原主义（reductism）的历史渊源讲起，带你认识这个术语代表的意义，通过神经科学中的“祖母细胞”，即该神经元细胞只针对你祖母的照片才会被激活来说明还原主义发展到极致后产生的无法解释的现象，进而过渡到混沌的概念，介绍了分形，蝴蝶效应，并对比了混沌科学及还原论在看问题和做研究方面的不同视角和看法。通过学习该课程，你可以弄懂基础概念，领略大师风采。课程的老师留着长胡子，语言风趣，很有魅力的。&lt;/p&gt;&lt;p&gt;温馨提示：这个教授讲课语速快，信息量大，全是干货，讲话旁征博引，推荐看至少俩遍。这一套课程也是很不错的，推荐下。&lt;/p&gt;&lt;p&gt;课程链接：&lt;a class="" data-title="斯坦福大学公开课:从生物学看人类行为" data-editable="true" href="http://v.youku.com/v_show/id_XNTE2OTg4MzUy.html"&gt;斯坦福大学公开课:从生物学看人类行为&lt;/a&gt;&lt;/p&gt;&lt;p&gt;纸上得来终觉浅，要认识一门学科，不止需要了解概念，还需要亲自动手，get your hand dirty。 &lt;/p&gt;&lt;p&gt;所以之后推荐的书，以python为基础，演示了多种复杂系统的模型，让在计算机诞生之前难以验证的理论得以模拟，并逐步建立起复杂演绎基础之上的新认知模式。Python语言简单易懂，但书中的很多代码、练习有时间还得需要仔细研究实践。本书内容短小，但是信息量很大，关键看你是走马观花的读，还是一行行代码地进行实践了，收获是不一样的。 &lt;/p&gt;&lt;p&gt;复杂性科学涵盖了各种主题。这些主题之间相互关联，但需要花费不少时间才能搞清楚这些联系。为了帮助读者看到全景，这本书阅读列表，这些都来自于该领域最流行的研究成果。阅读列表以及关于如何使用它的建议在附录B中。这本书提供了一系列练习；很多练习都要求读者重新实现一些开创性实验并对其进行扩展。复杂性吸引人的一个地方在于我们可以通过适当的编程技能与数学知识接触研究前沿。 &lt;/p&gt;&lt;p&gt;这本书的内容覆盖：小世界图，无标度网络，细胞自动机，生命游戏，分形，自组织临界性，基于主体的模型(agent based model) 及几个现实中的案例分析。是复杂性研究入门参考好书。&lt;/p&gt;&lt;p&gt;本书的翻译版已有，原版可以在网上轻易下载。推荐看原版。另外本书还可以用作Python编程与算法的大学中级课程教材。既是你对python和算法一无所知，其前三章的内容也可以让你能够接着看下去。 &lt;/p&gt;&lt;p&gt;顺便推荐下作者的另外一本书&lt;/p&gt;&lt;p&gt;贝叶斯思维：统计建模的Python学习法&lt;/p&gt;&lt;p&gt;同一个作者的作品，同样的方式，生活中的案例+代码实现+分析，同样是根据作者在美国大学讲授相关课程的讲义编撰而成的。 &lt;/p&gt;&lt;p&gt;贝叶斯思维的威力可以帮助你在生活的各个方面获得清晰的思维，参考 &lt;a class="" data-title="玩转贝叶斯分析" data-editable="true" href="http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381623&amp;amp;idx=1&amp;amp;sn=079563ccdd30186de04e4826b5b37d18&amp;amp;scene=21#wechat_redirect"&gt;玩转贝叶斯分析&lt;/a&gt;&lt;a class="" data-title="贝叶斯理论在医学数据分析中的应用" data-editable="true" href="http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381637&amp;amp;idx=1&amp;amp;sn=32c9a72e3835b68ac67b8d4c211535ed&amp;amp;scene=21#wechat_redirect"&gt;贝叶斯理论在医学数据分析中的应用&lt;/a&gt; 举书中的例子 战争环境下（二战德军坦克问题），法律问题上（肾肿瘤的假设验证），体育博彩领域（棕熊队和加人队NFL比赛问题），通过阅读，作者潜移默化的帮助读者形成了建模决策的方法论，建模误差和数值误差怎么取舍，怎样为具体问题建立数学模型，如何抓住问题中的主要矛盾（模型中的关键参数），再一步一步的优化或者验证模型的有效性或者局限性。&lt;/p&gt;&lt;p&gt;本文首发于微信公众号混沌巡洋舰（chaoscruiser）。&lt;/p&gt;&lt;p&gt;商业转载请联系作者，非商业转载请注明出处。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22839875&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sun, 09 Oct 2016 19:00:10 GMT</pubDate></item><item><title>一个来自武林的类比 论文故事-深度学习的几个猜想，关于文化与认知</title><link>https://zhuanlan.zhihu.com/p/22819277</link><description>Yoshua Bengio的这篇”Evolving Culture vs Local Minima“ 是一篇很容易被人误解的文章，其中跨界的思考固然引人入胜，但更值得学习的是其文章中表现出的谦逊，文章的开头第一句话是：We propose a theory that relates difficulty of learn-ing in deep architectures to culture and language.    这里的theory很容易让人们以为文中的理论是坚不可摧的，然而，这篇文章的关键字是 Hypothesis. 猜想，这其实是一切理论的本质，在一个理论还没有被证实之前，其不过是等待验证的猜想，有了证据，一项猜想的可行度就增一分。这一点是值得被铭记的，每一个跨界思考，产生的都只会是猜想。&lt;p&gt;        接着是说说这篇文章的conclusion。很多的文章的conclusion，只是在总结自己做了什么，而这篇文章主要在写还需要做什么，才能证实书中的猜想，这里体现了好的科学理论的另一个特点，可证伪性，理论是廉价的，证实或者证伪一项理论却一定会是艰苦的。我们接触各种跨界的理论，只是为了让我们在日常生活的观察中，能够将某些看似平常的事用来证伪或证佐证我们之前接触的猜想，从而获得属于自己的智慧。&lt;/p&gt;&lt;p&gt;        接着说说该怎么做科普，这篇文章的讲法，可以是很专业的去一段一段的去解读，但我觉得科普的作用相当于“师傅引进门”的过程。科普只负责有趣，只负责吸引人去阅读原文，只要其没有明显的科学性的错误，尽可以异想天开。所以这里我的解读是一次冒险，可能的误解包括“量子佛学”这种可笑的chimera。所以在开始解读之前，我需要再重复一遍，好的科普只负责引起读者的好奇心，吸引他去看原文。&lt;/p&gt;&lt;p&gt;       这篇文章中的第一个要点，是local minimum，既然是要来自武侠的类比，那么看官要的类比来了，神雕侠侣的40回，一群跳梁小丑在东施效颦的搞“华山论剑”，被杨过的长啸赶走，但金庸很快就引入了九阳真经，这里所谓的山外有山，就是local minimum，你以为你已经达到了“武学”的顶峰，但你所到的不过是一个局部的最优解。        &lt;/p&gt;&lt;p&gt;        接着说书中的另一个概念，high abstraction，We call high-level abstraction the kind of concept or feature that could be computed efficiently only through a deep structure in the brain .这里想举的例子在武侠中也有很多，最典型的是张三丰教无忌太极拳，问他还有几招没有忘掉，这里背后的道理就是这里需要的学习必须出现在更高的抽象层次上，如同一个只能看到直线，直角等视觉元素的人，是肯定无法欣赏油画的，无忌学到的太极拳，是无法用一招一式的组合来涵盖的，这样的具有高抽象度的概念，是难以学习的，这也是这篇文章的核心观点和论述主线。        &lt;/p&gt;&lt;p&gt;        A single human learner is unlikely to dis-cover high-level abstractions by chance because these are represented by a deep sub-network in the brain.这篇文章接着得出的第一个结论，其实不令人意外，既然每个人都会不可避免的面对局部最优的困扰，那么就如同没有免费的午餐一样，也不会有轻松的顿悟，所有的巧妙的总结或是比喻，都可以看成是一种high-level abstraction，无论是苯环的结构，还是梦中吟诗，其背后都是长时间的思考与观察。&lt;/p&gt;&lt;p&gt;        这里作者接下来说道，这个苦难只对单个人是这样的，也不排除某些高级的抽象概念被进化写进了我们的基因，使我们能轻易明白这些观点。这也将引入接下来的观点。&lt;/p&gt;&lt;p&gt;        要走出局部最优，一个办法是先走一段下坡路，之后才能登上更Labeled Examples as Hints高的山峰。比如杨过失去了拿剑的右手，这对他是一个打击，但若他一生的武功都只是拿着长剑，那么他就无法达到他后来的成就。对于杨过，失去右手，是走出局部最优的第一步。而接下来，是需要有人去给出指导，这里的指导，是一种解释，而解释，用机器学习的观点来看，是hidden variable。这里便会引入这篇文章最关键的假设&lt;strong&gt;Guided Learning Hypothesis. &lt;/strong&gt;A human brain can learn high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions.&lt;/p&gt;&lt;p&gt;        这里对应的是交流思想的重要性，作者列出了几种交换的方式，包括Labeled Examples as Hints，比如将常用的剑招命名，来降低信息的信噪比；Language for Supervised Training，这里可以看成是杨过跟独孤求败隔代学艺的故事，无论是练习的方法，还是要学习的高抽象概念，都是有指导的有例子的。&lt;/p&gt;&lt;p&gt;        接下来的一种通过交流来学习的方式是Learning by Predicting the Linguistic Output of Other Agents，作者觉得例子是科学家通过严谨的论述，完备的研究方法来通过实验证实假说，从而使科学共同体接受某一概念。而这里的对应在武侠世界的例子是杨过在华山顶看到的洪七公和欧阳锋的比武，两人通过对招式的描述，来试图说服对方，而在这个过程中的旁观者杨过从中收益颇多。其实人类社会的进步很多也是类似形式的，如何更有效的让linguistic output产生更多的认知盈余，是评价一个时代，一个国家制度好坏的标准之一。        &lt;/p&gt;&lt;p&gt;        接下来的一种学习是Language to Evoke Training Examples at Will，作者说到，你不必处在危险的环境下，但你可以通过语言来明白危险的含义。就如同张三丰在初学九阳时不懂得后发先至的道理，他只是记住了这个概念，而后在之后的实践中去加深了对这一概念的理解。交流可以让我们将之前视为噪音的例子看成是有用的训练数据，从而在我们交流的带宽很低时能表达很多言外之意。&lt;/p&gt;&lt;p&gt;        接着就到了meme，也就是迷因进化的部分了。迷因，就如同剑招，是一个可以传播的观念，是不断要被copy，不断经过修改的，也不断被重新组合的最小可分割单位。作者说道重组可以使好的迷因组合呈现出比其信息上的父母都要好的特性，这一点放到武林中更好理解，一个人学了两派的剑招，就有一定概率组合形成更强的剑法，不过可不一定总是这样的哦。这引入了这篇文章给出的对于个人学习的固有困难的解决方案 &lt;strong&gt;Memes Divide-and-Conquer Hypothesis.&lt;/strong&gt; Language, individual learning, and the recombination of memes constitute an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of memes, that helps humans build up better high-level internal representations of their world.&lt;/p&gt;&lt;p&gt;        最后，作者在这篇文章中给出了新观点从何而来，两个关三词是随机，模仿和组合。随机，指的是我们要持续性的阅读不同文化，不同领域的研究成果，因为我们很可能如同开篇的在华山论剑的小丑，陷入局部最优而不自知。模仿，是要在多将其他领域的方法用到自己从事的行业，自己观察到的生活中。模仿的目的不是为了生搬硬套，而是产生新的组合，然后在实践中去检验这套新组合。&lt;/p&gt;&lt;p&gt;        风清扬道：“活学活使，只是第一步。要做到出手无招，那才真是踏入了高手的境界。你说‘各招浑成，敌人便无法可破’，这句话还只说对了一小半。不是‘浑成’，而是根本无招。所谓无招，就是学会了更高的抽象层次的概念，就是超越了前人总结的捷径。这是人类智慧的体现，也是每一个想要成功的人都躲不开的一步。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22819277&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sat, 08 Oct 2016 17:45:02 GMT</pubDate></item><item><title>找回生物界的暗物质-读《消失的微生物》</title><link>https://zhuanlan.zhihu.com/p/22788250</link><description>&lt;p&gt;《missing microbe》这本书是美国纽约大学微生物学家Martin J. Blaser所著。2016年9月被翻译为《消失的微生物》，由赵立平教授作序，在国内也日渐流行起来。这本书的有些类似几十年前的《寂静的春天》，指出了一个影响深远却并不广为人知的可怕未来。寂静的春天说的是杀虫剂滥用造成的生态灾难，而missing microbe说的则是抗生素滥用造成的微生态灾难。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7eb314af41335ce850d9e70b0477b2a4.jpg" data-rawwidth="268" data-rawheight="379"&gt;&lt;p&gt;关于人体类的微生物，我这里将其称作生物界的暗物质，之所以这么说，是因为微生物对人体各个系统的影响，这些年来虽频见于报道，但大多数人仍对其全景知之甚少。微生物群落，可以看成是人体内的一个隐形器官，其对健康的影响力，不亚于心脏，肠胃等我们熟悉的器官，不同于这些与生俱来的器官，微生物是在我们出身后才获得的，每一个婴儿的微生物菌群，在其生命最初的几年定型，这是值得我们牢记的第一点，微生物群落相对稳定，婴幼儿时期的生活经历对微生物影响巨大。 &lt;/p&gt;&lt;p&gt;关于菌群本身的特点，这里我总结出四个字“&lt;b&gt;多快好省&lt;/b&gt;”。所谓多，首先就是说微生物的分布地点很多，你的皮肤，口腔，胃，肠，还有女性的阴道，都包含着微生物菌群；多还表现在菌群的细胞数目上，拿研究的较多的肠道菌群说，虽然不同的估算差异较大，但肠道中生长的菌群细胞数和人体的细胞个数在同一个数量级上；多的第三个表现是这些肠道菌编码的基因数目巨大，相比于人自身的基因个数，肠道菌群的基因数目数倍于其，编码更多的基因，即说明肠道菌群参与的代谢活动更多，最后，肠道菌群的种类还很多，菌的进化时间长突变快，这导致不同菌之间的差异很大。 &lt;/p&gt;&lt;p&gt;所谓快，是指菌的繁殖很快，在适宜的条件下，一种菌可以在一周内由一千个繁殖到数十亿个，所谓好，是指菌不止是让我们生病的坏家伙，而是能够给我们帮助的伙伴。而最后一点省，说的则是菌的影响更多时是隐藏的，只有当我们缺失某种菌的时候，我们才能够看出其与疾病的影响。 &lt;/p&gt;&lt;p&gt;就如同生态系统的多样性是一个生态系统是否健壮的指标，菌群的多样性也对人体的健康影响深远，菌群与消化系统，免疫系统，神经系统都有着密切的联系。而且这种联系不是非黑即白的，例如作者在这本书中指出的，人类载有幽门杆菌有1万年的历史了。当人逐渐变老，幽门杆菌增加胃溃疡和胃癌的几率。但它对食管有好处，保护人免得胃食管反流病及其造成的其他病和癌症。幽门杆菌消失后，胃癌几率下降，食管癌和哮喘患病几率增加。 &lt;/p&gt;&lt;p&gt;说完了背景知识，再来说这本书的核心观点。首先是抗生素的滥用很危险，尤其在儿童上。这里作者举的是美国的数据，而中国抗生素的滥用则更为严重，许多人小感冒都会去医院打包含抗生素的点滴。而抗生素滥用的后果，从个体上说是广谱抗生素降低了人体内的菌群多样性，使你更容易成为超重，糖尿病等现代病的受害者，从群体上说是增加了超级抗药细菌出现的几率，如同经常叫狼来了的孩子，当我们滥用抗生素，更大的进化压力也在加速抗药菌的出现。其次是在工厂化的动物养殖线上，低剂量的抗生素被用来让动物更快的增加体重，这些最终进入我们餐桌的动物体内就包含着抗生素。 &lt;/p&gt;&lt;p&gt;针对这些问题，在个人层面，作者给出的建议包括&lt;/p&gt;&lt;p&gt;1，尽量少用抗生素&lt;/p&gt;&lt;p&gt;2，尽量少用消毒水洗手&lt;/p&gt;&lt;p&gt;3 尝试使用排泄物微生物移植，弥补失去的微生物（主要在肠道）.&lt;/p&gt;&lt;p&gt;在政府层面，作者给出的建议是，&lt;/p&gt;&lt;p&gt;1 禁止饲养场使用抗生素饲喂牲畜&lt;/p&gt;&lt;p&gt;2， 开发诊断方法，辨别是病毒还是细菌感染&lt;/p&gt;&lt;p&gt;3，研发针对具体细菌的抗生素，而不是使用广谱抗生素。 &lt;/p&gt;&lt;p&gt;这本书中还有很多研究，这里就不一一列举。需要指出的是，作者不是反对使用抗生素，当危及生命的疾病时，抗生素是必须的，作者提倡的是我们对我们身体内的微生物菌群有更全面的了解，更审慎的使用抗生素。记住多块好省这四个字，当下次医生给你建议打抗生素时，明白抗生素不是仙丹，这篇文章的作用也达到了。&lt;/p&gt;&lt;p&gt;本文首发于微信公众号混沌巡洋舰（chaoscruiser）。&lt;/p&gt;&lt;p&gt;商业转载请联系作者，非商业转载请注明出处。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22788250&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Thu, 06 Oct 2016 21:06:12 GMT</pubDate></item><item><title>推荐一部纪录片 BBC 算法现代生活的秘密规律</title><link>https://zhuanlan.zhihu.com/p/22766591</link><description>&lt;p&gt;“算法（Algorithm）是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。”看过了维基百科的解释，你是不是依旧云里雾里，今天给大家推荐的是15年BBC的纪录片。片长1小时，可以在acFun上在线观看。搜索BBC 算法就好了。&lt;/p&gt;&lt;p&gt;在这个时代，我们无时无刻不在与算法打交道，从最近红透半边天的AlphaGo，到手机拍照，再到交通信号灯……可以说，只要有计算机的地方，就有算法。&lt;strong&gt;算法，在引导，甚至支配着我们的行为&lt;/strong&gt;&lt;strong&gt;。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本片深入浅出，不涉及具体的算式、代码，&lt;strong&gt;即使没有数学基础看着也不会吃力&lt;/strong&gt;。而你就算是对算法有所了解的，也能从这部纪录片中温故而知新。这部纪录片把什么是算法、人脸识别算法、最大公约数算法、冒泡排序、归并排序，page rank算法等的基本原理，进行了清晰的阐释。&lt;/p&gt;&lt;p&gt;之后由旅行商（TSP）问题，引出了启发式算法，启发式算法用来解决那些目前无法给出最优解的问题，这类算法可以在较短的时间内给出相对好的解决方案，但其给出的答案并不总是最好的。&lt;/p&gt;&lt;p&gt;本片还通过蜂群是如何解决TSP问题的，介绍了自然界中的算法。遗传算法，蚁群算法，包括最近热的不能再热的深度学习，都是我们从自然界中偷师学来的算法。随着我们对自然界的了解的深入，未来会涌现出更多更好这类的仿生算法。&lt;/p&gt;&lt;p&gt;机器学习是最近很火的话题，本片也对此有所提及，机器学习让算法可以被程序自动生成的，从而极大的扩展了算法的运用范围。本片结尾处以一个全自动的电商仓库举例，形象的展示了算法对现代生活的影响。更多的内容就不剧透了。&lt;/p&gt;&lt;p&gt;下面是这部片中的几幅图，各位能否猜出这是些什么算法了？&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.acfun.tv/v/ac2741983" data-editable="true" data-title="点击观看这部纪录片"&gt;点击观看这部纪录片&lt;/a&gt;&lt;/p&gt;&lt;p&gt;更多阅读&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22455079" data-editable="true" data-title="玩转贝叶斯分析"&gt;玩转贝叶斯分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22097796" data-editable="true" data-title="说说随机森林"&gt;说说随机森林&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21549488" data-editable="true" data-title="当牛逼顿遇到李嘉图---物理模型能否去薅经济学"&gt;当牛逼顿遇到李嘉图---物理模型能否去薅经济学&lt;/a&gt;&lt;/p&gt;&lt;p&gt;本文首发于微信公众号混沌巡洋舰（chaoscruiser）。&lt;/p&gt;&lt;p&gt;商业转载请联系作者，非商业转载请注明出处。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22766591&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Wed, 05 Oct 2016 09:24:52 GMT</pubDate></item><item><title>数据科学是如何帮我们发现心理问题的</title><link>https://zhuanlan.zhihu.com/p/22735631</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cc8e20107fb6986290a4883f25d54293_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;心理问题的诊断， 一直都是难度很大的问题， 我们培养专门的心理医生， 但是专业的心理医生永远供不应求。 大量得不到顾及的心理问题， 不仅给个人和家庭带来巨大的损失， 而且给企业和社会带来巨大的负担。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-eb7e73a7aa705e52c83a71920159c37d.png" data-rawwidth="1066" data-rawheight="568"&gt;&lt;b&gt;图： 大量不得及时诊断的抑郁情况给社会造成巨大损失&lt;/b&gt;&lt;/p&gt;&lt;p&gt;心理问题和各种身体问题一样， 尽早发现永远比晚期发现好，但是由于我们往往很难发现自身的心理问题， 而且更很少有人在问题不严重的时候有去看心理医生的习惯。&lt;/p&gt;&lt;p&gt;这时候AI可以帮助我们。 一般你发现不了的东西， 机器往往可以较早的发现insights。随着手机的普及和智能硬件的推广，我们获取各种人类行为和健康的数据越来越多， 比如手环可以全面记录我们的心跳，手机可以记录我们的位置， 我们的运动， 我们的声音，我们社交活动的频率等。 
这些数据可能为我们提供各种心理问题的早期检测。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-45b4b6b6bcb89b038d6071fb2978a6c1.png" data-rawwidth="1031" data-rawheight="574"&gt;&lt;b&gt;图: 抑郁引起一系列可观测的行为变化&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 那么，
我就来找几种有趣的心理疾病机器学习诊断方法，以抛砖引玉。 第一个例子谈谈心率测量， 大家知道的最简单的生理测量是心跳吧，通过心跳我们不仅可以指导身体健康状况还可以测量心理压力。 
我们看似规律平稳的心跳， 其实包含着众多信息， 这其中重要的就是心跳变化率HRV（heart rate variability）， 大家以为心跳是平稳周期性的，其实不然， 如果你精细的看心跳每个时刻到每个时刻的变化， 你会发现这里面在周期之外包含丰富的变化。 
HRV通常认为受人类非意识控制神经系统的影响， 因而是一个认识神经系统的通道。 我们在紧张， 放松，
工作， 睡眠时候HRV都会有变化。 科学家设计了一组实验， 实验人员让每个被测者（35名IT从业者）带上一个胸带，睡觉时候胸带可以记录被测者在睡觉时候的状态，并连续记录4个月时间， 并回答标准问卷系统以确证心理状态。实验者发现了睡眠时HRV和工作时压力的相关系， 提取长短期的心跳变化率特征和频域特征， 并用一个简单的logistic 模型测量了这个指数。最终测试可以达到59%准确率（低度，中度，高度压力分类任务，如果只分类高压和低压则可达66%，如果在更加可控的人群里， 比如应考压力下的学生，则可达90%） 
。 HRV可以反应一个人长期的承受压力指数， 虽然我们的大脑都有自愈能力， 但是长期连续的压力会让这种自愈无法承担， 而造成多钟心理疾病如抑郁症的潜在温床。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0a0c354ab9cacbaa60aaa5961a4c7bbd.png" data-rawwidth="524" data-rawheight="387"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a552ee1ec75da8fc3505139aae90f64f.png" data-rawwidth="510" data-rawheight="650"&gt;&lt;b&gt;图：被测者被要求汇报自己的心情和行为，以和心电检测的结果联系起来&lt;/b&gt;&lt;/p&gt;&lt;p&gt; 第二种有趣的例子是是社交网络测量抑郁症。什么， 社交网络还能测量抑郁症？首先，一个人在社交网络的行为往往反映其性格， 这已经与神经症的发病率有直接的关系， 
第二，一个人所表现的社交网络的行为变化往往体现他在一段时间的状态变化， 自然语言识别可以较有效的发现这些模式。 
2013的一篇论文讲述了作者通过twitter预测抑郁症的方法。 这件事的历史最早追溯到2012年Park发现人们有时会把自己接受抑郁症诊断和吃药的信息发到社交网络。 之后13年Choudhury分析了产前和产后妇女社交网络行为的变化， 并根据产前的行为极好的预测了产后的行为， 而这样的结果提示了我们这种方法的可行性。 这篇文章的研究方法是抽取476个用户（一半男一半女，171个具有抑郁症的人和305个不具有抑郁症的人）一年（对于患者是发病1年前的数据）的twitter数据并做特征工程。 我们可以从engagement（参与度，各种发信息的频率和时间），社交网络结构（中心性，双向性，自我趋向度）。 
情绪（正，负）和语言风格， 抑郁高频词诊断和预测抑郁症的发生。&lt;/p&gt;&lt;p&gt;一些基本的社交网络特征与抑郁症高度相关， 如下图现实了非抑郁症组（蓝色）和抑郁症组（红色）发状态的密度随一天时间的变化， 明显的看出抑郁症的人更偏向于夜间活跃而正常组白天活跃。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-24b883863b9f5ba2b96a01a489a45572.png" data-rawwidth="548" data-rawheight="370"&gt;&lt;p&gt;科学家利用这组数据总结了几种重要特征在一年时间里的辩护， 比如下图从上到下反映了抑郁症患者粽子社交网络的活动数量是递减的， 负面情绪表达的增加，  第一人称表达的增加和第三人称表达的减少，抑郁高频词的增加。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2ab9e7de50dcdd82bbfcb49d591f8167.png" data-rawwidth="626" data-rawheight="1044"&gt;研究发现抑郁症患者的社交网络有封闭化趋势， 趋向于自我中心，下图描写了用图论和网络中心度来测量抑郁的各项指数， 包括向内， 向外链接的个数， 相互度， 自我中心度等， 最终可以看到抑郁症患者倾向于建立高度自我中心的小网络， 在其中获取情感支持。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e14f05a5641124abd2d32646c4ec80a1.png" data-rawwidth="663" data-rawheight="284"&gt;最终综合各项指标，社交网络分析可以预测下一段时间抑郁的发病率，正确率高达70%&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b27cab6e3f4174cc635b83dba6954eb0.png" data-rawwidth="615" data-rawheight="260"&gt;第三个例子是利用instagram图片库来分析抑郁症的例子。科学家发现， 正常人和患者在instagam上post的照片有显著的色调区别，抑郁症患者明显对图像色调的偏好趋于清冷，灰白，而这种联系之强可以足够一种预测性的方法。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-850ca6748c6a62420062bd199cfafc84.png" data-rawwidth="329" data-rawheight="431"&gt;&lt;b&gt;图： 正常和抑郁对不同色调的偏好， 上图为正常，下图为抑郁。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ef98544aadcbfec702a452030b219f1f.png" data-rawwidth="1134" data-rawheight="665"&gt;&lt;b&gt;图： 科学家可以通过抑郁和非抑郁被测人群喜好使用的图片滤镜判断其类型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;                  第四个例子是利用视线转移来分析自闭症。  同样的图片， 病患和正常人的视线转移是不同的。&lt;/p&gt;

                
第五个例子声音检验， 声音可以帮助我们检验多种心理疾病， 以及心理压力。 最早的通过声音检测压力的实验表情即使在室外手机记录下， 这种检测精度也可以达到76% 之高。声音检测方法几乎可以不通过理解语言本身的含义就知道你大脑的状态， 所以这也是一种比较成熟和稳定的方法。&lt;p&gt;各种各样的通过数据特征检验心理疾病的方法， 为大数据辅助心理健康的工作奠定了基础。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22735631&amp;pixel&amp;useReferer"/&gt;</description><author>许铁-巡洋舰科技</author><pubDate>Sun, 02 Oct 2016 16:29:07 GMT</pubDate></item></channel></rss>