<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>远东轶事 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/yuandong</link><description>“前进！前进！不择手段地前进！”——托马斯·维德</description><lastBuildDate>Sat, 01 Oct 2016 16:16:34 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>一些问题的回答</title><link>https://zhuanlan.zhihu.com/p/21620869</link><description>&lt;p&gt;本次问答应“将门创业”之邀所写，专栏版本内容有扩增。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问一：能否和大家简要介绍一下你的背景，以及你现在在Facebook的工作内容和强度？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;我之前在谷歌无人车组工作了一年三个月，自从2015年1月加入Facebook任人工智能研究所研究员，之前在做围棋，现在在做深度学习和增强学习(reinforcement learning)方面的工作。强度上来说基本上一天十一二个小时吧，一周七天，具体时间上比较自由，可能早上先干一会儿再去上班，或者晚上吃个饭然后继续干到晚上十一二点，或者早晨去健身，周末出去打个球都有可能。睡眠时间一般在七小时左右。&lt;/p&gt;&lt;p&gt;这样长的工作时间当然不是强制的，一方面因为我自己比较喜欢做研究，所以自然而然会比较长；另一方面毕竟年轻无孩，多干点是应该的。工作的内容基本上是看文章想办法写代码，另外还有开会聊天。后者听起来不是正事，但却是与别人沟通的重要一环。&lt;/p&gt;&lt;p&gt;我很少熬夜。熬夜效率非常低，我试过几次，眼睁睁地看着时钟从凌晨一点走到两点，再从两点走到三点，但手上工作还没做多少，伴随着一种非常强烈的挫败感。一般来说，这只适合最后期限（deadline）在第二天早晨的情况，但是如果真是这样，那说明投稿的时间部署还有优化的空间。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问二：如果用3个词来形容你自己的性格，你觉得是什么？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;三个词概括不了一个人的性格的，写下来反而会给大家刻板印象。如果硬要说的话，我是个不会闲下来的人吧。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问三：你曾在一篇文章中提到过，希望奔跑中的自己，不要忘了“梦想”这个词的含义。你的梦想是什么？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;我的梦想是在人工智能领域做出影响世界的工作。这其中包括两方面的工作。在理论上，理解复杂人工智能系统，比方说深度学习的工作原理；在应用上，做出效果更好并且切实可用的人工智能系统。目前在这两方面都有尝试并且也有一些成果，比如说在博士阶段做的对特定条件下非凸问题全局最优解的理论分析，还有最近做的DarkForest围棋系统。前者拿了ICCV2013的马尔奖提名，后者获得了很多国内外的媒体报道，拿了一些比赛名次，并且在开源之后惠及他人。&lt;/p&gt;&lt;p&gt;但说实话，目前的这些工作离实现梦想还有很大差距，需要一点一滴的不懈努力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问四：目前你在知乎的粉丝也有3万多了，在圈内也算小有名气，作为一名科研人员，你觉得出名对你产生了哪些正面和负面的影响？&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为一个科研人员，正面的影响是说话有人愿意听，写的文章也有人欣赏。我本质上比较内向，写文章是自己的事情，目的是整理总结过去的经验，以为将来开辟道路，当然我非常高兴有很多人喜欢我的文章，这也算是影响世界的一种办法了吧（笑），从这点来说，出名是很有意义的。希望我以后能给大家带来更多更高质量的文章。负面的影响就是会有很多人来找，会变得忙一些。但另一方面，一个内向的人多和别人聊聊是非常有好处的。所以总的来说没什么负面影响吧。&lt;/p&gt;&lt;p&gt;一句话，做好自己的事，不要被名声牵着走就好了，管自己是涨粉还是掉粉，第二天太阳还是照常升起来的。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问五：在AlphaGo战胜李世石之后，你觉得除了Google在此获得了具大的曝光外，对整个产业的发展起到了怎样的作用？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;主要还是给大众一个冲击。围棋在国内一直被认为是人类智慧的最后堡垒，是人的智能比机器智能厉害的标杆，现在这个标杆突然间被征服了，多年以来咱们的教科书上写的不对，这个震撼力是不言而喻的。这样就会让普罗大众都来关注人工智能，能起到加速产业发展的作用。&lt;/p&gt;&lt;p&gt;这有好有坏，好的是流进这个领域的钱会变多，需求会增长，工资会变高；但是坏的是大家对我们的期望也变高了。其实如同我之前写的那样，进步没有想像得那么大，很多时候大众看到的只是连成系统的最后一击，这一击非常震撼，但是背后几年甚至十几年的积累大众没有看到，以为会继续连击，结果发现老本都吃完了。&lt;/p&gt;&lt;p&gt;现在就要看我们是不是工作够努力以达成这个期望了，以目前的迭代速度和丰富的资源和工具来看，确实有可能加速发展；但也有可能在基本理论没有突破的情况下试不出什么新货色来，于是发展停滞。在这两种可能中，我个人持乐观主义态度，倾向于前者，当然这个观点是带有偏向性的，因为要是我持悲观态度，那为什么还要在这个领域继续做下去呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;问六：现在深度学习在工业界越来越蔓延，其实现在有名的几家DL的公司，都在探索DL可以为社会创造什么样的价值，多大的价值。你认为DL将对社会产生什么样的价值？&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深度学习（DL）的价值在于它能够免除很多人工设计的麻烦，极大地提高效率。以前花十年手工打造一个为特定场景设计的系统，现在可能花一年就做成通用系统，并且不管是性能还是可维护性都胜过以前手工打造的系统，这个可以说是革命性的。当然目前的问题是DL是黑箱不能解释，在定制和调试方面不如手工打造灵活。不过我相信随着大家的使用，理解会越来越深，更好的理论迟早也会出现，所以从长远上来说不是问题，我相信DL未来的影响会越来越深远，现在仅仅只是个如黎明曙光一般的开始，以后的路还很长。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;问七：技术的竞争很激烈，快速迭代，而且算法门槛越来越低，大家都在刷数据，简单粗暴。你认为作为技术人员，竞争优势在哪里？&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;因为DL的便利性，现在是存在这个问题。大家估计都看过这个笑话，Caffe10块钱安装一次，CNN5块钱一行，RNN8块钱一行，好像深度学习门槛很低很低，高中生都可以试试。其实门槛并没有那么低，DL解决了以前的很多问题（比如说设计特征），但是带来了更多的问题（比如设计网络结构，从训练的结果里看出下一步要怎么调整）。&lt;/p&gt;&lt;p&gt;对于工程人员来说，如何以最快速度学习现有工具，掌握它们的脾气，利用它们解决新出现的问题；再上一步，如何设计灵活便利，效率更高的工具，这些都是要思考的。工具变强之后，人就会自然地思考更难的问题，是一直以来的趋势。而计算机这个行业的好处，就是不管工具内部有多复杂的逻辑，接口做好了用起来都一样方便，这是它之前一直在火，以后也会一直火下去的原因。&lt;/p&gt;&lt;p&gt;对于研究人员来说，虽然是绕着DL做文章，但功夫却在DL之外。遍历各大会议的文章，DL虽是很大的主题，但是在DL上翻的花样都和以前的各领域有关，比如说图模型，增强学习，等等，至于理论分析，则更离不开基本功，矩阵论，微分方程，动力系统，随机矩阵谱估计，张量分解，凸优化等等，现在既然不知道它为什么效果好，那么任何领域都得试一下。在这种情况下，能在各个分支上来回切换，并且迅速找到问题的难点，就是研究员的核心能力了。这种能力往往在求学阶段时，静下心来花很多年的时间积累得到，目前还很难被机器被取代。如果翻我的履历，就会发现我以前不是做深度学习的，后来在谷歌无人车组时自己在业余时间做了下，最后拿到了我现在组研究员的Offer。&lt;/p&gt;&lt;p&gt;另一种重要的，目前难以替代的能力是交流能力。其一是业务交流能力及管理能力，单独自己做是永远赶不上一个训练有素的团队的速度的（AlphaGo就是一例）。为此需要广泛地与同行交流与下属交流，明白什么事是自己需要干的，什么事可以交给别人解决，知道什么时候可以妥协需要妥协，什么时候要坚持原则，以最快最有效率的办法解决问题。&lt;/p&gt;&lt;p&gt;其二是跨领域交流能力。如之前我写的《快速迭代的人工智能》所说，相比其它领域以年为单位来衡量的迭代速度，AI这里完全可以用技术爆炸来形容：每天都会出现值得一读的文章，所有教科书都相对过时，各种结论随时可能被推翻，准确率的冠军往往只能保持几月甚至几天。我相信，以后作为高效率工具，机器学习特别是深度学习肯定要进入其它领域的，而如何通过不同领域间的交流，让它充分发挥作用，这是个难点。这就要求博士能真正做到学识渊博，在自己的领域有深入了解的同时，还要对其它领域有所了解，特别对学科框架有所了解，遇到问题能纲举目张，分析到点子上。随便找一个学电子，金融，机械，化工，材料，土木，生物的同学，能不能和他们聊起来？他们现在可能和我们做的一点关系也没有，但还是需要处理数据，需要数学建模，需要模式识别，需要最优化，需要高效地分配和完成任务。这些都是以后的增长点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;问八：你是如何看待现在大量学术界的人才流向工业界这个现象的？&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;学术界相对来说比较自由，迭代没有那么快，可以做相对长期的课题（比如说一年两年）和相对抽象的课题（比如说建框架，做理论分析），当然资源没有工业界丰富。这里的资源包括计算资源，大量的数据还有有经验的人。工业界这边给钱会比较多，但是要求出活快有实际效果，最好能赚钱。&lt;/p&gt;&lt;p&gt;我们组（Facebook人工智能研究所）目前看起来兼顾学术界和工业界的优点，既有学术界的自由度，又有工业界的资源，是相当不错的，欢迎大家申请。我们这里有以下几类职位。&lt;/p&gt;&lt;p&gt;(1) 研究员（Research Scientist），要求有比较长的相关研究经验（计算机视觉，机器学习，增强学习），并且在自己的研究领域做出有影响力的好成果。主要做基础研究工作。&lt;/p&gt;&lt;p&gt;(2) 研究工程师（Research Engineer），要求代码能力强，对相关方向有一定经验。有相关开源项目更好。&lt;/p&gt;&lt;p&gt;(3) 博士后（Postdoc），与(1)的要求相近但是低些。一般博士毕业应届生只能申请这个职位。&lt;/p&gt;&lt;p&gt;(4) 实习生 (Intern)，为期三个月，一般要求博士在读，已有好工作发表在顶会上，及具备一定的代码能力。&lt;/p&gt;&lt;p&gt;注意(1)(3)(4)都没有文章数量的要求，只看质量。若是看到简历里列出一堆烂会或者低质量的工作，只会是扣分项，有一两篇烂会大家就要考虑一下是不是招，有很多篇烂会那肯定无脑拒了，而反过来，只要有一篇文章在领域里获得了巨大的影响力，那不管是不是中稿都会有对作者有极大的兴趣。总的来说，看一个人的影响力，往往看最好的两三篇文章的质量，看他的招牌成名作，这是以前我导师教我的，也是在美国学术圈里一直看到的评价体系。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问九：目前机器学习方法的成功运用需要依赖于利用大量数据进行算法训练，然而对于不拥有海量数据资源的企业，尤其是初创企业而言门槛很高，这在一定程度上限制了创新的机会。机器学习是否存在减少对数据的依赖的路径？在学术界和工业界最新的实践进展如何？ &lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;减少对数据依赖的办法一定是有的，人类智能就是个现成的例子：一个训练好的模型，能通过几十个或者几百个样本迅速学会新任务。如果以后我们能搞出一个这样的模型来，放在Github上给大家下载，那对初创企业是非常好的帮助。当然，近期内还办不到。&lt;/p&gt;&lt;p&gt;目前减少数据依赖有各种办法，比如说人工设计特征和提炼规则再接以简单的模型训练，各种正则化方法，对模型顶层权值进行fine-tune，绑定权值shared weight, 各种形式的transfer learning，及最近比较流行的建立虚拟世界然后从里面进行数据采样的办法，这些办法各有各的优缺点，不存在万灵药。本质上来说，这是因为我们对DL的机理不清楚，只有模糊的直觉理解，而没有定量理解，所以只好用大量数据把模型硬生生学出来。若是对DL有更深的掌握，那原则上来说能用很少的数据去随意微调已有模型得到新的，将又会是一个突破。对初创公司来说，大数据的限制确实会让人难受，但技术上的问题通过仔细分析，应该是有各种折中办法的，这就要靠各家人才的聪明才智了，基本上是具体问题具体分析，没有什么统一的办法。这是DL研究人员能拿高工资的原因之一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;u&gt;问十：现在越来越多的企业开始进行人工智能专用处理器的研发。例如IBM公司的TrueNorth，高通公司的Zeroth，Google公司的TPU，KnuEdge公司的KnuPath和中国科学院的寒武纪，中星微的NPU等。这些专用处理器在实际应用中效果如何？人工智能专用处理器的细分市场发展上会有什么样的趋势？&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我不是做硬件的所以无法详细回答这个问题。目前通用的GPU已经让训练速度快10多倍了（这数字可能过时），我相信再往下走可能会出现更快的通用DL芯片，和针对特定应用背景的专用DL芯片吧。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21620869&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Tue, 02 Aug 2016 02:01:54 GMT</pubDate></item><item><title>第一次半马感想</title><link>https://zhuanlan.zhihu.com/p/21809552</link><description>&lt;p&gt;原来主办方通过Tracker还是能知道冲线的时间。万幸没有白跑这一次半程马拉松，最后成绩是2小时2分47秒。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b49035403ec3068d05ed396a5ca6c029.png" data-rawwidth="598" data-rawheight="373"&gt;&lt;p&gt;=========================&lt;/p&gt;&lt;p&gt;早上刚跑完旧金山的半程马拉松（21.1公里或13.1迈），用时约2小时1分钟，乘热乎写篇博客（以下一迈等于1.6公里）。&lt;/p&gt;&lt;p&gt;从家里出发去跑步的出发点，事先预约下附近的停车场，然后早上4点35分吃了个面包出门，约5点10分到达，一路走过去，人越来越多，最后到跑点简直是人山人海。整个三番马拉松赛事是早晨5点半开始，运动员们先跑，每隔十分钟再放一波一波的参赛者。周围全都是短打装束，各种装备一应俱全，手机固定在手臂上，腰包上固定各种证件和饮料，还有饮料吸管可用，而相比之下我只穿了条平时锻炼用的短裤，钱包手机和车钥匙放在松松垮垮的裤子口袋里，一晃一晃的，也没有带运动饮料，相比之下业余很多。还好主办方在路上都设有补给点。我在第五波，约6点5分起跑。主席宣布起跑后，整个方队是不动的，要等到前面的人走了，后面的才能慢慢加速。&lt;/p&gt;&lt;p&gt;这次半程马拉松的风景相当不错，先沿着旧金山的海港从Pier 1到Pier 59，然后上金门大桥跑个来回，再一路向南穿过旧金山多山的地形，到达金州公园（Golden State Park）。在金门大桥附近看，细雨霏霏，清晨雾重，将大桥遮去了一半，从高处望下去，长跑的队伍浩浩荡荡，已经有先前跑得快的人从那里回来了。&lt;/p&gt;&lt;p&gt;前面11迈总的来说比较轻松（跑到11迈约7点44分，只用了1小时40分钟左右）。一开始不停地超人，跑得略快。大约在5到6迈时出现了第一个极限，又正是上坡路，腿比较酸，速度慢了下来，但并不难克服。约6迈后上了金门大桥，第一个极限之后跑起来很惬意，金门大桥眨眼就过，当我快下桥看到9迈的标志的时候，有一种“半马也不过如此”的感觉。但后来发现高兴得太早，一过11迈这个坎，第二个极限就来了，虽然心脏还在说“继续跑，没问题”，但肌肉受不了了，迈步和铅一样重，尤其是遇到旧金山市区的上坡路，有一种马上就要大腿抽筋的感觉，我甚至觉得自己坚持不下来。之前一直在超人，但到这个位置，看到别人一个一个地超过自己，什么也干不了。这时候只能对自己说坚持住不能停下来，可以跑得很慢，但是一定要在跑。从11迈到12迈花了12分钟左右，几乎就是快步走的速度。之前训练过两次一小时零几分钟跑7迈（11.2公里），但并没有进行过长达两小时的训练，所以第一次跑起半马就见真章了。&lt;/p&gt;&lt;p&gt;12迈之后稍好一些，但速度仍然上不去，这时候已经进了补给点，喝了口水看到半马/全马的分界线，预示着终点的到来。希望来了，动作快了点。周围的观众越来越多，气氛越来越热烈，我抬着快要断的腿，终于看到了13迈的标志。这对于快跑不动的人来说简直是救命稻草，我憋一口气开始冲刺，超过了之前的几个人，听着旁边的解说员说道“This guy picks a bit speed..."冲过了最后的0.1迈（160米）到达终点，正是8点06分。整个半程马拉松总花时2小时1分钟，对第一次参加的我而言，是非常令人满意的成绩了。&lt;/p&gt;&lt;p&gt;对于长跑的人来说，跑完是很幸福的事情。站着喝水，什么也不用想，什么也不用做，完成了一件大事，真是美好。大家坐在草地上发呆，或者兴奋地交流。回到起始点查成绩，发现随身带的Tracker坏了，只记录了前2.3迈的速度和位置，后面的啥也没有。问了工作人员，他们说Tracker得系在鞋上，而我放在胸前，跑了一阵后被汗水打湿就不起作用了。这是这次美中不足的地方。希望主办方有照片或者视频证明冲线的时间。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/341f964914c49c92e0ffe55dcb383fa0.png" data-rawwidth="851" data-rawheight="1134"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21809552&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 01 Aug 2016 03:41:01 GMT</pubDate></item><item><title>两篇DeepMind ICML的点评</title><link>https://zhuanlan.zhihu.com/p/21432542</link><description>&lt;p&gt;【原文应新智元之邀所写】&lt;/p&gt;&lt;p&gt;&lt;b&gt;点评 Dueling Network Architecture for Deep Reinforcement Learning (ICML Best paper)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DQN系列的方法用的是Reinforcement Learning中的Q-learning, 即给定状态s，给下一步的行动a打分，分数记为Q(s, a)，然后选取分高者作为这一步的最优策略。Q这个函数可以很复杂，特别是当状态由当前图像的像素值直接表示的时候，所以现在流行的方法是用卷积神经网络读取图像s，得到中间特征，然后再加上若干层全相连层去预测Q(s, a)在每个a上的值。而这篇文章的主要贡献很简单，在这个神经网络上稍微改进了一下，在得到中间特征后兵分两路，一路预测估值函数V(s)，另一路预测相对优势函数Advantage function A(s, a)，两个相加才是最终的Q(s, a)。这样做的好处是V(s)和A(s, a)有各自的意义，V(s)是对当前状态的长远判断（Expected Return），而A(s, a)则衡量在当前状态s下，不同行为的相对好坏，一个是远期目标，另一个是近期目标，这就是所谓的Dueling Network Architecture。图2很清楚的显示了这一点，也是本文最有意思的地方。如果状态s1比状态s2总体要好，那么每个Q(s1, a)相对每个Q(s2, a)要高，而需要Q(s, a)的每项都去拟合这种“低频分量”，会在某种程度上费去神经网络的“容量”，不是最优的办法；而将Q(s, a)分解为V(s)及A(s, a)的和就没有这个问题。当然这个只是直觉印象，不一定是真实情况。实验上用了57个Atari Games，算是比较多，也是比较靠谱的，从表1看起来，新方法和老方法相比略好些，相反是各种其它的技巧，如Gradient Clip，和Prioritized Experience Replay，对结果的影响似乎更大。&lt;/p&gt;&lt;p&gt;&lt;b&gt;点评 Continuous Deep Q-Learning with Model-based Acceleration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这篇也是用Advantage Function去做增强学习的工作，但这次对付的是变量连续的行动空间，这样就直接和自动控制及机器人相关。连续空间上的一个大问题是，即使通过训练得到了用深度网络表达的Q(s, a)，但因为a可以取无穷多个值，在通常情况下无法通过穷举a得到最优的行为。对此该文将Q(s, a)先分解成V(s)和A(s, a)的和，然后将A(s, a)建模成一个关于a的二次函数，而建模这个二次函数的方法是通过建模条件均值mu(a|s)和方差P(a|s)进行的，这样可以用解析方法直接得到给定s后a的最优解。注意该文中用x代表状态，u代表行动，而非s和a，这个是控制论中的通用做法。如果大家想一想可能马上发现mu(a|s)其实就是策略函数pi(a|s)（对计算机围棋而言就是走子网络），那为啥他要这么绕？因为这样的话可以用一个模型同时建模pi(a|s), V(s)和Q(s, a)，这个是比较有意思的地方。&lt;/p&gt;&lt;p&gt;有了这个模型之后，该文另一个大的贡献在于用卡尔曼滤波器配以局部线性模型，来加快经验的获取。在增强学习中训练深度网络是比较慢的，因为一开始深度网络的策略很糟糕，在自我模拟中得到的经验完全没有意义，用这些经验训练得到的网络也就不会太好，如此循环往复，需要很久才能走出这个圈。一个办法是用好的经验去训练策略和估值函数，这可以通过专家已有的经验来获取（如围棋），也可以像该文那样，通过简单模型来获取。局部线性模型是用来预测行动后的下一个状态的，而在有了局部线性模型之后，卡尔曼滤波则告诉你为了达成某个状态（如让猎豹达到下一个跑步的姿势），目前需要采取什么样的控制策略，这两者都是经典控制论中常用的方法。最后的测试是在开源的模拟环境中进行的。效果要比以前他们自己的方法，即使用actor-critic模型的Deep Deterministic Policy Gradient (DDPG)，要好些，我很期望看到只用局部线性加卡尔曼滤波后的结果（即经典控制论的baseline），可惜似乎文章中并未给出。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21432542&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 27 Jun 2016 15:16:16 GMT</pubDate></item><item><title>围棋引擎DarkForest开源了</title><link>https://zhuanlan.zhihu.com/p/21381527</link><description>我们的围棋引擎DarkForest开源了。见以下Github链接：&lt;a href="https://github.com/facebookresearch/darkforestGo" class=""&gt;https://github.com/facebookresearch/darkforestGo&lt;/a&gt;，目前主要是围棋的MCTS引擎和训练好的DCNN模型（不加搜索可以直上KGS 3D）及playout模型。另外训练代码会在稍后发布。希望这份代码能给有志于计算机围棋研究和深度学习研究的同仁们一些小小帮助。&lt;p&gt;这个项目从去年五月开始零零碎碎做起，到现在终于公布了内部细节，算来是整整一年了。全心投入的时间大约有四到五个月，其间一波三折，有初始成功的喜悦，拼命工作的漫长，被人甩开的哀伤，对世纪大战的复杂心情和期许，及最后尘埃落地的平静。不管怎么说，我们是中途从视觉研究方向转行，用了别人百分之一甚至千分之一的资源来做这样一项工作，并且能够获得大家的喜欢，认识了很多朋友，已经十分满足了。这次开源后能在短短的一周内收获一千多颗星，一百多次Fork和watch，实在超乎想像。&lt;/p&gt;&lt;p&gt;感谢大家的支持和鼓励！&lt;/p&gt;&lt;p&gt;当然了，第一才有最多的鲜花和掌声，第一之后的人和事，则免不了被批评指摘以及被遗忘的命运。然而即便如此，努力向前总是必要的，因为我们永远也不知道，下一次面对的，是无法战胜的大军，还是无人涉足的宝藏。&lt;/p&gt;&lt;p&gt;能做的，就是再来一次。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/105a99601bf119a6d722847ce1562604.png" data-rawwidth="796" data-rawheight="597"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/212cb8750d87c013f6780b757e1a2904.png" data-rawwidth="790" data-rawheight="578"&gt;&lt;p&gt;---------------------------&lt;/p&gt;&lt;p&gt;PS..用纯C写的原因纯粹偶然：在项目的一开始，我并不知道怎么在Lua里面调用C++的函数，图方便就直接用了C，于是大家看到了现在这个样子。如果我有时间的话，会尽量改成C++11的风格。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21381527&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 20 Jun 2016 08:15:47 GMT</pubDate></item><item><title>快速迭代的人工智能</title><link>https://zhuanlan.zhihu.com/p/21315591</link><description>&lt;p&gt;本来我们这领域步调就快，因为杂志论文的内容滞后，大家都看会议论文，一年两三个顶会，半年一年的研究周期。但因为是双盲评审，就算有别人和自己做得相近，投稿时也是两不知晓，大不了同时中稿同时发文。Arxiv的出现，允许任何人将自己的工作发表在网上，让同行们即时看到，让这一领域的竞争变本加厉。一个例行的功课是每周甚至每天都要刷一下Arxiv看看有没有好文章，更重要的是看看有没有工作和自己目前正在做的重合。每当有一个新想法新思路，必须得马上做完，只要拖一两个月就不再是你的了，因为别人已经捷足先登，直接放篇文章出来把你一闷棍打死，叫也叫不出来。之前的计算机围棋已经血淋淋地展示了这一点，一切在几个月内发生，我们让其它做围棋的同行们郁闷，然后AlphaGo又让我们郁闷，昨天还是众星捧月，今日便少人问津，虽说是世态炎凉，却正是人之常情。&lt;/p&gt;&lt;p&gt;程序和框架的开源也加剧了这一过程，本来重现别人的方法不仅痛苦更是耗时，但现在都是拿来即用，各项成本为零。而想要保持领先优势，隐藏自己的小技巧小把戏那是没有意义的，一年发表的文章多如过江之鲫，谁不开源或者晚开源，谁就被遗忘在历史的角落里了。去年NIPS大家都在谈论ResNet，我们组在ResNet出来的24小时之内写完了程序并且已经开始了在ImageNet上的训练，并且很快确认了它的效果。TensorFlow刚刚开源，组里的骨干就忙到凌晨三点半完成了评测。大凡说自己的工作厉害却不发布程序的，除非像AlphaGo那样动用大量硬件资源的系统，不然自会有人评判，而且速度飞快。有一次有一篇Arxiv报告了CIFAR更好的结果，马上在reddit上被人质疑，然后公开代码，被人发现在测试集上训练的低级错误，只得黯然撤稿。&lt;/p&gt;&lt;p&gt;接锺而来的是开会的味道也发生了变化。开会的原意，是去领略最新进展的，现在去开会则是怀旧，满眼都是半年一年前的老文章，有一种穿越回去的即视感和莫名喜感。海报上写着“我们是最牛的”，观众在一旁看着，笑而不语。大家谈论的往往不是目力所及的演讲或是海报，而是刚刚在Arxiv上冒出来的新作，这个“刚刚”，可能指上周，可能指昨天，也可能指几小时之前，方才还为小小的成果而沾沾自喜，转眼间就如一桶冰水当头浇下，魂不守舍，夜不能寐，食不知味。相比其它方向动辄半年一年的审稿周期，迭代之快，更新之速，史无前例。我有时候在想，以后除了饿醒胖醒春梦醒，还要加个Arxiv醒，梦见自己的神思路被别人做了，或是梦见投稿前几天遭遇当头棒喝，大汗淋漓之下猛然醒来，而后抹一把脸谢天谢地。&lt;/p&gt;&lt;p&gt;在这样的压力下，人的思维方式也发生了改变。目前为止，我们还没有对深度学习有深层次的本质理解，大量的研究思路都是简单粗暴的，在计算资源极大丰富的今天，对于它们质量的主观判断，经常远不及写代码上机实验来得快与准。很多想法不论对错，不论它看起来有多离谱荒谬，先试一下再说，往往蒙得比想得快，动手比动脑快。和我们每天刷手机类似，思维方式的这种转变是好是坏，目前还无从评判。这样的做法，完全可以归入浮躁和急功近利之流，与传统学术做法背道而驰——但与口舌之争相比，这样的方案还在不断地出成果，还在推动着技术的进步，做成更好的系统。我们一边抱怨谷歌机器太多调参太猛，免不了批判如此风气，另一边还在准备着大量GPU雄心勃勃地上战场。不管怎样，可以预计的是，凭一人一纸一笔去战胜难题，令人敬仰却鲜有效率；而让计算机参与推理过程和判断，终会成为每个人生来的必修课。&lt;/p&gt;&lt;p&gt;从这个趋势上来看，能快速利用各种工具达成目标的，能快速阅读及写出代码的，有强大工程能力的，在就业市场上都有巨大优势；若是再有基础扎实，能快速读懂文章，从中获取新知识并举一反三，经验丰富，看文能抓住要点痛点下手，从纷繁想法中修剪思路，那就是老手了；再上一层，有战略眼光，能从万千文章中看出大势远景，巧妙定题，以最小代价获取最大收益，那就是一流高手，能站在前沿火线，直接推动领域的发展。然而要做到这一步，需要在这样的快速迭代中静下心来积累经验，日久方能见效。&lt;/p&gt;&lt;p&gt;人工智能火爆的背后，是多少天才你来我往的战斗，在这个高维空间里攻城拔寨，刺刀见红。虽然累点苦点，却正是这个领域的诱人之处和希望所在。与苦与累相比更可怕的，是舒适安逸时却因技能陈旧而被解职，徘徊天地间，无处可去。我无法想像那种绝望感，所以还是紧追时代的脚步吧，至少我们还能看见技术发展的轨迹，为它出一份力。在这个领域里，只要工具运用得当，思路巧妙，每个人都有成为超人的潜质，一个好算法好框架可服务千万人，且24小时工作，不会出错，不会疲劳，这在以前完全不可想像。正所谓：&lt;/p&gt;&lt;p&gt;灵工巧技汇涓流，奔腾到今浩无垠；&lt;/p&gt;&lt;p&gt;谁料硅砂一捧土，更胜周郎百万兵。&lt;/p&gt;&lt;p&gt;------------------------&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/77ff8774fe59c2383a3a8847260c1f36" data-hash="77ff8774fe59c2383a3a8847260c1f36" class="member_mention" data-editable="true" data-title="@罗珠钦哲" data-tip="p$b$77ff8774fe59c2383a3a8847260c1f36"&gt;@罗珠钦哲&lt;/a&gt; 的评论：前辈，如果可以的话，我希望你能让心静一点，再静一点。我觉得你在失去你写博士生涯总结时的那种状态，面对挑战拿得起放得下的那种状态。现在你有点放不下了。世间的名利和旁人的期望是会让一个原本很出色的人迷失的。这种状态是不利于你的长远发展的。我希望你找回你的 "zone"。&lt;/p&gt;&lt;p&gt;回复：多谢你的关注~这话说得太好了，说到我骨子里去了。理想情况下我希望找一个安静的地方，一张纸一支笔，有一些资源给我调配，花个三年五年时间去静静思考我想要做的问题。然而大势不由人，现在已经不是以前的时代了。一个落户硅谷有家有室的人，在一个快速迭代的公司里干活，还是要照顾一下外部期望的。可以做的，是去平衡短期和长期目标，在高度竞争的环境下，还能给自己留一方净土。出世难入世也难，在入世中出世则更难。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21315591&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Tue, 07 Jun 2016 13:46:05 GMT</pubDate></item><item><title>UEC比赛总结</title><link>https://zhuanlan.zhihu.com/p/20680393</link><description>&lt;p&gt;上周我和朱岩去日本电气通信大学（University of Electro-Communications, UEC)参加了第9届UEC杯计算机围棋比赛，获得了第二名的成绩和与小林光一九段下让3子棋的资格，由我代替DarkForest摆子体会“肉臂”的感觉，开局稍缓，中盘战斗非常有力，但打劫还是有问题，最后在赢棋的情况下认输了，有点可惜。而Zen同样被让3子赢了五目左右。&lt;/p&gt;&lt;p&gt;这次收获还挺大的，主要是能和诸位做计算机围棋的前辈进行面对面的交流。Zen的作者加藤英树和CrazyStone的作者Remi Coulom给了我很多建议和意见。他们毕竟做了很多年的计算机围棋，经验是非常丰富的，而且非常愿意分享。&lt;/p&gt;&lt;p&gt;DarkForest目前为止的主要问题还在快速走子上，快速走子有误，则最后盘面和初始盘面会天差地别，对最后的胜者估计错误，统计上对胜率的估计也就不准。这样在选定下一招的时候就会有错。UEC的比赛分为两天，第一天我们输给了Zen和CrazyStone，全是快速走子的问题，对Zen的这盘将左边的死棋下成活棋，对CrazyStone的这盘则是在模拟时，未能在每次都让对手点上死活的关键点，导致对大龙的死活判断出了问题，一半模拟认为对手大优，另一半模拟认为己方大优，最后平均下来反而是己方略好，但事实上对手早就赢定了。晚上复盘发现了这个问题，终于在凌晨一点修好了一部分，不过说实在离完全修好，还有很长的路要走。所谓Devil
is in the details，就是这个意思，虽然AlphaGo的大框架很简单，但要做到它这个水平，还是要花大量时间，两位一作在计算机围棋上多年的功力在此展现。&lt;/p&gt;&lt;p&gt;快速走子因为是需要快（速度达到微秒级别），神经网络目前还是使不上劲的。至少在Zen和CrazyStone看来，在通过模拟对盘面进行估计这一方面，围棋仍然需要大量的手工规则去解决各种特殊情况。作者们花了五至十年的时间一点一点地收集各种特殊情况，不停地积累和调试规则，并且感叹目前为止都没有好的系统性的解决方案。AlphaGo的文章说的是用大量局部模式加上多类Logistics回归，然而我这里实现出来并不是这样，一些死活和对杀的情况必须要百分之百做对，而Logistics回归输出的则是概率，所以经常有下错的时候。我猜测AlphaGo可能也会有类似的规则，但是文章中似乎并未提及。如果他们的估值网络非常厉害，那这些规则都可以去掉了。但是看他们最新在UCI的演讲，由快速走子和估值网络分别得到的两类胜率似乎都比较重要。&lt;/p&gt;&lt;p&gt;第一天台湾团队的CGI
Intelligence在小组赛中七战全胜，最后一局还赢了Zen，但在第二天的淘汰赛中输给了我们。我猜测他们也是基于相似框架的算法，但是DCNN或者说大局观可能有所不及，在模拟盘面方面，则同样有一些特殊情况未能处理。所以导致一开始布局就落后，然后为了获胜而冒险侵入对方领地，却因为计算不够精确而失败。&lt;/p&gt;&lt;p&gt;===========================&lt;/p&gt;&lt;p&gt;棋谱：&lt;/p&gt;&lt;p&gt;第一天 &lt;a href="http://jsb.cs.uec.ac.jp/~igo/eng/result1.html" data-editable="true" data-title="First-day results"&gt;First-day results&lt;/a&gt;&lt;/p&gt;&lt;p&gt;第二天 &lt;a href="http://jsb.cs.uec.ac.jp/~igo/eng/result2.html" data-editable="true" data-title="Second-day results" class=""&gt;Second-day results&lt;/a&gt;&lt;/p&gt;&lt;p&gt;电圣战网上还没有官方的，这个是DF输给小林的一局：&lt;a href="http://yuandong-tian.com/densei4_1_en.sgf" data-editable="true" data-title="yuandong-tian.com 的页面"&gt;http://yuandong-tian.com/densei4_1_en.sgf&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20680393&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 28 Mar 2016 20:53:14 GMT</pubDate></item><item><title>赛后感言</title><link>https://zhuanlan.zhihu.com/p/20648612</link><description>&lt;p&gt;AlphaGo的比赛以4:1的比分结束了，说几句吧。&lt;/p&gt;&lt;p&gt;大家可能一直有误解， 计算机解决问题靠暴力，而人则靠智能。其实在面对指数级别的解题空间时，机器的暴力搜索所能覆盖的范围，不过是沧海一粟。我现在越来越觉得，连接主义和符号主义的合并，强直觉加上适当搜索才是解决问题较好的方案。人工智能迄今为止的历程，就是人类认识到这个宏大空间并且往这个方向靠拢的过程。从一开始的规则驱动的暴力搜索，到特征驱动的线性模型，再到数据驱动的深度学习，越来越强的模式识别能力让“直觉”两字从神秘莫测，变成了通过大量样本能学到的模型。机器因为有了更强的直觉，才能在图像识别和围棋上打败人类，毕竟围棋太难了，每一步都需要高效的剪枝才行。我在赛前说DCNN＋MCTS这样的框架大局观非常强，正是这个原因，不知不觉棋筋就连着了，不知不觉棋就非常厚了，怎么也断不了。因为MCTS是全盘估计分数的，DCNN又长于大局观。&lt;/p&gt;&lt;p&gt;与大家通常的理解相反，要让机器像人一样作理性推理，目前还比较困难。有效的逻辑和理性思维能力同样是依赖大量的直觉去找到正确的逻辑链条和理性判断，然后再回头验证。直觉的错误率是很高的，就像围棋的DCNN经常给出不靠谱的着法一样，需要MCTS的价值判断来纠正。逻辑思维的强弱，不是说想得有多深，脑袋转得有多快，而是在过去的经验中能找到解决方案，并且能有意识地用理性去判断解决方案的正确性，这样的框架一但在脑里建立起来，就有了自我学习自我纠错的能力。人类在这方面，仍然比计算机要强得多。我在赛前说局部死活对杀劫争会有问题，看完了五盘棋，大家也都惊叹计算机其实不会计算，因为AlphaGo里并没有显式建模局部死活对杀，而人可以依据当前盘面的状况，积极地改变搜索的指向，而机器会因为全局搜索所累，不知道在合适时候集中火力到某个重要的局部上。如何将DCNN和MCTS深度结合起来，仍然是很大的研究课题。&lt;/p&gt;&lt;p&gt;我以前写过《给人工智能泼点冷水》，现在看来这点冷水肯定浇不灭大家的热情。但我还是要说人工智能还有很长的路要走。围棋是固定规则下的完全信息博弈，再加上大量的人类对局样本，及两位一作长年对计算机围棋的坚持，还有谷歌大量的资源时间和人力，才造就了AlphaGo现在的辉煌。相比之下，人类每天在非对称信息中过活，在未知的世界中摸索，在充满噪音、模糊甚至错误的指导信息中学习，在稀缺样本中寻找规律，在极窄的通讯带宽下相互交流，依然有远超纸面数据的判断与推理，远胜大量机器的直觉和洞察力。另一方面，对于神经网络模型的解读，我们仍然处于初级阶段，多少人问为什么计算机能下出妙手？职业棋手有自己的判断和逻辑，并且能在一定程度上阐述清楚；可遗憾的是，人工智能研究者这边，只能在堆完大量的数据之后，两手一摊露出无奈的笑容。如何让机器学会人类的各项能力，依旧是一座需要攀登很多年的大山。&lt;/p&gt;&lt;p&gt;我记得在比赛期间有张图让人印象深刻，一边是需要千台机器的AlphaGo，另一边是李世石和一杯咖啡。大自然的鬼斧神工，一直让人肃然起敬；而最杰出的造物，莫过于我们人类自己。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20648612&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Wed, 16 Mar 2016 03:00:06 GMT</pubDate></item><item><title>第四局AlphaGo败招的分析</title><link>https://zhuanlan.zhihu.com/p/20644427</link><description>&lt;p&gt;第四局李世石的78手L11挖被大家誉为“神之一手”，在DarkForest的策略网络输出里排第31位，而J11靠排第10位。因此我觉得可能是AlphaGo没有算到这一步。如果对方下了一手机器没算到的棋，则蒙特卡罗（MCTS）搜索树会清空，然后重新开始搜索，不应该会太快做出结论。李喆六段告诉我K10这一手是秒下，那有可能是时间管理子系统在搜索树清空时有程序上的漏洞，因此过早地将搜索结果返回了。MCTS在一开始搜索的时候，因为模拟次数不够多，每步的胜率方差非常大，所以返回一个不够好的着法如K10是很正常的（在DarkForest里面这着排在前四）。这个比较容易修正。 &lt;/p&gt;&lt;p&gt;另一种可能是，AlphaGo的估值网络出了问题。因为估值网络的权重是0.5，而不管快速走子从一个局面开始重复了多少次，它的权值也是0.5。对于一个局面，估值网络只得到一个数，而从这个局面往下走子，走多后会得到很多个数，统计上应该更为重要，但是AlphaGo不是这样想的，两边各自算得胜率后直接对半平均了。所以如果估值网络对某个局面得到的结果不对，则会极大地影响对该局面的胜率估计。注意这里得到很多个数的原因是按照文章，叶结点在积累了一定盘数后（40）才展开，而不是第一次访问就展开，以提高DCNN的效率。DarkForest没有用到估值网络，在L11的挖之后正确地返回了L12和L10这两个应手，据李喆六段说，都是正确的应手，这间接支持了这个推断。AlphaGo在87手之后才意识到自己已经大大落后，可能也是由于同样的问题，比如说把右边的黑大龙看成活的。&lt;/p&gt;&lt;p&gt;那为什么估值网络会出问题呢？可能是用于训练估值网络的自学习（Self-Play）的样本分布有盲点。为了提高样本生成速度，AlphaGo的自学习样本是通过用两个纯粹的DCNN互搏来生成的（完全没有搜索），而DCNN下出来的棋因为是纯模式识别，一个大问题是死活不正确，经常是在死棋里面下子。如果黑白两方都犯了死活不分的毛病，然后一方比如说白侥幸胜了，那估值网络就会认为方才白的死棋局面是好的。这样估值网络就会染上同样毛病，在中盘复杂的对杀局面中判断失误。若是这种情况就不好处理，AlphaGo下一局可能还会有同样的问题。这里可以看到，电脑本身也不是靠穷举来下棋的，围棋毕竟太复杂，每一步都要剪枝，离当前局面近的仔细剪（用DCNN），离当前局面远的快速剪（快速走子），直到终局得到胜负为止。剪枝的好坏直接关系到棋力的高低，DCNN只是一个有大局观的非常好的剪枝手段，它的盲点也会通过败着反映出来。&lt;/p&gt;&lt;p&gt;关于DCNN＋MCTS打劫。首先因为MCTS是全局估计分数的，劫争本身和其它局面在程序看来没有本质区别，都只是一步棋而已。劫的特殊性在DarkForest上表现为碰到有劫可提的情况时，DCNN经常会以非常高的概率（0.8以上）返回提劫这一手。可能的原因是，劫点是作为单独的特征输入的，所以DCNN学习到了它和输出（提劫）的强关联性。这样在MCTS搜索时会强烈偏向这一手。这在很多情况下是正确的，但有时劫很小可以不予理会，或者碰到两个或者多个劫需要放弃一个，那“遇劫必提”的偏向性就会给搜索带来麻烦。有时连环劫电脑反复提就是这个原因。AlphaGo可能会有这个问题，或者是反向的问题（比如说提劫概率很小），这样在下棋时大家就会感觉到它在避免开劫，或者在含劫的变化中计算失误。&lt;/p&gt;&lt;p&gt;关于地平线效应(Horizon Effect)。国象的AI里面会有这个效应，比如说只搜索10步，计算到别人的后被自己的后吃了结束，然后用简单的加和法估计下盘面发现自己多个后特别爽，觉得这个分支特别好。其实再往下走一步自己的后也被别人吃了，或者掉入陷阱，这样就误算盘面价值。但是围棋因为每次模拟都是走到底的，可能前30步是用DCNN，之后就是用快速走子，虽然走子质量上有差距，但是大方向上不会错，所以地平线效应在某种程度上是减弱了。而且这次AlphaGo的失误在20步以内，应该还在DCNN的范围里面，所以地平线效应的可能性比较低。&lt;/p&gt;&lt;p&gt;＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝&lt;/p&gt;&lt;p&gt;应大家要求，这里放Game3的胜率，李世石毫无机会。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0d3d2473432940e96cca246120d79f9c.png" data-rawwidth="821" data-rawheight="536"&gt;&lt;p&gt;Game4的还没空全跑，以下是关键的一段。注意DarkForest只是5d，所以没有及早看出来黑挂了也是正常的。&lt;/p&gt;&lt;p&gt;[84] Suggest: W O9, Winrate: 0.405329 Actual move: W O9&lt;/p&gt;&lt;p&gt;[85] Suggest: B O8, Winrate: 0.599069 Actual move: B P8&lt;/p&gt;&lt;p&gt;[86] Suggest: W O8, Winrate: 0.400797 Actual move: W P9&lt;/p&gt;&lt;p&gt;[87] Suggest: B L10, Winrate: 0.600354 Actual move: B Q9&lt;/p&gt;&lt;p&gt;[88] Suggest: W Q8, Winrate: 0.430756 Actual move: W Q8&lt;/p&gt;&lt;p&gt;[89] Suggest: B R9, Winrate: 0.567363 Actual move: B R9&lt;/p&gt;&lt;p&gt;[90] Suggest: W O8, Winrate: 0.446702 Actual move: W O8&lt;/p&gt;&lt;p&gt;[91] Suggest: B P7, Winrate: 0.556039 Actual move: B L10&lt;/p&gt;&lt;p&gt;[92] Suggest: W N7, Winrate: 0.421326 Actual move: W J11&lt;/p&gt;&lt;p&gt;[93] Suggest: B P7, Winrate: 0.588889 Actual move: B S9&lt;/p&gt;&lt;p&gt;[94] Suggest: W P7, Winrate: 0.465145 Actual move: W P7&lt;/p&gt;&lt;p&gt;[95] Suggest: B R7, Winrate: 0.570279 Actual move: B Q13&lt;/p&gt;&lt;p&gt;[96] Suggest: W P12, Winrate: 0.453528 Actual move: W R8&lt;/p&gt;&lt;p&gt;[97] Suggest: B S8, Winrate: 0.474463 Actual move: B C4&lt;/p&gt;&lt;p&gt;[98] Suggest: W C5, Winrate: 0.492770 Actual move: W C5&lt;/p&gt;&lt;p&gt;[99] Suggest: B C3, Winrate: 0.507522 Actual move: B P15&lt;/p&gt;&lt;p&gt;[100] Suggest: W P12, Winrate: 0.528700 Actual move: W S8&lt;/p&gt;&lt;p&gt;[101] Suggest: B J10, Winrate: 0.402717 Actual move: B T9&lt;/p&gt;&lt;p&gt;[102] Suggest: W P12, Winrate: 0.604598 Actual move: W S10&lt;/p&gt;&lt;p&gt;[103] Suggest: B P14, Winrate: 0.462454 Actual move: B H13&lt;/p&gt;&lt;p&gt;[104] Suggest: W J14, Winrate: 0.566891 Actual move: W J10&lt;/p&gt;&lt;p&gt;[105] Suggest: B J12, Winrate: 0.476526 Actual move: B L7&lt;/p&gt;&lt;p&gt;[106] Suggest: W J7, Winrate: 0.597039 Actual move: W G11&lt;/p&gt;&lt;p&gt;[107] Suggest: B F10, Winrate: 0.408352 Actual move: B F10&lt;/p&gt;&lt;p&gt;[108] Suggest: W J7, Winrate: 0.600918 Actual move: W K8&lt;/p&gt;&lt;p&gt;[109] Suggest: B J12, Winrate: 0.419091 Actual move: B L8&lt;/p&gt;&lt;p&gt;[110] Suggest: W G8, Winrate: 0.633512 Actual move: W G8&lt;/p&gt;&lt;p&gt;[111] Suggest: B F8, Winrate: 0.370351 Actual move: B F8&lt;/p&gt;&lt;p&gt;[112] Suggest: W G7, Winrate: 0.630929 Actual move: W G7&lt;/p&gt;&lt;p&gt;[113] Suggest: B E6, Winrate: 0.373719 Actual move: B C12&lt;/p&gt;&lt;p&gt;[114] Suggest: W D12, Winrate: 0.602646 Actual move: W E15&lt;/p&gt;&lt;p&gt;[115] Suggest: B D16, Winrate: 0.397657 Actual move: B E18&lt;/p&gt;&lt;p&gt;[116] Suggest: W B12, Winrate: 0.612667 Actual move: W B13&lt;/p&gt;&lt;p&gt;[117] Suggest: B F7, Winrate: 0.450077 Actual move: B D13&lt;/p&gt;&lt;p&gt;[118] Suggest: W E13, Winrate: 0.587001 Actual move: W E13&lt;/p&gt;&lt;p&gt;[119] Suggest: B J12, Winrate: 0.440845 Actual move: B E6&lt;/p&gt;&lt;p&gt;[120] Suggest: W D6, Winrate: 0.546337 Actual move: W F5&lt;/p&gt;&lt;p&gt;[121] Suggest: B H9, Winrate: 0.454207 Actual move: B D14&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20644427&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 14 Mar 2016 14:11:16 GMT</pubDate></item><item><title>DarkForest对AlphaGo和李世石前两局每步的胜率估计（仅供参考）</title><link>https://zhuanlan.zhihu.com/p/20639694</link><description>&lt;p&gt;DarkForest 75k rollouts. DarkForest现在是KGS 5d，一开始黑棋胜率都会低点，但于棋力关系不大。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0fbf120b27e447d553ce1eb7079c5e79.png" data-rawwidth="825" data-rawheight="530"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cb5d81be32500db9c7715bda5a8e2841.png" data-rawwidth="806" data-rawheight="532"&gt;&lt;p&gt;首先说明一下每个数据点是DF在当前局面下给出最优应手，同时给出的胜率。这个最优应手和选手的应手不一定一样。如果大家要看DF给的应手，可以下载以下两个文件：&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.yuandong-tian.com/win_rate1.txt"&gt;http://www.yuandong-tian.com/win_rate1.txt&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.yuandong-tian.com/win_rate2.txt"&gt;http://www.yuandong-tian.com/win_rate2.txt&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20639694&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Sat, 12 Mar 2016 10:32:31 GMT</pubDate></item><item><title>AlphaGo的分析</title><link>https://zhuanlan.zhihu.com/p/20607684</link><description>&lt;p&gt;最近我仔细看了下AlphaGo在《自然》杂志上发表的文章，写一些分析给大家分享。&lt;/p&gt;&lt;p&gt;AlphaGo这个系统主要由几个部分组成：&lt;/p&gt;&lt;p&gt;1. 走棋网络（Policy Network），给定当前局面，预测/采样下一步的走棋。&lt;/p&gt;&lt;p&gt;2. 快速走子（Fast rollout），目标和1一样，但在适当牺牲走棋质量的条件下，速度要比1快1000倍。 &lt;/p&gt;&lt;p&gt;3. 估值网络（Value Network），给定当前局面，估计是白胜还是黑胜。&lt;/p&gt;&lt;p&gt;4. 蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS)，把以上这三个部分连起来，形成一个完整的系统。&lt;/p&gt;&lt;p&gt;我们的DarkForest和AlphaGo同样是用4搭建的系统。DarkForest较AlphaGo而言，在训练时加强了1，而少了2和3，然后以开源软件Pachi的缺省策略
(default policy)部分替代了2的功能。以下介绍下各部分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.走棋网络：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;走棋网络把当前局面作为输入，预测/采样下一步的走棋。它的预测不只给出最强的一手，而是对棋盘上所有可能的下一着给一个分数。棋盘上有361个点，它就给出361个数，好招的分数比坏招要高。DarkForest在这部分有创新，通过在训练时预测三步而非一步，提高了策略输出的质量，和他们在使用增强学习进行自我对局后得到的走棋网络（RL network）的效果相当。当然，他们并没有在最后的系统中使用增强学习后的网络，而是用了直接通过训练学习到的网络（SL
network）,理由是RL
network输出的走棋缺乏变化，对搜索不利。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ea120155d6658e3f813f956c63d21bc4.png" data-rawwidth="575" data-rawheight="320"&gt;&lt;p&gt;有意思的是在AlphaGo为了速度上的考虑，只用了宽度为192的网络，而并没有使用最好的宽度为384的网络（见图2(a))，所以要是GPU更快一点（或者更多一点），AlphaGo肯定是会变得更强的。&lt;/p&gt;&lt;p&gt;所谓的0.1秒走一步，就是纯粹用这样的网络，下出有最高置信度的合法着法。这种做法一点也没有做搜索，但是大局观非常强，不会陷入局部战斗中，说它建模了“棋感”一点也没有错。我们把DarkForest的走棋网络直接放上KGS就有3d的水平，让所有人都惊叹了下。可以说，这一波围棋AI的突破，主要得益于走棋网络的突破。这个在以前是不可想像的，以前用的是基于规则，或者基于局部形状再加上简单线性分类器训练的走子生成法，需要慢慢调参数年，才有进步。&lt;/p&gt;&lt;p&gt;当然，只用走棋网络问题也很多，就我们在DarkForest上看到的来说，会不顾大小无谓争劫，会无谓脱先，不顾局部死活，对杀出错，等等。有点像高手不经认真思考的随手棋。因为走棋网络没有价值判断功能，只是凭“直觉”在下棋，只有在加了搜索之后，电脑才有价值判断的能力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 快速走子&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那有了走棋网络，为什么还要做快速走子呢？有两个原因，首先走棋网络的运行速度是比较慢的，AlphaGo说是3毫秒，我们这里也差不多，而快速走子能做到几微秒级别，差了1000倍。所以在走棋网络没有返回的时候让CPU不闲着先搜索起来是很重要的，等到网络返回更好的着法后，再更新对应的着法信息。&lt;/p&gt;&lt;p&gt;其次，快速走子可以用来评估盘面。由于天文数字般的可能局面数，围棋的搜索是毫无希望走到底的，搜索到一定程度就要对现有局面做个估分。在没有估值网络的时候，不像国象可以通过算棋子的分数来对盘面做比较精确的估值，围棋盘面的估计得要通过模拟走子来进行，从当前盘面一路走到底，不考虑岔路地算出胜负，然后把胜负值作为当前盘面价值的一个估计。这里有个需要权衡的地方：在同等时间下，模拟走子的质量高，单次估值精度高但走子速度慢；模拟走子速度快乃至使用随机走子，虽然单次估值精度低，但可以多模拟几次算平均值，效果未必不好。所以说，如果有一个质量高又速度快的走子策略，那对于棋力的提高是非常有帮助的。&lt;/p&gt;&lt;p&gt;为了达到这个目标，神经网络的模型就显得太慢，还是要用传统的局部特征匹配（local pattern matching）加线性回归（logistic
regression）的方法，这办法虽然不新但非常好使，几乎所有的广告推荐，竞价排名，新闻排序，都是用的它。与更为传统的基于规则的方案相比，它在吸纳了众多高手对局之后就具备了用梯度下降法自动调参的能力，所以性能提高起来会更快更省心。AlphaGo用这个办法达到了2微秒的走子速度和24.2%的走子准确率。24.2%的意思是说它的最好预测和围棋高手的下子有0.242的概率是重合的，相比之下，走棋网络在GPU上用2毫秒能达到57%的准确率。这里，我们就看到了走子速度和精度的权衡。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/85f83f679dc9a8893eeb30287a37646b.png" data-rawwidth="1349" data-rawheight="566"&gt;&lt;p&gt;和训练深度学习模型不同，快速走子用到了局部特征匹配，自然需要一些围棋的领域知识来选择局部特征。对此AlphaGo只提供了局部特征的数目（见Extended Table 4)，而没有说明特征的具体细节。我最近也实验了他们的办法，达到了25.1%的准确率和4-5微秒的走子速度，然而全系统整合下来并没有复现他们的水平。我感觉上24.2%并不能完全概括他们快速走子的棋力，因为只要走错关键的一步，局面判断就完全错误了；而图2(b)更能体现他们快速走子对盘面形势估计的精确度，要能达到他们图2(b)这样的水准，比简单地匹配24.2%要做更多的工作，而他们并未在文章中强调这一点。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/b9e68dd3b2836afe0980498668ce45bf.png" data-rawwidth="568" data-rawheight="322"&gt;&lt;p&gt;在AlphaGo有了快速走子之后，不需要走棋网络和估值网络，不借助任何深度学习和GPU的帮助，不使用增强学习，在单机上就已经达到了3d的水平（见Extended Table 7倒数第二行)，这是相当厉害的了。任何使用传统方法在单机上达到这个水平的围棋程序，都需要花费数年的时间。在AlphaGo之前，Aja Huang曾经自己写过非常不错的围棋程序，在这方面相信是有很多的积累的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 估值网络&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/f99c6bb0982c57d9dd2f404c12a16a13.png" data-rawwidth="1374" data-rawheight="535"&gt;&lt;p&gt;AlphaGo的估值网络可以说是锦上添花的部分，从Fig 2(b)和Extended
Table 7来看，没有它AlphaGo也不会变得太弱，至少还是会在7d-8d的水平。少了估值网络，等级分少了480分，但是少了走棋网络，等级分就会少掉800至1000分。特别有意思的是，如果只用估值网络来评估局面（2177），那其效果还不及只用快速走子（2416），只有将两个合起来才有更大的提高。我的猜测是，估值网络和快速走子对盘面估计是互补的，在棋局一开始时，大家下得比较和气，估值网络会比较重要；但在有复杂的死活或是对杀时，通过快速走子来估计盘面就变得更重要了。考虑到估值网络是整个系统中最难训练的部分（需要三千万局自我对局），我猜测它是最晚做出来并且最有可能能进一步提高的。&lt;/p&gt;&lt;p&gt;关于估值网络训练数据的生成，值得注意的是文章中的附录小字部分。与走棋网络不同，每一盘棋只取一个样本来训练以避免过拟合，不然对同一对局而言输入稍有不同而输出都相同，对训练是非常不利的。这就是为什么需要三千万局，而非三千万个盘面的原因。对于每局自我对局，取样本是很有讲究的，先用SL network保证走棋的多样性，然后随机走子，取盘面，然后用更精确的RL
network走到底以得到最正确的胜负估计。当然这样做的效果比用单一网络相比好多少，我不好说。&lt;/p&gt;&lt;p&gt;一个让我吃惊的地方是，他们完全没有做任何局部死活/对杀分析，纯粹是用暴力训练法训练出一个相当不错的估值网络。这在一定程度上说明深度卷积网络（DCNN）有自动将问题分解成子问题，并分别解决的能力。&lt;/p&gt;&lt;p&gt;另外，我猜测他们在取训练样本时，判定最终胜负用的是中国规则。所以说三月和李世石对局的时候也要求用中国规则，不然如果换成别的规则，就需要重新训练估值网络（虽然我估计结果差距不会太大）。至于为什么一开始就用的中国规则，我的猜测是编程非常方便（我在写DarkForest的时候也是这样觉得的）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. 蒙特卡罗树搜索&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分基本用的是传统方法，没有太多可以评论的，他们用的是带先验的UCT，即先考虑DCNN认为比较好的着法，然后等到每个着法探索次数多了，选择更相信探索得来的胜率值。而DarkForest则直接选了DCNN推荐的前3或是前5的着法进行搜索。我初步试验下来效果差不多，当然他们的办法更灵活些，在允许使用大量搜索次数的情况下，他们的办法可以找到一些DCNN认为不好但却对局面至关重要的着法。&lt;/p&gt;&lt;p&gt;一个有趣的地方是在每次搜索到叶子节点时，没有立即展开叶子节点，而是等到访问次数到达一定数目(40)才展开，这样避免产生太多的分支，分散搜索的注意力，也能节省GPU的宝贵资源，同时在展开时，对叶节点的盘面估值会更准确些。除此之外，他们也用了一些技巧，以在搜索一开始时，避免多个线程同时搜索一路变化，这部分我们在DarkForest中也注意到了，并且做了改进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. 总结&lt;/b&gt;&lt;/p&gt;&lt;p&gt;总的来说，这整篇文章是一个系统性的工作，而不是一两个小点有了突破就能达到的胜利。在成功背后，是作者们，特别是两位第一作者David Silver和Aja Huang，在博士阶段及毕业以后五年以上的积累，非一朝一夕所能完成的。他们能做出AlphaGo并享有现在的荣誉，是实至名归的。&lt;/p&gt;&lt;p&gt;从以上分析也可以看出，与之前的围棋系统相比，AlphaGo较少依赖围棋的领域知识，但还远未达到通用系统的程度。职业棋手可以在看过了寥寥几局之后明白对手的风格并采取相应策略，一位资深游戏玩家也可以在玩一个新游戏几次后很快上手，但到目前为止，人工智能系统要达到人类水平，还是需要大量样本的训练的。可以说，没有千年来众多棋手在围棋上的积累，就没有围棋AI的今天。&lt;/p&gt;&lt;p&gt;在AlphaGo中，增强学习（Reinforcement Learning）所扮演的角色并没有想像中那么大。在理想情况下，我们希望人工智能系统能在对局中动态地适应环境和对手的招式并且找到办法反制之，但是在AlphaGo中增强学习更多地是用于提供更多质量更好的样本，给有监督学习（Supervised Learning）以训练出更好的模型。在这方面增强学习还有很长的路要走。&lt;/p&gt;&lt;p&gt;另外，据他们的文章所言，AlphaGo整个系统在单机上已具有了职业水平，若是谷歌愿意开几万台机器和李世石对决（这对它来说再容易不过了，改个参数就行），相信比赛会非常精彩。 &lt;/p&gt;&lt;p&gt;===========================&lt;/p&gt;&lt;p&gt;一些更新。&lt;/p&gt;&lt;p&gt;问题1：“Alphago的MCTS做rollout的时候，除了使用快速走子，还用了搜索树的已有部分，看起来像是AMAF/RAVE反过来：AMAF是把快速走子的信息传导到树的其它无关部分，Alphago是把树的其它无关部分拿来增强快速走子。我怀疑这是不是它棋力比其它DCNN+MCTS强的原因之一。"&lt;/p&gt;&lt;p&gt;这个办法在解死活题的文章中出现过，会在一定程度上提高搜索效率，但是提高多少还不知道。&lt;/p&gt;&lt;p&gt;问题2：“rollout的走法质量变好可能会导致棋力下降。”&lt;/p&gt;&lt;p&gt;这里要分两种情况，tree policy和default policy。在AlphaGo的文章里面已经说过了，tree policy的分布不能太尖，不然在搜索时太过重视一些看起来的好着，可能使得棋力下降。但是除了这种原因，一般来说tree policy变好棋力还是会变强的。&lt;/p&gt;&lt;p&gt;default policy这边，即（半）随机走子到最后然后判分，就很复杂了，质量变好未必对局面能估得更准。default policy需要保证的是每块棋的死活大体正确，不要把死的棋下成活的或者反之，而对大局观的要求反而没有那么高。双方完全可以配合着把每块棋下完，然后转战另一块，而不是说抢在对方前去别处占先手。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20607684&amp;pixel&amp;useReferer"/&gt;</description><author>田渊栋</author><pubDate>Mon, 29 Feb 2016 07:26:06 GMT</pubDate></item></channel></rss>