<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Datartisan数据工匠 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/datartisan</link><description></description><lastBuildDate>Mon, 10 Oct 2016 20:16:46 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>机器学习通用框架</title><link>https://zhuanlan.zhihu.com/p/22833471</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5ed55fdec803bf29c368e83917832630_r.png"&gt;&lt;/p&gt;每个数据科学家每天都要处理成吨的数据，而他们60%~70%的时间都在进行数据清洗和数据格式调整，将原始数据转变为可以用机器学习所识别的形式。本文主要集中在数据清洗后的过程，也就是机器学习的通用框架。这个框架是我在参加了百余场机器学习竞赛后的一个总结。尽管这个框架是非常笼统和概括的，但是绝对能发挥强大的作用，仍然可以在专业人员的运用下变成复杂、高效的方法。整个过程使用Python来实现。&lt;h2&gt;数据&lt;/h2&gt;&lt;p&gt;在用机器学习的方法之前，我们应该先把数据转变为表格的形式，这个过程是最耗时、最复杂的。我们用下图来表示这一过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7e116860a704a65392553fc64eb40615.png" data-rawwidth="800" data-rawheight="222"&gt;&lt;p&gt;这一过程也就是将原始数据的所有的变量量化，进一步转变为含数据（Data）和标签（Labels）的数据框形式。这样处理过的数据就可以用来机器学习建模了。数据框形式的数据是机器学习和数据挖掘中最为通用的数据表现形式，它的行是数据抽样得到的样本，列代表数据的标签Y和特征X，其中标签根据我们要研究的问题不同，有可能是一列或多列。 &lt;/p&gt;&lt;h2&gt;标签的类型&lt;/h2&gt;&lt;p&gt;根据我们要研究的问题，标签的类型也不一： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;单列0-1值(&lt;strong&gt;二分类问题&lt;/strong&gt;，一个样本只属于一类并且一共只有两类）&lt;/li&gt;&lt;li&gt;单列连续值（&lt;strong&gt;单回归问题&lt;/strong&gt;，要预测的值只有一个）&lt;/li&gt;&lt;li&gt;多列0-1值（&lt;strong&gt;多分类问题&lt;/strong&gt;，同样是一个样本只属于一类但是一共有多类）&lt;/li&gt;&lt;li&gt;多列连续值(&lt;strong&gt;多回归问题&lt;/strong&gt;，能够预测多个值）&lt;/li&gt;&lt;li&gt;多标签（&lt;strong&gt;多标签分类问题&lt;/strong&gt;，但是一个样本可以属于多类）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;评价指标&lt;/h2&gt;&lt;p&gt;对于多个机器学习方法，我们必须找到一个评价指标来衡量它们的好坏。比如一个二元分类的问题我们一般选用AUC ROC或者仅仅用AUC曲线下面的面积来衡量。在多标签和多分类问题上，我们选择交叉熵或对数损失函数。在回归问题上我们选择常用的均方误差(MSE)。&lt;/p&gt;&lt;h2&gt;Python库&lt;/h2&gt;&lt;p&gt;在安装机器学习的几个库之前，应该安装两个基础库：numpy和scipy。 &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Pandas&lt;/strong&gt; 处理数据最强大的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt; 涵盖机器学习几乎所有方法的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;xgboost&lt;/strong&gt; 优化了传统的梯度提升算法 &lt;/li&gt;&lt;li&gt;&lt;strong&gt;keras&lt;/strong&gt; 神经网络&lt;/li&gt;&lt;li&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;用来作图的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;tpdm&lt;/strong&gt; 显示过程&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;机器学习框架&lt;/h2&gt;&lt;p&gt;2015年我想出了一个自动式机器学习的框架，直到今天还在开发阶段但是不久就会发布，本文就是以这个框架作为基础的。下图展示了这个框架：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5ed55fdec803bf29c368e83917832630.png" data-rawwidth="800" data-rawheight="605"&gt;&lt;p&gt;上面展示的这个框架里面，粉红色的线就是一些通用的步骤。在处理完数据并把数据转为数据框格式后，我们就可以进行机器学习过程了。 &lt;/p&gt;&lt;h3&gt;确定问题&lt;/h3&gt;&lt;p&gt;确定要研究的问题，也就是通过观察标签的类别确定究竟是分类还是回归问题。 &lt;/p&gt;&lt;h3&gt;划分样本&lt;/h3&gt;&lt;p&gt;第二步是将所有的样本划分为&lt;strong&gt;训练集(training data)&lt;/strong&gt;和&lt;strong&gt;验证集(validation data)&lt;/strong&gt;。过程如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7e153d8877ffb6b6549888cacf8ebea5.png" data-rawwidth="800" data-rawheight="406"&gt;划分样本的这一过程必须要根据标签来做。比如对于一个类别不平衡的分类问题，必须要用分层抽样的方法，比如每种标签抽多少，这样才能保证抽出来的两个样本子集分布类似。在Python中，我们可以用scikit-learn轻松实现。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-fe714934ca0c0c46506a0abf03a34538.png" data-rawwidth="496" data-rawheight="130"&gt;对于回归问题，那么一个简单的K折划分就足够了。但是仍然有一些复杂的方法可以使得验证集和训练集标签的分布接近，这个问题留给读者作为练习。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-274aedb004ffb1123a80f28523812d38.png" data-rawwidth="504" data-rawheight="121"&gt;&lt;p&gt;上面我用了样本全集中的10%作为验证集的规模，当然你可以根据你的样本量做相应的调整。划分完样本以后，我们就把这些数据放在一边。接下来我们使用的任何一种机器学习的方法都要先在训练集上使用然后再用验证集检验效果。验证集和训练集永远都不能掺和在一起。这样才能得到有效的评价得分，否则将会导致过拟合的问题。 &lt;/p&gt;&lt;h3&gt;识别特征&lt;/h3&gt;&lt;p&gt;一个数据集总是带有很多的变量(variables)，或者称之为特征（features)，他们对应着数据框的维度。一般特征的值有三种类型：数值变量、属性变量和文字变量。我们用经典的&lt;a href="https://www.kaggle.com/c/titanic/data" data-editable="true" data-title="泰坦尼克号数据集"&gt;泰坦尼克号数据集&lt;/a&gt;来示例。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ddcec0bb516d5ac00699f42ae7f6a4d0.png" data-rawwidth="800" data-rawheight="441"&gt;&lt;p&gt;在这里生还（survival）就是标签，船舱等级（pclass）、性别（sex）和登船港口（embarked）是属性变量。而像年龄（age）、船上兄弟姐妹数量（sibsp）、船上父母孩子数量（parch）是数值变量。而姓名（name）这种文字变量我们认为这和生还与否没什么关系，所以我们决定不考虑。首先处理数值型变量，这些变量几乎不需要任何的处理，常见的方式是正规化（normalization)。处理属性变量通常有两步： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;把属性变量转变为标签&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-5f8e47281c1989d51fdfb15f9d46d96e.png" data-rawwidth="533" data-rawheight="111"&gt;&lt;/li&gt;&lt;li&gt;把标签转变为二元数值&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-210a2f9867eb2da40f2f84166b66d9f5.png" data-rawwidth="531" data-rawheight="108"&gt;由于泰坦尼克号数据集没有很好的文字变量来示范，那么我们就制定一个通用的规则来处理文字变量。把所有的文字变量组合到一起，然后用某种算法来处理并转变为数字&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4b80388cfcf90e904a3ae345a66f680a.png" data-rawwidth="531" data-rawheight="90"&gt;&lt;p&gt;我们可以用CountVectorizer或者TfidfVectorizer来实现&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-c0a5835bbe0ca5ccc70e7cb5585c8f29.png" data-rawwidth="530" data-rawheight="92"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1de24e5d3486617db516dcae18e89df4.png" data-rawwidth="530" data-rawheight="91"&gt;一般来说第二种方法往往比较优越，下面代码框中所展示的参数长期以来都取得了良好的效果。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-4576414bd3937f529680b4d1db9547f9.png" data-rawwidth="530" data-rawheight="230"&gt;如果你对训练集数据采用了上述处理方式，那么也要保证对验证及数据做相同处理。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6d7b9382604d0b687aa166b06057c435.png" data-rawwidth="533" data-rawheight="59"&gt;&lt;h3&gt;特征融合&lt;/h3&gt;&lt;p&gt;特征融合是指将不同的特征融合，要区别对待密集型变量和稀疏型变量。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-89154eb1d05095710ec9d96192474ef6.png" data-rawwidth="611" data-rawheight="134"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-69a7a2a787669ed9a72efd22c36747df.png" data-rawwidth="531" data-rawheight="126"&gt;&lt;p&gt;当我们把特征融合好以后，可以开始机器学习的建模过程了，在这里我们都是选择以决策树为基学习器的集成算法，主要有： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;RandomForestClassifier&lt;/li&gt;&lt;li&gt;RandomForestRegressor&lt;/li&gt;&lt;li&gt;ExtraTreesClassifier&lt;/li&gt;&lt;li&gt;ExtraTreesRegressor&lt;/li&gt;&lt;li&gt;XGBClassifier&lt;/li&gt;&lt;li&gt;XGBRegressor但是不能直接把没有经过规范化的数值变量直接用线性模型拟合，可以用scikitlearn里面的&lt;strong&gt;规范化（Normalized）&lt;/strong&gt;和&lt;strong&gt;标准化（StandardScaler）&lt;/strong&gt;命令分别对密集和稀疏的数据进行相应的处理。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;特征降维和特征选择&lt;/h3&gt;&lt;p&gt;如果以上方式处理后的数据可以产生一个优秀的模型，那就可以直接进行参数调整了。如果不行则还要继续进行特征降维和特征选择。降维的方法有以下几种：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a994abcea4bab5ec498a5e1010c8890d.png" data-rawwidth="547" data-rawheight="158"&gt;简单起见，这里不考虑LDA和QDA。对于高维数据来说，PCA是常用的降维方式，对于图像数据一般我们选用10~15组主成分，当然如果模型效果会提升的话也可以选择更多的主成分。对于其他类型的数据我们一般选择50~60个主成分。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f88985c9186ec217a9e5fa4f77e87819.png" data-rawwidth="533" data-rawheight="93"&gt;文字变量转变为稀疏矩阵后进行奇异值分解，奇异值分解对应scikit learn库中的TruncatedSVD语句。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-db5aa99256a24a68e67f071e32eec5c1.png" data-rawwidth="535" data-rawheight="110"&gt;一般在TF-IDF中SVD主成分的数目大约在120~200之间，但是也可以采用更多的成分，但是相应的计算成本也会增加。在特征降维之后我们可以进行建模的训练过程了，但是有的时候如果这样降维后的结果仍不理想，可以进行特征选择：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fd94e51c03ce073dd8a32e8b8be11a88.png" data-rawwidth="543" data-rawheight="156"&gt;特征选择也有很多实用的方法，比如说常用的向前或向后搜索。那就是一个接一个地把特征加入模型训练，如果加入一个新的特征后模型效果不好，那就不加入这一特征。直到选出最好的特征子集。对于这种方法有一个提升的方式是&lt;a href="https://github.com/abhishekkrthakur/greedyFeatureSelection" data-editable="true" data-title="用AUC作为评价指标"&gt;用AUC作为评价指标&lt;/a&gt;，当然这个提升也不是尽善尽美的，还是需要实际应用进行改善和调整的。还有一种特征选择的方式是在建模的过程中就得到了最佳特征子集。比如我们可以观察logit模型的系数或者拟合一个随机森林模型从而直接把这些甄选后的特征用在其它模型中。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e5ebb4859ca5bfa4fad12bc2c62ae2c9.png" data-rawwidth="527" data-rawheight="107"&gt;在上面的处理中应该选择一个小的estimator数目这样不会导致过拟合。还可以用梯度提升算法来进行特征选择，在这里我们建议用xgboost的库而不是sklearn库里面的梯度提升算法，因为前者速度快且有着更好的延展性。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e19417a5546f66fad1a8654761c4e953.png" data-rawwidth="534" data-rawheight="114"&gt;对于稀疏的数据集我们可以用随机森林、xgboost或卡方等方式来进行特征选择。下面的例子中我们用了卡方的方法选择了20个特征出来。当然这个参数值20也是可以进一步优化的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-262c04afcbb8625145ff89273c9c66d9.png" data-rawwidth="537" data-rawheight="111"&gt;&lt;p&gt;同样，以上我们用的所有方法都要记录储存用以交叉验证。 &lt;/p&gt;&lt;h3&gt;模型选择和参数调整&lt;/h3&gt;&lt;p&gt;一般而言，常用的机器学习模型有以下几种，我们将在这些模型中选择最好的模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分类问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;Logistic 回归&lt;/li&gt;&lt;li&gt;朴素贝叶斯分类器&lt;/li&gt;&lt;li&gt;支持向量机&lt;/li&gt;&lt;li&gt;k临近分类器&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;回归问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;线性回归&lt;/li&gt;&lt;li&gt;岭回归&lt;/li&gt;&lt;li&gt;Lasso&lt;/li&gt;&lt;li&gt;支持向量回归&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下表中展示了每种模型分别需要优化的参数，这其中包含的问题太多太多了。究竟参数取什么值才最优，很多人往往有经验但是不会甘愿把这些秘密分享给别人。但是在这里我会把我的经验跟大家分享。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-fbaebe0e8a9f3ee4230e12a304c8b746.png" data-rawwidth="785" data-rawheight="800"&gt;&lt;blockquote&gt;&lt;p&gt;RS*是指没有一个确切的值提供给大家。 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在我看来上面的这些模型基本会完爆其他的模型，当然这只是我的一家之言。下面是上述过程的一个总结，主要是强调一下要保留训练的结果用来给验证集验证，而不是重新用验证集训练！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-46a7880b2130eab916ddd2e2d4830c3b.png" data-rawwidth="800" data-rawheight="444"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7ebb886225a95536fac0bc0785d92882.png" data-rawwidth="800" data-rawheight="184"&gt;&lt;/p&gt;&lt;p&gt;在我长时间的实践过程中，我发现这些总结出来的规则和框架还是很有用的，当然在一些极其复杂的工作中这些方法还是力有不逮。生活从来不会完美，我们只能尽自身所能去优化，机器学习也是一样。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;原文作者：Abhishek Thakur原文链接：&lt;a href="http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/?sukey=3997c0719f151520eec92d4e7022d429d473ec41dadc46d670acc94d1f94abd4231fd776bc3c18126eea39e6b5a67350" data-editable="true" data-title="Approaching (Almost) Any Machine Learning Problem" class=""&gt;Approaching (Almost) Any Machine Learning Problem&lt;/a&gt;译者：Cup&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22833471&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Sun, 09 Oct 2016 14:24:17 GMT</pubDate></item><item><title>机器学习系列-Logistic回归：我看你像谁 （下篇）</title><link>https://zhuanlan.zhihu.com/p/22692266</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者：向日葵&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;Logistic回归&lt;/h2&gt;&lt;p&gt;书接上回，在我们有了最小二乘法与极大似然估计做基础之后，这样我们就做好了Logistic回归的准备，渐渐的进入到我们的主题Logistic回归。 很多都属于分类的问题了，邮件（垃圾邮件/非垃圾邮件），肿瘤（良性/恶性）。二分类问题，可以用如下形式来定义它： y∈{0,1},其中0属于负例，1属于正例。 现在来构造一种状态，一个向量来代表肿瘤（良性/恶性）和肿瘤大小的关系。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-569e40432a36bb5c6524ec03d501892a.png" data-rawwidth="819" data-rawheight="413"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-56666b6e14927636bd71b71ba81d2e64.png" data-rawwidth="818" data-rawheight="302"&gt;Sigmoid 函数在有个很漂亮的“S"形，如下图所示（引自维基百科）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b6c0a14d298c4857cabc80bb27aecba1.png" data-rawwidth="738" data-rawheight="492"&gt;综合上述两式，我们得到逻辑回归模型的数学表达式：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8c8064514b0d41ebd9f3222d4fec169d.png" data-rawwidth="885" data-rawheight="355"&gt;Cost函数和J函数如下，它们是基于最大似然估计推导得到的。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2d30ec70ae197c7aba524cb38db64134.png" data-rawwidth="884" data-rawheight="197"&gt;下面详细说明推导的过程：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-2a3c65b90fd23715566dee0420567baf.png" data-rawwidth="882" data-rawheight="348"&gt;最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-36071a33031756d36876df600daaa644.png" data-rawwidth="885" data-rawheight="178"&gt;梯度下降法求的最小值 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-631267a13f028b13e3c75b07a8b963f2.png" data-rawwidth="886" data-rawheight="755"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d00dbcb3572b291fb75efe1cf84648d3.png" data-rawwidth="885" data-rawheight="133"&gt;&lt;h4&gt;向量化Vectorization&lt;/h4&gt;&lt;p&gt;Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。 如上式，Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。 &lt;/p&gt;&lt;p&gt;下面介绍向量化的过程： 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ce662d1d53f309092795720afdefdca1.png" data-rawwidth="881" data-rawheight="323"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7d70c69c5a409fa9c2357dce8910fcc8.png" data-rawwidth="884" data-rawheight="390"&gt;&lt;p&gt;Logistic回归的推导过程，采用的是极大似然法和梯度下降法取得各个参数的迭代过程。以后很多公式的推导也是类似这个过程，机器学习的过程大部分的算法都归结到概率论，如果概率论不是很熟，可以继续温习一下。所以很多人都在总觉，机器学习的问题，归宗到底就是概率论的问题。而采用极大似然的算法，其中隐藏着一个道理：求出来的参数会是最符合我们观察到的结果，实验数据决定了我们的参数。 &lt;/p&gt;&lt;h3&gt;TensorFlow下的Logistic回归&lt;/h3&gt;&lt;p&gt;现在有大量的机器学习的框架，个人开发者，大公司等都有。比较出名的还是FaceBook和谷歌的开源框架。 &lt;/p&gt;&lt;p&gt;TensorFlow是谷歌2015年开源的学习框架，结合了大量的机器学习的算法，官方的文档也比较清楚，开篇的初学者入门讲的就是关于Logistic回归的问题，这里简单的介绍一下，主要是想说明TensorFlow还是属于比较强大的工具，可以进行工具的学习。 &lt;/p&gt;&lt;p&gt;这篇文档的主要介绍如何使用TensorFlow识别MNIST，关于MNIST在之前神经网络的介绍有介绍过。MNIST里存放着一些手写的数据：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b1f5ef1030bc74cf5ed3396f0f2c64a9.png" data-rawwidth="486" data-rawheight="295"&gt;每个数字都可以用二进制向量数组来表示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f08519ed8e0c407856860f8e26d19251.png" data-rawwidth="573" data-rawheight="287"&gt;这些数据为神经网络的输入：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153.png" data-rawwidth="559" data-rawheight="240"&gt;&lt;p&gt;利用Logistic回归的训练求解上面的参数。 代码在&lt;a href="https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py" data-editable="true" data-title="githubusercontent.com 的页面"&gt;https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&lt;/a&gt;下可以自己参看。 &lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;这个章节里介绍了Logistic回归和推导的这个过程，Logistic回归是机器学习里最经常用到的算法，也是最基础的算法，通过推导Logistic回归就能够清楚机器学习的基础知识，后面有些算法的思想也和Logistic回归算法类似。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22692266&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 29 Sep 2016 12:21:00 GMT</pubDate></item><item><title>机器学习系列（一）:Logistic回归-我看你像谁（上篇）</title><link>https://zhuanlan.zhihu.com/p/22564500</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d065e83e77b87560b8256916a5ca7719_r.png"&gt;&lt;/p&gt;&lt;h2&gt;引子：&lt;/h2&gt;&lt;p&gt;依旧是一个“烧烤”的天气，我在的城市厦门，已经达到了37度的高温了，不过也好，只有这样的暑假，这样的天气才有时间和心思，静下心来思考一些东西。 &lt;/p&gt;&lt;p&gt;关于机器学习的教程确实是太多了，处于这种变革的时代，出去不说点机器学习的东西，都觉得自己落伍了，但总觉得网上的东西并不系统，无法让人串联在一起，总有很多人读了几篇机器学习的东西，就自以为机器学习就那些东西，认为机器学习也就那么一回事，想把这几年关于机器学习的东西做一些总结，能够跟大家一起学习和交流。 &lt;/p&gt;&lt;p&gt;如果需要用几句话来简单的总结机器学习是什么意思，也许可以用：让机器学会决策。对比我们人来说，每天都会碰到这个问题，比如菜市场里买芒果，总要挑出哪些是甜的。这就是所谓的决策，再通俗来讲就是分类问题了，把一堆芒果，分出甜和不甜的。而机器学习就是学会把甜和不甜的芒果分出来，那如何分呢？模拟人类的思考方式。凭经验，我们可以按照芒果皮的颜色，大小等来对芒果的酸甜进行分类，对于机器来说，把芒果的颜色，大小等当成变量输入到电脑模型里，就能推出芒果的酸甜性，这样就对芒果进行分类。 &lt;/p&gt;&lt;p&gt;机器学习的分类算法有非常的多，这篇主要介绍的是Logistic回归。&lt;/p&gt;&lt;h2&gt;最小二乘法和极大似然估计&lt;/h2&gt;&lt;p&gt;从一个最简单的数学问题开始。 &lt;/p&gt;&lt;p&gt;1801年，意大利天文学家朱赛普·皮亚齐发现了第一颗小行星谷神星。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥尔伯斯根据高斯计算出来的轨道重新发现了谷神星。 &lt;/p&gt;&lt;p&gt;从这段历史记录可以看出，高斯当时观察了很多小行星谷神星的记录，也就是我们常说的观察数据，并使用了最小二乘法模拟了这条线，预测了小行星谷神星的轨迹。 &lt;/p&gt;&lt;p&gt;高斯最小二乘法的方法发表于1809年他的著作《天体运动论》中。其实在1806年法国科学家勒让德就提出了最小二乘法相应的想法，所以勒让德曾与高斯为谁最早创立最小二乘法原理发生争执。 &lt;/p&gt;&lt;p&gt;最小二乘法的思想是什么呢？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-98ba3a21018f105332eb9f681bf59332.png" data-rawwidth="593" data-rawheight="450"&gt;&lt;p&gt;假设观察数据(x&lt;em&gt;1,y&lt;/em&gt;1 ), (x&lt;em&gt;2,y&lt;/em&gt;2 ), (x&lt;em&gt;3,y&lt;/em&gt;3 )…,(x&lt;em&gt;n,y&lt;/em&gt;n ),而默认这些数据是符合最常见的规律，既x和y符合线性关系，用方程可以表示为 y=a+bx &lt;/p&gt;&lt;p&gt;其中，a,b是我们需要通过观察数据确定的参数。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bbb5854ec9c8e6a90510ca229a4091bb.png" data-rawwidth="578" data-rawheight="429"&gt;&lt;p&gt;所谓最小二乘法就是这样的一个法则，按照这样法则，最好是拟合于各个数据点的最佳曲线应该使个数据点与曲线偏差的平均和为最小。 &lt;/p&gt;&lt;p&gt;用数学公式表示为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-779ad18638f92dec4c93cb8354d5cf12.png" data-rawwidth="551" data-rawheight="176"&gt;&lt;p&gt;其公式的值为最小，这里的a,b是参数。有两个参数，求最小值，即为求偏差平方和对a和b分别求出偏导数，得：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cb4b6370947f742108e3d596160e72fc.png" data-rawwidth="743" data-rawheight="239"&gt;&lt;/p&gt;&lt;p&gt;根据公式可以推出a和b的值： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-417a869e7ba51d488de65b1f910a7ecc.png" data-rawwidth="542" data-rawheight="504"&gt;&lt;p&gt;这样就可以求出了a和b的值。 &lt;/p&gt;&lt;p&gt;既我们可以通过观察的数据，来拟合我们的直线，既可以在给定某个x，有效的预测y。通常来说求出的值a和b跟实际本身来说是有一定的误差了，是不是给定的观察值越多就越准确呢？这不一定，这也是大学概率论和数值统计中一直讨论的问题。 &lt;/p&gt;&lt;p&gt;再次的强调最小二乘法是使用误差最小来进行估计的。 &lt;/p&gt;&lt;p&gt;回头过来看，可能会觉得最小二乘法跟我们讨论中的芒果酸甜问题，并不是一回事。但从另外一种退化的角度来讲： &lt;/p&gt;&lt;p&gt;在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。 &lt;/p&gt;&lt;p&gt;继续一个简单的故事：某位同学与一位猎人一起出去打猎，一只兔子从前方窜过。只听见一声枪响，野兔应声倒下，如果要你来推测，这一发命中的子弹是谁打的？你会怎么想呢？正常的情况下，猎人的枪法肯定比你的同学的枪法好，也就是说猎人的命中率比你的同学高。 而一枪就打死兔子，命中率是100%的，这么高的命中率，应该是谁打中的呢？显然，猎人开的枪比较符合我们观察的想象了吧。如果是开了三枪才打中兔子的话，那枪法就不怎么样了，某同学开的枪比较符合已经发生的现象了。 &lt;/p&gt;&lt;p&gt;这就是要讲的，极大似然法。如果试验n次，我们得到n个样本，极大似然估计是要是所求的概率，最大限制的符合我们现在所发生的。这里我们这样定义似然函数：假设{y&lt;em&gt;1,…,y&lt;/em&gt;n }为独立同分布，则样本数据的联合密度函数为 f(y&lt;em&gt;1;θ)f(y&lt;/em&gt;2;θ)∙∙∙f(y_n;θ)，定义“似然函数”为， &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f21f5222704a5ce6b99dce3cd20cf0d9.png" data-rawwidth="594" data-rawheight="243"&gt;&lt;p&gt;为了较好的说明，举一个很简单的例子：两点分布的情况。设某工序生产的产品不合格率为p，抽n个产品作检验，发现有T个不合格，试求p的极大似然估计值。在这里我们做了n次的试验，我们所求的概率p要符合我们试验的结果，也就是通过极大似然函数来求解。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-cb8315a2372d40b85f67619302818fa1.png" data-rawwidth="441" data-rawheight="218"&gt;&lt;/p&gt;&lt;p&gt;最大似然函数的思想也就是想使我们求得的概率符合我们所观察的。而最大似然法看起来，好像只是为了求得某个概率，但恰恰是我们Logistic回归中用到的一种方法。 了解了最小二乘法与极大似然估计之后，我们暂时先休息一下，在下半部分我们会进入正题介绍 Logistic 回归，以及简单介绍 Logistic 在 TensorFlow 中的用法。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22564500&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 22 Sep 2016 10:34:28 GMT</pubDate></item><item><title>永不过时的 K 均值算法</title><link>https://zhuanlan.zhihu.com/p/22431592</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3e1438ca8892ad5da4011ccf51e8b032_r.jpg"&gt;&lt;/p&gt;&lt;h3&gt;引言&lt;/h3&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/3e1438ca8892ad5da4011ccf51e8b032.jpg" data-rawwidth="561" data-rawheight="420"&gt;&lt;p&gt;众所周知，数据挖掘算法并非十全十美，在某些情况下他们也会失效。 使用 K 均值算法（K-Means）时就可能会出现这种情况，当然此时你可以尝试一下另一种方法—— K 中心聚类算法（K-Medoids），也许效果会更好。&lt;/p&gt;&lt;p&gt;在该网站之前的文章《揭开机器学习的面纱》中，已经指出， K 均值算法用于聚类时效果良好，而且在数据挖掘和机器学习领域，它也有着重要的地位。Psanchezcri 就曾在他的文章《将 K 均值方法用于金融时序回报率聚类》中，将 K 均值算法用于分析金融时间序列的趋势。&lt;/p&gt;&lt;p&gt;然而，即使在网络上有关算法的文档浩如烟海的情况下，关于机器学习算法有时会失效的讨论却并不多见。&lt;/p&gt;&lt;p&gt;因此，本文借由一个金融案例来反映这个问题。&lt;/p&gt;&lt;h3&gt;思路&lt;/h3&gt;&lt;p&gt;1）首先，我们在欧洲斯托克600指数的成分股中选择三组共6只股票（在三个不同的部门中各选两只）： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;金融部门：　　西班牙毕尔巴鄂比斯开银行 &amp;amp; 桑坦德银行&lt;/li&gt;&lt;li&gt;非必需消费品：　　法国酩悦·轩尼诗－路易·威登 &amp;amp; 迪奥&lt;/li&gt;&lt;li&gt;能源部门：　　英国石油公司 &amp;amp; 锡尼什港能源公司&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2）搜集数据，并绘出在2013/01/01至2015/12/31期间这六只股票的价格走势曲线。如下所示：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/99af96c71097923cdbf46af0bf7e01b3.png" data-rawwidth="640" data-rawheight="336"&gt;&lt;/p&gt;&lt;p&gt;3）选择日回报率作为计算指标，我们算出三组股票序列的相关距离。然后通过距离矩阵降维的方法，在二维欧氏空间中绘出每个点。&lt;/p&gt;&lt;p&gt;结果显示这六只股票可以按部门进行分类效果显著。下图以蓝色菱形点、绿色正方形点、红色圆点来标记六只股票，明显可以按部门分为三类：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/9f4441afc56a520c157a228a09780e1d.png" data-rawwidth="502" data-rawheight="292"&gt;&lt;p&gt;4）最后，我们将 K 均值算法运用于距离矩阵，聚类目标预先设定分成3类。由于 K 均值算法是从随机点开始的，每次运行结果可能有所不同，本文我们预先设定运行这个算法15次，即产生15个结果。当然，我们希望得到聚类结果符合股票所属部门的实际情况。 &lt;/p&gt;&lt;h3&gt;结论&lt;/h3&gt;&lt;p&gt;１）在约80%的聚类结果中，K 均值聚类算法取得了理想的结果，聚类结果与这六只股票所属部门相符，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/08af0acea4385818a84e736638e17ecd.png" data-rawwidth="502" data-rawheight="292"&gt;２）在剩下的20%的聚类结果中，算法则出现了聚类的错误。例如，下图中错将两个不同部门的四只股票聚为一类（图中蓝色菱形点和绿色正方形点），而将同一部门的两只股票分为两类（图中红色圆点）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b520bafdcbb1dbf8860df4439a73aece.png" data-rawwidth="502" data-rawheight="292"&gt;如果我们使用与之思想类似的 K 中心聚类算法，结果则可以达到100%的正确聚类率。这表明在聚类时，似乎使用重心会比用均值来衡量距离，效果更好。 &lt;/p&gt;&lt;p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接： &lt;a href="http://quantdare.com/2016/04/k-means-vs-k-medoids/" data-editable="true" data-title="“K-Means never fails”, they said…"&gt;“K-Means never fails”, they said…&lt;/a&gt;原文作者：Fjrodriguez2译者：Vector&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22431592&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 14 Sep 2016 10:27:57 GMT</pubDate></item><item><title>利用 Pandas 来分析 MovieLens 数据集</title><link>https://zhuanlan.zhihu.com/p/21268937</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/fd1ba61f2d32bc752f0bf181d43dbabe_r.png"&gt;&lt;/p&gt;&lt;h1&gt;利用 Pandas 来分析 MovieLens 数据集&lt;a class="" href="#%E5%88%A9%E7%94%A8-Pandas-%E6%9D%A5%E5%88%86%E6%9E%90-MovieLens-%E6%95%B0%E6%8D%AE%E9%9B%86" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;为了展现 Pandas 的实用性，本文将利用 Pandas 来解决 MovieLen 数据集的一些问题。我们首先回顾下如何将数据集读进 DataFrame 中并将其合并：&lt;/p&gt;In [1]:&lt;pre&gt;&lt;code lang="text"&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;In [2]:&lt;pre&gt;&lt;code lang="text"&gt;# pass in column names for each CSV
u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']
users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols,
                    encoding='latin-1')

r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']
ratings = pd.read_csv('ml-100k/u.data', sep='\t', names=r_cols,
                      encoding='latin-1')

# the movies file contains columns indicating the movie's genres
# let's only load the first five columns of the file with usecols
m_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']
movies = pd.read_csv('ml-100k/u.item', sep='|', names=m_cols, usecols=range(5),
                     encoding='latin-1')

# create one merged DataFrame
movie_ratings = pd.merge(movies, ratings)
lens = pd.merge(movie_ratings, users)
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;评价最多的 25 部电影&lt;a class="" href="#%E8%AF%84%E4%BB%B7%E6%9C%80%E5%A4%9A%E7%9A%84-25-%E9%83%A8%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;In [3]:&lt;pre&gt;&lt;code lang="text"&gt;most_rated = lens.groupby('title').size().sort_values(ascending=False)[:25]
most_rated
&lt;/code&gt;&lt;/pre&gt;Out[3]:&lt;pre&gt;&lt;code lang="text"&gt;title
Star Wars (1977)                             583
Contact (1997)                               509
Fargo (1996)                                 508
Return of the Jedi (1983)                    507
Liar Liar (1997)                             485
English Patient, The (1996)                  481
Scream (1996)                                478
Toy Story (1995)                             452
Air Force One (1997)                         431
Independence Day (ID4) (1996)                429
Raiders of the Lost Ark (1981)               420
Godfather, The (1972)                        413
Pulp Fiction (1994)                          394
Twelve Monkeys (1995)                        392
Silence of the Lambs, The (1991)             390
Jerry Maguire (1996)                         384
Chasing Amy (1997)                           379
Rock, The (1996)                             378
Empire Strikes Back, The (1980)              367
Star Trek: First Contact (1996)              365
Back to the Future (1985)                    350
Titanic (1997)                               350
Mission: Impossible (1996)                   344
Fugitive, The (1993)                         336
Indiana Jones and the Last Crusade (1989)    331
dtype: int64&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上述代码的含义是先将 DataFrame 按电影标题分组，接下来利用 &lt;em&gt;size&lt;/em&gt; 方法计算每组样本的个数，最后按降序方式输出前 25 条观测值。&lt;/p&gt;&lt;p&gt;在 SQL 中，等价的代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, count(1)
FROM lens
GROUP BY title
ORDER BY 2 DESC
LIMIT 25;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，在 Pandas 中有一个非常好用的替代函数—— &lt;em&gt;value_counts&lt;/em&gt;：&lt;/p&gt;In [4]:&lt;pre&gt;&lt;code lang="text"&gt;lens.title.value_counts()[:25]
&lt;/code&gt;&lt;/pre&gt;Out[4]:&lt;pre&gt;&lt;code lang="text"&gt;Star Wars (1977)                             583
Contact (1997)                               509
Fargo (1996)                                 508
Return of the Jedi (1983)                    507
Liar Liar (1997)                             485
English Patient, The (1996)                  481
Scream (1996)                                478
Toy Story (1995)                             452
Air Force One (1997)                         431
Independence Day (ID4) (1996)                429
Raiders of the Lost Ark (1981)               420
Godfather, The (1972)                        413
Pulp Fiction (1994)                          394
Twelve Monkeys (1995)                        392
Silence of the Lambs, The (1991)             390
Jerry Maguire (1996)                         384
Chasing Amy (1997)                           379
Rock, The (1996)                             378
Empire Strikes Back, The (1980)              367
Star Trek: First Contact (1996)              365
Titanic (1997)                               350
Back to the Future (1985)                    350
Mission: Impossible (1996)                   344
Fugitive, The (1993)                         336
Indiana Jones and the Last Crusade (1989)    331
Name: title, dtype: int64&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;评价最高的电影&lt;a class="" href="#%E8%AF%84%E4%BB%B7%E6%9C%80%E9%AB%98%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;In [6]:&lt;pre&gt;&lt;code lang="text"&gt;movie_stats = lens.groupby('title').agg({'rating': [np.size, np.mean]})
movie_stats.head()
&lt;/code&gt;&lt;/pre&gt;Out[6]:ratingsizemeantitle'Til There Was You (1997)92.3333331-900 (1994)52.600000101 Dalmatians (1996)1092.90825712 Angry Men (1957)1254.344000187 (1997)413.024390&lt;p&gt;我们可以利用 &lt;em&gt;agg&lt;/em&gt; 方法来进行分组汇总计算，其参数包括键值和汇总方法。接下来我们对汇总结果进行排序即可得到评价最高的电影：&lt;/p&gt;In [7]:&lt;pre&gt;&lt;code lang="text"&gt;# sort by rating average
movie_stats.sort_values([('rating', 'mean')], ascending=False).head()
&lt;/code&gt;&lt;/pre&gt;Out[7]:ratingsizemeantitleThey Made Me a Criminal (1939)15Marlene Dietrich: Shadow and Light (1996)15Saint of Fort Washington, The (1993)25Someone Else's America (1995)15Star Kid (1997)35&lt;p&gt;由于 &lt;em&gt;movie_stats&lt;/em&gt; 是一个 DataFrame，因此我们可以利用 &lt;em&gt;sort&lt;/em&gt; 方法来排序——Series 对象则使用&lt;em&gt;order&lt;/em&gt; 方法。此外，由于该数据集包含多层索引，所以我们需要传递一个元组数据来指定排序变量。&lt;/p&gt;&lt;p&gt;上表列出来的电影中评价数量都非常少，以致于我们无法从中得到一些有价值的信息。因此我们考虑对数据集进行筛选处理，只分析评价数量大于 100 的电影：&lt;/p&gt;In [9]:&lt;pre&gt;&lt;code lang="text"&gt;atleast_100 = movie_stats['rating']['size'] &amp;gt;= 100
movie_stats[atleast_100].sort_values([('rating', 'mean')], ascending=False)[:15]
&lt;/code&gt;&lt;/pre&gt;Out[9]:ratingsizemeantitleClose Shave, A (1995)1124.491071Schindler's List (1993)2984.466443Wrong Trousers, The (1993)1184.466102Casablanca (1942)2434.456790Shawshank Redemption, The (1994)2834.445230Rear Window (1954)2094.387560Usual Suspects, The (1995)2674.385768Star Wars (1977)5834.35849112 Angry Men (1957)1254.344000Citizen Kane (1941)1984.292929To Kill a Mockingbird (1962)2194.292237One Flew Over the Cuckoo's Nest (1975)2644.291667Silence of the Lambs, The (1991)3904.289744North by Northwest (1959)1794.284916Godfather, The (1972)4134.283293&lt;p&gt;这个结果看起来比较靠谱，需要注意的是在这里我们利用布尔索引来筛选数据。&lt;/p&gt;&lt;p&gt;在SQL中，等价的代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, COUNT(1) size, AVG(rating) mean
FROM lens
GROUP BY title
HAVING COUNT(1) &amp;gt;= 100
ORDER BY 3 DESC
LIMIT 15;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;筛选部分数据&lt;a class="" href="#%E7%AD%9B%E9%80%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;为了便于进一步分析，我们从数据集中筛选出评价数最高的 50 部电影：&lt;/p&gt;In [10]:&lt;pre&gt;&lt;code lang="text"&gt;most_50 = lens.groupby('movie_id').size().sort_values(ascending=False)[:50]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;SQL 中的等价代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;CREATE TABLE most_50 AS(
    SELECT movie_id, COUNT(1)
    FROM lens
    GROUP BY movie_id
    ORDER BY 2 DESC
    LIMIT 50
);&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，我们也可以利用 EXISTS, IN 或者 JOIN 来过滤数据：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT *
FROM lens
WHERE EXISTS (SELECT 1 FROM most_50 WHERE lens.movie_id = most_50.movie_id);&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;不同年龄段观众之间争议最大的电影&lt;a class="" href="#%E4%B8%8D%E5%90%8C%E5%B9%B4%E9%BE%84%E6%AE%B5%E8%A7%82%E4%BC%97%E4%B9%8B%E9%97%B4%E4%BA%89%E8%AE%AE%E6%9C%80%E5%A4%A7%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;首先，我们来看下数据集中用户的年龄分布情况：&lt;/p&gt;In [16]:&lt;pre&gt;&lt;code lang="text"&gt;users.age.plot.hist(bins=30)
plt.title("Distribution of users' ages")
plt.ylabel("count of users")
plt.xlabel("age");
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pandas 整合了 matplotlib 的基础画图功能，我们只需要对列变量调用 &lt;em&gt;hist&lt;/em&gt; 方法即可绘制直方图。当然了，我们也可以利用 matplotlib.pyplot 来自定义绘图。&lt;/p&gt;&lt;h2&gt;对用户进行分箱处理&lt;a class="" href="#%E5%AF%B9%E7%94%A8%E6%88%B7%E8%BF%9B%E8%A1%8C%E5%88%86%E7%AE%B1%E5%A4%84%E7%90%86" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我认为直接对比不同年龄用户的行为无法得到有价值的信息，所以我们应该根据用户的年龄情况利用&lt;em&gt;pandas.cut&lt;/em&gt; 将所有用户进行分箱处理。&lt;/p&gt;In [17]:&lt;pre&gt;&lt;code lang="text"&gt;labels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79']
lens['age_group'] = pd.cut(lens.age, range(0, 81, 10), right=False, labels=labels)
lens[['age', 'age_group']].drop_duplicates()[:10]
&lt;/code&gt;&lt;/pre&gt;Out[17]:ageage_group06060-693972120-294593330-395243030-397822320-299952920-2912292620-2916643130-3919422420-2922703230-39&lt;p&gt;上述代码中，我们首先创建分组标签名，然后根据年龄变量将用户分成八组（0-9, 10-19, 20-29,...）,其中参数 &lt;em&gt;right=False&lt;/em&gt; 用于剔除掉区间上界数据，即30岁的用户对应的标签为 30-39。&lt;/p&gt;&lt;p&gt;现在我们可以比较不同年龄组之间的评分情况：&lt;/p&gt;In [18]:&lt;pre&gt;&lt;code lang="text"&gt;lens.groupby('age_group').agg({'rating': [np.size, np.mean]})
&lt;/code&gt;&lt;/pre&gt;Out[18]:ratingsizemeanage_group0-9433.76744210-1981813.48612620-29395353.46733330-39256963.55444440-49150213.59177250-5987043.63580060-6926233.64887570-791973.649746&lt;p&gt;从上表中我们可以看出，年轻用户比其他年龄段的用户更加挑剔。接下来让我们看下这 50 部热评电影中不同年龄组用户的评价情况。&lt;/p&gt;In [19]:&lt;pre&gt;&lt;code lang="text"&gt;lens.set_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [21]:&lt;pre&gt;&lt;code lang="text"&gt;by_age = lens.loc[most_50.index].groupby(['title', 'age_group'])
by_age.rating.mean().head()
&lt;/code&gt;&lt;/pre&gt;Out[21]:&lt;pre&gt;&lt;code lang="text"&gt;title                 age_group
Air Force One (1997)  10-19        3.647059
                      20-29        3.666667
                      30-39        3.570000
                      40-49        3.555556
                      50-59        3.750000
Name: rating, dtype: float64&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要注意的是，此处的电影标题和年龄组都是索引值，平均评分为 Series 对象。如果你觉得这个展示结果不直观的话，我们可以利用 &lt;em&gt;unstack&lt;/em&gt; 方法将其转换成表格形式。&lt;/p&gt;In [27]:&lt;pre&gt;&lt;code lang="text"&gt;by_age.rating.mean().unstack(1).fillna(0)[10:20]
&lt;/code&gt;&lt;/pre&gt;Out[27]:age_group0-910-1920-2930-3940-4950-5960-6970-79titleE.T. the Extra-Terrestrial (1982)03.6800003.6090913.8068184.1600004.3684214.3750000.000000Empire Strikes Back, The (1980)44.6428574.3116884.0520834.1000003.9090914.2500005.000000English Patient, The (1996)53.7391303.5714293.6218493.6346153.7746483.9047624.500000Fargo (1996)03.9375004.0104714.2307694.2941184.4423084.0000004.333333Forrest Gump (1994)54.0476193.7857143.8617023.8478264.0000003.8000000.000000Fugitive, The (1993)04.3200003.9699253.9814814.1904764.2400003.6666670.000000Full Monty, The (1997)03.4210534.0568183.9333333.7142864.1463414.1666673.500000Godfather, The (1972)04.4000004.3450704.4128443.9294124.4634154.1250000.000000Groundhog Day (1993)03.4761903.7982463.7866673.8510643.5714293.5714294.000000Independence Day (ID4) (1996)03.5952383.2914293.3893813.7187503.8888892.7500000.000000&lt;p&gt;&lt;em&gt;unstack&lt;/em&gt; 方法主要用于拆分多层索引，此例中我们将移除第二层索引然后将其转换成列向量，并用 0 来填补缺失值。&lt;/p&gt;&lt;h2&gt;男士与女士分歧最大的电影&lt;a class="" href="#%E7%94%B7%E5%A3%AB%E4%B8%8E%E5%A5%B3%E5%A3%AB%E5%88%86%E6%AD%A7%E6%9C%80%E5%A4%A7%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;首先思考下你会如何利用 SQL 来解决这个问题，你可能会利用判断语句和汇总函数来旋转你的数据集，你的查询语句大概会是这个样子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, AVG(IF(sex = 'F', rating, NULL)), AVG(IF(sex = 'M', rating, NULL))
FROM lens
GROUP BY title;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;想象下，如果你必须处理多列数据的话，这样运算是多么的麻烦。DataFrame 提供了一个简便的方法—— &lt;em&gt;pivot_table&lt;/em&gt; 。&lt;/p&gt;In [29]:&lt;pre&gt;&lt;code lang="text"&gt;lens.reset_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [30]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted = lens.pivot_table(index = ['movie_id', 'title'], columns = ['sex'],
                          values = 'rating', fill_value = 0)
pivoted.head()
&lt;/code&gt;&lt;/pre&gt;Out[30]:sexFMmovie_idtitle1Toy Story (1995)3.7899163.9099102GoldenEye (1995)3.3684213.1785713Four Rooms (1995)2.6875003.1081084Get Shorty (1995)3.4000003.5914635Copycat (1995)3.7727273.140625In [31]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted['diff'] = pivoted.M - pivoted.F
pivoted.head()
&lt;/code&gt;&lt;/pre&gt;Out[31]:sexFMdiffmovie_idtitle1Toy Story (1995)3.7899163.9099100.1199942GoldenEye (1995)3.3684213.178571-0.1898503Four Rooms (1995)2.6875003.1081080.4206084Get Shorty (1995)3.4000003.5914630.1914635Copycat (1995)3.7727273.140625-0.632102In [32]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted.reset_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [34]:&lt;pre&gt;&lt;code lang="text"&gt;disagreements = pivoted[pivoted.movie_id.isin(most_50.index)]['diff']
disagreements.sort_values().plot(kind='barh', figsize=[9, 15])
plt.title('Male vs. Female Avg. Ratings\n(Difference &amp;gt; 0 = Favored by Men)')
plt.ylabel('Title')
plt.xlabel('Average Rating Difference');
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上图中我们可以看出，男性喜欢《终结者》的程度远高于女性，女性用户则更喜欢《独立日》。这个结果可靠吗？&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/"&gt;http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：Greg Reda&lt;/p&gt;&lt;p&gt;译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21268937&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 08 Sep 2016 10:44:57 GMT</pubDate></item><item><title>Pandas中的链式方法</title><link>https://zhuanlan.zhihu.com/p/22238376</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c0de4843c8d39b533a4279958de366f0_r.png"&gt;&lt;/p&gt;&lt;p&gt;链式方法是当前比较流行的一种语法规则。 在过去的几个版本中，我们已经提到了几个支持链式方法的函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;assign (0.16.0): 用于往 DataFrame 中增加新变量(类似于 dplyr 中的 mutate 函数)&lt;/li&gt;&lt;li&gt;pipe (0.16.2): 用于包含用户自定义的链式方法&lt;/li&gt;&lt;li&gt;rename (0.18.0): 用于改变轴名称&lt;/li&gt;&lt;li&gt;Window methods (0.18): 利用类似于 groupby 的 API 接口调用 pd.rolling* 和 pd.expanding* 顶层函数的 NDFrame 方法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文将从一个简单的例子说起： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/7e4f72b9a6995046642344c62688d314.png" data-rawwidth="429" data-rawheight="710"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/9eb112f7c3631863fe3377a849445dd2.png" data-rawwidth="445" data-rawheight="480"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c55505d56eeebad2f1b0d43905c24fe0.png" data-rawwidth="633" data-rawheight="708"&gt;&lt;p&gt;我觉得链式方法的代码非常易读，但是有些人却并了解它。它并不像重嵌套函数那样循环调用参数，它的所有代码和流程都是自上而下运行的，这大大增强了代码的可读性。&lt;/p&gt;&lt;p&gt;我最喜欢的示例来自 Jeff Allen，比较以下这两段功能相同但风格迥异的代码：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d7179d23493012ad018483bf08842cc4.png" data-rawwidth="560" data-rawheight="174"&gt;&lt;/p&gt;&lt;p&gt;和&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0903e08ec50c54113a0e40f6d9f871d2.png" data-rawwidth="555" data-rawheight="130"&gt;&lt;p&gt;对比上述两种风格的代码，你会发现即使你不知道 R 语言中管道符号 %&amp;gt;% 的功能，你也能很轻易地看懂第二段代码。而对于第一段代码而言，你需要弄清楚代码的执行顺序以及如何处理相应的函数参数。 &lt;/p&gt;&lt;p&gt;作为读者，你可能会说你不会写出类似于重嵌套风格的代码，但是大多数情况下你的代码应该是如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dfef7df084846fa9da378a06c6e30fe6.png" data-rawwidth="563" data-rawheight="114"&gt;&lt;p&gt;我非常不喜欢这个风格的代码，因为我需要花费很多时间来思考如何对变量进行命名。这是非常令人困扰的事情，因为我们根本不关心 on_hill 这些中间变量。 &lt;/p&gt;&lt;p&gt;上述代码的第四种实现方法是可行的，假设你拥有一个 JackAndJill 对象并且你可以自定义一些方法。那么你可以实现类似于 R 语言中的管道功能：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/28dcbfd36824238ab0a85d9181d0130c.png" data-rawwidth="560" data-rawheight="158"&gt;&lt;/p&gt;&lt;p&gt;但是这种方法的问题在于如果的数据不是 ndarray 或者 DataFrame 或者 DataArray，那么上述的方法就不存在了。而且我们很难对 DataFrame 的子类进行拓展从而来适应自定义的方法。同时，你所创建的从 DataFrame 中继承的子类可能仅适用于你自己的代码，无法和其他方法进行交互操作，因此你的代码将会非常零散。 &lt;/p&gt;&lt;p&gt;或者你可以往 pandas 的项目中提交新的 pull request，从而实现自己的方法。但是你需要说服该项目的维护者，你的新方法值得加入到该项目中并维护之。而且 DataFrame 目前已经拥有超过 250 种的方法，因此我们不愿意增加更多的方法。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/3646050ed15aa39528cf0ebecd3c9f82.png" data-rawwidth="632" data-rawheight="151"&gt;DataFrame.pipe 的第一个参数是 DataFrame，我们只需要指明后续的参数即可。&lt;/p&gt;&lt;h2&gt;成本&lt;/h2&gt;&lt;p&gt;过长的链式代码的缺点是调试比较麻烦。由于没有生成中间变量值，所以如果代码出问题了，我们无法直接定位出问题在哪。Python 中的生成器也有类似的问题，借助生成器机制我们可以降低计算机内存消耗，但是此时我们比较难调试程序。 &lt;/p&gt;&lt;p&gt;就我常用的探索分析过程而言，这并不是一个大问题。我平常处理的都是不会再更新的数据集，而且对原始数据集进行加工的步骤也不多。 &lt;/p&gt;&lt;p&gt;对于规模较大的工作流程，你可能需要借助 pandas 的其他功能，比如 Airflow 或者 Luigi。 对于需要重复运行的中等规模 ETL 工作流程，我将借助装饰器来审查 DataFrame 每个工作步骤所产生的属性日志。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/dabaa7560ef4bd132f3551579a96834f.png" data-rawwidth="565" data-rawheight="659"&gt;&lt;/p&gt;&lt;p&gt;借助我之前制作的一个用于验证管道中数据集有效性的软件库 engarde，我们可以很好地完成工作。 &lt;/p&gt;&lt;h2&gt;Inplace?&lt;/h2&gt;&lt;p&gt;大多数 pandas 的方法都有一个默认值为 False 的关键词 inplace。通常来说，你不应该做 inplace 运算。&lt;/p&gt;&lt;p&gt;首先，如果你喜欢用链式规则来写代码的话，你肯定不会用 inplace 运算，因为这会导致最终返回的结果是 None，并中断相应的管道链。 &lt;/p&gt;&lt;p&gt;其次，我怀疑存在一个适合 inplace 运算的构思模型。也就是说，最终结果并不会被分配到额外的存储器中。但实际上这可能是不真实的，pandas 中还存在许多下述用法：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/46de628b0a4c3cc4fc71b0f6a5d66af3.png" data-rawwidth="563" data-rawheight="159"&gt;&lt;/p&gt;&lt;p&gt;最后，类似于 ibis 或者 dask 这种类型的项目 inplace 运算并没有任何意义，因为此时你需要处理表达式或者建立可执行的 DAG 任务，而不仅仅是处理数据而已。 &lt;/p&gt;&lt;p&gt;我觉得到此为止我并没有怎么写代码，更多的是在介绍一些额外的东西，我对此感到非常抱歉。接下来，让我们做一些探索性分析吧。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/bbdd2d512287764ca2e69df997eb7792.png" data-rawwidth="496" data-rawheight="91"&gt;一架一天执行多趟航班执飞任务的飞机“堵机”了，会导致靠后的航班延误更长时间吗？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/acfa569ec544481c13d5d579fd7ccc7a.png" data-rawwidth="484" data-rawheight="318"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/7976cf95b8b0b1fc9d025d4c626e1861.png" data-rawwidth="908" data-rawheight="339"&gt;一天中较晚起飞的航班会延误更长时间吗？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/f0e1301cc1a1d6fac776ebc66a8e292a.png" data-rawwidth="483" data-rawheight="231"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/13910c8a27b2993b9b0cb84844c05a78.png" data-rawwidth="901" data-rawheight="339"&gt;我们将延误超过十小时的数据视为异常值并将其剔除掉。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/57165e540bbf3b950a1e3f1c41c7ea5d.png" data-rawwidth="485" data-rawheight="134"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/92c31f96b9604638b4090bd7e9f03195.png" data-rawwidth="908" data-rawheight="339"&gt;接下来，我们仅考虑确实发生延误的航班数据。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/aeadc5bed6b3d5662f023524507734c9.png" data-rawwidth="480" data-rawheight="127"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/13910c8a27b2993b9b0cb84844c05a78.png" data-rawwidth="901" data-rawheight="339"&gt;哪个航班的延误情况最严重呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6bbabbca6b9356558c7420998b14be84.png" data-rawwidth="484" data-rawheight="226"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/a96186c8c288cbb7f4547c9940c164cd.png" data-rawwidth="506" data-rawheight="362"&gt;哪个航空公司的延误情况最严重呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7ea2ec9a136b4ced2f4fd6ea598561f2.png" data-rawwidth="544" data-rawheight="98"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d7ed93db045bfd74d1e66492bd42a3f6.png" data-rawwidth="651" data-rawheight="434"&gt;&lt;/p&gt;&lt;p&gt;B6 是美国捷蓝航空公司。 &lt;/p&gt;&lt;p&gt;I wanted to try out scikit-learn's new Gaussian Process module so here's a pretty picture.&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4269e43cbb28ccf025bbec763e907482.png" data-rawwidth="513" data-rawheight="867"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/633c8f67ae06d7753029ba6b8b5e621b.png" data-rawwidth="561" data-rawheight="226"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d9f5d39a24f83f315b9e6d8d8b8339cb.png" data-rawwidth="380" data-rawheight="211"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d408a1cd7f6250e7a3885144583e9635.png" data-rawwidth="561" data-rawheight="366"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/96185ff7b0539cb04b978beab12d132a.png" data-rawwidth="561" data-rawheight="527"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1b229c6f7e3bed45edd29278dc4af5fb.png" data-rawwidth="845" data-rawheight="412"&gt;&lt;/p&gt;&lt;p&gt;谢谢阅读本文！由于我们更多地讨论了关于代码风格的问题而不是介绍实际案例操作，所以本文所介绍的内容比较抽象。谢谢你们的包容，下次我将介绍一个偏实务的话题！&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接：&lt;a href="http://tomaugspurger.github.io/method-chaining.html" data-editable="true" data-title="DatasFrame"&gt;DatasFrame&lt;/a&gt;原文作者：Tom Augspurger译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22238376&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 01 Sep 2016 10:57:52 GMT</pubDate></item><item><title>在python 中如何将 list 转化成 dictionary</title><link>https://zhuanlan.zhihu.com/p/21981759</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0658b34b4498516ba1014e6fe6490b88_r.jpg"&gt;&lt;/p&gt;&lt;h4&gt;问题1：如何将一个list转化成一个dictionary？&lt;/h4&gt;问题描述：比如在python中我有一个如下的list，其中奇数位置对应字典的key，偶数位置为相应的value&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c505279f860a835ad201780d3a663ddd.png" data-rawwidth="361" data-rawheight="198"&gt;解决方案:&lt;p&gt;1.利用zip函数实现&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cb7209edd847833afc81b01e45b0f87c.png" data-rawwidth="361" data-rawheight="218"&gt;2.利用循环来实现&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f3c0e4da9add83554be645fd93f214cb.png" data-rawwidth="371" data-rawheight="261"&gt;3.利用 enumerate 函数生成index来实现&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0efd2a1e482e594943ec765e95832a02.png" data-rawwidth="371" data-rawheight="277"&gt;&lt;h4&gt;问题2 我们如何将两个list 转化成一个dictionary？&lt;/h4&gt;问题描述：假设你有两个list&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d1e4cb13992b6bebf444345205b5a015.png" data-rawwidth="495" data-rawheight="210"&gt;解决方案：还是常见的zip函数&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/3b9ab721966176dd65daccfa32835d8d.png" data-rawwidth="365" data-rawheight="219"&gt;这里我们看到了zip函数确实在配对上面起到了很不错的效果，如果两个list都很大，你需要引入itertools.izip来解决问题。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/79fe85b7f1c81366a965acc303ae8da5.png" data-rawwidth="363" data-rawheight="164"&gt;或者下面的直接使用dict函数&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/381d4be7e04ae723a86fd2d8b7432454.png" data-rawwidth="362" data-rawheight="362"&gt;&lt;/p&gt;那么如果我们有三个lsit呢？比如我们有时候会遇到这样的问题比如在一个经纬度下面记录某个数据，这个时候又该怎么实现呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7c3300f15f1f32f401f92e9ec1f70054.png" data-rawwidth="365" data-rawheight="339"&gt;&lt;p&gt;我们可以看到这个时候 zip函数还是可以帮助我们成功的实现所需要的功能，首先将经纬度一一配对整合到一起，随后再将val连起来，最后使用dict函数放在一起。&lt;/p&gt;&lt;p&gt;通过上面的例子，我们知道可以通过zip函数的多次调用来整合数据，最终解决问题。&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21981759&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Fri, 12 Aug 2016 11:21:29 GMT</pubDate></item><item><title>一个计算我的妻子是否怀孕的贝叶斯模型</title><link>https://zhuanlan.zhihu.com/p/21956061</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cb124fbccfd34346eaf63e59429767be_r.png"&gt;&lt;/p&gt;&lt;p&gt;在2015年的二月21日，我的妻子已经33天没有来月经了，她怀孕了，这真是天大的好消息！通常月经的周期是大约一个月，如果你们夫妇打算怀孕，那么月经没来或许是一个好消息。但是33天，这还无法确定这是一个消失的月经周期，或许只是来晚了，那么它是否真的是一个好消息？  &lt;/p&gt;&lt;p&gt;为了能获得结论我建立了一个简单的贝叶斯模型，基于这个模型，可以根据你当前距离上一次经期的天数、你历史经期的起点数据来计算在当前经期周期中你怀孕的可能性。在此篇文章中我将阐述我所使用的数据、先验思想、模型假设以及如何使用重点抽样法获取数据并用R语言运算出结果。在最后，我将解释为什么模型的运算结果最终并不重要。另外，我将附上简便的脚本以供读者自行计算.&lt;/p&gt;&lt;h2&gt;数据&lt;/h2&gt;&lt;p&gt;非常幸运的是，在2014年的下半年间我的妻子一直在记录她经期起始日期，否则我只能以仅拥有小量数据而告终。总体上我们拥有8个经期的起始日期数据，但是我采用的数据不是日期而是相邻经期起始日间相隔的天数。 已经有33天。  &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/db5d823ab1f049cc29c65cbda9d52464.png" data-rawwidth="829" data-rawheight="556"&gt;&lt;/p&gt;&lt;p&gt;所以日期发生得相对规律，以28天为一个周期循环。最后一次月经开始日期是在1月19日，所以在2月21日，距离最后一次经期发生日。&lt;/p&gt;&lt;h2&gt;模型的建立&lt;/h2&gt;&lt;p&gt;我要建立一个涵盖生理周期的模型，包括受孕期和不受孕期，这显然需要做大量的简化。我做了一些&lt;strong&gt;总体假设&lt;/strong&gt;如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一对情侣受孕与否不受其他因素的影响。&lt;/li&gt;&lt;li&gt;女方拥有固定的经期。&lt;/li&gt;&lt;li&gt;该对想要受孕的夫妻正在积极地尝试受孕。换言之，如Wilcox et al. (2000) 推荐的每周两次到三次受精。&lt;/li&gt;&lt;li&gt;一旦怀孕，期间将不会有经期。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;接下来是我所做的具体假设：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;假设两个相邻经期间隔的天数（days_between_periods）服从的正态分布，其中均值（mean_period）和标准差（sd_period）未知。&lt;/li&gt;&lt;li&gt;假设在一对生育能力的夫妻（is_fertile为真 ）受孕时一个周期内怀上孕的概率是0.19（更多关于选定该值的由来见参考文献）。不幸的是，并非所有的夫妻都具备生育能力，没有生育能力则怀孕的几率为零。如果生育率被编码为 0-1，那么可生育率可以被简洁的写为 0.19* is_fertile.&lt;/li&gt;&lt;li&gt;在某一些不能受孕的时期（n_non_pregnant_periods）的怀孕失败率则为(1 - 0.19 * is_fertile)^n_non_pregnant_periods&lt;/li&gt;&lt;li&gt;最后，如果你在这一个周期内（从上一次生理期至这一次生理期为一个周期）将不会怀孕；那么最新一次经期距离下一个经期的天数（next_period）将必然会大于最新一次经期距离当前日期的天数（days_since_last_perio）。即，next_period &amp;lt; days_since_last_period的概率为零。这么做看上去很奇怪因为这个事件是显然的，但是我们在模型中将会要用到它。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;基本的假设就是这样了。但是为了使其更加实际，需要考虑使用一个似然函数，一个给定了参数和一些数据、计算在给定参数下数据的概率，通常而言是一个与概率成正比例的数值——似然值。因为这个似然值可能极小所以我需要对其取对数，从而避免引起数值问题。当用R语言设计似然函数时，总体上的模式如下：  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;方程将数据和参数作为选项。&lt;/li&gt;&lt;li&gt;通过预处理，将似然值的初始值设为1.0，相应的对数为0.0。（log_like &amp;lt;- 0.0）&lt;/li&gt;&lt;li&gt;用R语言调用概率密度分布函数（比如dnorm, dbinom and dpois），用该函数计算模型中不同部分的似然值。然后将这些似然值相乘。对应地，将取对数后的似然值log_like相加。&lt;/li&gt;&lt;li&gt;为了让d*函数返回对数似然值，只需添加参数log=TRUE。并且注意似然值0.0对应的取对值为-inf  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以，综上所述该模型的对数似然函数如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/80d15aa464ab58b8d28a59b0b777afa7.png" data-rawwidth="1087" data-rawheight="455"&gt;这里数据有标量days_since_last_period以及向量days_between_periods，而其他的参数将会被被估计出来。使用这个函数，我能从任意一个数据+参数的组合中得出对数似然函数值。但是，到这里我只完成了建模的一半工作，我还需要先验信息！ &lt;/p&gt;&lt;p&gt;关于经期，受孕和生育的先验信息&lt;/p&gt;&lt;p&gt;为了完善这个模型，我需要所有参数的先验信息。换言之，我需要明确在获取数据之前这个模型包含了哪些信息。具体上，我需要实验开始前mean_period, sd_period, is_fertile, and is_pregnant的初始值。（虽然next_period也是一个参数，我不需要给出一个它的确切初始值，因为它的分布完全由mean_period 和sd_period确定。另外，我还需要找到在一个周期内能受孕的可能值（上文中我设定为0.19）。这里我使用了模糊、主观的数据吗？不！我到生育文献中去寻找了更加有信息价值的依据！   &lt;/p&gt;&lt;p&gt;对于days_between_periods的分布，其参数为mean_period和sd_period。这里我使用了来自文章The normal variabilities of the menstrual cycle Cole et al, 2009 中的估计值，该文测量了184个年龄来自18-36岁的女性的经期规律。相邻经期间天数的总平均值为27.7天。每一个参与实验者的标准差的平均值为2.4。总体样本的间隔天数的标准差为1.6。给定了这些估计值以后我令mean_period服从（27.7,2.4）的正态分布，令sd_period服从均值为1.6，标准差为2.05的半正态分布。如下：  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/52bbc1c7828f2c92789eb1b7561dbe63.png" data-rawwidth="957" data-rawheight="460"&gt;&lt;p&gt;对于参数is_fertile a以及参数is_pregnant我考虑了受精频率作为先验。想要确定可育的夫妻的比例几乎是不可能的事情，因为这里对于不育有各种不同的定义。 Van Geloven et al. (2013)做了一个小范围的文献回顾然后得出结论所有夫妻中有2%至5%的人被认为是不孕的。因为曾看到高达10%的情况，我决定取该范围的上限。设定初始数据100%-5%=95%的夫妻是可孕的。  &lt;/p&gt;&lt;p&gt;is_pregnant 是 0 1变量表示这对夫妻在最近的一轮周期中是否将要（或者说已经）受孕。在这里我使用的先验值是在一个周期内成功受孕的概率。当这对夫妇没有生育能力时这个概率值显然为0.0，但是积极地尝试、可育的夫妇在一个周期内成功受孕的比例有多大呢？不幸的是我并没有找到明确说明这一数据的文献，但是我找到了比较接近的参照依据。在Increased Infertility With Age in Men and Women Dunson et al. (2004) 一书的第53页，给出了在12个月中一直尝试受孕但是没有怀上的夫妻的比例，同时该数据也提供了女性不同年龄段的数据。  &lt;/p&gt;​&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7bdce6415e8c8927e5b3eb3f8e2967d6.png" data-rawwidth="821" data-rawheight="113"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8f6aaafdf2b660c3952d998324e59662.png" data-rawwidth="774" data-rawheight="276"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/e6cf04dcb58e4991a42f04b9ca5fb5bf.png" data-rawwidth="715" data-rawheight="171"&gt;故上述即为对数似然函数中19%的怀孕概率值的由来，19%亦作为is_pregnant的先验值。现在我有了所有参数的先验，可以建立一个由先验函数的抽样函数了。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5396232132a99a9bcf938e5dabe2fe5d.png" data-rawwidth="1007" data-rawheight="270"&gt;这里使用了一个参数(n)，它输出了一个n行的数据框，每一行是基于先验数值得出的样本数据。输出结果如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d5ae4a9df47d5eb709c20627efb393d7.png" data-rawwidth="824" data-rawheight="308"&gt;&lt;h2&gt;使用重要性抽样来拟合模型&lt;/h2&gt;&lt;p&gt;现在，我已经收集了贝叶斯统计分析的三大要素：先验信息，似然函数以及数据。为了拟合模型我有很多方法，但是这里有一个非常方便的方法——重要性抽样。我之前曾写文提及过重要性抽样法，这里我们来回顾一下：重要性抽样法是一种蒙特卡洛实验法，它建立起来非常简单并且适用于以下情况：（1）参数空间非常小（2）先验分布与后验分布的形式区别不大。因为我的参数空间比较小，加之我使用了信息量包含得比较丰富的先验数据。因此，认为重点抽样法在此例中是可用的。在重要性抽样法中三个基本的步骤为: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;由先验分布产生大样本（这里可以通过sample&lt;em&gt;from&lt;/em&gt;prior得到）&lt;/li&gt;&lt;li&gt;给定了参数时，对每一个与似然值成比例的先验数据进行赋权。（这里可以通过 calc&lt;em&gt;log&lt;/em&gt;like 得到）&lt;/li&gt;&lt;li&gt;将权重归一化，从而在先验分布的情况下形成了新的概率分布。最终，根据此概率分布对先验分布的样本进行重新抽样。（这里可以用R函数抽样）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;（ 注意存在与该过程不同的多种方法，但是在用来拟合贝叶斯模型时，这是重要性抽样法的常用版本） &lt;/p&gt;&lt;p&gt;因为我已经定义过 sample&lt;em&gt;from&lt;/em&gt;prior 和 calc&lt;em&gt;log&lt;/em&gt;like，因此需要定义一个新的方程来做后验抽样： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/96f1358269fcac79b7362a0a46e1e1ce.png" data-rawwidth="1094" data-rawheight="381"&gt;&lt;h2&gt;结果：怀孕的可能性&lt;/h2&gt;&lt;p&gt;因此，在2月21日，2015，我的妻子已经没有来月经33天了。这是一个好休息吗？让我们运行这个模型看看结果吧！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9958b54312e3800790e3e867b86ecadc.png" data-rawwidth="846" data-rawheight="92"&gt;post这里是一个长数据框，其中数值的表示基于这些参数得出的后验分布信息。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0d47873a10f531ecea5dec4ff01daffa.png" data-rawwidth="851" data-rawheight="382"&gt;让我们来看看各个周期中间隔天数的均值和方差的变化吧。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/96d0216720592b2d864f084b0e378efd.png" data-rawwidth="864" data-rawheight="376"&gt;像期望的那样，后验分布的图像比先验数据更狭长；并且观察后验数据，大致得出平均的经期周期天数在29天左右，其标准差在2-3天左右。那么重要问题来了：我们是可育夫妻的概率为多少，以及我们在2月21日确定已经怀孕的概率为多少？为了计算这个我们取 `postisfertile` 与`post is_pregnant`，并计算众数。当然一个捷径是直接采用均值。  &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e26446d46f9fd2e64571ae0645b4c551.png" data-rawwidth="440" data-rawheight="229"&gt;因此这是一个相当好的消息：我们极有可能是可孕的夫妻，并且我们已经受孕的可能性高达84%！用这个模型我可以了解到：当经期的来临再多延迟几天，我们确定怀孕的概率是如何随之而变化的。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a674e2f991a107fea7f88d73125d91ee.png" data-rawwidth="520" data-rawheight="225"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f5921bc80eacd6cae3ff13b82eae5267.png" data-rawwidth="523" data-rawheight="215"&gt;是的，既然我们已经得到了好消息，为何不看看在我们之前尝试受孕的数月中我们可孕和受孕成功的可能性是如何变动的呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3975346bc3a4fcf1bb544f43bb28120f.png" data-rawwidth="576" data-rawheight="700"&gt;&lt;/p&gt;&lt;p&gt;因此，这能够解释得通了。在距离“最后一次经期”的时间变长时，我们在当前周期怀孕成功的几率增加了，但是一旦这里有经期发生时可能性会跌回至基线。我们看到的可孕率曲线是几乎相同的，但在我们没有怀孕成功的每一个周期里，可孕率曲线稍稍有所下降。这两个指标的图像均呈轻微的锯齿状，但这只是由重要性抽样算法的偏差导致的。另外需指出的是，虽然上述的图像非常美观，但是查看未怀孕期间的概率是无效的，唯一具重要性和信息价值的是当前的概率。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4a6b16415ef142078fa9380b549d8d03.png" data-rawwidth="613" data-rawheight="459"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/cb124fbccfd34346eaf63e59429767be.png" data-rawwidth="803" data-rawheight="498"&gt;&lt;/p&gt;&lt;h3&gt;一些关于这个模型的批评 但其实并不重要&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;当然，比起我相对简单粗糙的计算，别人有可能能够得到更优越的先验值。还有很多可以加入考虑的预测因子，如男性的年龄，健康因子等等。&lt;/li&gt;&lt;li&gt;每个月受孕的概率本应被视作一个不确定的值而不是一个固定值，而我把它设为了固定值。但是在拥有的给定数据很少的情况下，我将其视作一个适用于多个参数的参数值。&lt;/li&gt;&lt;li&gt;没有事物是完全符合正态分布的，两个经期间的天数亦然。这里我认为假设是适用的，但是还有比我的假设远要复杂得多经期间隔天数模型，比如 Bortot et al (2010)建立的模型。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21956061&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 10 Aug 2016 17:38:23 GMT</pubDate></item><item><title>数据驱动的出口电商品类行情分析</title><link>https://zhuanlan.zhihu.com/p/21948882</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4c920fbc4f7468900b798d437bc644d4_r.png"&gt;&lt;/p&gt;作为一个出口电商卖家，逐条点击商品链接、逐个翻看商品的详情页面无疑是我们最直接可靠的品类分析手段，但是在品类数量较多、所需跟踪期限较长的情况下，这种做法却往往使我们迷失在海量的信息之中。许多电商卖家也许会有这样的体验：翻了一下午的竞品页面，但是最后对竞品的了解程度还是相当有限。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/749cf0f2538190a3835a831c4177fa3d.png" data-rawwidth="1918" data-rawheight="951"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a9606a338066367fa9184b8621ce7d49.png" data-rawwidth="1917" data-rawheight="1007"&gt;为什么花了那么多时间了解品类，但总是感觉对品类的把握还是很模糊。其实，问题的根本在于：通过翻看页面，我们只能获得当前的品类信息，却无法把这些信息和过去的历史信息做对比。这时我们需要一个可以把品类历史信息绘制成如下图表的工具，帮助我们清晰的展示品类的历史变化趋势。（如下图）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/bc0d0cc584c947feea7b939046c1b53b.png" data-rawwidth="721" data-rawheight="438"&gt;那么今天我来介绍下如何利用『数据脉』进行更加深入的品类分析。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/79d0681d879ead39ae55cd5ae50bc7e9.png" data-rawwidth="1115" data-rawheight="632"&gt;打开数据脉品类信息追踪功能，选择好要观测的时间范围，比如2016年7月15号到31号，就可以得到相应的价格、BRS、评论星级、评论总量的变动趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f1a010bd16dcdec67812ed4bba9e87ab.png" data-rawwidth="539" data-rawheight="109"&gt;【价格】：对价格跟踪，生成的价格变化趋势图。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/af86435a13db7f8ade8aeedb476dd2b5.png" data-rawwidth="704" data-rawheight="422"&gt;【BSR】：对BSR的跟踪，生成的BSR变化趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/bc0d0cc584c947feea7b939046c1b53b.png" data-rawwidth="721" data-rawheight="438"&gt;【评论星级】：对评论星级的跟踪，生成该商品星级的变化趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d01f0bd86f3b557021792a9973878d78.png" data-rawwidth="707" data-rawheight="430"&gt;【评论总量】对评论总量的跟踪，生成的评论总量的变化趋势图。从评论量的变化，就可以清晰的看出，该商品现在受到的关注度到底怎么样了。  &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e506e1c8c40ce45b52ed68097af76476.png" data-rawwidth="695" data-rawheight="431"&gt;&lt;/p&gt;&lt;p&gt;我想到这边，你应该对这款商品的总体趋势有个很好的把握了。可是还没完，要知道，在亚马逊是没有办法获得商品销量的信息的。但是数据脉可以通过数据跟踪，探测到这款商品的「销量」！同时进行跟踪记录，获得销量的趋势图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1e82cbca4f338c2e38d9ddb7f98982e5.png" data-rawwidth="1269" data-rawheight="432"&gt;&lt;/p&gt;&lt;p&gt;如果你还觉得不够，数据脉还将根据这些数据通过特定的数据算法推算出一组参考数据，来评价商品的综合表现，如下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cba21050651825c072d92da5ba392c5f.png" data-rawwidth="1155" data-rawheight="97"&gt;&lt;p&gt;如果你要关注的商品特别多，说真的参考这些指标就够了。这些指标通常情况下比用文章开篇时说的那种「翻看网页」的方法肯定是更科学和更准确的。因为这些指标的生成所依据就是之前我们所说的被跟踪记录下来的商品数据，而翻看页面所依据的是通过人为的浏览而形成的主观判断。  &lt;/p&gt;&lt;p&gt;【上帝视角般的纵览竞品】:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/74fad89483dace5a258bb1984c53bdce.png" data-rawwidth="1353" data-rawheight="776"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/01c671a4bade5419de93bc7e954c842a.png" data-rawwidth="1662" data-rawheight="864"&gt;如果你需要一款像『数据脉』这样的工具，通过以下方式了解更多：&lt;/p&gt;&lt;p&gt;http://qm.qq.com/cgi-bin/qm/qr?k=y0sgE3_qKUd_DhplBTYiV667XDMPNONR (二维码自动识别)&lt;/p&gt;&lt;p&gt;https://datartery.com (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21948882&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 10 Aug 2016 11:36:31 GMT</pubDate></item><item><title>福布斯深度专访Jeff Dean——谷歌人工智能背后的大脑丨数据工匠简报（8.08）</title><link>https://zhuanlan.zhihu.com/p/21915770</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/82208a7502d944ee81b46f6ed487d637_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;福布斯深度专访Jeff Dean——谷歌人工智能背后的大脑&lt;/p&gt;&lt;p&gt;Jeff Dean 在获得华盛顿大学计算机科学博士学位的三年之后（1999 年）加入了谷歌公司，成为了该公司最早的员工之一。在谷歌的成长过程中，他一直是该公司的头面人物——设计和实现了支撑谷歌大部分产品的许多分布式计算基础设施。&lt;/p&gt;&lt;p&gt;谷歌 CEO Sundar Pichai 曾说过谷歌将会变成一家人工智能优先的公司；作为系统和基础设施组（Systems and Infrastructure Group）的高级成员，Dean 及其团队对实现这样的目标至关重要。在这个涉及范围广泛的访谈中，Dean 描述了他在谷歌的多种角色、该公司的人工智能愿景、他对谷歌如何在作为科技巨头的同时保持创业精神的想法，以及其它许多主题。&lt;/p&gt;&lt;p&gt;http://www.geekpark.net/topics/216282 (二维码自动识别)&lt;/p&gt;&lt;h2&gt;互联网金融中的数据科学&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/998c2ae3affb835cc129ac77a918161b.png" data-rawwidth="702" data-rawheight="470"&gt;作者：张云松,毕业于中科院，多年咨询公司和互联网公司从事数据算法、决策分析、风险管理和产品设计的工作，目前是融360风控总监，负责纯线上小额微贷信用贷款产品。&lt;/p&gt;&lt;p&gt;最近几年，这波在资本撬动的互联网金融的浪潮极大地提升了数据科学的行业应用价值，数据分析师不再是苦逼的跑数的，摇身一变成了风控模型专家、数据科学家。尤其是大数据风控、大数据征信领域一片火热的场景，数据挖掘、机器学习相关专业同学的数量也翻番上涨，越来越多的计算机和统计领域的同学加入互联网金融行业。&lt;/p&gt;&lt;p&gt;面试中发现很多同学的梦想工作都是我要做机器学习相关工作、我要做算法、我要做模型……但其实以一个互联网金融从业者角度看，我们大量的时间还是在做数据理解、数据处理、重复验证特征、不停的在做实验，我对模型师的定义基本就是半个蓝领，只不过很多学术和一些五花八门的算法和方法可以真正有机会应用到商业领域并且产生价值。&lt;/p&gt;&lt;p&gt;http://cos.name/2016/08/data-science-in-itfin/#more-12962 (二维码自动识别)&lt;/p&gt;&lt;h2&gt;合理使用元数据工具，解决大数据治理落地难点&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/45dec2c41396f889439edc2e620fa5ca.png" data-rawwidth="538" data-rawheight="280"&gt;元数据并不止存在于数据领域，近年来，元数据管理的范围在不断扩大，从简单的库表，到整个数据平台，再到服务管理，不断突破传统元数据管理的范畴。InfoQ采访了普元软件产品部副总、大数据产品线总经理王轩，了解如下问题：“元数据”和“大数据”之间的异同之处在哪里？大数据环境下的元数据管理有什么特点？元数据元数据驱动的微服务架构有何特点？企业大数据治理难点在哪里？&lt;/p&gt;&lt;p&gt;http://www.infoq.com/cn/news/2016/08/big-data-solve-management-diffic (二维码自动识别)&lt;/p&gt;&lt;p&gt;以上简讯由数据工匠提供，感兴趣的小伙伴可以通过扫描简报后的二维码链接原文，更多数据科学资讯尽在数据工匠，扫码关注Datartisan数据工匠公众号！如果你看到什么与“数据科学”有关的好文或者信息科技优质的文章，可以随手转发给我们，让更多热爱数据科学的小伙伴一起成长！&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;​&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21915770&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Mon, 08 Aug 2016 10:34:19 GMT</pubDate></item></channel></rss>