<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Datartisan数据工匠 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/datartisan</link><description></description><lastBuildDate>Sun, 16 Oct 2016 06:16:43 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>短文本主题建模方法</title><link>https://zhuanlan.zhihu.com/p/22332099</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/45b65581fc651b93dc1f6c9212833f6f_r.png"&gt;&lt;/p&gt;&lt;h2&gt;1. 引言&lt;/h2&gt;&lt;p&gt;许多数据分析应用都会涉及到从短文本中提取出潜在的主题，比如微博、短信、日志文件或者评论数据。一方面，提取出潜在的主题有助于下一步的分析，比如情感评分或者文本分类模型。另一方面，短文本数据存在一定的特殊性，我们无法直接用传统的主题模型算法来处理它。短文本数据的主要难点在于： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;短文本数据中经常存在多词一义的现象[1]，比如 “dollar”, "$", "$$", "fee", "charges" 拥有相同的含义，但是受限于文本篇幅的原因，我们很难直接从短文本数据中提取出这些信息。&lt;/li&gt;&lt;li&gt;与长文档不同的地方在于，短文本数据中通常只包含一个主题。这看似很好处理，但是传统的主题模型算法都假设一篇文档中包含多个主题，这给建模分析带来了不小的麻烦。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;主题提取模型通常包含多个流程，比如文本预处理、文本向量化、主题挖掘和主题表示过程。每个流程中都有多种处理方法，不同的组合方法将会产生不同的建模结果。 &lt;/p&gt;&lt;p&gt;本文将主要从实际操作的角度来介绍不同的短文本主题建模算法的优缺点，更多理论上的探讨可以参考以下文章。 &lt;/p&gt;&lt;p&gt;下文中我将自己创建一个数据集，并利用 Python scikit-learn 来拟合相应的主题模型。 &lt;/p&gt;&lt;h2&gt;2. 主题发现模型&lt;/h2&gt;&lt;p&gt;本文主要介绍三个主题模型, LDA(Latent Dirichlet Allocation), NMF(Non-Negative Matrix Factorization)和SVD(Singular Value Decomposition)。本文主要采用 scikit-learn 来实现这三个模型。 &lt;/p&gt;&lt;p&gt;除了这三个模型外，还有其他一些模型也可以用来发现文档的结构。其中最重要的一个模型就是 KMeans 聚类模型，本文将对比 KMeans 聚类模型和其他主题模型的拟合效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dd56a7cb6c13d05cb6001441e6551aec.png" data-rawwidth="444" data-rawheight="217"&gt;&lt;p&gt;首先，我们需要构建文本数据集。本文将以四个自己构建的文本数据集为例来构建主题模型： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;clearcut topics: 该数据集中只包含两个主题—— "berger-lovers" 和 "sandwich-haters"。&lt;/li&gt;&lt;li&gt;unbalanced topics: 该数据集与第一个数据集包含的主题信息一致，但是此数据集的分布是有偏的。&lt;/li&gt;&lt;li&gt;semantic topics: 该数据集包含四个主题，分别是 "berger-lovers"， "berger-haters"，"sandwich-lovers" 和 "sandwich-haters"。此外，该数据集中还包含了两个潜在的主题 “food” 和 “feelings”。&lt;/li&gt;&lt;li&gt;noisy topics: 正如前文所说的，短文本数据中经常存在多词一义的现象，该数据集主要用于模拟两个主题不同类型的文本。该数据集文本的篇幅小于其他三个数据集，这可以用来检验模型是否能够很好地处理短文本数据。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/00d8dd77ea06f0372f19f7beeca09cb8.png" data-rawwidth="450" data-rawheight="677"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/10a3c69c01a2801ae89c479548073233.png" data-rawwidth="449" data-rawheight="565"&gt;&lt;p&gt;首先，我们需要考虑下如何评估一个主题模型建模效果的好坏程度。多数情况下，每个主题中的关键词有以下两个特征： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;关键词出现的频率得足够大&lt;/li&gt;&lt;li&gt;足以区分不同的主题 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一些研究表明：关键词还需具备以下两个特征： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;相同主题的文档中关键词共同出现的频率应该差不多&lt;/li&gt;&lt;li&gt;每个主题中关键词的语义应该十分接近，比如水果主题中的 “apples” 和 “oranges”，或者情感主题中的 “love” 和 “hate”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来，我们将介绍如何实现上述的四个模型——NMF, SVD, LDA 和 KMEANS。对于每个主题模型，我们将分别采用两种文本向量化的方法—— TF(Term Frequence) 和 TFIDF(Term-frequence-inverse-document-frequence)。通常情况下，如果你的数据集中有许多词语在多篇文档中都频繁出现，那么你应该选择采用 TFIDF 的向量化方法。此时这些频繁出现的词语将被视为噪声数据，这些数据会影响模型的拟合效果。然而对于短文本数据而言，TF和TFIDF方法并没有显著的区别，因为短文本数据集中很难碰到上述情况。如何将文本数据向量化是个非常热门的研究领域，比如 基于word embedding模型的方法——word2vec和doc2vec。 &lt;/p&gt;&lt;p&gt;主题模型将选择主题词语分布中频率最高的词语作为该主题的关键词，但是对于 SVD 和 KMEANS 算法来说，模型得到的主题词语矩阵中既包含正向值也包含负向值，我们很难直接从中准确地提取出主题关键词。为了解决这个问题，我选择从中挑出绝对数值最大的几个词语作为关键词，并且根据正负值的情况加上相应的标签，即对负向词语加上 "^" 的前缀，比如"^bergers"。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/06878f42ddaa8b873ac42f621064c06f.png" data-rawwidth="444" data-rawheight="374"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/69b9594b0bac370b9d24d76eda1d4513.png" data-rawwidth="435" data-rawheight="575"&gt;&lt;h3&gt;2.1 SVD: 正交分解&lt;/h3&gt;&lt;p&gt;sklearn 中的 truncated SVD implementation 类似于主成分分析算法，它们都试图利用正交分解的方法选择出具有最大方差的变量信息。 &lt;/p&gt;&lt;p&gt;对于 clearcut-topic 数据集来说，我们分别利用 TF 和 TFIDF方法来向量化文本数据，并构建 SVD 模型，模型的拟合结果如下所示。正如我们之前所提到的，SVD 模型所提取的关键词中包含正负向词语。为了简单起见， 我们可以理解为该主题包含正向词语，不包含负向的词语。 &lt;/p&gt;&lt;p&gt;比如，对于 "Topic 1: bergers | ^hate | love | ^sandwiches" 来说，该文本的主题中包含 "love bergers" 但是不包含 "hate sandwiches"。 &lt;/p&gt;&lt;p&gt;由于模型的随机效应，所以每次运行模型得到的结果都会存在细微的差异。在 SVD 的拟合结果中我们发现发现 Topic 3: bergers | ^hate | ^love | sandwiches 成功地提取了 “food” 的主题。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1ecc76e6b9450551513af3752342b792.png" data-rawwidth="437" data-rawheight="467"&gt;在上述的例子中，我们设定了过多的主题数量，这是因为大多数时候我们无法事先知道某个文档包含多少个主题。如果我们令主题个数等于2，可以得到下述结果：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dc3ec2767306e12c9ed185caa89000dc.png" data-rawwidth="443" data-rawheight="158"&gt;&lt;p&gt;当我们在解释 SVD 模拟的拟合结果时，我们需要对比多个主题的信息。比如上述的模型拟合结果可以解释成：数据集中文档的主要差异是文档中包含 “love bergers” 但不包含 “hate sandwiches”。 &lt;/p&gt;&lt;p&gt;接下来我们将利用 SVD 来拟合 unbalanced topic 数据集，检验该模型处理非平衡数据集的效果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/62d8b038dc4d11879375779b2611062f.png" data-rawwidth="437" data-rawheight="166"&gt;从下述结果中可以看出，SVD无法处理噪声数据，即无法从中提取出主题信息。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1598eefc59132b72b2788d2e245bb863.png" data-rawwidth="437" data-rawheight="297"&gt;&lt;/p&gt;&lt;h3&gt;2.2 LDA: 根据词语的共现频率来提取主题&lt;/h3&gt;&lt;p&gt;LDA 是最常用的主题提取模型之一，因为该模型能够处理多种类型的文本数据，而且模拟的拟合效果非常易于解释。 &lt;/p&gt;&lt;p&gt;直观上来看，LDA 根据不同文档中词语的共现频率来提取文本中潜在的主题信息。另一方面，具有相同主题结构的文本之间往往非常相似，因此我们可以根据潜在的主题空间来推断词语之间的相似性和文档之间的相似性。 &lt;/p&gt;&lt;p&gt;LDA 算法中主要有两类参数： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;每个主题中各个关键词的分布参数&lt;/li&gt;&lt;li&gt;每篇文档中各个主题的分布参数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来我们将研究这些参数是如何影响 LDA 模型的计算过程，人们更多的是根据经验来选择最佳参数。 &lt;/p&gt;&lt;p&gt;与 SVD 模型不同的是，LDA 模型所提取的主题非常好解释。以 clearcut-topics 数据集为例，LDA 模型中每个主题都有明确的关键词，它和SVD主要有以下两个区别： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;LDA 模型中可能存在重复的主题&lt;/li&gt;&lt;li&gt;不同的主题可以共享相同的关键词，比如单词 “we” 在所有的主题中都出现了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外，对 LDA 模型来说，采用不同的文本向量化方法也会得到不同的结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2e2974b2b0434b623125f2b209585202.png" data-rawwidth="445" data-rawheight="349"&gt;&lt;p&gt;在 sklearn 中，参数 topic&lt;em&gt;word&lt;/em&gt;prior 和 doc&lt;em&gt;topic&lt;/em&gt;prior 分别用来控制 LDA 模型的两类参数。 &lt;/p&gt;&lt;p&gt;其中 topic&lt;em&gt;word&lt;/em&gt;prior 的默认值是(1/n&lt;em&gt;topics)，这意味着主题中的每个词语服从均匀分布。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d3da2eabf4428c7545318776b8a2e8d1.png" data-rawwidth="435" data-rawheight="202"&gt;&lt;em&gt;选择更小的 topic&lt;/em&gt;word_prior 参数值可以提取粒度更小的主题信息，因为每个主题中都会选择更少的词语。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/9d269da811a94d8d603b0924c03f0538.png" data-rawwidth="454" data-rawheight="183"&gt;LDA 模型同样无法很好地处理 noisy topics 数据集，从下述结果中可以看出 LDA 模型提取的主题相当模糊：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/188d61474f54376df385430e96dd6c50.png" data-rawwidth="448" data-rawheight="400"&gt;&lt;h3&gt;2.3 NMF&lt;/h3&gt;&lt;p&gt;NMF 可以视为 LDA模型的特例，从理论上来说，这两个模型之间的联系非常复杂。但是在实际应用中，NMF 经常被视为参数固定且可以获得稀疏解的 LDA 模型。虽然 NMF 模型的灵活性不如 LDA 模型，但是该模型可以很好地处理短文本数据集。 &lt;/p&gt;&lt;p&gt;另一方面，NMF 最大的缺点是拟合结果的不一致——当我们设置过大的主题个数时，NMF 拟合的结果非常糟糕。相比之下，LDA模型的拟合结果更为稳健。 &lt;/p&gt;&lt;p&gt;首先我们来看下 NMF 模型不一致的拟合结果。对于 clearcut topics 数据集来说，当我们设置提取5个主题时，其结果和真实结果非常相似：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e394d64b6412d6eb8b3d6da6d755d767.png" data-rawwidth="442" data-rawheight="178"&gt;但是当我们增加主题个数时（远大于真实主题数2），NMF 模型将会得到一些奇异的结果：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/956e6a758e5de7218440a057ca1fcde0.png" data-rawwidth="444" data-rawheight="512"&gt;相比之下，LDA模型的结果十分稳健。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c99c3663425c41fc62de71a3b2c52a65.png" data-rawwidth="444" data-rawheight="640"&gt;对于非平衡数据集，设置好合适的主题个数，NMF 可以很好地提取出文档中的主题信息。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/390b3509368b4b3a77a13099939e6ca8.png" data-rawwidth="452" data-rawheight="179"&gt;值得注意的是，NMF 是本文提到的四个模型中唯一一个能够较好地处理 noisy topics 数据的模型：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/85b28398d17a31a72849b3fadc76f446.png" data-rawwidth="441" data-rawheight="296"&gt;&lt;/p&gt;&lt;h3&gt;2.4 KMeans&lt;/h3&gt;&lt;p&gt;类似于 KMeans 模型的聚类方法能够根据文档的向量形式对其进行分组。然而这个模型无法看成是主题模型，因为我们很难解释聚类结果中的关键词信息。 &lt;/p&gt;&lt;p&gt;但是如果结合 TF或TFIDF方法，我们可以将 KMeans 模型的聚类中心视为一堆词语的概率组合：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2e976c586bd65756b7375dd8e0330b90.png" data-rawwidth="446" data-rawheight="548"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cce452500aeed3f9d6373bd95b837350.png" data-rawwidth="440" data-rawheight="303"&gt;&lt;/p&gt;&lt;h3&gt;2.5 寻找具有高语义相关的主题&lt;/h3&gt;&lt;p&gt;最后，我将简单比较下不同的主题提取模型。大多数情况下，我们倾向于根据文档的主题分布情况对其进行分组，并根据关键词的分布情况来提取主题的信息。 &lt;/p&gt;&lt;p&gt;大多数研究者都认为词语的语义信息是由其上下文信息所决定的，比如 “love” 和 “hate”可以看成是语义相似的词语，因为这两个词都可以用在 “I _ apples” 的语境中。事实上，词向量最重要的一个研究就是如何构建词语、短语或者文档的向量形式，使得新的向量空间中仍然保留着语义信息。 &lt;/p&gt;&lt;p&gt;找寻语义相同的词语不同于计算词语的共现频率。从下述的结果中可以看出，大多数主题提取模型只涉及到词语的共现频率，并没有考虑词语的语义信息，只有 SVD 模型简单涉及到语义信息。 &lt;/p&gt;&lt;p&gt;需要注意的是，本文所采用的数据集是根据一定的规则随机生成的，所以下述结果更多的是用于说明不同模型之间的区别：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/126bf908c01b80419a94181336662b9d.png" data-rawwidth="454" data-rawheight="755"&gt;&lt;h2&gt;3. 总结&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;短文本数据集具有其独特的性质，建模时需要特别注意。&lt;/li&gt;&lt;li&gt;模型的选择依赖于主题的定义（共现频率高或者语义相似性）和主题提取的目的（文档表示或者是异常值检验）&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们可以首先采用 KMeans 或者 NMF 模型来快速获取文档的结构信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;主题中词语的分布情况&lt;/li&gt;&lt;li&gt;文档中主题的分布情况&lt;/li&gt;&lt;li&gt;主题个数&lt;/li&gt;&lt;li&gt;每个主题中词语的个数&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LDA 模型具有很好的灵活性，可以处理多种类型的文本数据。但是调参过程需要很好地理解数据结构，因此如果你想构建 LDA 模型，你最好先构建一个基准模型（KMEANS 或 NMF）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;SVD 模型可以很好地提取出文本的主要信息。比如 SVD 模型可以很好地分析半结构化的数据（模板数据、截图或者html中的表格数据）。 &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb"&gt;http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：dolaameng  &lt;/p&gt;&lt;p&gt;译者：Fibears  &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22332099&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 13 Oct 2016 11:51:22 GMT</pubDate></item><item><title>机器学习通用框架</title><link>https://zhuanlan.zhihu.com/p/22833471</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5ed55fdec803bf29c368e83917832630_r.png"&gt;&lt;/p&gt;每个数据科学家每天都要处理成吨的数据，而他们60%~70%的时间都在进行数据清洗和数据格式调整，将原始数据转变为可以用机器学习所识别的形式。本文主要集中在数据清洗后的过程，也就是机器学习的通用框架。这个框架是我在参加了百余场机器学习竞赛后的一个总结。尽管这个框架是非常笼统和概括的，但是绝对能发挥强大的作用，仍然可以在专业人员的运用下变成复杂、高效的方法。整个过程使用Python来实现。&lt;h2&gt;数据&lt;/h2&gt;&lt;p&gt;在用机器学习的方法之前，我们应该先把数据转变为表格的形式，这个过程是最耗时、最复杂的。我们用下图来表示这一过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7e116860a704a65392553fc64eb40615.png" data-rawwidth="800" data-rawheight="222"&gt;&lt;p&gt;这一过程也就是将原始数据的所有的变量量化，进一步转变为含数据（Data）和标签（Labels）的数据框形式。这样处理过的数据就可以用来机器学习建模了。数据框形式的数据是机器学习和数据挖掘中最为通用的数据表现形式，它的行是数据抽样得到的样本，列代表数据的标签Y和特征X，其中标签根据我们要研究的问题不同，有可能是一列或多列。 &lt;/p&gt;&lt;h2&gt;标签的类型&lt;/h2&gt;&lt;p&gt;根据我们要研究的问题，标签的类型也不一： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;单列0-1值(&lt;strong&gt;二分类问题&lt;/strong&gt;，一个样本只属于一类并且一共只有两类）&lt;/li&gt;&lt;li&gt;单列连续值（&lt;strong&gt;单回归问题&lt;/strong&gt;，要预测的值只有一个）&lt;/li&gt;&lt;li&gt;多列0-1值（&lt;strong&gt;多分类问题&lt;/strong&gt;，同样是一个样本只属于一类但是一共有多类）&lt;/li&gt;&lt;li&gt;多列连续值(&lt;strong&gt;多回归问题&lt;/strong&gt;，能够预测多个值）&lt;/li&gt;&lt;li&gt;多标签（&lt;strong&gt;多标签分类问题&lt;/strong&gt;，但是一个样本可以属于多类）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;评价指标&lt;/h2&gt;&lt;p&gt;对于多个机器学习方法，我们必须找到一个评价指标来衡量它们的好坏。比如一个二元分类的问题我们一般选用AUC ROC或者仅仅用AUC曲线下面的面积来衡量。在多标签和多分类问题上，我们选择交叉熵或对数损失函数。在回归问题上我们选择常用的均方误差(MSE)。&lt;/p&gt;&lt;h2&gt;Python库&lt;/h2&gt;&lt;p&gt;在安装机器学习的几个库之前，应该安装两个基础库：numpy和scipy。 &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Pandas&lt;/strong&gt; 处理数据最强大的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt; 涵盖机器学习几乎所有方法的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;xgboost&lt;/strong&gt; 优化了传统的梯度提升算法 &lt;/li&gt;&lt;li&gt;&lt;strong&gt;keras&lt;/strong&gt; 神经网络&lt;/li&gt;&lt;li&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;用来作图的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;tpdm&lt;/strong&gt; 显示过程&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;机器学习框架&lt;/h2&gt;&lt;p&gt;2015年我想出了一个自动式机器学习的框架，直到今天还在开发阶段但是不久就会发布，本文就是以这个框架作为基础的。下图展示了这个框架：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-5ed55fdec803bf29c368e83917832630.png" data-rawwidth="800" data-rawheight="605"&gt;&lt;p&gt;上面展示的这个框架里面，粉红色的线就是一些通用的步骤。在处理完数据并把数据转为数据框格式后，我们就可以进行机器学习过程了。 &lt;/p&gt;&lt;h3&gt;确定问题&lt;/h3&gt;&lt;p&gt;确定要研究的问题，也就是通过观察标签的类别确定究竟是分类还是回归问题。 &lt;/p&gt;&lt;h3&gt;划分样本&lt;/h3&gt;&lt;p&gt;第二步是将所有的样本划分为&lt;strong&gt;训练集(training data)&lt;/strong&gt;和&lt;strong&gt;验证集(validation data)&lt;/strong&gt;。过程如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7e153d8877ffb6b6549888cacf8ebea5.png" data-rawwidth="800" data-rawheight="406"&gt;划分样本的这一过程必须要根据标签来做。比如对于一个类别不平衡的分类问题，必须要用分层抽样的方法，比如每种标签抽多少，这样才能保证抽出来的两个样本子集分布类似。在Python中，我们可以用scikit-learn轻松实现。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fe714934ca0c0c46506a0abf03a34538.png" data-rawwidth="496" data-rawheight="130"&gt;对于回归问题，那么一个简单的K折划分就足够了。但是仍然有一些复杂的方法可以使得验证集和训练集标签的分布接近，这个问题留给读者作为练习。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-274aedb004ffb1123a80f28523812d38.png" data-rawwidth="504" data-rawheight="121"&gt;&lt;p&gt;上面我用了样本全集中的10%作为验证集的规模，当然你可以根据你的样本量做相应的调整。划分完样本以后，我们就把这些数据放在一边。接下来我们使用的任何一种机器学习的方法都要先在训练集上使用然后再用验证集检验效果。验证集和训练集永远都不能掺和在一起。这样才能得到有效的评价得分，否则将会导致过拟合的问题。 &lt;/p&gt;&lt;h3&gt;识别特征&lt;/h3&gt;&lt;p&gt;一个数据集总是带有很多的变量(variables)，或者称之为特征（features)，他们对应着数据框的维度。一般特征的值有三种类型：数值变量、属性变量和文字变量。我们用经典的&lt;a href="https://www.kaggle.com/c/titanic/data" data-editable="true" data-title="泰坦尼克号数据集"&gt;泰坦尼克号数据集&lt;/a&gt;来示例。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ddcec0bb516d5ac00699f42ae7f6a4d0.png" data-rawwidth="800" data-rawheight="441"&gt;&lt;p&gt;在这里生还（survival）就是标签，船舱等级（pclass）、性别（sex）和登船港口（embarked）是属性变量。而像年龄（age）、船上兄弟姐妹数量（sibsp）、船上父母孩子数量（parch）是数值变量。而姓名（name）这种文字变量我们认为这和生还与否没什么关系，所以我们决定不考虑。首先处理数值型变量，这些变量几乎不需要任何的处理，常见的方式是正规化（normalization)。处理属性变量通常有两步： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;把属性变量转变为标签&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5f8e47281c1989d51fdfb15f9d46d96e.png" data-rawwidth="533" data-rawheight="111"&gt;&lt;/li&gt;&lt;li&gt;把标签转变为二元数值&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-210a2f9867eb2da40f2f84166b66d9f5.png" data-rawwidth="531" data-rawheight="108"&gt;由于泰坦尼克号数据集没有很好的文字变量来示范，那么我们就制定一个通用的规则来处理文字变量。把所有的文字变量组合到一起，然后用某种算法来处理并转变为数字&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4b80388cfcf90e904a3ae345a66f680a.png" data-rawwidth="531" data-rawheight="90"&gt;&lt;p&gt;我们可以用CountVectorizer或者TfidfVectorizer来实现&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c0a5835bbe0ca5ccc70e7cb5585c8f29.png" data-rawwidth="530" data-rawheight="92"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1de24e5d3486617db516dcae18e89df4.png" data-rawwidth="530" data-rawheight="91"&gt;一般来说第二种方法往往比较优越，下面代码框中所展示的参数长期以来都取得了良好的效果。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-4576414bd3937f529680b4d1db9547f9.png" data-rawwidth="530" data-rawheight="230"&gt;如果你对训练集数据采用了上述处理方式，那么也要保证对验证及数据做相同处理。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6d7b9382604d0b687aa166b06057c435.png" data-rawwidth="533" data-rawheight="59"&gt;&lt;h3&gt;特征融合&lt;/h3&gt;&lt;p&gt;特征融合是指将不同的特征融合，要区别对待密集型变量和稀疏型变量。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-89154eb1d05095710ec9d96192474ef6.png" data-rawwidth="611" data-rawheight="134"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-69a7a2a787669ed9a72efd22c36747df.png" data-rawwidth="531" data-rawheight="126"&gt;&lt;p&gt;当我们把特征融合好以后，可以开始机器学习的建模过程了，在这里我们都是选择以决策树为基学习器的集成算法，主要有： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;RandomForestClassifier&lt;/li&gt;&lt;li&gt;RandomForestRegressor&lt;/li&gt;&lt;li&gt;ExtraTreesClassifier&lt;/li&gt;&lt;li&gt;ExtraTreesRegressor&lt;/li&gt;&lt;li&gt;XGBClassifier&lt;/li&gt;&lt;li&gt;XGBRegressor但是不能直接把没有经过规范化的数值变量直接用线性模型拟合，可以用scikitlearn里面的&lt;strong&gt;规范化（Normalized）&lt;/strong&gt;和&lt;strong&gt;标准化（StandardScaler）&lt;/strong&gt;命令分别对密集和稀疏的数据进行相应的处理。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;特征降维和特征选择&lt;/h3&gt;&lt;p&gt;如果以上方式处理后的数据可以产生一个优秀的模型，那就可以直接进行参数调整了。如果不行则还要继续进行特征降维和特征选择。降维的方法有以下几种：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a994abcea4bab5ec498a5e1010c8890d.png" data-rawwidth="547" data-rawheight="158"&gt;简单起见，这里不考虑LDA和QDA。对于高维数据来说，PCA是常用的降维方式，对于图像数据一般我们选用10~15组主成分，当然如果模型效果会提升的话也可以选择更多的主成分。对于其他类型的数据我们一般选择50~60个主成分。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f88985c9186ec217a9e5fa4f77e87819.png" data-rawwidth="533" data-rawheight="93"&gt;文字变量转变为稀疏矩阵后进行奇异值分解，奇异值分解对应scikit learn库中的TruncatedSVD语句。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-db5aa99256a24a68e67f071e32eec5c1.png" data-rawwidth="535" data-rawheight="110"&gt;一般在TF-IDF中SVD主成分的数目大约在120~200之间，但是也可以采用更多的成分，但是相应的计算成本也会增加。在特征降维之后我们可以进行建模的训练过程了，但是有的时候如果这样降维后的结果仍不理想，可以进行特征选择：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-fd94e51c03ce073dd8a32e8b8be11a88.png" data-rawwidth="543" data-rawheight="156"&gt;特征选择也有很多实用的方法，比如说常用的向前或向后搜索。那就是一个接一个地把特征加入模型训练，如果加入一个新的特征后模型效果不好，那就不加入这一特征。直到选出最好的特征子集。对于这种方法有一个提升的方式是&lt;a href="https://github.com/abhishekkrthakur/greedyFeatureSelection" data-editable="true" data-title="用AUC作为评价指标"&gt;用AUC作为评价指标&lt;/a&gt;，当然这个提升也不是尽善尽美的，还是需要实际应用进行改善和调整的。还有一种特征选择的方式是在建模的过程中就得到了最佳特征子集。比如我们可以观察logit模型的系数或者拟合一个随机森林模型从而直接把这些甄选后的特征用在其它模型中。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e5ebb4859ca5bfa4fad12bc2c62ae2c9.png" data-rawwidth="527" data-rawheight="107"&gt;在上面的处理中应该选择一个小的estimator数目这样不会导致过拟合。还可以用梯度提升算法来进行特征选择，在这里我们建议用xgboost的库而不是sklearn库里面的梯度提升算法，因为前者速度快且有着更好的延展性。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e19417a5546f66fad1a8654761c4e953.png" data-rawwidth="534" data-rawheight="114"&gt;对于稀疏的数据集我们可以用随机森林、xgboost或卡方等方式来进行特征选择。下面的例子中我们用了卡方的方法选择了20个特征出来。当然这个参数值20也是可以进一步优化的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-262c04afcbb8625145ff89273c9c66d9.png" data-rawwidth="537" data-rawheight="111"&gt;&lt;p&gt;同样，以上我们用的所有方法都要记录储存用以交叉验证。 &lt;/p&gt;&lt;h3&gt;模型选择和参数调整&lt;/h3&gt;&lt;p&gt;一般而言，常用的机器学习模型有以下几种，我们将在这些模型中选择最好的模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分类问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;Logistic 回归&lt;/li&gt;&lt;li&gt;朴素贝叶斯分类器&lt;/li&gt;&lt;li&gt;支持向量机&lt;/li&gt;&lt;li&gt;k临近分类器&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;回归问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;线性回归&lt;/li&gt;&lt;li&gt;岭回归&lt;/li&gt;&lt;li&gt;Lasso&lt;/li&gt;&lt;li&gt;支持向量回归&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下表中展示了每种模型分别需要优化的参数，这其中包含的问题太多太多了。究竟参数取什么值才最优，很多人往往有经验但是不会甘愿把这些秘密分享给别人。但是在这里我会把我的经验跟大家分享。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-fbaebe0e8a9f3ee4230e12a304c8b746.png" data-rawwidth="785" data-rawheight="800"&gt;&lt;blockquote&gt;&lt;p&gt;RS*是指没有一个确切的值提供给大家。 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在我看来上面的这些模型基本会完爆其他的模型，当然这只是我的一家之言。下面是上述过程的一个总结，主要是强调一下要保留训练的结果用来给验证集验证，而不是重新用验证集训练！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-46a7880b2130eab916ddd2e2d4830c3b.png" data-rawwidth="800" data-rawheight="444"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7ebb886225a95536fac0bc0785d92882.png" data-rawwidth="800" data-rawheight="184"&gt;&lt;/p&gt;&lt;p&gt;在我长时间的实践过程中，我发现这些总结出来的规则和框架还是很有用的，当然在一些极其复杂的工作中这些方法还是力有不逮。生活从来不会完美，我们只能尽自身所能去优化，机器学习也是一样。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;原文作者：Abhishek Thakur原文链接：&lt;a href="http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/?sukey=3997c0719f151520eec92d4e7022d429d473ec41dadc46d670acc94d1f94abd4231fd776bc3c18126eea39e6b5a67350" data-editable="true" data-title="Approaching (Almost) Any Machine Learning Problem" class=""&gt;Approaching (Almost) Any Machine Learning Problem&lt;/a&gt;译者：Cup&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22833471&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Sun, 09 Oct 2016 14:24:17 GMT</pubDate></item><item><title>机器学习系列-Logistic回归：我看你像谁 （下篇）</title><link>https://zhuanlan.zhihu.com/p/22692266</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者：向日葵&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;Logistic回归&lt;/h2&gt;&lt;p&gt;书接上回，在我们有了最小二乘法与极大似然估计做基础之后，这样我们就做好了Logistic回归的准备，渐渐的进入到我们的主题Logistic回归。 很多都属于分类的问题了，邮件（垃圾邮件/非垃圾邮件），肿瘤（良性/恶性）。二分类问题，可以用如下形式来定义它： y∈{0,1},其中0属于负例，1属于正例。 现在来构造一种状态，一个向量来代表肿瘤（良性/恶性）和肿瘤大小的关系。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-569e40432a36bb5c6524ec03d501892a.png" data-rawwidth="819" data-rawheight="413"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-56666b6e14927636bd71b71ba81d2e64.png" data-rawwidth="818" data-rawheight="302"&gt;Sigmoid 函数在有个很漂亮的“S"形，如下图所示（引自维基百科）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b6c0a14d298c4857cabc80bb27aecba1.png" data-rawwidth="738" data-rawheight="492"&gt;综合上述两式，我们得到逻辑回归模型的数学表达式：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8c8064514b0d41ebd9f3222d4fec169d.png" data-rawwidth="885" data-rawheight="355"&gt;Cost函数和J函数如下，它们是基于最大似然估计推导得到的。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2d30ec70ae197c7aba524cb38db64134.png" data-rawwidth="884" data-rawheight="197"&gt;下面详细说明推导的过程：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2a3c65b90fd23715566dee0420567baf.png" data-rawwidth="882" data-rawheight="348"&gt;最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-36071a33031756d36876df600daaa644.png" data-rawwidth="885" data-rawheight="178"&gt;梯度下降法求的最小值 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-631267a13f028b13e3c75b07a8b963f2.png" data-rawwidth="886" data-rawheight="755"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d00dbcb3572b291fb75efe1cf84648d3.png" data-rawwidth="885" data-rawheight="133"&gt;&lt;h4&gt;向量化Vectorization&lt;/h4&gt;&lt;p&gt;Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。 如上式，Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。 &lt;/p&gt;&lt;p&gt;下面介绍向量化的过程： 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ce662d1d53f309092795720afdefdca1.png" data-rawwidth="881" data-rawheight="323"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7d70c69c5a409fa9c2357dce8910fcc8.png" data-rawwidth="884" data-rawheight="390"&gt;&lt;p&gt;Logistic回归的推导过程，采用的是极大似然法和梯度下降法取得各个参数的迭代过程。以后很多公式的推导也是类似这个过程，机器学习的过程大部分的算法都归结到概率论，如果概率论不是很熟，可以继续温习一下。所以很多人都在总觉，机器学习的问题，归宗到底就是概率论的问题。而采用极大似然的算法，其中隐藏着一个道理：求出来的参数会是最符合我们观察到的结果，实验数据决定了我们的参数。 &lt;/p&gt;&lt;h3&gt;TensorFlow下的Logistic回归&lt;/h3&gt;&lt;p&gt;现在有大量的机器学习的框架，个人开发者，大公司等都有。比较出名的还是FaceBook和谷歌的开源框架。 &lt;/p&gt;&lt;p&gt;TensorFlow是谷歌2015年开源的学习框架，结合了大量的机器学习的算法，官方的文档也比较清楚，开篇的初学者入门讲的就是关于Logistic回归的问题，这里简单的介绍一下，主要是想说明TensorFlow还是属于比较强大的工具，可以进行工具的学习。 &lt;/p&gt;&lt;p&gt;这篇文档的主要介绍如何使用TensorFlow识别MNIST，关于MNIST在之前神经网络的介绍有介绍过。MNIST里存放着一些手写的数据：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b1f5ef1030bc74cf5ed3396f0f2c64a9.png" data-rawwidth="486" data-rawheight="295"&gt;每个数字都可以用二进制向量数组来表示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f08519ed8e0c407856860f8e26d19251.png" data-rawwidth="573" data-rawheight="287"&gt;这些数据为神经网络的输入：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153.png" data-rawwidth="559" data-rawheight="240"&gt;&lt;p&gt;利用Logistic回归的训练求解上面的参数。 代码在&lt;a href="https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py" data-editable="true" data-title="githubusercontent.com 的页面"&gt;https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&lt;/a&gt;下可以自己参看。 &lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;这个章节里介绍了Logistic回归和推导的这个过程，Logistic回归是机器学习里最经常用到的算法，也是最基础的算法，通过推导Logistic回归就能够清楚机器学习的基础知识，后面有些算法的思想也和Logistic回归算法类似。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22692266&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 29 Sep 2016 12:21:00 GMT</pubDate></item><item><title>机器学习系列（一）:Logistic回归-我看你像谁（上篇）</title><link>https://zhuanlan.zhihu.com/p/22564500</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d065e83e77b87560b8256916a5ca7719_r.png"&gt;&lt;/p&gt;&lt;h2&gt;引子：&lt;/h2&gt;&lt;p&gt;依旧是一个“烧烤”的天气，我在的城市厦门，已经达到了37度的高温了，不过也好，只有这样的暑假，这样的天气才有时间和心思，静下心来思考一些东西。 &lt;/p&gt;&lt;p&gt;关于机器学习的教程确实是太多了，处于这种变革的时代，出去不说点机器学习的东西，都觉得自己落伍了，但总觉得网上的东西并不系统，无法让人串联在一起，总有很多人读了几篇机器学习的东西，就自以为机器学习就那些东西，认为机器学习也就那么一回事，想把这几年关于机器学习的东西做一些总结，能够跟大家一起学习和交流。 &lt;/p&gt;&lt;p&gt;如果需要用几句话来简单的总结机器学习是什么意思，也许可以用：让机器学会决策。对比我们人来说，每天都会碰到这个问题，比如菜市场里买芒果，总要挑出哪些是甜的。这就是所谓的决策，再通俗来讲就是分类问题了，把一堆芒果，分出甜和不甜的。而机器学习就是学会把甜和不甜的芒果分出来，那如何分呢？模拟人类的思考方式。凭经验，我们可以按照芒果皮的颜色，大小等来对芒果的酸甜进行分类，对于机器来说，把芒果的颜色，大小等当成变量输入到电脑模型里，就能推出芒果的酸甜性，这样就对芒果进行分类。 &lt;/p&gt;&lt;p&gt;机器学习的分类算法有非常的多，这篇主要介绍的是Logistic回归。&lt;/p&gt;&lt;h2&gt;最小二乘法和极大似然估计&lt;/h2&gt;&lt;p&gt;从一个最简单的数学问题开始。 &lt;/p&gt;&lt;p&gt;1801年，意大利天文学家朱赛普·皮亚齐发现了第一颗小行星谷神星。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥尔伯斯根据高斯计算出来的轨道重新发现了谷神星。 &lt;/p&gt;&lt;p&gt;从这段历史记录可以看出，高斯当时观察了很多小行星谷神星的记录，也就是我们常说的观察数据，并使用了最小二乘法模拟了这条线，预测了小行星谷神星的轨迹。 &lt;/p&gt;&lt;p&gt;高斯最小二乘法的方法发表于1809年他的著作《天体运动论》中。其实在1806年法国科学家勒让德就提出了最小二乘法相应的想法，所以勒让德曾与高斯为谁最早创立最小二乘法原理发生争执。 &lt;/p&gt;&lt;p&gt;最小二乘法的思想是什么呢？&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-98ba3a21018f105332eb9f681bf59332.png" data-rawwidth="593" data-rawheight="450"&gt;&lt;p&gt;假设观察数据(x&lt;em&gt;1,y&lt;/em&gt;1 ), (x&lt;em&gt;2,y&lt;/em&gt;2 ), (x&lt;em&gt;3,y&lt;/em&gt;3 )…,(x&lt;em&gt;n,y&lt;/em&gt;n ),而默认这些数据是符合最常见的规律，既x和y符合线性关系，用方程可以表示为 y=a+bx &lt;/p&gt;&lt;p&gt;其中，a,b是我们需要通过观察数据确定的参数。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bbb5854ec9c8e6a90510ca229a4091bb.png" data-rawwidth="578" data-rawheight="429"&gt;&lt;p&gt;所谓最小二乘法就是这样的一个法则，按照这样法则，最好是拟合于各个数据点的最佳曲线应该使个数据点与曲线偏差的平均和为最小。 &lt;/p&gt;&lt;p&gt;用数学公式表示为：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-779ad18638f92dec4c93cb8354d5cf12.png" data-rawwidth="551" data-rawheight="176"&gt;&lt;p&gt;其公式的值为最小，这里的a,b是参数。有两个参数，求最小值，即为求偏差平方和对a和b分别求出偏导数，得：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-cb4b6370947f742108e3d596160e72fc.png" data-rawwidth="743" data-rawheight="239"&gt;&lt;/p&gt;&lt;p&gt;根据公式可以推出a和b的值： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-417a869e7ba51d488de65b1f910a7ecc.png" data-rawwidth="542" data-rawheight="504"&gt;&lt;p&gt;这样就可以求出了a和b的值。 &lt;/p&gt;&lt;p&gt;既我们可以通过观察的数据，来拟合我们的直线，既可以在给定某个x，有效的预测y。通常来说求出的值a和b跟实际本身来说是有一定的误差了，是不是给定的观察值越多就越准确呢？这不一定，这也是大学概率论和数值统计中一直讨论的问题。 &lt;/p&gt;&lt;p&gt;再次的强调最小二乘法是使用误差最小来进行估计的。 &lt;/p&gt;&lt;p&gt;回头过来看，可能会觉得最小二乘法跟我们讨论中的芒果酸甜问题，并不是一回事。但从另外一种退化的角度来讲： &lt;/p&gt;&lt;p&gt;在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。 &lt;/p&gt;&lt;p&gt;继续一个简单的故事：某位同学与一位猎人一起出去打猎，一只兔子从前方窜过。只听见一声枪响，野兔应声倒下，如果要你来推测，这一发命中的子弹是谁打的？你会怎么想呢？正常的情况下，猎人的枪法肯定比你的同学的枪法好，也就是说猎人的命中率比你的同学高。 而一枪就打死兔子，命中率是100%的，这么高的命中率，应该是谁打中的呢？显然，猎人开的枪比较符合我们观察的想象了吧。如果是开了三枪才打中兔子的话，那枪法就不怎么样了，某同学开的枪比较符合已经发生的现象了。 &lt;/p&gt;&lt;p&gt;这就是要讲的，极大似然法。如果试验n次，我们得到n个样本，极大似然估计是要是所求的概率，最大限制的符合我们现在所发生的。这里我们这样定义似然函数：假设{y&lt;em&gt;1,…,y&lt;/em&gt;n }为独立同分布，则样本数据的联合密度函数为 f(y&lt;em&gt;1;θ)f(y&lt;/em&gt;2;θ)∙∙∙f(y_n;θ)，定义“似然函数”为， &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f21f5222704a5ce6b99dce3cd20cf0d9.png" data-rawwidth="594" data-rawheight="243"&gt;&lt;p&gt;为了较好的说明，举一个很简单的例子：两点分布的情况。设某工序生产的产品不合格率为p，抽n个产品作检验，发现有T个不合格，试求p的极大似然估计值。在这里我们做了n次的试验，我们所求的概率p要符合我们试验的结果，也就是通过极大似然函数来求解。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cb8315a2372d40b85f67619302818fa1.png" data-rawwidth="441" data-rawheight="218"&gt;&lt;/p&gt;&lt;p&gt;最大似然函数的思想也就是想使我们求得的概率符合我们所观察的。而最大似然法看起来，好像只是为了求得某个概率，但恰恰是我们Logistic回归中用到的一种方法。 了解了最小二乘法与极大似然估计之后，我们暂时先休息一下，在下半部分我们会进入正题介绍 Logistic 回归，以及简单介绍 Logistic 在 TensorFlow 中的用法。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22564500&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 22 Sep 2016 10:34:28 GMT</pubDate></item><item><title>永不过时的 K 均值算法</title><link>https://zhuanlan.zhihu.com/p/22431592</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3e1438ca8892ad5da4011ccf51e8b032_r.jpg"&gt;&lt;/p&gt;&lt;h3&gt;引言&lt;/h3&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3e1438ca8892ad5da4011ccf51e8b032.jpg" data-rawwidth="561" data-rawheight="420"&gt;&lt;p&gt;众所周知，数据挖掘算法并非十全十美，在某些情况下他们也会失效。 使用 K 均值算法（K-Means）时就可能会出现这种情况，当然此时你可以尝试一下另一种方法—— K 中心聚类算法（K-Medoids），也许效果会更好。&lt;/p&gt;&lt;p&gt;在该网站之前的文章《揭开机器学习的面纱》中，已经指出， K 均值算法用于聚类时效果良好，而且在数据挖掘和机器学习领域，它也有着重要的地位。Psanchezcri 就曾在他的文章《将 K 均值方法用于金融时序回报率聚类》中，将 K 均值算法用于分析金融时间序列的趋势。&lt;/p&gt;&lt;p&gt;然而，即使在网络上有关算法的文档浩如烟海的情况下，关于机器学习算法有时会失效的讨论却并不多见。&lt;/p&gt;&lt;p&gt;因此，本文借由一个金融案例来反映这个问题。&lt;/p&gt;&lt;h3&gt;思路&lt;/h3&gt;&lt;p&gt;1）首先，我们在欧洲斯托克600指数的成分股中选择三组共6只股票（在三个不同的部门中各选两只）： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;金融部门：　　西班牙毕尔巴鄂比斯开银行 &amp;amp; 桑坦德银行&lt;/li&gt;&lt;li&gt;非必需消费品：　　法国酩悦·轩尼诗－路易·威登 &amp;amp; 迪奥&lt;/li&gt;&lt;li&gt;能源部门：　　英国石油公司 &amp;amp; 锡尼什港能源公司&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2）搜集数据，并绘出在2013/01/01至2015/12/31期间这六只股票的价格走势曲线。如下所示：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/99af96c71097923cdbf46af0bf7e01b3.png" data-rawwidth="640" data-rawheight="336"&gt;&lt;/p&gt;&lt;p&gt;3）选择日回报率作为计算指标，我们算出三组股票序列的相关距离。然后通过距离矩阵降维的方法，在二维欧氏空间中绘出每个点。&lt;/p&gt;&lt;p&gt;结果显示这六只股票可以按部门进行分类效果显著。下图以蓝色菱形点、绿色正方形点、红色圆点来标记六只股票，明显可以按部门分为三类：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9f4441afc56a520c157a228a09780e1d.png" data-rawwidth="502" data-rawheight="292"&gt;&lt;p&gt;4）最后，我们将 K 均值算法运用于距离矩阵，聚类目标预先设定分成3类。由于 K 均值算法是从随机点开始的，每次运行结果可能有所不同，本文我们预先设定运行这个算法15次，即产生15个结果。当然，我们希望得到聚类结果符合股票所属部门的实际情况。 &lt;/p&gt;&lt;h3&gt;结论&lt;/h3&gt;&lt;p&gt;１）在约80%的聚类结果中，K 均值聚类算法取得了理想的结果，聚类结果与这六只股票所属部门相符，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/08af0acea4385818a84e736638e17ecd.png" data-rawwidth="502" data-rawheight="292"&gt;２）在剩下的20%的聚类结果中，算法则出现了聚类的错误。例如，下图中错将两个不同部门的四只股票聚为一类（图中蓝色菱形点和绿色正方形点），而将同一部门的两只股票分为两类（图中红色圆点）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b520bafdcbb1dbf8860df4439a73aece.png" data-rawwidth="502" data-rawheight="292"&gt;如果我们使用与之思想类似的 K 中心聚类算法，结果则可以达到100%的正确聚类率。这表明在聚类时，似乎使用重心会比用均值来衡量距离，效果更好。 &lt;/p&gt;&lt;p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接： &lt;a href="http://quantdare.com/2016/04/k-means-vs-k-medoids/" data-editable="true" data-title="“K-Means never fails”, they said…"&gt;“K-Means never fails”, they said…&lt;/a&gt;原文作者：Fjrodriguez2译者：Vector&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22431592&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 14 Sep 2016 10:27:57 GMT</pubDate></item><item><title>利用 Pandas 来分析 MovieLens 数据集</title><link>https://zhuanlan.zhihu.com/p/21268937</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/fd1ba61f2d32bc752f0bf181d43dbabe_r.png"&gt;&lt;/p&gt;&lt;h1&gt;利用 Pandas 来分析 MovieLens 数据集&lt;a class="" href="#%E5%88%A9%E7%94%A8-Pandas-%E6%9D%A5%E5%88%86%E6%9E%90-MovieLens-%E6%95%B0%E6%8D%AE%E9%9B%86" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;为了展现 Pandas 的实用性，本文将利用 Pandas 来解决 MovieLen 数据集的一些问题。我们首先回顾下如何将数据集读进 DataFrame 中并将其合并：&lt;/p&gt;In [1]:&lt;pre&gt;&lt;code lang="text"&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;In [2]:&lt;pre&gt;&lt;code lang="text"&gt;# pass in column names for each CSV
u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']
users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols,
                    encoding='latin-1')

r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']
ratings = pd.read_csv('ml-100k/u.data', sep='\t', names=r_cols,
                      encoding='latin-1')

# the movies file contains columns indicating the movie's genres
# let's only load the first five columns of the file with usecols
m_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']
movies = pd.read_csv('ml-100k/u.item', sep='|', names=m_cols, usecols=range(5),
                     encoding='latin-1')

# create one merged DataFrame
movie_ratings = pd.merge(movies, ratings)
lens = pd.merge(movie_ratings, users)
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;评价最多的 25 部电影&lt;a class="" href="#%E8%AF%84%E4%BB%B7%E6%9C%80%E5%A4%9A%E7%9A%84-25-%E9%83%A8%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;In [3]:&lt;pre&gt;&lt;code lang="text"&gt;most_rated = lens.groupby('title').size().sort_values(ascending=False)[:25]
most_rated
&lt;/code&gt;&lt;/pre&gt;Out[3]:&lt;pre&gt;&lt;code lang="text"&gt;title
Star Wars (1977)                             583
Contact (1997)                               509
Fargo (1996)                                 508
Return of the Jedi (1983)                    507
Liar Liar (1997)                             485
English Patient, The (1996)                  481
Scream (1996)                                478
Toy Story (1995)                             452
Air Force One (1997)                         431
Independence Day (ID4) (1996)                429
Raiders of the Lost Ark (1981)               420
Godfather, The (1972)                        413
Pulp Fiction (1994)                          394
Twelve Monkeys (1995)                        392
Silence of the Lambs, The (1991)             390
Jerry Maguire (1996)                         384
Chasing Amy (1997)                           379
Rock, The (1996)                             378
Empire Strikes Back, The (1980)              367
Star Trek: First Contact (1996)              365
Back to the Future (1985)                    350
Titanic (1997)                               350
Mission: Impossible (1996)                   344
Fugitive, The (1993)                         336
Indiana Jones and the Last Crusade (1989)    331
dtype: int64&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上述代码的含义是先将 DataFrame 按电影标题分组，接下来利用 &lt;em&gt;size&lt;/em&gt; 方法计算每组样本的个数，最后按降序方式输出前 25 条观测值。&lt;/p&gt;&lt;p&gt;在 SQL 中，等价的代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, count(1)
FROM lens
GROUP BY title
ORDER BY 2 DESC
LIMIT 25;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，在 Pandas 中有一个非常好用的替代函数—— &lt;em&gt;value_counts&lt;/em&gt;：&lt;/p&gt;In [4]:&lt;pre&gt;&lt;code lang="text"&gt;lens.title.value_counts()[:25]
&lt;/code&gt;&lt;/pre&gt;Out[4]:&lt;pre&gt;&lt;code lang="text"&gt;Star Wars (1977)                             583
Contact (1997)                               509
Fargo (1996)                                 508
Return of the Jedi (1983)                    507
Liar Liar (1997)                             485
English Patient, The (1996)                  481
Scream (1996)                                478
Toy Story (1995)                             452
Air Force One (1997)                         431
Independence Day (ID4) (1996)                429
Raiders of the Lost Ark (1981)               420
Godfather, The (1972)                        413
Pulp Fiction (1994)                          394
Twelve Monkeys (1995)                        392
Silence of the Lambs, The (1991)             390
Jerry Maguire (1996)                         384
Chasing Amy (1997)                           379
Rock, The (1996)                             378
Empire Strikes Back, The (1980)              367
Star Trek: First Contact (1996)              365
Titanic (1997)                               350
Back to the Future (1985)                    350
Mission: Impossible (1996)                   344
Fugitive, The (1993)                         336
Indiana Jones and the Last Crusade (1989)    331
Name: title, dtype: int64&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;评价最高的电影&lt;a class="" href="#%E8%AF%84%E4%BB%B7%E6%9C%80%E9%AB%98%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;In [6]:&lt;pre&gt;&lt;code lang="text"&gt;movie_stats = lens.groupby('title').agg({'rating': [np.size, np.mean]})
movie_stats.head()
&lt;/code&gt;&lt;/pre&gt;Out[6]:ratingsizemeantitle'Til There Was You (1997)92.3333331-900 (1994)52.600000101 Dalmatians (1996)1092.90825712 Angry Men (1957)1254.344000187 (1997)413.024390&lt;p&gt;我们可以利用 &lt;em&gt;agg&lt;/em&gt; 方法来进行分组汇总计算，其参数包括键值和汇总方法。接下来我们对汇总结果进行排序即可得到评价最高的电影：&lt;/p&gt;In [7]:&lt;pre&gt;&lt;code lang="text"&gt;# sort by rating average
movie_stats.sort_values([('rating', 'mean')], ascending=False).head()
&lt;/code&gt;&lt;/pre&gt;Out[7]:ratingsizemeantitleThey Made Me a Criminal (1939)15Marlene Dietrich: Shadow and Light (1996)15Saint of Fort Washington, The (1993)25Someone Else's America (1995)15Star Kid (1997)35&lt;p&gt;由于 &lt;em&gt;movie_stats&lt;/em&gt; 是一个 DataFrame，因此我们可以利用 &lt;em&gt;sort&lt;/em&gt; 方法来排序——Series 对象则使用&lt;em&gt;order&lt;/em&gt; 方法。此外，由于该数据集包含多层索引，所以我们需要传递一个元组数据来指定排序变量。&lt;/p&gt;&lt;p&gt;上表列出来的电影中评价数量都非常少，以致于我们无法从中得到一些有价值的信息。因此我们考虑对数据集进行筛选处理，只分析评价数量大于 100 的电影：&lt;/p&gt;In [9]:&lt;pre&gt;&lt;code lang="text"&gt;atleast_100 = movie_stats['rating']['size'] &amp;gt;= 100
movie_stats[atleast_100].sort_values([('rating', 'mean')], ascending=False)[:15]
&lt;/code&gt;&lt;/pre&gt;Out[9]:ratingsizemeantitleClose Shave, A (1995)1124.491071Schindler's List (1993)2984.466443Wrong Trousers, The (1993)1184.466102Casablanca (1942)2434.456790Shawshank Redemption, The (1994)2834.445230Rear Window (1954)2094.387560Usual Suspects, The (1995)2674.385768Star Wars (1977)5834.35849112 Angry Men (1957)1254.344000Citizen Kane (1941)1984.292929To Kill a Mockingbird (1962)2194.292237One Flew Over the Cuckoo's Nest (1975)2644.291667Silence of the Lambs, The (1991)3904.289744North by Northwest (1959)1794.284916Godfather, The (1972)4134.283293&lt;p&gt;这个结果看起来比较靠谱，需要注意的是在这里我们利用布尔索引来筛选数据。&lt;/p&gt;&lt;p&gt;在SQL中，等价的代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, COUNT(1) size, AVG(rating) mean
FROM lens
GROUP BY title
HAVING COUNT(1) &amp;gt;= 100
ORDER BY 3 DESC
LIMIT 15;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;筛选部分数据&lt;a class="" href="#%E7%AD%9B%E9%80%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;为了便于进一步分析，我们从数据集中筛选出评价数最高的 50 部电影：&lt;/p&gt;In [10]:&lt;pre&gt;&lt;code lang="text"&gt;most_50 = lens.groupby('movie_id').size().sort_values(ascending=False)[:50]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;SQL 中的等价代码为：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;CREATE TABLE most_50 AS(
    SELECT movie_id, COUNT(1)
    FROM lens
    GROUP BY movie_id
    ORDER BY 2 DESC
    LIMIT 50
);&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，我们也可以利用 EXISTS, IN 或者 JOIN 来过滤数据：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT *
FROM lens
WHERE EXISTS (SELECT 1 FROM most_50 WHERE lens.movie_id = most_50.movie_id);&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;不同年龄段观众之间争议最大的电影&lt;a class="" href="#%E4%B8%8D%E5%90%8C%E5%B9%B4%E9%BE%84%E6%AE%B5%E8%A7%82%E4%BC%97%E4%B9%8B%E9%97%B4%E4%BA%89%E8%AE%AE%E6%9C%80%E5%A4%A7%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;首先，我们来看下数据集中用户的年龄分布情况：&lt;/p&gt;In [16]:&lt;pre&gt;&lt;code lang="text"&gt;users.age.plot.hist(bins=30)
plt.title("Distribution of users' ages")
plt.ylabel("count of users")
plt.xlabel("age");
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pandas 整合了 matplotlib 的基础画图功能，我们只需要对列变量调用 &lt;em&gt;hist&lt;/em&gt; 方法即可绘制直方图。当然了，我们也可以利用 matplotlib.pyplot 来自定义绘图。&lt;/p&gt;&lt;h2&gt;对用户进行分箱处理&lt;a class="" href="#%E5%AF%B9%E7%94%A8%E6%88%B7%E8%BF%9B%E8%A1%8C%E5%88%86%E7%AE%B1%E5%A4%84%E7%90%86" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;我认为直接对比不同年龄用户的行为无法得到有价值的信息，所以我们应该根据用户的年龄情况利用&lt;em&gt;pandas.cut&lt;/em&gt; 将所有用户进行分箱处理。&lt;/p&gt;In [17]:&lt;pre&gt;&lt;code lang="text"&gt;labels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79']
lens['age_group'] = pd.cut(lens.age, range(0, 81, 10), right=False, labels=labels)
lens[['age', 'age_group']].drop_duplicates()[:10]
&lt;/code&gt;&lt;/pre&gt;Out[17]:ageage_group06060-693972120-294593330-395243030-397822320-299952920-2912292620-2916643130-3919422420-2922703230-39&lt;p&gt;上述代码中，我们首先创建分组标签名，然后根据年龄变量将用户分成八组（0-9, 10-19, 20-29,...）,其中参数 &lt;em&gt;right=False&lt;/em&gt; 用于剔除掉区间上界数据，即30岁的用户对应的标签为 30-39。&lt;/p&gt;&lt;p&gt;现在我们可以比较不同年龄组之间的评分情况：&lt;/p&gt;In [18]:&lt;pre&gt;&lt;code lang="text"&gt;lens.groupby('age_group').agg({'rating': [np.size, np.mean]})
&lt;/code&gt;&lt;/pre&gt;Out[18]:ratingsizemeanage_group0-9433.76744210-1981813.48612620-29395353.46733330-39256963.55444440-49150213.59177250-5987043.63580060-6926233.64887570-791973.649746&lt;p&gt;从上表中我们可以看出，年轻用户比其他年龄段的用户更加挑剔。接下来让我们看下这 50 部热评电影中不同年龄组用户的评价情况。&lt;/p&gt;In [19]:&lt;pre&gt;&lt;code lang="text"&gt;lens.set_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [21]:&lt;pre&gt;&lt;code lang="text"&gt;by_age = lens.loc[most_50.index].groupby(['title', 'age_group'])
by_age.rating.mean().head()
&lt;/code&gt;&lt;/pre&gt;Out[21]:&lt;pre&gt;&lt;code lang="text"&gt;title                 age_group
Air Force One (1997)  10-19        3.647059
                      20-29        3.666667
                      30-39        3.570000
                      40-49        3.555556
                      50-59        3.750000
Name: rating, dtype: float64&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要注意的是，此处的电影标题和年龄组都是索引值，平均评分为 Series 对象。如果你觉得这个展示结果不直观的话，我们可以利用 &lt;em&gt;unstack&lt;/em&gt; 方法将其转换成表格形式。&lt;/p&gt;In [27]:&lt;pre&gt;&lt;code lang="text"&gt;by_age.rating.mean().unstack(1).fillna(0)[10:20]
&lt;/code&gt;&lt;/pre&gt;Out[27]:age_group0-910-1920-2930-3940-4950-5960-6970-79titleE.T. the Extra-Terrestrial (1982)03.6800003.6090913.8068184.1600004.3684214.3750000.000000Empire Strikes Back, The (1980)44.6428574.3116884.0520834.1000003.9090914.2500005.000000English Patient, The (1996)53.7391303.5714293.6218493.6346153.7746483.9047624.500000Fargo (1996)03.9375004.0104714.2307694.2941184.4423084.0000004.333333Forrest Gump (1994)54.0476193.7857143.8617023.8478264.0000003.8000000.000000Fugitive, The (1993)04.3200003.9699253.9814814.1904764.2400003.6666670.000000Full Monty, The (1997)03.4210534.0568183.9333333.7142864.1463414.1666673.500000Godfather, The (1972)04.4000004.3450704.4128443.9294124.4634154.1250000.000000Groundhog Day (1993)03.4761903.7982463.7866673.8510643.5714293.5714294.000000Independence Day (ID4) (1996)03.5952383.2914293.3893813.7187503.8888892.7500000.000000&lt;p&gt;&lt;em&gt;unstack&lt;/em&gt; 方法主要用于拆分多层索引，此例中我们将移除第二层索引然后将其转换成列向量，并用 0 来填补缺失值。&lt;/p&gt;&lt;h2&gt;男士与女士分歧最大的电影&lt;a class="" href="#%E7%94%B7%E5%A3%AB%E4%B8%8E%E5%A5%B3%E5%A3%AB%E5%88%86%E6%AD%A7%E6%9C%80%E5%A4%A7%E7%9A%84%E7%94%B5%E5%BD%B1" data-editable="true" data-title="知乎专栏 - 随心写作，自由表达"&gt;知乎专栏 - 随心写作，自由表达&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;首先思考下你会如何利用 SQL 来解决这个问题，你可能会利用判断语句和汇总函数来旋转你的数据集，你的查询语句大概会是这个样子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;SELECT title, AVG(IF(sex = 'F', rating, NULL)), AVG(IF(sex = 'M', rating, NULL))
FROM lens
GROUP BY title;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;想象下，如果你必须处理多列数据的话，这样运算是多么的麻烦。DataFrame 提供了一个简便的方法—— &lt;em&gt;pivot_table&lt;/em&gt; 。&lt;/p&gt;In [29]:&lt;pre&gt;&lt;code lang="text"&gt;lens.reset_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [30]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted = lens.pivot_table(index = ['movie_id', 'title'], columns = ['sex'],
                          values = 'rating', fill_value = 0)
pivoted.head()
&lt;/code&gt;&lt;/pre&gt;Out[30]:sexFMmovie_idtitle1Toy Story (1995)3.7899163.9099102GoldenEye (1995)3.3684213.1785713Four Rooms (1995)2.6875003.1081084Get Shorty (1995)3.4000003.5914635Copycat (1995)3.7727273.140625In [31]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted['diff'] = pivoted.M - pivoted.F
pivoted.head()
&lt;/code&gt;&lt;/pre&gt;Out[31]:sexFMdiffmovie_idtitle1Toy Story (1995)3.7899163.9099100.1199942GoldenEye (1995)3.3684213.178571-0.1898503Four Rooms (1995)2.6875003.1081080.4206084Get Shorty (1995)3.4000003.5914630.1914635Copycat (1995)3.7727273.140625-0.632102In [32]:&lt;pre&gt;&lt;code lang="text"&gt;pivoted.reset_index('movie_id', inplace=True)
&lt;/code&gt;&lt;/pre&gt;In [34]:&lt;pre&gt;&lt;code lang="text"&gt;disagreements = pivoted[pivoted.movie_id.isin(most_50.index)]['diff']
disagreements.sort_values().plot(kind='barh', figsize=[9, 15])
plt.title('Male vs. Female Avg. Ratings\n(Difference &amp;gt; 0 = Favored by Men)')
plt.ylabel('Title')
plt.xlabel('Average Rating Difference');
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上图中我们可以看出，男性喜欢《终结者》的程度远高于女性，女性用户则更喜欢《独立日》。这个结果可靠吗？&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/"&gt;http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：Greg Reda&lt;/p&gt;&lt;p&gt;译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21268937&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 08 Sep 2016 10:44:57 GMT</pubDate></item><item><title>Pandas中的链式方法</title><link>https://zhuanlan.zhihu.com/p/22238376</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c0de4843c8d39b533a4279958de366f0_r.png"&gt;&lt;/p&gt;&lt;p&gt;链式方法是当前比较流行的一种语法规则。 在过去的几个版本中，我们已经提到了几个支持链式方法的函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;assign (0.16.0): 用于往 DataFrame 中增加新变量(类似于 dplyr 中的 mutate 函数)&lt;/li&gt;&lt;li&gt;pipe (0.16.2): 用于包含用户自定义的链式方法&lt;/li&gt;&lt;li&gt;rename (0.18.0): 用于改变轴名称&lt;/li&gt;&lt;li&gt;Window methods (0.18): 利用类似于 groupby 的 API 接口调用 pd.rolling* 和 pd.expanding* 顶层函数的 NDFrame 方法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文将从一个简单的例子说起： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7e4f72b9a6995046642344c62688d314.png" data-rawwidth="429" data-rawheight="710"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9eb112f7c3631863fe3377a849445dd2.png" data-rawwidth="445" data-rawheight="480"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c55505d56eeebad2f1b0d43905c24fe0.png" data-rawwidth="633" data-rawheight="708"&gt;&lt;p&gt;我觉得链式方法的代码非常易读，但是有些人却并了解它。它并不像重嵌套函数那样循环调用参数，它的所有代码和流程都是自上而下运行的，这大大增强了代码的可读性。&lt;/p&gt;&lt;p&gt;我最喜欢的示例来自 Jeff Allen，比较以下这两段功能相同但风格迥异的代码：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d7179d23493012ad018483bf08842cc4.png" data-rawwidth="560" data-rawheight="174"&gt;&lt;/p&gt;&lt;p&gt;和&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0903e08ec50c54113a0e40f6d9f871d2.png" data-rawwidth="555" data-rawheight="130"&gt;&lt;p&gt;对比上述两种风格的代码，你会发现即使你不知道 R 语言中管道符号 %&amp;gt;% 的功能，你也能很轻易地看懂第二段代码。而对于第一段代码而言，你需要弄清楚代码的执行顺序以及如何处理相应的函数参数。 &lt;/p&gt;&lt;p&gt;作为读者，你可能会说你不会写出类似于重嵌套风格的代码，但是大多数情况下你的代码应该是如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dfef7df084846fa9da378a06c6e30fe6.png" data-rawwidth="563" data-rawheight="114"&gt;&lt;p&gt;我非常不喜欢这个风格的代码，因为我需要花费很多时间来思考如何对变量进行命名。这是非常令人困扰的事情，因为我们根本不关心 on_hill 这些中间变量。 &lt;/p&gt;&lt;p&gt;上述代码的第四种实现方法是可行的，假设你拥有一个 JackAndJill 对象并且你可以自定义一些方法。那么你可以实现类似于 R 语言中的管道功能：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/28dcbfd36824238ab0a85d9181d0130c.png" data-rawwidth="560" data-rawheight="158"&gt;&lt;/p&gt;&lt;p&gt;但是这种方法的问题在于如果的数据不是 ndarray 或者 DataFrame 或者 DataArray，那么上述的方法就不存在了。而且我们很难对 DataFrame 的子类进行拓展从而来适应自定义的方法。同时，你所创建的从 DataFrame 中继承的子类可能仅适用于你自己的代码，无法和其他方法进行交互操作，因此你的代码将会非常零散。 &lt;/p&gt;&lt;p&gt;或者你可以往 pandas 的项目中提交新的 pull request，从而实现自己的方法。但是你需要说服该项目的维护者，你的新方法值得加入到该项目中并维护之。而且 DataFrame 目前已经拥有超过 250 种的方法，因此我们不愿意增加更多的方法。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3646050ed15aa39528cf0ebecd3c9f82.png" data-rawwidth="632" data-rawheight="151"&gt;DataFrame.pipe 的第一个参数是 DataFrame，我们只需要指明后续的参数即可。&lt;/p&gt;&lt;h2&gt;成本&lt;/h2&gt;&lt;p&gt;过长的链式代码的缺点是调试比较麻烦。由于没有生成中间变量值，所以如果代码出问题了，我们无法直接定位出问题在哪。Python 中的生成器也有类似的问题，借助生成器机制我们可以降低计算机内存消耗，但是此时我们比较难调试程序。 &lt;/p&gt;&lt;p&gt;就我常用的探索分析过程而言，这并不是一个大问题。我平常处理的都是不会再更新的数据集，而且对原始数据集进行加工的步骤也不多。 &lt;/p&gt;&lt;p&gt;对于规模较大的工作流程，你可能需要借助 pandas 的其他功能，比如 Airflow 或者 Luigi。 对于需要重复运行的中等规模 ETL 工作流程，我将借助装饰器来审查 DataFrame 每个工作步骤所产生的属性日志。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/dabaa7560ef4bd132f3551579a96834f.png" data-rawwidth="565" data-rawheight="659"&gt;&lt;/p&gt;&lt;p&gt;借助我之前制作的一个用于验证管道中数据集有效性的软件库 engarde，我们可以很好地完成工作。 &lt;/p&gt;&lt;h2&gt;Inplace?&lt;/h2&gt;&lt;p&gt;大多数 pandas 的方法都有一个默认值为 False 的关键词 inplace。通常来说，你不应该做 inplace 运算。&lt;/p&gt;&lt;p&gt;首先，如果你喜欢用链式规则来写代码的话，你肯定不会用 inplace 运算，因为这会导致最终返回的结果是 None，并中断相应的管道链。 &lt;/p&gt;&lt;p&gt;其次，我怀疑存在一个适合 inplace 运算的构思模型。也就是说，最终结果并不会被分配到额外的存储器中。但实际上这可能是不真实的，pandas 中还存在许多下述用法：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/46de628b0a4c3cc4fc71b0f6a5d66af3.png" data-rawwidth="563" data-rawheight="159"&gt;&lt;/p&gt;&lt;p&gt;最后，类似于 ibis 或者 dask 这种类型的项目 inplace 运算并没有任何意义，因为此时你需要处理表达式或者建立可执行的 DAG 任务，而不仅仅是处理数据而已。 &lt;/p&gt;&lt;p&gt;我觉得到此为止我并没有怎么写代码，更多的是在介绍一些额外的东西，我对此感到非常抱歉。接下来，让我们做一些探索性分析吧。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/bbdd2d512287764ca2e69df997eb7792.png" data-rawwidth="496" data-rawheight="91"&gt;一架一天执行多趟航班执飞任务的飞机“堵机”了，会导致靠后的航班延误更长时间吗？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/acfa569ec544481c13d5d579fd7ccc7a.png" data-rawwidth="484" data-rawheight="318"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/7976cf95b8b0b1fc9d025d4c626e1861.png" data-rawwidth="908" data-rawheight="339"&gt;一天中较晚起飞的航班会延误更长时间吗？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f0e1301cc1a1d6fac776ebc66a8e292a.png" data-rawwidth="483" data-rawheight="231"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/13910c8a27b2993b9b0cb84844c05a78.png" data-rawwidth="901" data-rawheight="339"&gt;我们将延误超过十小时的数据视为异常值并将其剔除掉。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/57165e540bbf3b950a1e3f1c41c7ea5d.png" data-rawwidth="485" data-rawheight="134"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/92c31f96b9604638b4090bd7e9f03195.png" data-rawwidth="908" data-rawheight="339"&gt;接下来，我们仅考虑确实发生延误的航班数据。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/aeadc5bed6b3d5662f023524507734c9.png" data-rawwidth="480" data-rawheight="127"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/13910c8a27b2993b9b0cb84844c05a78.png" data-rawwidth="901" data-rawheight="339"&gt;哪个航班的延误情况最严重呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6bbabbca6b9356558c7420998b14be84.png" data-rawwidth="484" data-rawheight="226"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a96186c8c288cbb7f4547c9940c164cd.png" data-rawwidth="506" data-rawheight="362"&gt;哪个航空公司的延误情况最严重呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7ea2ec9a136b4ced2f4fd6ea598561f2.png" data-rawwidth="544" data-rawheight="98"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d7ed93db045bfd74d1e66492bd42a3f6.png" data-rawwidth="651" data-rawheight="434"&gt;&lt;/p&gt;&lt;p&gt;B6 是美国捷蓝航空公司。 &lt;/p&gt;&lt;p&gt;I wanted to try out scikit-learn's new Gaussian Process module so here's a pretty picture.&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4269e43cbb28ccf025bbec763e907482.png" data-rawwidth="513" data-rawheight="867"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/633c8f67ae06d7753029ba6b8b5e621b.png" data-rawwidth="561" data-rawheight="226"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d9f5d39a24f83f315b9e6d8d8b8339cb.png" data-rawwidth="380" data-rawheight="211"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d408a1cd7f6250e7a3885144583e9635.png" data-rawwidth="561" data-rawheight="366"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/96185ff7b0539cb04b978beab12d132a.png" data-rawwidth="561" data-rawheight="527"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1b229c6f7e3bed45edd29278dc4af5fb.png" data-rawwidth="845" data-rawheight="412"&gt;&lt;/p&gt;&lt;p&gt;谢谢阅读本文！由于我们更多地讨论了关于代码风格的问题而不是介绍实际案例操作，所以本文所介绍的内容比较抽象。谢谢你们的包容，下次我将介绍一个偏实务的话题！&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接：&lt;a href="http://tomaugspurger.github.io/method-chaining.html" data-editable="true" data-title="DatasFrame"&gt;DatasFrame&lt;/a&gt;原文作者：Tom Augspurger译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22238376&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 01 Sep 2016 10:57:52 GMT</pubDate></item><item><title>在python 中如何将 list 转化成 dictionary</title><link>https://zhuanlan.zhihu.com/p/21981759</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0658b34b4498516ba1014e6fe6490b88_r.jpg"&gt;&lt;/p&gt;&lt;h4&gt;问题1：如何将一个list转化成一个dictionary？&lt;/h4&gt;问题描述：比如在python中我有一个如下的list，其中奇数位置对应字典的key，偶数位置为相应的value&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c505279f860a835ad201780d3a663ddd.png" data-rawwidth="361" data-rawheight="198"&gt;解决方案:&lt;p&gt;1.利用zip函数实现&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/cb7209edd847833afc81b01e45b0f87c.png" data-rawwidth="361" data-rawheight="218"&gt;2.利用循环来实现&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/f3c0e4da9add83554be645fd93f214cb.png" data-rawwidth="371" data-rawheight="261"&gt;3.利用 enumerate 函数生成index来实现&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0efd2a1e482e594943ec765e95832a02.png" data-rawwidth="371" data-rawheight="277"&gt;&lt;h4&gt;问题2 我们如何将两个list 转化成一个dictionary？&lt;/h4&gt;问题描述：假设你有两个list&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d1e4cb13992b6bebf444345205b5a015.png" data-rawwidth="495" data-rawheight="210"&gt;解决方案：还是常见的zip函数&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3b9ab721966176dd65daccfa32835d8d.png" data-rawwidth="365" data-rawheight="219"&gt;这里我们看到了zip函数确实在配对上面起到了很不错的效果，如果两个list都很大，你需要引入itertools.izip来解决问题。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/79fe85b7f1c81366a965acc303ae8da5.png" data-rawwidth="363" data-rawheight="164"&gt;或者下面的直接使用dict函数&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/381d4be7e04ae723a86fd2d8b7432454.png" data-rawwidth="362" data-rawheight="362"&gt;&lt;/p&gt;那么如果我们有三个lsit呢？比如我们有时候会遇到这样的问题比如在一个经纬度下面记录某个数据，这个时候又该怎么实现呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7c3300f15f1f32f401f92e9ec1f70054.png" data-rawwidth="365" data-rawheight="339"&gt;&lt;p&gt;我们可以看到这个时候 zip函数还是可以帮助我们成功的实现所需要的功能，首先将经纬度一一配对整合到一起，随后再将val连起来，最后使用dict函数放在一起。&lt;/p&gt;&lt;p&gt;通过上面的例子，我们知道可以通过zip函数的多次调用来整合数据，最终解决问题。&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21981759&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Fri, 12 Aug 2016 11:21:29 GMT</pubDate></item><item><title>一个计算我的妻子是否怀孕的贝叶斯模型</title><link>https://zhuanlan.zhihu.com/p/21956061</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cb124fbccfd34346eaf63e59429767be_r.png"&gt;&lt;/p&gt;&lt;p&gt;在2015年的二月21日，我的妻子已经33天没有来月经了，她怀孕了，这真是天大的好消息！通常月经的周期是大约一个月，如果你们夫妇打算怀孕，那么月经没来或许是一个好消息。但是33天，这还无法确定这是一个消失的月经周期，或许只是来晚了，那么它是否真的是一个好消息？  &lt;/p&gt;&lt;p&gt;为了能获得结论我建立了一个简单的贝叶斯模型，基于这个模型，可以根据你当前距离上一次经期的天数、你历史经期的起点数据来计算在当前经期周期中你怀孕的可能性。在此篇文章中我将阐述我所使用的数据、先验思想、模型假设以及如何使用重点抽样法获取数据并用R语言运算出结果。在最后，我将解释为什么模型的运算结果最终并不重要。另外，我将附上简便的脚本以供读者自行计算.&lt;/p&gt;&lt;h2&gt;数据&lt;/h2&gt;&lt;p&gt;非常幸运的是，在2014年的下半年间我的妻子一直在记录她经期起始日期，否则我只能以仅拥有小量数据而告终。总体上我们拥有8个经期的起始日期数据，但是我采用的数据不是日期而是相邻经期起始日间相隔的天数。 已经有33天。  &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/db5d823ab1f049cc29c65cbda9d52464.png" data-rawwidth="829" data-rawheight="556"&gt;&lt;/p&gt;&lt;p&gt;所以日期发生得相对规律，以28天为一个周期循环。最后一次月经开始日期是在1月19日，所以在2月21日，距离最后一次经期发生日。&lt;/p&gt;&lt;h2&gt;模型的建立&lt;/h2&gt;&lt;p&gt;我要建立一个涵盖生理周期的模型，包括受孕期和不受孕期，这显然需要做大量的简化。我做了一些&lt;strong&gt;总体假设&lt;/strong&gt;如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一对情侣受孕与否不受其他因素的影响。&lt;/li&gt;&lt;li&gt;女方拥有固定的经期。&lt;/li&gt;&lt;li&gt;该对想要受孕的夫妻正在积极地尝试受孕。换言之，如Wilcox et al. (2000) 推荐的每周两次到三次受精。&lt;/li&gt;&lt;li&gt;一旦怀孕，期间将不会有经期。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;接下来是我所做的具体假设：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;假设两个相邻经期间隔的天数（days_between_periods）服从的正态分布，其中均值（mean_period）和标准差（sd_period）未知。&lt;/li&gt;&lt;li&gt;假设在一对生育能力的夫妻（is_fertile为真 ）受孕时一个周期内怀上孕的概率是0.19（更多关于选定该值的由来见参考文献）。不幸的是，并非所有的夫妻都具备生育能力，没有生育能力则怀孕的几率为零。如果生育率被编码为 0-1，那么可生育率可以被简洁的写为 0.19* is_fertile.&lt;/li&gt;&lt;li&gt;在某一些不能受孕的时期（n_non_pregnant_periods）的怀孕失败率则为(1 - 0.19 * is_fertile)^n_non_pregnant_periods&lt;/li&gt;&lt;li&gt;最后，如果你在这一个周期内（从上一次生理期至这一次生理期为一个周期）将不会怀孕；那么最新一次经期距离下一个经期的天数（next_period）将必然会大于最新一次经期距离当前日期的天数（days_since_last_perio）。即，next_period &amp;lt; days_since_last_period的概率为零。这么做看上去很奇怪因为这个事件是显然的，但是我们在模型中将会要用到它。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;基本的假设就是这样了。但是为了使其更加实际，需要考虑使用一个似然函数，一个给定了参数和一些数据、计算在给定参数下数据的概率，通常而言是一个与概率成正比例的数值——似然值。因为这个似然值可能极小所以我需要对其取对数，从而避免引起数值问题。当用R语言设计似然函数时，总体上的模式如下：  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;方程将数据和参数作为选项。&lt;/li&gt;&lt;li&gt;通过预处理，将似然值的初始值设为1.0，相应的对数为0.0。（log_like &amp;lt;- 0.0）&lt;/li&gt;&lt;li&gt;用R语言调用概率密度分布函数（比如dnorm, dbinom and dpois），用该函数计算模型中不同部分的似然值。然后将这些似然值相乘。对应地，将取对数后的似然值log_like相加。&lt;/li&gt;&lt;li&gt;为了让d*函数返回对数似然值，只需添加参数log=TRUE。并且注意似然值0.0对应的取对值为-inf  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以，综上所述该模型的对数似然函数如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/80d15aa464ab58b8d28a59b0b777afa7.png" data-rawwidth="1087" data-rawheight="455"&gt;这里数据有标量days_since_last_period以及向量days_between_periods，而其他的参数将会被被估计出来。使用这个函数，我能从任意一个数据+参数的组合中得出对数似然函数值。但是，到这里我只完成了建模的一半工作，我还需要先验信息！ &lt;/p&gt;&lt;p&gt;关于经期，受孕和生育的先验信息&lt;/p&gt;&lt;p&gt;为了完善这个模型，我需要所有参数的先验信息。换言之，我需要明确在获取数据之前这个模型包含了哪些信息。具体上，我需要实验开始前mean_period, sd_period, is_fertile, and is_pregnant的初始值。（虽然next_period也是一个参数，我不需要给出一个它的确切初始值，因为它的分布完全由mean_period 和sd_period确定。另外，我还需要找到在一个周期内能受孕的可能值（上文中我设定为0.19）。这里我使用了模糊、主观的数据吗？不！我到生育文献中去寻找了更加有信息价值的依据！   &lt;/p&gt;&lt;p&gt;对于days_between_periods的分布，其参数为mean_period和sd_period。这里我使用了来自文章The normal variabilities of the menstrual cycle Cole et al, 2009 中的估计值，该文测量了184个年龄来自18-36岁的女性的经期规律。相邻经期间天数的总平均值为27.7天。每一个参与实验者的标准差的平均值为2.4。总体样本的间隔天数的标准差为1.6。给定了这些估计值以后我令mean_period服从（27.7,2.4）的正态分布，令sd_period服从均值为1.6，标准差为2.05的半正态分布。如下：  &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/52bbc1c7828f2c92789eb1b7561dbe63.png" data-rawwidth="957" data-rawheight="460"&gt;&lt;p&gt;对于参数is_fertile a以及参数is_pregnant我考虑了受精频率作为先验。想要确定可育的夫妻的比例几乎是不可能的事情，因为这里对于不育有各种不同的定义。 Van Geloven et al. (2013)做了一个小范围的文献回顾然后得出结论所有夫妻中有2%至5%的人被认为是不孕的。因为曾看到高达10%的情况，我决定取该范围的上限。设定初始数据100%-5%=95%的夫妻是可孕的。  &lt;/p&gt;&lt;p&gt;is_pregnant 是 0 1变量表示这对夫妻在最近的一轮周期中是否将要（或者说已经）受孕。在这里我使用的先验值是在一个周期内成功受孕的概率。当这对夫妇没有生育能力时这个概率值显然为0.0，但是积极地尝试、可育的夫妇在一个周期内成功受孕的比例有多大呢？不幸的是我并没有找到明确说明这一数据的文献，但是我找到了比较接近的参照依据。在Increased Infertility With Age in Men and Women Dunson et al. (2004) 一书的第53页，给出了在12个月中一直尝试受孕但是没有怀上的夫妻的比例，同时该数据也提供了女性不同年龄段的数据。  &lt;/p&gt;​&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7bdce6415e8c8927e5b3eb3f8e2967d6.png" data-rawwidth="821" data-rawheight="113"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/8f6aaafdf2b660c3952d998324e59662.png" data-rawwidth="774" data-rawheight="276"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e6cf04dcb58e4991a42f04b9ca5fb5bf.png" data-rawwidth="715" data-rawheight="171"&gt;故上述即为对数似然函数中19%的怀孕概率值的由来，19%亦作为is_pregnant的先验值。现在我有了所有参数的先验，可以建立一个由先验函数的抽样函数了。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5396232132a99a9bcf938e5dabe2fe5d.png" data-rawwidth="1007" data-rawheight="270"&gt;这里使用了一个参数(n)，它输出了一个n行的数据框，每一行是基于先验数值得出的样本数据。输出结果如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d5ae4a9df47d5eb709c20627efb393d7.png" data-rawwidth="824" data-rawheight="308"&gt;&lt;h2&gt;使用重要性抽样来拟合模型&lt;/h2&gt;&lt;p&gt;现在，我已经收集了贝叶斯统计分析的三大要素：先验信息，似然函数以及数据。为了拟合模型我有很多方法，但是这里有一个非常方便的方法——重要性抽样。我之前曾写文提及过重要性抽样法，这里我们来回顾一下：重要性抽样法是一种蒙特卡洛实验法，它建立起来非常简单并且适用于以下情况：（1）参数空间非常小（2）先验分布与后验分布的形式区别不大。因为我的参数空间比较小，加之我使用了信息量包含得比较丰富的先验数据。因此，认为重点抽样法在此例中是可用的。在重要性抽样法中三个基本的步骤为: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;由先验分布产生大样本（这里可以通过sample&lt;em&gt;from&lt;/em&gt;prior得到）&lt;/li&gt;&lt;li&gt;给定了参数时，对每一个与似然值成比例的先验数据进行赋权。（这里可以通过 calc&lt;em&gt;log&lt;/em&gt;like 得到）&lt;/li&gt;&lt;li&gt;将权重归一化，从而在先验分布的情况下形成了新的概率分布。最终，根据此概率分布对先验分布的样本进行重新抽样。（这里可以用R函数抽样）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;（ 注意存在与该过程不同的多种方法，但是在用来拟合贝叶斯模型时，这是重要性抽样法的常用版本） &lt;/p&gt;&lt;p&gt;因为我已经定义过 sample&lt;em&gt;from&lt;/em&gt;prior 和 calc&lt;em&gt;log&lt;/em&gt;like，因此需要定义一个新的方程来做后验抽样： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/96f1358269fcac79b7362a0a46e1e1ce.png" data-rawwidth="1094" data-rawheight="381"&gt;&lt;h2&gt;结果：怀孕的可能性&lt;/h2&gt;&lt;p&gt;因此，在2月21日，2015，我的妻子已经没有来月经33天了。这是一个好休息吗？让我们运行这个模型看看结果吧！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/9958b54312e3800790e3e867b86ecadc.png" data-rawwidth="846" data-rawheight="92"&gt;post这里是一个长数据框，其中数值的表示基于这些参数得出的后验分布信息。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0d47873a10f531ecea5dec4ff01daffa.png" data-rawwidth="851" data-rawheight="382"&gt;让我们来看看各个周期中间隔天数的均值和方差的变化吧。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/96d0216720592b2d864f084b0e378efd.png" data-rawwidth="864" data-rawheight="376"&gt;像期望的那样，后验分布的图像比先验数据更狭长；并且观察后验数据，大致得出平均的经期周期天数在29天左右，其标准差在2-3天左右。那么重要问题来了：我们是可育夫妻的概率为多少，以及我们在2月21日确定已经怀孕的概率为多少？为了计算这个我们取 `postisfertile` 与`post is_pregnant`，并计算众数。当然一个捷径是直接采用均值。  &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e26446d46f9fd2e64571ae0645b4c551.png" data-rawwidth="440" data-rawheight="229"&gt;因此这是一个相当好的消息：我们极有可能是可孕的夫妻，并且我们已经受孕的可能性高达84%！用这个模型我可以了解到：当经期的来临再多延迟几天，我们确定怀孕的概率是如何随之而变化的。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a674e2f991a107fea7f88d73125d91ee.png" data-rawwidth="520" data-rawheight="225"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f5921bc80eacd6cae3ff13b82eae5267.png" data-rawwidth="523" data-rawheight="215"&gt;是的，既然我们已经得到了好消息，为何不看看在我们之前尝试受孕的数月中我们可孕和受孕成功的可能性是如何变动的呢？&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/3975346bc3a4fcf1bb544f43bb28120f.png" data-rawwidth="576" data-rawheight="700"&gt;&lt;/p&gt;&lt;p&gt;因此，这能够解释得通了。在距离“最后一次经期”的时间变长时，我们在当前周期怀孕成功的几率增加了，但是一旦这里有经期发生时可能性会跌回至基线。我们看到的可孕率曲线是几乎相同的，但在我们没有怀孕成功的每一个周期里，可孕率曲线稍稍有所下降。这两个指标的图像均呈轻微的锯齿状，但这只是由重要性抽样算法的偏差导致的。另外需指出的是，虽然上述的图像非常美观，但是查看未怀孕期间的概率是无效的，唯一具重要性和信息价值的是当前的概率。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4a6b16415ef142078fa9380b549d8d03.png" data-rawwidth="613" data-rawheight="459"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cb124fbccfd34346eaf63e59429767be.png" data-rawwidth="803" data-rawheight="498"&gt;&lt;/p&gt;&lt;h3&gt;一些关于这个模型的批评 但其实并不重要&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;当然，比起我相对简单粗糙的计算，别人有可能能够得到更优越的先验值。还有很多可以加入考虑的预测因子，如男性的年龄，健康因子等等。&lt;/li&gt;&lt;li&gt;每个月受孕的概率本应被视作一个不确定的值而不是一个固定值，而我把它设为了固定值。但是在拥有的给定数据很少的情况下，我将其视作一个适用于多个参数的参数值。&lt;/li&gt;&lt;li&gt;没有事物是完全符合正态分布的，两个经期间的天数亦然。这里我认为假设是适用的，但是还有比我的假设远要复杂得多经期间隔天数模型，比如 Bortot et al (2010)建立的模型。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21956061&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 10 Aug 2016 17:38:23 GMT</pubDate></item><item><title>数据驱动的出口电商品类行情分析</title><link>https://zhuanlan.zhihu.com/p/21948882</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4c920fbc4f7468900b798d437bc644d4_r.png"&gt;&lt;/p&gt;作为一个出口电商卖家，逐条点击商品链接、逐个翻看商品的详情页面无疑是我们最直接可靠的品类分析手段，但是在品类数量较多、所需跟踪期限较长的情况下，这种做法却往往使我们迷失在海量的信息之中。许多电商卖家也许会有这样的体验：翻了一下午的竞品页面，但是最后对竞品的了解程度还是相当有限。&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/749cf0f2538190a3835a831c4177fa3d.png" data-rawwidth="1918" data-rawheight="951"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a9606a338066367fa9184b8621ce7d49.png" data-rawwidth="1917" data-rawheight="1007"&gt;为什么花了那么多时间了解品类，但总是感觉对品类的把握还是很模糊。其实，问题的根本在于：通过翻看页面，我们只能获得当前的品类信息，却无法把这些信息和过去的历史信息做对比。这时我们需要一个可以把品类历史信息绘制成如下图表的工具，帮助我们清晰的展示品类的历史变化趋势。（如下图）&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/bc0d0cc584c947feea7b939046c1b53b.png" data-rawwidth="721" data-rawheight="438"&gt;那么今天我来介绍下如何利用『数据脉』进行更加深入的品类分析。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/79d0681d879ead39ae55cd5ae50bc7e9.png" data-rawwidth="1115" data-rawheight="632"&gt;打开数据脉品类信息追踪功能，选择好要观测的时间范围，比如2016年7月15号到31号，就可以得到相应的价格、BRS、评论星级、评论总量的变动趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f1a010bd16dcdec67812ed4bba9e87ab.png" data-rawwidth="539" data-rawheight="109"&gt;【价格】：对价格跟踪，生成的价格变化趋势图。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/af86435a13db7f8ade8aeedb476dd2b5.png" data-rawwidth="704" data-rawheight="422"&gt;【BSR】：对BSR的跟踪，生成的BSR变化趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/bc0d0cc584c947feea7b939046c1b53b.png" data-rawwidth="721" data-rawheight="438"&gt;【评论星级】：对评论星级的跟踪，生成该商品星级的变化趋势图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d01f0bd86f3b557021792a9973878d78.png" data-rawwidth="707" data-rawheight="430"&gt;【评论总量】对评论总量的跟踪，生成的评论总量的变化趋势图。从评论量的变化，就可以清晰的看出，该商品现在受到的关注度到底怎么样了。  &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e506e1c8c40ce45b52ed68097af76476.png" data-rawwidth="695" data-rawheight="431"&gt;&lt;/p&gt;&lt;p&gt;我想到这边，你应该对这款商品的总体趋势有个很好的把握了。可是还没完，要知道，在亚马逊是没有办法获得商品销量的信息的。但是数据脉可以通过数据跟踪，探测到这款商品的「销量」！同时进行跟踪记录，获得销量的趋势图：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1e82cbca4f338c2e38d9ddb7f98982e5.png" data-rawwidth="1269" data-rawheight="432"&gt;&lt;/p&gt;&lt;p&gt;如果你还觉得不够，数据脉还将根据这些数据通过特定的数据算法推算出一组参考数据，来评价商品的综合表现，如下图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/cba21050651825c072d92da5ba392c5f.png" data-rawwidth="1155" data-rawheight="97"&gt;&lt;p&gt;如果你要关注的商品特别多，说真的参考这些指标就够了。这些指标通常情况下比用文章开篇时说的那种「翻看网页」的方法肯定是更科学和更准确的。因为这些指标的生成所依据就是之前我们所说的被跟踪记录下来的商品数据，而翻看页面所依据的是通过人为的浏览而形成的主观判断。  &lt;/p&gt;&lt;p&gt;【上帝视角般的纵览竞品】:&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/74fad89483dace5a258bb1984c53bdce.png" data-rawwidth="1353" data-rawheight="776"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/01c671a4bade5419de93bc7e954c842a.png" data-rawwidth="1662" data-rawheight="864"&gt;如果你需要一款像『数据脉』这样的工具，通过以下方式了解更多：&lt;/p&gt;&lt;p&gt;http://qm.qq.com/cgi-bin/qm/qr?k=y0sgE3_qKUd_DhplBTYiV667XDMPNONR (二维码自动识别)&lt;/p&gt;&lt;p&gt;https://datartery.com (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21948882&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 10 Aug 2016 11:36:31 GMT</pubDate></item></channel></rss>