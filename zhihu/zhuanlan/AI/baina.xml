<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>黑斑马笔记 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/baina</link><description>黑斑马是百纳（武汉）信息技术有限公司（海豚浏览器）的游戏团队。
百纳致力于研究人工智能和计算机视觉。我们大量招聘上述领域专家和技术人员。</description><lastBuildDate>Mon, 26 Sep 2016 00:16:53 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>自然场景下脸部检测、姿态估计和特征点定位</title><link>https://zhuanlan.zhihu.com/p/22578841</link><description>本文译自Xiangxin Zhu  Deva Ramanan的《Face Detection,Pose Estimation,and Landmark Localization in the Wild》，有翻译不当的地方敬请指出。&lt;p&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~翻译：  &lt;a class="" data-title="刘畅@百纳.海豚浏览器" href="mailto:xxx@xn--bxy08o.xn--m7rv64cod312j7xc"&gt;刘畅@百纳.海豚浏览器&lt;/a&gt;校对：  &lt;a class="" data-title="吴刚@百纳.海豚浏览器" href="mailto:xxx@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;吴刚@百纳.海豚浏览器&lt;/a&gt;微信公众号： zero_zebraQQ交流群：   142961883大量职位虚位以待！详情请见公众号.职位列表~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/p&gt;&lt;h1&gt;&lt;b&gt;摘要&lt;/b&gt;&lt;/h1&gt;&lt;p&gt;我们提出了一个用于现实世界复杂背景图片中人脸检测、姿态估计和特征点估计的统一模型。我们的模型是基于树和shared pool of parts（译者注：parts共享池）混合模型；我们将每个面部特征点作为一个部分，利用全局混合来捕捉由于视点变化引起的拓扑结构变化。我们证明树结构的模型在捕捉全局弹性形变上是非常有效的，并且和dense graph structures（译者注：密集图形结构）相比，是非常易于优化。我们提出了在标准的脸部benchmark和一个新的“in the wild”（译者注：自然场景）带注释的数据集上的扩展结果，显示出对于所有的三项任务，我们的系统达到了先进的水准。即使我们的模型是用上百张脸进行适当训练，但是优于用数十亿样例训练的的商业系统（比如Google Picasa和face.com）。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-643eefc523114026ccef49c863f7bd10.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-123c520dbe285b22e48d78a2d1c80d62.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-487e229446ce9eb67ae7bfec64272842.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-bfc88c2b72da7bf9dc80ba796b1d3db0.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b7a950bcabc5b81e15328b3310360885.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d21223d949e36f3053a710d77f0c1946.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-45b18657b4302c253c77f2931a7bdb63.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-5a23555056ff5c27fa841c70f05faa34.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a63e59dedd7762a4dc8072c6345d0323.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-30400729de27fc8b6d97bf8de4890d98.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-16231dfe23bec2b7d78d3c92e530ddec.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-56044327bb64ec1cad8719fde39dd8b0.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-088cc7b11d94a10824cc837140dd3d7f.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b6e3f9b6c5d76f776cf368d47e83d7c1.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1ae52587ea30c8fa64cb7cdc89bd61cf.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-19b403f061650025c34214a6d3513b8c.jpg" data-rawwidth="2481" data-rawheight="3508"&gt;译者注：由于翻译水平有限，难免有些地方翻译不妥，有问题的欢迎提出来，大家共同学习，谢谢~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22578841&amp;pixel&amp;useReferer"/&gt;</description><author>海雅古慕</author><pubDate>Thu, 22 Sep 2016 23:12:31 GMT</pubDate></item><item><title>调整3D形变模型适应边缘：
强联系和弱联系的比较</title><link>https://zhuanlan.zhihu.com/p/22507938</link><description>&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;Anil Bas        William A. P. Smith           Timo Bolkarty            Stefanie Wuhrer&lt;/p&gt;&lt;p&gt;Department of
Computer Science, University of York, UK&lt;/p&gt;&lt;p&gt;Multimodal
Computing and Interaction, Saarland University, Germany&lt;/p&gt;&lt;p&gt;Morpheo Team,
INRIA Grenoble Rhˆone-Alpes, France&lt;/p&gt;&lt;h2&gt;译者&lt;/h2&gt;&lt;p&gt;翻译：  &lt;a href="mailto:%E5%90%B4%E5%88%9A@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;吴刚@百纳.海豚浏览器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;校对：  &lt;a href="mailto:%E5%88%98%E7%95%85@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;刘畅@百纳.海豚浏览器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;微信公众号： zero_zebra&lt;/p&gt;&lt;p&gt;QQ交流群：   142961883&lt;/p&gt;&lt;p&gt;大量职位虚位以待！详情请见公众号.职位列表&lt;/p&gt;&lt;h2&gt;摘要&lt;/h2&gt;&lt;p&gt;我们提出了一个全自动的方法，可以用来调整3D形变模型去适应任意角度和光照的单个人脸。我们的方法依赖于几何特征（边缘和特征点），启发于迭代最近点算法，基于计算模型顶点和边缘像素的强联系。之前的工作是利用弱联系来形成一个edge-derived cost surface，这个代价函数通过非线性优化来最小化。我们证明了我们的方法是优于这个方法的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;索引项&lt;/b&gt;——3D morphable model，edge detection，iterated closest point，face shape estimation。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-39361b3e0fc29d9853b15c72c47e4f08.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-caa7b684110b8f548be959197048f73a.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-d7e8c6e757059fedf57f99638b42cf2a.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-0f484ea5b2c14271c3d6d58134fa7a2b.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e5fbe57304159fc0e6134c7a4905ee49.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-bf4f23b8a8afb40332cbef4dd9b67a3d.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3fbd01f199a4ad59c956f8750c1f2063.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a1d10f95552c4fe4ee254e7b651653a0.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-78212584781bfe552255bd3359159ff1.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5bb344f45bb0a14a12faae894028a312.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0bd2098fdb3958a9ed6aedabd66210cc.jpeg" data-rawwidth="826" data-rawheight="1169"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22507938&amp;pixel&amp;useReferer"/&gt;</description><author>kiukotsu</author><pubDate>Mon, 19 Sep 2016 14:51:41 GMT</pubDate></item><item><title>Tensorflow的GPU支持模式下的安装要点</title><link>https://zhuanlan.zhihu.com/p/22410507</link><description>&lt;p&gt;其实Tensorflow在GPU支持模式下的安装并不困难，严格按照其官方文档就可以了。但整个 过程比较长，中间一些步骤注意不到也可能出错。这里列出要点和排错指南。&lt;/p&gt;&lt;h2&gt;确保Ecosystem一致性&lt;/h2&gt;&lt;p&gt;要确保Tensorflow能真正利用GPU的算力，就需要保证驱动、CUDA库和Tensorflow的版本相兼容。首先要确定你的硬件支持到哪一个版本的cuda和cudnn。比如我们一个比较初级的机器使用的GPU的是GeForce GTX 1070，这个机器不是Pascal架构的，所以安装Cuda的版本到7.5就ok了，最新的8.0则不是必须的。实际在安装中我们还发现了一个安装了8.0以后的问题，这个会在后面提到。&lt;/p&gt;&lt;h2&gt;安装完成后的测试&lt;/h2&gt;&lt;p&gt;安装完成后，需要运行一小段tensorflow脚本来测试安装是否正确。Tensorflow的官方教程里给出了两个阶段的测试，第一个是hello world性质的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;$ python
...
&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; hello = tf.constant('Hello, TensorFlow!')
&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()
&amp;gt;&amp;gt;&amp;gt; print(sess.run(hello))
Hello, TensorFlow!
&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)
&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)
&amp;gt;&amp;gt;&amp;gt; print(sess.run(a + b))
42
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;                                           
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally                               
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH:                               
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally                                
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally                               
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在这一步可能报的错误有libcudart.so.(x.x) cannot open share object file。这意味着CUDA并未正确安装（*1），或者未配置路径（*2），或者cuda的版本不正确（*3）。&lt;/p&gt;&lt;p&gt; 第二个阶段的检查是cudnn是否正确安装了，以及tensorflow在运算时，是否真正将计算分配到了GPU上。注意在上面的hello world中，尽管cudnn并未安装正确，程序只会报一个libcudnn并未找到的警告，程序还会继续正常去行。要检查cudnn是否正确安装，需要使用用到cudnn的库，可以用下面的代码来检查：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import tensorflow as tf

input = tf.Variable(tf.random_normal([100, 28, 28, 1]))
filter = tf.Variable(tf.random_normal([5, 5, 1, 6]))

sess = tf.Session()
sess.run(tf.initialize_all_variables())

op = tf.nn.conv2d(input, filter, strides = [1, 1, 1, 1], padding = 'VALID')
out = sess.run(op)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在前面通过了“hello world”测试的环境下运行上面的代码，会导致程序崩溃：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: 
...
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.759
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.84GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating ...
...
F tensorflow/stream_executor/cuda/cuda_dnn.cc:208] could not find cudnnCreate in cudnn DSO; dlerror: /home/yyang/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在我的环境里是由于LD_LIBRARY_PATH未配置好。&lt;/p&gt;&lt;p&gt;&lt;i&gt;注：由于运行在一个多人共享的环境上，所以我使用了screen jupter notebook来创建一个可以从browser上远程运行桌面命令和ipynb会话的环境。但是，screen并没有一开始完全继承我的环境设置，所以导致libcuda.so可以加载，而libcudnn.so不能加载。而这种情况，刚好可以通过hello world测试，如果你忽略警告的话。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;实际上你还应该运行第三个阶段的测试--确保当你的tensorflow运行时，它是真正运行在GPU上，这需要在创建session时就加上配置信息：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import tensorflow as tf

input = tf.Variable(tf.random_normal([100, 28, 28, 1]))
filter = tf.Variable(tf.random_normal([5, 5, 1, 6]))

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
sess.run(tf.initialize_all_variables())

op = tf.nn.conv2d(input, filter, strides = [1, 1, 1, 1], padding = 'VALID')
out = sess.run(op)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重点在第四行，我们加上了log_device_placement信息。&lt;/p&gt;&lt;p&gt;现在运行上面的命令，会得到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
...
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.759
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.84GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&amp;gt; (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -&amp;gt; device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0
I tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -&amp;gt; device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0

Variable_1: /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1: /job:localhost/replica:0/task:0/gpu:0
Variable_1/read: /job:localhost/replica:0/task:0/gpu:0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上面的输出可以看出，执行任务已经和gpu绑定了。&lt;/p&gt;&lt;h2&gt;查错&lt;/h2&gt;&lt;p&gt;&lt;b&gt;检查显卡设备、驱动是否安装正确&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们使用lspic | grep -i nvidia来检查显卡安装是否正确，以及驱动是否加载：&lt;/p&gt;&lt;pre&gt;&lt;code lang="bash"&gt;01:00.0 VGA compatible controller: NVIDIA Corporation Device 1b81 (rev a1) (prog-if 00 [VGA controller])
	Subsystem: Device 7377:0000
	Flags: bus master, fast devsel, latency 0, IRQ 16
	Memory at f6000000 (32-bit, non-prefetchable) [size=16M]
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at f0000000 (64-bit, prefetchable) [size=32M]
	I/O ports at e000 [size=128]
	[virtual] Expansion ROM at f7000000 [disabled] [size=512K]
	Capabilities: [60] Power Management version 3
	Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+
	Capabilities: [78] Express Legacy Endpoint, MSI 00
	Capabilities: [100] Virtual Channel
	Capabilities: [250] Latency Tolerance Reporting
	Capabilities: [128] Power Budgeting &amp;lt;?&amp;gt;
	Capabilities: [420] Advanced Error Reporting
	Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024 &amp;lt;?&amp;gt;
	Capabilities: [900] #19
	Kernel driver in use: nvidia
	Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从最后两行来看，这块显卡使用的是nvidia的驱动，且驱动已加载到内核。&lt;/p&gt;&lt;p&gt;驱动版本可以通过下面的语句查询到：&lt;/p&gt;&lt;pre&gt;&lt;code lang="bash"&gt;cat /proc/driver/nvidia/version 
NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看上去完全正确，因为显卡的驱动就是367.44。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查cuda是否安装正确&lt;/b&gt;&lt;/p&gt;&lt;p&gt;要检查cuda库是否安装正确，可以使用cuda编译后产生的deviceQuery工具。同时，你还要检查LD_LIBRARY_PATH是否正确设置：&lt;/p&gt;&lt;p&gt;echo $LD_LIBRARY_PATH&lt;/p&gt;&lt;p&gt;输出中应该包括/usr/local/cuda-x-x/lib64。如果没有，你需要编辑/etc/profile或者~/.bashrc，加入export LD_LIBRARY_PATH=xx。&lt;/p&gt;&lt;p&gt;在我的环境里还出现过上述配置都正确，但tensorflow还是报找不到cuda库的错误，即上文中提到的错误*3。最后发现错误是因为cuda安装了8.0的版本，但tensorflow的0.10版本，如果不是自己编译的话，它的发行版是链接的7.5的版本，所以会出错。最后的解决方案是将cuda版本回退到7.5（因为硬件并不支持8.0所需要的pascal架构）。&lt;/p&gt;&lt;h2&gt;其它&lt;/h2&gt;&lt;p&gt;我使用了一台多人共享的机器，通过远程连接访问。为了访问更方便快捷，我设置了以下环境：&lt;/p&gt;&lt;p&gt;1） virtualenv。以保证不跟其它人冲突。 &lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;pip3 install virtualenv
virtualenv --system-site-packages tensorflow_pyenv
#激活virtualenv环境后，再继续tensorflow安装
source tensorflow_pyenv/bin/activate
pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl
pip3 install jupyer notebook
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2）配置jupyter，启用远程服务&lt;/p&gt;&lt;p&gt; 为了使jupyter启动的服务能够在远程浏览器中打开，需要配置jupter:&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;jupyter notebook --generate-config&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 这样会在当前目标下生成.jupyter的目标，里面有jupyter_notebook_config.py文件，打开并编辑：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;c.NotebookApp.password=u''
c.NotebookApp.ip = '*'
c.NotebookApp.open_browser = False

# It is a good idea to set a known, fixed port for server access
c.NotebookApp.port = 9999&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 其中password可以使用下面的程序生成：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;In [1]: from notebook.auth import passwd
In [2]: passwd()
Enter password:
Verify password:
Out[2]: 'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 将out[2]中的字符串完整地输入到c.NotebookApp.password=u''中的引号中去。&lt;/p&gt;&lt;p&gt;3）通过screen来运行jupyter，使之象一个服务。&lt;/p&gt;&lt;p&gt;安装完成jupter和notebook以后，使用screen来运行jupyter。使用screen的好处是它能将程序放在后台运行，这样终端连接退出也不影响程序运行。在我的环境下，需要在用户目标~下创建一个.screenrc，加上这样的内容：&lt;/p&gt;&lt;pre&gt;&lt;code lang="bash"&gt;setenv LD_LIBRARY_PATH /usr/local/cuda-7.5/lib64:$LD_LIBRARY_PATH
source /etc/screenrc&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在启动程序：&lt;/p&gt;&lt;pre&gt;&lt;code lang="bash"&gt;screen jupyter notebook&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在可以打开浏览器，输入ip:端口来访问notebook了。&lt;/p&gt;&lt;p&gt;注意，你需要有64bit的screen来运行jupyter。如果使用了32bit的screen，在对一些稍大的一点数据集进行训练时，会因内存不足挂掉。另一个办法是将jupyter notebook的运行daemonize化。你可以修改&lt;a href="https://github.com/hbaaron/code_snippets/blob/master/daemonize/daemon.sh"&gt;这个脚本&lt;/a&gt;，使得jupyter可以以类似服务的方式来运行。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22410507&amp;pixel&amp;useReferer"/&gt;</description><author>杨勇</author><pubDate>Mon, 12 Sep 2016 17:56:18 GMT</pubDate></item><item><title>多玩家随机不完全信息博弈的均衡计算</title><link>https://zhuanlan.zhihu.com/p/22378565</link><description>&lt;p&gt;本文译自卡内基梅隆的Sam Ganzfried 和Tuomas W. Sandholm 的论文《Computing Equilibria in Multiplayer Stochastic Games of Imperfect Information 
》
&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/898c18eb66fae759f307bf6d1d79bebc.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7bea4bfbb46da1d12c450c3043731da6.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6d379975ba2d4007fe0c3d1879f23a9c.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/53483b6a75ee55d930842a560b93fa62.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/08ef1cf89676cff6e97b8fc4b408e8e3.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/235f453c8efeaa6c9c5d3d53c97c0bb5.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a12f62c4b9d5989276107b8ebc71f09a.jpg" data-rawheight="2234" data-rawwidth="1580"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9371f737ffc5d3d7fd4934ae78aeaa1b.jpg" data-rawheight="1497" data-rawwidth="1059"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e4a0a49a5e163219847e6daedb9940cc.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/619db6fd28ffcf290bfc28a8a6c77484.jpg" data-rawheight="2000" data-rawwidth="1414"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/e703bd69fe1826c15329c5779dc099d4.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/12dfaca291e32e578d0395cb7b6c49b9.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/601e098c74820812dc97fdf123798843.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7e86e73a40c25b89a891730b922edac3.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1b7aa28988e42a7e6aecee2f55dd4a6c.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/25903d79b66f03347dd2aedeac447c8e.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2b1cd43db683c15df92886812637cc8f.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e9a7cedbd0d26d5bdccfcaa011bb1308.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/245234ef14c6935708a97c3b605a6378.jpg" data-rawheight="1486" data-rawwidth="1051"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1c48cb6a49756e2182364352434c8e4c.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ce8c5a8cb4de84240d331bb169601558.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/baa153a0198b2cabcf86334643741139.jpg" data-rawheight="2339" data-rawwidth="1654"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22378565&amp;pixel&amp;useReferer"/&gt;</description><author>喵的熊爪饼</author><pubDate>Fri, 09 Sep 2016 14:23:05 GMT</pubDate></item><item><title>用于语义分割的全卷积网络</title><link>https://zhuanlan.zhihu.com/p/22280115</link><description>&lt;p&gt;      本文译自UC Berkeley的Jonathan Long、Evan Shelhamer、Trevor Darrell的《Fully Convolutional Networks for Semantic Segmentation》，2015 CVPR best paper。~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~翻译：  &lt;a class="" data-title="刘畅@百纳.海豚浏览器" href="mailto:xxx@xn--bxy08o.xn--m7rv64cod312j7xc"&gt;刘畅@百纳.海豚浏览器&lt;/a&gt;校对：  &lt;a class="" data-title="吴刚@百纳.海豚浏览器" href="mailto:xxx@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;吴刚@百纳.海豚浏览器&lt;/a&gt;微信公众号： zero_zebraQQ交流群：   142961883大量职位虚位以待！详情请见公众号.职位列表~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/p&gt;&lt;p&gt;摘要： 卷积网络在特征分层领域是非常强大的视觉模型。在语义分割中，我们证明了经过端到端、像素到像素训练的卷积网络胜过目前最先进的技术。我们的核心观点是建立“全卷积”网络，输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。我们定义并指定全卷积网络的空间，解释它们在空间范围内dense prediction task(译者注：预测每个像素所属的类别)中的应用并联系之前的模型。我们修改了当前的分类网络(AlexNet [&lt;a data-title="22" data-editable="true" href="https://zhuanlan.zhihu.com/#%E6%96%87%E7%8C%AE22"&gt;22&lt;/a&gt;],the VGG net [&lt;a data-title="34" data-editable="true" href="https://zhuanlan.zhihu.com/#%E6%96%87%E7%8C%AE34"&gt;34&lt;/a&gt;], and GoogLeNet [&lt;a data-title="35" data-editable="true" href="https://zhuanlan.zhihu.com/#%E6%96%87%E7%8C%AE35"&gt;35&lt;/a&gt;])到全卷积网络和通过fine-tuning（译者注：精调）[&lt;a data-title="5" data-editable="true" href="https://zhuanlan.zhihu.com/#%E6%96%87%E7%8C%AE5"&gt;5&lt;/a&gt;] 将learned representation 迁移到分割任务中。然后我们定义了一个skip architecture(译者注：跨层架构)，结合来自深、粗层的语义信息和来自浅、细层的表征信息来产生准确和精细的分割。我们的全卷积网络在PASCAL VOC， NYUDv2以及SIFT Flow等测试集中均达到了顶尖的分割结果，同时对于一个特定图片的推理花费时间少于0.2秒。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="41e94d11f0c384a9fdfabbecad058b0c.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="d87349b2b02221f70f495a6c743c9b00.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="ad88e6d17a70a0f1099b6b3d07657b04.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="7cf9a2b6bfa1ee619ddfc48b7a76474a.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="210f65b4cc3d29001853d7300790d6b9.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="f4915f737fe720f396d25c06116bb968.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="d7b77c1d084b6196705609c06dc56729.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="24946a0d3cfe0a021599a73d84f57093.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="32a1eb93e53f4fccfb77c22cb719368c.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="74de566ac034d5aefe14ece2e105712f.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="51006ca633addeea18fbb49bf4dd47ea.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="63cd4d5ff806d62216be1dba87655b57.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="a4d539725a767ae3cc4f14ed23a298b4.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="879a3e787cf2a41c05717f15b340ca38.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="51188d9f6538bb50cd60b1cdc7d7cf95.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="1169" data-rawwidth="826" src="c72edc634b80a87edbaa3c356a155832.jpg"&gt;&lt;img rel="noreferrer" data-rawheight="159" data-rawwidth="789" src="0dbf74ab2bb4da90f6b4888851611c9c.jpg"&gt;译者注：由于翻译水平有限，难免有些地方翻译不妥，有问题的欢迎提出来，大家共同学习，谢谢~&lt;/p&gt;&lt;p&gt;最后，再贴个广告~~&lt;/p&gt;&lt;p&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~翻译：  &lt;a class="" data-title="刘畅@百纳.海豚浏览器" href="mailto:xxx@xn--bxy08o.xn--m7rv64cod312j7xc"&gt;刘畅@百纳.海豚浏览器&lt;/a&gt;校对：  &lt;a class="" data-title="吴刚@百纳.海豚浏览器" href="mailto:xxx@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;吴刚@百纳.海豚浏览器&lt;/a&gt;微信公众号： zero_zebraQQ交流群：   142961883大量职位虚位以待！详情请见公众号.职位列表~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22280115&amp;pixel&amp;useReferer"/&gt;</description><author>海雅古慕</author><pubDate>Fri, 02 Sep 2016 12:04:50 GMT</pubDate></item><item><title>基于R-FCN的物体检测</title><link>https://zhuanlan.zhihu.com/p/22261216</link><description>&lt;h1&gt;基于R-FCN的物体检测&lt;/h1&gt;&lt;h2&gt;作者&lt;/h2&gt;&lt;p&gt;     Jifeng
Dai               Yi Li                        Kaiming He                   Jian Sun&lt;/p&gt;&lt;p&gt;Microsoft Research    Tsinghua University      Microsoft Research       Microsoft Research&lt;/p&gt;&lt;h2&gt;译者&lt;/h2&gt;&lt;p&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/p&gt;&lt;p&gt;翻译：  &lt;a href="mailto:%E5%90%B4%E5%88%9A@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;吴刚@百纳.海豚浏览器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;校对：  &lt;a href="mailto:%E5%88%98%E7%95%85@%E7%99%BE%E7%BA%B3.%E6%B5%B7%E8%B1%9A%E6%B5%8F%E8%A7%88%E5%99%A8"&gt;刘畅@百纳.海豚浏览器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;微信公众号： zero_zebra&lt;/p&gt;&lt;p&gt;QQ交流群：   142961883&lt;/p&gt;&lt;p&gt;大量职位虚位以待！详情请见公众号.职位列表&lt;/p&gt;&lt;p&gt;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/p&gt;&lt;h2&gt;摘要&lt;/h2&gt;&lt;p&gt;我们使用R-FCN
（region-based，fully convolutional networks）进行精确和有效的物体检测。对比之前的区域检测（Fast/Faster R-CNN [6&lt;b&gt;&lt;u&gt;]&lt;/u&gt;&lt;/b&gt;[&lt;b&gt;&lt;u&gt;18&lt;/u&gt;&lt;/b&gt;]应用于每一个区域子网格要花费数百次），我们的区域检测是基于整幅图片的全卷积计算。为了达到这个目标，我们使用了一个“位敏得分地图”（position-sensitive score maps）来处理在图像分类中的平移不变性和在目标检测中的平移变换性这样一种两难境地。因此我们的方法采用了全卷积图片分类主干部分，例如最新的残差网络（Residual Networks） (ResNets）[9&lt;b&gt;&lt;u&gt;]&lt;/u&gt;&lt;/b&gt;，用于目标检测。在PASCAL
VOC（e.g.，83.6% mAP on the 2007 set） 数据集的实验上，我们使用了101层ResNet达到了很好的效果。同时，我们仅仅使用了170ms/每张图片，比Faster R-CNN匹配快了2.5~20倍左右。公开的代码可以在此网站中访问到：&lt;a href="https://github.com/daijifeng001/r-fcn" data-editable="true" data-title="GitHub - daijifeng001/R-FCN: R-FCN: Object Detection via Region-based Fully Convolutional Networks"&gt;GitHub - daijifeng001/R-FCN: R-FCN: Object Detection via Region-based Fully Convolutional Networks&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/33e545ad67231323b08453bcd7b8f256.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/3dac54033df74414ced74a891401251c.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/fa28ab2ae2cbedc35518ea6fc4fa3e99.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/053c673e81f89eb0ba846026f479eb1e.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ce8d31c9bfc9fb9dfa42b468aa0c5606.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9d333508c45b450bd87bb43806bc94c8.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f7665d8232f85de06081168f852e8f5e.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d333329c145d244d7ed40ec978eb2b2a.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4f57d21f461eb112f699e9ccd1fb6922.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ca5569d6c9d8f4f0b7fab9e5fd56bee1.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/474ed25af3b0676d6b28465992135bbe.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d623c25c8457f9c7f2a073bd0a7ac51f.jpg" data-rawwidth="877" data-rawheight="1240"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f18e4b3471a885885b562a55511c5299.jpg" data-rawwidth="877" data-rawheight="1240"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6fd371fe4b69d17f41efe6d6154b2eba.jpg" data-rawwidth="885" data-rawheight="1252"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/031761fd5e986908d205400fa195d294.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/73bb5421fd91b61bd2926259816ac075.jpg" data-rawwidth="1654" data-rawheight="2339"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22261216&amp;pixel&amp;useReferer"/&gt;</description><author>kiukotsu</author><pubDate>Wed, 31 Aug 2016 23:16:52 GMT</pubDate></item><item><title>不完全信息博弈中自我博弈的深度增强学习</title><link>https://zhuanlan.zhihu.com/p/22203823</link><description>本文译自伦敦大学Johannes Heinrich 和 David Silver 发表的论文《不完全信息博弈中自我博弈的深度增强学习》。&lt;p&gt;由于对论文重新排版实在有些麻烦，需要重新录入大量公式，以及调整段落图片版式。在这里直接上传的图片版本。请善用缩放功能，以获得最佳的阅读体验（注，刚打开文章时，图片可能比较模糊，可以通过点击图片来查看清晰版本，可以进行适当缩放）。&lt;/p&gt;&lt;p&gt;文章中的图表可以参见文末附注的图片， 给出了一些中文注释，希望能够起到一些帮助作用。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a838439c15c9541b13cd0295b4b5bdc3.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/5a16c0b974f648ed8755577ae1b92c93.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ff1b6c1d636dd56bd861935f16fd0224.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4fd5fc3e2f37f6770ff48a03bd87be23.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b4a858e22aeb323192ec4017a47c9fce.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a10420bb6e4bfb93d223cbe980a07ecc.jpg" data-rawwidth="2484" data-rawheight="3513"&gt;&lt;p&gt;文章中的一些图表：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/70139626a76e141bc22f287a61a88de7.png" data-rawwidth="1239" data-rawheight="445"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b2900f99b62aa14c6f5dd99bf5c65b5e.png" data-rawwidth="1242" data-rawheight="575"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6be678ea7fb8650332cb133714e0b99c.png" data-rawwidth="1244" data-rawheight="567"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22203823&amp;pixel&amp;useReferer"/&gt;</description><author>平衡木</author><pubDate>Sat, 27 Aug 2016 17:21:28 GMT</pubDate></item><item><title>DeepMind的Neural Stack Machine及其代码实现（二）</title><link>https://zhuanlan.zhihu.com/p/22099077</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3d3055c10ba737164b445d6935604cbe_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;本文译自Trask的&lt;a class="" data-title="博客文章" data-editable="true" href="https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/"&gt;博客文章&lt;/a&gt;，为原文中的Part II部分。Andrew Trask是Digital Reasoning的Product Manager，著有《Modeling Order in Neural Word Embeddings at Scale》和《Modeling Order in Neural Word Embeddings at ScalPredicting Stock Change using Twitter and Artificial Neural Networks》。本文翻译未获授权，不保证正确性。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;如篇头所述，我将就如何实现学术论文中的方法对元学习（meta-learning，如何学会学习）作一点讨论。现在，请点击打开&lt;a data-title="这篇论文" data-editable="true" href="http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf"&gt;这篇论文&lt;/a&gt;（《Learning to Transduce with Unbounded Memory》），粗略地看看。&lt;b&gt;声明：并不存在该如何阅读论文的正确打开方式！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;初轮：&lt;/b&gt;我认识的很多人在这一轮会从头到尾把论文看一遍。不要强迫自己能理解所有的内容。只要对这些内容有一个概要性的理解就可以了：这篇论文的成就何在，有哪些关键术语，对使用的方法有一个基本观念。不要过于担心公式。花点时间多看看图表。这篇论文有相当多的图表，这对读者帮助颇大。如果这篇论文是关于造车的，那么初论阅读应该理解到这样的概念：”我们将制造一个可驾驶的机器，它将能在一个弯曲的道路上以60KPM的速度移动和转弯。它有轮子，使用汽油作燃料。应该由人来控制。"。在这一轮不要去管什么引擎，变速箱，和火花塞，更不要说什么最佳燃烧温度了。只要得到一般化的思想即可。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二轮：&lt;/b&gt;这一轮中，如果你觉得已经理解了论文背景（通常是最前面几节，贴着简介或者相关工作的标签），那么就可以跳到方法一节。在这篇论文中，方法一节从第2页底部的”3 Models"开始。在这一节中，要逐句细读。这些章节一般总是内容极为饱满。每一句都是精心雕琢，如果不理解上一句，那么通常下一句读起来就全无意义。在这一轮中，仍然不要太过关注公式，相反，只要理解算法中的“主要动作部件”就好。关注“什么”而不是”怎么“。同样，如果我们要是造一辆车，那么这一轮是要弄理出一个部件清单，并且载明每个部件叫什么，看上去象什么，如下所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="199" data-rawwidth="865" src="4aa16bcba572514279adfc43bcc4c971.png"&gt;作为一个附注，这一时段你需要创造一些助记符（metal pneumonics)来帮助记住这些变量谁是谁。比如，u_t中的u是向上开口的， 就象它是被"弹”开的一样。（译注：这一段没有往下译。主要是讲如何通过一些记忆钩子帮助你迅速记住各个变量的用法和含义。这种方法并非阅读论文必须）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多轮阅读&lt;/b&gt;。现在反复阅读方法一节，直到你有一个可行的实现。（你可以在下一篇验证你的成果）。&lt;/p&gt;&lt;p&gt;上一篇： &lt;a data-editable="true" data-title="DeepMind的Neural Stack Machine及其代码实现（一）" class="" href="https://zhuanlan.zhihu.com/p/22090568"&gt;DeepMind的Neural Stack Machine及其代码实现（一）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;下一篇：&lt;a data-title="eepMind的Neural Stack Machine及其代码实现（三）" class="" href="https://zhuanlan.zhihu.com/p/22102144"&gt;DeepMind的Neural Stack Machine及其代码实现（三）&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22099077&amp;pixel&amp;useReferer"/&gt;</description><author>杨勇</author><pubDate>Sat, 20 Aug 2016 16:53:36 GMT</pubDate></item><item><title>DeepMind的Neural Stack Machine及其代码实现（一）</title><link>https://zhuanlan.zhihu.com/p/22090568</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/3d3055c10ba737164b445d6935604cbe_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;本文译自Trask的&lt;a class="" data-title="博客文章" data-editable="true" href="https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/"&gt;博客文章&lt;/a&gt;，
为原文中的Part II部分。Andrew Trask是Digital Reasoning的Product 
Manager，著有《Modeling Order in Neural Word Embeddings at Scale》和《Modeling 
Order in Neural Word Embeddings at ScalPredicting Stock Change using 
Twitter and Artificial Neural Networks》。本文翻译未获授权，不保证正确性。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这篇文章不仅帮助读者理解DeepMind的Neural Stack Machine的结果，还以这篇&lt;a data-title="论文" data-editable="true" href="http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf"&gt;论文&lt;/a&gt;为例，详细地讲述了如何从论文出发，完成其代码实现。文中的方法可以当成将论文转化为代码实现的经典的套路。&lt;/p&gt;&lt;h2&gt;（一）什么是Neural Stack？&lt;/h2&gt;&lt;p&gt;&lt;b&gt;一个简单的堆栈&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在接触神经元堆栈前，让我们先从一个常规的栈定义开始。在计算机科学中，堆栈是数据结构的一种。在下面的代码中，我们把一些哈利.波特的书“摞”在一张（通过字符艺术画出的）桌子上。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;class VerySimpleStack:
    
    def __init__(self):
        self.contents = list()
        
    def push(self,item):
        self.contents.append(item)
    
    def pop(self):
        item = self.contents[-1]
        self.contents = self.contents[:-1]
        return item
    
    def pretty_print(self):
      i = 1
      print "\n--------TOP---------"
      for item in (self.contents):
          if(i != 1):
              print "--------------------"            
          print self.contents[-i]
          i+=1

      print  "-----------------------"
      print  "---------(table)-------"
      print  "-----------------------"
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "--                --"            
      print  "--                --\n\n"            
      
        
print "1) Creating an empty stack of books..."
stack = VerySimpleStack()

print "2) Pushing two books onto our stack..."
stack.push("Harry Potter and the Sorcerer's Stone")
stack.push("Harry Potter and the Chamber of Secrets")

print "3) Let's look at our stack..."
stack.pretty_print() # what order will this print?

print "4)Let's Pop a few...\n"
print "POP: " + stack.pop() # which one does this remove?
print "POP: " + stack.pop() # how bout this one?

print "\n5)Let's push a few more!"

stack.push("Harry Potter and the Prisoner of Azkaban")
stack.push("Harry Potter and the Goblet of Fire")
stack.push("Harry Potter and the Order of the Phoenix")
stack.push("Harry Potter and the Half-Blood Prince")
stack.push("Harry Potter and the Deathly Hallows")


print "\n6)Let's look at our stack again..."
stack.pretty_print()

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;想象一下你在桌子上摞了一堆哈利.波特的书。堆栈与列表极其相似，但有一点不同：你只能堆栈的顶部增加或者移除一本书。所以，你可以增加另一本书到栈顶（stack.push(book))，或者从栈顶移除一本书(stack.pop())，然而你无法操作这堆书中间的任何一本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Neural Stack&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Neural Stack也是一种堆栈。但是，我们将要实现的Neural Stack，将能够学习到如何使用堆栈来实现一个算法。它将学习到根据输入数据，在何时进行压栈和出栈操作，以正确地模态化输出数据。&lt;/p&gt;&lt;p&gt;神经网络如何学习到何时压栈，何时出栈？&lt;/p&gt;&lt;p&gt;神经网络将使用反向传播来学习。因此在读这篇文章前，必须要对神经网络和反向传播有一个直觉性的理解。读完这篇&lt;a data-title="博文" data-editable="true" href="http://iamtrask.github.io/2015/07/12/basic-python-network/"&gt;博文&lt;/a&gt;应该足够了。&lt;/p&gt;&lt;p&gt;因此，要回答神经网络如何学习何时做压栈和出栈操作的问题，我们需要理解一个正确的压栈和出栈序列看起来应该长什么样儿。因此，我们的输入数据和输出数据都是序列。那么，哪种数据序列是堆栈易于建模的呢？&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;class VerySimpleStack:
    
    def __init__(self):
        self.contents = list()
        
    def push(self,item):
        self.contents.append(item)
    
    def pop(self):
        item = self.contents[-1]
        self.contents = self.contents[:-1]
        return item
    
    def pretty_print(self):
      i = 1
      print "\n--------TOP---------"
      for item in (self.contents):
          if(i != 1):
              print "--------------------"            
          print self.contents[-i]
          i+=1

      print  "-----------------------"
      print  "---------(table)-------"
      print  "-----------------------"
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "-- --             -- --"            
      print  "--                --"            
      print  "--                --\n\n"            
      
        

print "1) Creating an empty stack of numbers..."
stack = VerySimpleStack()

print "\n2) Create an ordered sequence... "
sequence = [0,1,2,3,4,5]
print "sequence = " + str(sequence)

print "\n3) Push sequence onto my stack"
stack.push(sequence[0])
stack.push(sequence[1])
stack.push(sequence[2])
stack.push(sequence[3])
stack.push(sequence[4])
stack.push(sequence[5])
stack.pretty_print()

print "\n4) Create New Sequence By Popping From Stack"
new_sequence = list()
new_sequence.append(stack.pop())
new_sequence.append(stack.pop())
new_sequence.append(stack.pop())
new_sequence.append(stack.pop())
new_sequence.append(stack.pop())
new_sequence.append(stack.pop())

print "\n5) Print new sequence... notice anything?"
print "new_sequence = " + str(new_sequence)

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;输出结果如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;1) Creating an empty stack of numbers...

2) Create an ordered sequence... 
sequence = [0, 1, 2, 3, 4, 5]

3) Push sequence onto my stack

--------TOP---------
5
--------------------
4
--------------------
3
--------------------
2
--------------------
1
--------------------
0
-----------------------
---------(table)-------
-----------------------
-- --             -- --
-- --             -- --
-- --             -- --
-- --             -- --
--                --
--                --

4) Create New Sequence By Popping From Stack

5) Print new sequence... notice anything?
new_sequence = [5, 4, 3, 2, 1, 0]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;究竟什么是Neural Stack？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Neural Stack&lt;/b&gt;是一种通过学习，能记忆输入序列并根据从数据中学习到的pattern来做出正确变换的栈。&lt;/p&gt;&lt;p&gt;&lt;b&gt;如何学习？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Neural Stack通过以下方式进行学习：&lt;/p&gt;&lt;p&gt;1） 接收输入数据，根据神经网络的指令进行出栈和入栈。这样生成了输出数据序列（预测）。&lt;/p&gt;&lt;p&gt;2） 将输出数据与输入数据进行比较，看有多少数据是Neural Stack漏掉的。&lt;/p&gt;&lt;p&gt;3） 使用反向传播算法更新神经网络，以使得下次入栈和出栈操作能更加正确。&lt;/p&gt;&lt;p&gt;问题：如果错误发生在栈的输出上，而神经网络作用在栈的输入上，反向传播如何能学习到该如何入栈和出栈？一般来说我们将网络输出端的错误反向传播到权重值，从而我们可以更新权重。这里看起来neural stack正好阻塞了神经网络（正是神经网络控制着入栈和出栈)的决策。&lt;/p&gt;&lt;p&gt;回答：要使得neural stack是可微分的。如果我们能找出只使用加、减、乘法就能模拟栈的行为的工具，那么我们就能够象在神经网络里那样将错误反向传播通过隐藏层一样，将错误反向传播通过堆栈。而这些对我们来讲很熟悉。我们已经做过使用加、减、乘的序列来做反向传播，现在最难的部分只是要如何以一个完全可微分的方式来模拟栈的操作--Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman和Phil Blunsom做到了，这也正是他们如此光芒夺目的原因！&lt;/p&gt;&lt;p&gt;下一篇：&lt;a data-title="DeepMind的Neural Stack Machine及其代码实现（二）" class="" href="https://zhuanlan.zhihu.com/p/22099077"&gt;DeepMind的Neural Stack Machine及其代码实现（二）&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22090568&amp;pixel&amp;useReferer"/&gt;</description><author>杨勇</author><pubDate>Sat, 20 Aug 2016 12:08:17 GMT</pubDate></item><item><title>斯坦福CS231N课程学习笔记（一）.课程简介与准备</title><link>https://zhuanlan.zhihu.com/p/21353567</link><description>前言&lt;p&gt;开这个系列是因为工作中需要用到计算机视觉相关知识。几经淘洗，发现了斯坦福大学的CS231N课程。为了强制自己学习，强化学习效果，将学习中的笔记整理出来，与大家一起分享，也希望借此与同在学习这门课程、以及其他计算机视觉的学习者、研究者一起探讨和进步。&lt;/p&gt;&lt;p&gt;本人此前没有接触过这一领域，IT从业以来多以工程为主，少有接触学术和算法研究，所以学习笔记也会因为本人理解能力原因，存在谬误，恳请阅读者指正。&lt;/p&gt;&lt;p&gt;&lt;i&gt;请注意：本系列是以CS231N为蓝本进行学习的学习笔记，并不是对CS231N的翻译。在学习过程中不可避免地会针对个人知识体系特点补充学习相关内容。关于CS231N的课程翻译，可以参见知乎网友&lt;a data-title="@杜客" data-editable="true" class="member_mention" href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt; 的&lt;a data-title="翻译" data-editable="true" href="https://zhuanlan.zhihu.com/intelligentunit" class=""&gt;翻译&lt;/a&gt;。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;CS231N课程简介&lt;/h2&gt;&lt;p&gt;CS231N课程的全称是卷积神经网络在视觉辨识中的应用（Convolutional Neural Networks for Visual Recognition），是一个学习时长跨度为两个月的课程。这门课程从2015年起第一次开设，授课者是&lt;a data-title="李飞飞" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//profiles.stanford.edu/fei-fei-li%3Ftab%3Dbio"&gt;李飞飞&lt;/a&gt;，&lt;a data-title="Andrej Karpathy" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//karpathy.github.io/"&gt;Andrej Karpathy&lt;/a&gt;，Justin Johnson。&lt;a data-title="李飞飞" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//profiles.stanford.edu/fei-fei-li%3Ftab%3Dbio"&gt;李飞飞&lt;/a&gt;，斯坦福大学计算机科学系副教授，入选2015年“全球百大思想者”，现为斯坦福人工智能实验室（SAIL）主任。斯坦福大学在机器学习和计算机视觉上都非常牛。著名的人工智能专家， Google Brain之父吴恩达也是斯坦福大学副教授。&lt;/p&gt;&lt;p&gt;计
算机视觉在搜索，图像理解，地图，医疗，无人机和无人驾驶汽车等方面的应用越来越重要和广泛。这些任务的核心就是视觉辨识，即图像分类，本地化
（localization）和检测。而神经网络（即深度学习）在这一领域的应用又大大提高的视觉辨识系统的最新水平。这门课程将以上述任务，特别是图像
分类为研究对象，以端到端的模式解析深度学习架构在视觉辨识领域的实现。&lt;/p&gt;&lt;p&gt;这门课程将教会学生如何实现、训练和调试他们自己的神经网络，并获
得对计算机视觉这一前沿科学深入了解。在课程最后，你将训练一个有几百万参数的神经网络并将其应用于全球最大的图像分类数据库--ImageNet. 
具体而言，课程重点将会是图像识别问题的设定，学习算法（即后向传播），神经网络训练和调优中的工程技术难题和技巧，以及如何上手完成布置的作业及最终的
课业项目（final course project）。&lt;/p&gt;&lt;p&gt;学习这门课程需要对python很熟练，以及对C/C++有High-level
的熟悉程度。作业主要使用python（以及python的库如numpy等），但一些关于深度学习的库，也可能使用C/C++。需要有一些大学微积分知
识及线性代数知识，需要能看懂求导及矩阵运算。也需要一些基础的概率知识，如高斯分布，均值，标准差等等。这门课程还将讲述代价函数，求导和使用梯度下降
法进行优化等，如果学习过CS229（机器学习），这些知识将直接可用。&lt;/p&gt;&lt;h2&gt;课程资源&lt;/h2&gt;&lt;p&gt;课程的主页在&lt;a data-title="这里" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/" class=""&gt;这里&lt;/a&gt;。在主页上使用一段JS向来访者显示一个正在进行的图像分类任务。这段javascript，被称之为&lt;a data-title="ConvNetJS" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/convnetjs/" class=""&gt;ConvNetJS&lt;/a&gt;，由课程讲授者Andrej Karpathy贡献，在后面的学习中会专门提到。&lt;/p&gt;&lt;p&gt;课程的大纲和课程表见&lt;a data-title="这里" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/syllabus.html" class=""&gt;这里&lt;/a&gt;。这个课程表可以供自己学习时作为进度参考，同时，这个而面也列举了课程中使用的资源的链接地址。这些资源包括授课用的课件，工具使用指南及一些课程笔记。这些课程笔记非常详细，对于不能现场听课的人来讲，非常重要。&lt;a data-title="这里" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/classification/" class=""&gt;这里&lt;/a&gt;是课程笔记的一个例子。 &lt;/p&gt;&lt;p&gt;这些课件也可以在google的&lt;a data-title="云盘" data-editable="true" href="https://link.zhihu.com/?target=https%3A//drive.google.com/folderview%3Fid%3D0B62MBK9B2knSY3ZmeHktSEhJNXM%26usp%3Ddrive_web" class=""&gt;云盘&lt;/a&gt;中获取。如果你要给其它人讲课，这些课件倒是很好的资源，如果仅用于自己学习，建议多从它的课程笔记开始，或者从本笔记开始。&lt;/p&gt;&lt;p&gt;这里有一份讲课的&lt;a data-title="视频播放清单" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLLvH2FwAQhnpj1WEB-jHmPuUeQ8mX-XXG"&gt;视频播放清单&lt;/a&gt;，是 youtube 的。如果无法访问youtube，也可以访问&lt;a data-title="百度云盘" data-editable="true" href="https://link.zhihu.com/?target=http%3A//url.cn/29YirMo" class=""&gt;百度云盘&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Andrej Karpathy的&lt;a data-title="博客" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//karpathy.github.io/"&gt;博客&lt;/a&gt;及课程教职员的&lt;a data-title="twitter" data-editable="true" href="https://link.zhihu.com/?target=https%3A//twitter.com/cs231n" class=""&gt;twitter&lt;/a&gt;也值得关注，提供了最新的一些资讯。另外，你也可以访问&lt;a data-title="Reddit." data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//www.reddit.com/r/cs231n"&gt;Reddit.&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;课程准备 &lt;/h2&gt;&lt;h2&gt;编程和课程实践工具&lt;/h2&gt;&lt;p&gt;CS231N课程作业主要使用python。使用python 2.7版本就可以完成这些作业。安装完python之后，检查一下是否安装了numpy、scipy、Pillow和matplotlib:&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;bogon:~ aaron$ pip list |grep numpy
numpy (1.8.0rc1)
bogon:~ aaron$ pip list |grep scipy
scipy (0.17.1)
bogon:~ aaron$ pip list |grep matplot
matplotlib (1.3.1)
bogon:~aaron$ pip list |grep Pillow
Pillow (3.2.0)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里有一个trick，如果你要使用 
scipy.misc.imread等图像文件操作函数（正如本文例子中所示），那么实际上需要导入Pillow。但是 
scipy安装文件并没有把这个依赖写进来，所以如果你的系统中没有安装Pillow，在执行下面的语句时会出错：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;from scipy.misc import imread, imsave, imresize
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; 错误是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;&amp;gt;&amp;gt;&amp;gt; from scipy.misc import imread
Traceback (most recent call last):
  File "&amp;lt;stdin&amp;gt;", line 1, in &amp;lt;module&amp;gt;
ImportError: cannot import name imread
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果没有安装，使用下面的命令安装：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;pip install numpy
pip install scipy
pip install matplotlib
pip install Pillow
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;如何使用&lt;a data-title="scipy" data-editable="true" href="https://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/" class=""&gt;scipy&lt;/a&gt;全家桶 &lt;/h2&gt;&lt;p&gt;SciPy提供用于科学计算的核心库。在我们的研究中，比较常用的有图像操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;from scipy.misc import imread, imsave, imresize

# Read an JPEG image into a numpy array
img = imread('assets/cat.jpg')
print img.dtype, img.shape  # Prints "uint8 (400, 248, 3)"

# We can tint the image by scaling each of the color channels
# by a different scalar constant. The image has shape (400, 248, 3);
# we multiply it by the array [1, 0.95, 0.9] of shape (3,);
# numpy broadcasting means that this leaves the red channel unchanged,
# and multiplies the green and blue channels by 0.95 and 0.9
# respectively.
img_tinted = img * [1, 0.95, 0.9]

# Resize the tinted image to be 300 by 300 pixels.
img_tinted = imresize(img_tinted, (300, 300))

# Write the tinted image back to disk
imsave('assets/cat_tinted.jpg', img_tinted)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以先花一点时间过一下它的quick start tutorial，对numpy的基本用法有个大致了解。当你需要完成某个任务，不知道numpy是否支持时，可以查看它的&lt;a data-title="参考文档" data-editable="true" href="https://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/index.html%23reference" class=""&gt;参考文档&lt;/a&gt;。如果你明确知道某个方法，需要详尽了解其具体用法，可以查看按字母顺序索引的&lt;a data-title="索引表" data-editable="true" class="" href="https://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/genindex.html"&gt;索引表&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;当然也可以使用python的终极帮助大法，通过全局函数dir()来查看一个对象（或者类）提供的属性和方法，然后通过 全局函数help() 来查看其用法。&lt;/p&gt;&lt;p&gt;在
numpy中，最重要的数据类型是同构多维数组ndarray。它支持建立矩阵、reshape,copy等操作。linalg是numpy中处理线性代
数运算的包，比如对矩阵进行转置，求逆， 
点乘，求迹，求特征值和特征向量等。numpy还有用于傅立叶变换的库numpy.fft，与随机数、概率相关的库numpy.random。&lt;/p&gt;&lt;p&gt;matplotlib的主要工作是提供绘图操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
import matplotlib.pyplot as plt

# Compute the x and y coordinates for points on sine and cosine curves
x = np.arange(0, 3 * np.pi, 0.1)
y_sin = np.sin(x)
y_cos = np.cos(x)

# Plot the points using matplotlib
plt.plot(x, y_sin)
plt.plot(x, y_cos)
plt.xlabel('x axis label')
plt.ylabel('y axis label')
plt.title('Sine and Cosine')
plt.legend(['Sine', 'Cosine'])
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;安装IPython notebook&lt;/h2&gt;&lt;p&gt;课程简介里提到了安装IPython 
notebook。CS231N的课程作业使用这个工具来布置。他们的作业布置方法是下发一些后缀为ipnb的文件，通过在IPython 
notebook中加载这些文件，你就能得到完成这些作业所必须的skeleton代码和作业指导，如下例所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="484" data-rawwidth="811" src="a7299924d780ffdb8d0938139aafae3d.png"&gt; 可以看到上方有详细的指示，告诉你需要完成的代码是实现一个K=5的KNN分类器，并且代码实现的位置已经指定，你需要做的就是在指定的位置填写上代码。这里面要求的一些skeleton的代码，已经事先写好了，比如第二课要用到的load_CIFAR10等。&lt;/p&gt;&lt;p&gt;从这些地方可以看出，这门课程的设计是多么精心，不由得让人感叹一下国内的大学跟世界一流大学的差距，其实不仅仅是在科研上，就连教学上也有很大的差距。&lt;/p&gt;&lt;p&gt;具体安装方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;pip install "ipython[notebook]"
python -m IPython notebook
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后一个命令会运行一个本地服务器，注意启动时提示的端口。打开浏览器，输入&lt;a data-title="http://localhost:8888/tree" data-editable="true" href="https://link.zhihu.com/?target=http%3A//localhost%3A8888/tree" class=""&gt;http://localhost:8888/tree&lt;/a&gt;即可以查看提供的服务。&lt;/p&gt;&lt;h2&gt;安装ConvNetJS&lt;/h2&gt;&lt;p&gt;从Github上下载convnetjs的&lt;a data-title="代码" data-editable="true" class="" href="https://link.zhihu.com/?target=https%3A//github.com/karpathy/convnetjs/releases"&gt;代码&lt;/a&gt;。在本地生成这个html文件(命名为index.html)：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;minimal demo&amp;lt;/title&amp;gt;
 
&amp;lt;!-- CSS goes here --&amp;gt;
&amp;lt;style&amp;gt;
body {
  background-color: #FFF; /* example... */
}
&amp;lt;/style&amp;gt;
 
&amp;lt;!-- import convnetjs library --&amp;gt;
&amp;lt;script src="convnet.js"&amp;gt;&amp;lt;/script&amp;gt;
 
&amp;lt;!-- javascript goes here --&amp;gt;
&amp;lt;script type="text/javascript"&amp;gt;
 
function periodic() {
  var d = document.getElementById('egdiv');
  d.innerHTML = 'Random number: ' + Math.random()
}
 
var net; // declared outside -&amp;gt; global variable in window scope
function start() {
  // this gets executed on startup
  //... 
  net = new convnetjs.Net();
  // ...
 
  // example of running something every 1 second
  setInterval(periodic, 1000);
}
 
&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
 
&amp;lt;body onload="start()"&amp;gt;
&amp;lt;div id="egdiv"&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意这里引用的JS文件是convnet.js，所以你需要在与本HTML相同的位置处保存一份从Github上下载的convnet.js.&lt;/p&gt;&lt;p&gt;现在运行命令：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;python -m SimpleHTTPServer 8000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在浏览器中打开&lt;a data-title="http://localhost:8000" data-editable="true" href="https://link.zhihu.com/?target=http%3A//localhost%3A8000" class=""&gt;http://localhost:8000&lt;/a&gt;,如果看到一串随机数在不停跳动，说明部署成功了。这个简单的demo基本上不包含任何有用的内容，但我们后面需要用到它。现在你可以通过它来观察convnetjs的对象封装，并对Vol, Net等模块的代码进行阅读和调试。&lt;/p&gt;&lt;h2&gt;基础数学知识 &lt;/h2&gt;&lt;p&gt;&lt;b&gt;高斯分布&lt;/b&gt;，又称正态分布。可以使用Box-Muller方法来生成一个符合高斯分布的随机数。这个方法的核心是，如果在值域(0, 1]内有两个独立同分布的变量U,V, 那么可用以下两个等式之一生成服从高斯分布的随机变量Z:&lt;/p&gt;&lt;p&gt;或者，这里满足：&lt;/p&gt;&lt;p&gt;参考实现代码如下（引用至Karpathy的ConvNetJs):&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;var gaussRandom = function() {
    if(return_v) { 
      return_v = false;
      return v_val; 
    }
    var u = 2*Math.random()-1;
    var v = 2*Math.random()-1;
    var r = u*u + v*v;
    if(r == 0 || r &amp;gt; 1) return gaussRandom();
    var c = Math.sqrt(-2*Math.log(r)/r);
    v_val = v*c; // cache this
    return_v = true;
    return u*c;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个随机数可以通过numpy生成，代码是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
np.random.normal()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在我们来检验一下它生成的随机数是否真的是正态分布：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import matplotlib.pyplot as plt
import numpy as np
a = []
for i in range(50000):
    a.append(np.random.normal())
plt.hist(a, 10000)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;矩阵相关知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;矩阵乘法。如果A是矩阵和B是矩阵，则A可以乘以B，即。在numpy里，矩阵的乘法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;b = a * a
#or
c = np.dot(a, a)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;转置。在python中使用下面的代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;A = np.array([[1,2,3],[4,5,6],[7,8,9]])
print A.T
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;行列式&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="393" data-rawwidth="830" src="f575d2c482107cd76160fd2144aa34c8.png"&gt;&lt;img rel="noreferrer" data-rawheight="221" data-rawwidth="841" src="b38c274f79f01c075eb2b4204319fbe9.png"&gt; 在numpy里，计算行列式的方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
A = np.array([[1, 2, 3], [4, 5,6], [7,8,9]])
np.linalg.det(A)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;代数余子式&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;img rel="noreferrer" data-rawheight="173" data-rawwidth="815" src="5ce991876478866dbad69b6ab4ca11ea.png"&gt; 伴随矩阵&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="520" data-rawwidth="810" src="794504727fbafdf8e1bc458be4a9b0f1.png"&gt;在python中求伴随矩阵（)&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;A = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.linalg.inv(A)*np.linalg.det(A)
[[ -3.   6.  -3.]
 [  6. -12.   6.]
 [ -3.   6.  -3.]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;逆矩阵（) &lt;/p&gt;&lt;p&gt; 在python中求矩阵的逆：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
A = np.array([[1, 2, 3], [4, 5, 6], [7,8,9]])
print np.linalg.inv(A)
&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" data-rawheight="160" data-rawwidth="755" src="6d8db82a6bb2583bb5f4a4b154a1f720.png"&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
A = np.array([[1, 2, 3], [4, 5, 6], [7,8,9]])
print np.linalg.trace(A)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;特征值和特征向量 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="113" data-rawwidth="789" src="bad0c867979868bdecf91809162f68fd.png"&gt;在python中求特征值和特征向量&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;import numpy as np
x = numpy.array([[1, 0, 0], [0, 2,0], [0, 0,3]])
a,b = numpy.linalg.eig(x)
print a, b
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;b&gt;计算机视觉简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;请见&lt;a data-title="课程讲义" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/slides/winter1516_lecture1.pdf" class=""&gt;课程讲义&lt;/a&gt;。这部分CS231N主页未提供任何课堂笔记和讲课视频。所以本笔记以下内容完全是基于个人理解，并跳过了很多计算机视觉这门学科的发展史上的内容。&lt;/p&gt;&lt;p&gt;计算机视觉发展到现阶段，就是机器学习在可视化数据上的应用。它与数字图像处理、计算图形学等是相邻学科。见下图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="742" data-rawwidth="1474" src="e2f3b3751d083b5c8e415b24058b2a3a.png"&gt;计算机视觉，就是让计算机能理解它所处理的图像内容，1966年，图灵奖得主Minksy给出以下描述：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="649" data-rawwidth="1289" src="5bc77acd3aa446045f0709eb7d0ff6ec.png"&gt;然
而半个世纪过去了， 我们依然很难说完全解决了这一问题。在本课程授课者之一的Karpathy的博客中有一篇文章： The state of 
Computer Vision and AI: we are really, really far away. 
这篇文章里有一张美国总统奥巴马的照片：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="450" data-rawwidth="675" src="3dc2dd7b04aef805d6cf628f53d69af6.jpg"&gt; 这篇文章列举了近十个计算机视觉理解这张图片的难点。 &lt;/p&gt;&lt;h2&gt;其它 &lt;/h2&gt;&lt;p&gt;百纳（武汉）信息技术有限公司在武汉组织CS231N线下学习课堂，并通过QQ群（142961883）和腾讯课堂向不能到现场的同学进行直播。每周组织一次授课，具体学习时间地点请见微信公众号黑斑马团队(zero_zebra)及QQ群发布。课程学习中的相关知识点和笔记通过公众号、知乎专栏发布。&lt;/p&gt;&lt;p&gt;本课程也可以参考CS231N的在线资料及视频来自学。我们组织的课程学习将额外提供：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;很多人需要良好的学习环境才能更有效率地学习。本课程通过组织大家集体学习，相互鼓励和监督，提供各种形式地交流互助，使得学习更有效率。&lt;/li&gt;&lt;li&gt;原课程录像是英文的，且部分课时不全。我们讲课全程使用中文，并且会根据学习者的程度不同，补充必要的预备知识。&lt;/li&gt;&lt;li&gt;提供对作业题的答疑和讨论，一些工具软件在使用中的困难帮助等等。&lt;/li&gt;&lt;li&gt;对课程体系的宏观把握，以方便大家入门，和根据自己的实际情况补充预备知识。 &lt;/li&gt;&lt;/ol&gt;&lt;b&gt;关于百纳（武汉）&lt;/b&gt;&lt;p&gt;&lt;a data-title="百纳（武汉）信息技术有限公司" data-editable="true" href="https://link.zhihu.com/?target=http%3A//cn.dolphin.com/" class=""&gt;百纳（武汉）信息技术有限公司&lt;/a&gt;（以下简称武汉百纳）成立于2010年。武汉百纳是武汉移动互联网行业的领先企业，公司专注于自主创新，以出色的市场前瞻力和卓越的技术创新力为依托，研发了著名的海豚浏览器（2015年全球用户超过2亿），先后得到红杉资本、经纬创投、高通及畅游的战略投资。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="74" data-rawwidth="491" src="bad8209916ae3be389a01f14cd389176.png"&gt; 公司现面向全球招聘计算机视觉及虚拟现实项目相关人才，有意者请向recruiting@bainainfo.com投递简历。&lt;/p&gt;&lt;p&gt;JD请看&lt;a data-title="这里" data-editable="true" href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI1NzA2MDYwMw%3D%3D%26mid%3D503004410%26idx%3D5%26sn%3Dc4686a3e67ee6f9c2c8504e891d22ff7%26scene%3D18%26uin%3DMTMxNTMxNDU2Mw%253D%253D%26key%3Df5c31ae61525f82ea6db4457c282b778e18b8f056c5794869f42ebe3f0413b532e8c99efba0fb6309b254cd3bca96c85%26devicetype%3DiMac%2BMacBookAir6%252C1%2BOSX%2BOSX%2B10.11.5%2Bbuild%252815F34%2529%26version%3D11020201%26lang%3Dzh_CN%26pass_ticket%3DM%252B%252FzUdojRkqAB8sHUODJYKEUGkHV5DihFacpEgtzyTS2iTotVuWn6JsSqoc9kknX" class=""&gt;这里&lt;/a&gt;。 



&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21353567&amp;pixel&amp;useReferer"/&gt;</description><author>杨勇</author><pubDate>Fri, 19 Aug 2016 17:10:25 GMT</pubDate></item></channel></rss>