<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>斯坦福CS231n官方教程笔记翻译连载。

深度增强学习领域论文和项目的原创思考和Demo复现。

领域内其他感兴趣论文和项目的原创思考解读。</description><lastBuildDate>Thu, 08 Sep 2016 03:15:11 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>关于图像语义分割的总结和感悟</title><link>https://zhuanlan.zhihu.com/p/22308032</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/cb5e078e5008907cb04b300369b7d621_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;前言 &lt;/h2&gt;&lt;p&gt;
(呕血制作啊！)前几天刚好做了个图像语义分割的汇报，把最近看的论文和一些想法讲了一下。所以今天就把它总结成文章啦，方便大家一起讨论讨论。本文只是展示了一些比较经典和自己觉得比较不错的结构，毕竟这方面还是有挺多的结构方法了。&lt;/p&gt;&lt;h2&gt;介绍 &lt;/h2&gt;&lt;blockquote&gt;&lt;b&gt;图像语义分割&lt;/b&gt;，简单而言就是给定一张图片，对图片上的每一个像素点分类&lt;/blockquote&gt;

从图像上来看，就是我们需要将实际的场景图分割成下面的分割图： &lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/cb5e078e5008907cb04b300369b7d621.jpg" data-rawwidth="738" data-rawheight="472"&gt;不同颜色代表不同类别。

经过我阅读“大量”论文（羞涩）和查看&lt;a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;amp;compid=6" data-editable="true" data-title="PASCAL VOC Challenge performance evaluation server" class=""&gt;PASCAL VOC Challenge performance evaluation server&lt;/a&gt;，我发现图像语义分割从深度学习引入这个任务（FCN）到现在而言，一个通用的框架已经大概确定了。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3adeadf2a20b0cc9cd68553a95f00552.png" data-rawwidth="1963" data-rawheight="781"&gt;&lt;ul&gt;&lt;li&gt;FCN-全卷积网络 &lt;/li&gt;&lt;li&gt;CRF-条件随机场 &lt;/li&gt;&lt;li&gt;MRF-马尔科夫随机场 &lt;/li&gt;&lt;/ul&gt;
前端使用FCN进行特征粗提取，后端使用CRF/MRF优化前端的输出，最后得到分割图。 &lt;p&gt;
接下来，我会从前端和后端两部分进行总结。&lt;/p&gt;&lt;h2&gt;前端 &lt;/h2&gt;&lt;h2&gt;为什么需要FCN？&lt;/h2&gt;&lt;p&gt;我们分类使用的网络通常会在最后连接几层全连接层，它会将原来二维的矩阵（图片）压扁成一维的，从而丢失了空间信息，最后训练输出一个标量，这就是我们的分类标签。 &lt;/p&gt;&lt;p&gt;
而图像语义分割的输出需要是个分割图，且不论尺寸大小，但是至少是二维的。所以，我们需要丢弃全连接层，换上全卷积层，而这就是全卷积网络了。具体定义请参看论文：&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-title="Fully Convolutional Networks for Semantic Segmentation" class="" data-editable="true"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;前端结构&lt;/h2&gt;&lt;h2&gt;FCN &lt;/h2&gt;&lt;p&gt;此处的FCN特指&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-title="Fully Convolutional Networks for Semantic Segmentation" class="" data-editable="true"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;论文中提出的结构，而非广义的全卷积网络。 &lt;/p&gt;&lt;p&gt;
作者的FCN主要使用了三种技术：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;卷积化（Convolutional） &lt;/li&gt;&lt;li&gt;上采样（Upsample） &lt;/li&gt;&lt;li&gt;跳跃结构（Skip Layer） &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;卷积化 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/42d85c5f7ddcb3f527666b250f62f5d6.png" data-rawwidth="630" data-rawheight="355"&gt;&lt;p&gt;&lt;b&gt;上采样 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;此处的上采样即是反卷积（Deconvolution）。当然关于这个名字不同框架不同，Caffe和Kera里叫Deconvolution，而tensorflow里叫conv_transpose。CS231n这门课中说，叫conv_transpose更为合适。 &lt;/p&gt;&lt;p&gt;
众所诸知，普通的池化（为什么这儿是普通的池化请看后文）会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。 &lt;/p&gt;&lt;p&gt;
反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。图解如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c18522f52e930a3f83748a73a829f0ad.jpg" data-rawwidth="403" data-rawheight="138"&gt;&lt;p&gt;但是，虽然文中说是可学习的反卷积，但是作者实际代码并没有让它学习，可能正是因为这个一对多的逻辑关系。代码如下： &lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer {
  name: "upscore"
  type: "Deconvolution"
  bottom: "score_fr"
  top: "upscore"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 21
    bias_term: false
    kernel_size: 64
    stride: 32
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到lr_mult被设置为了0. &lt;/p&gt;&lt;p&gt;&lt;b&gt;跳跃结构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（这个奇怪的名字是我翻译的，好像一般叫忽略连接结构）这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。具体结构如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ccb6dd0a7f207134ae7690974c3e88a5.png" data-rawwidth="1254" data-rawheight="864"&gt;&lt;p&gt;而不同上采样结构得到的结果对比如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5df8b118a7f77a2222d343a852b46034.png" data-rawwidth="594" data-rawheight="250"&gt;&lt;p&gt;当然，你也可以将pool1， pool2的输出再上采样输出。不过，作者说了这样得到的结果提升并不大。 &lt;/p&gt;&lt;p&gt;这是第一种结构，也是深度学习应用于图像语义分割的开山之作，所以得了CVPR2015的最佳论文。但是，还是有一些处理比较粗糙的地方，具体和后面对比就知道了。 &lt;/p&gt;&lt;h2&gt;SegNet/DeconvNet &lt;/h2&gt;&lt;p&gt;这样的结构总结在这儿，只是我觉得结构上比较优雅，它得到的结果不一定比上一种好。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;SegNet &lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6cab0e3643d16ccab0a1bf1909813484.png" data-rawwidth="877" data-rawheight="250"&gt;&lt;p&gt;&lt;b&gt;DeconvNet&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/99f62dbfe0e39aea5674deeaa2d8363d.jpg" data-rawwidth="828" data-rawheight="313"&gt;&lt;p&gt;这样的对称结构有种自编码器的感觉在里面，先编码再解码。这样的结构主要使用了反卷积和上池化。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c18522f52e930a3f83748a73a829f0ad.jpg" data-rawwidth="403" data-rawheight="138"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/f2da827523ecaa6b8c96b73464ba4c5c.jpg" data-rawwidth="411" data-rawheight="130"&gt;&lt;p&gt;
反卷积如上。而上池化的实现主要在于池化时记住输出值的位置，在上池化时再将这个值填回原来的位置，其他位置填0即OK。 &lt;/p&gt;&lt;h2&gt;DeepLab &lt;/h2&gt;&lt;p&gt;
接下来介绍一个很成熟优雅的结构，以至于现在的很多改进是基于这个网络结构的进行的。&lt;/p&gt;&lt;p&gt;

首先这里我们将指出一个第一个结构FCN的粗糙之处：为了保证之后输出的尺寸不至于太小，FCN的作者在第一层直接对原图加了100的padding，可想而知，这会引入噪声。 &lt;/p&gt;&lt;p&gt;
而怎样才能保证输出的尺寸不会太小而又不会产生加100 padding这样的做法呢？可能有人会说减少池化层不就行了，这样理论上是可以的，但是这样直接就改变了原先可用的结构了，而且最重要的一点是就不能用以前的结构参数进行fine-tune了。所以，Deeplab这里使用了一个非常优雅的做法：将pooling的stride改为1，再加上 1 padding。这样池化后的图片尺寸并未减小，并且依然保留了池化整合特征的特性。 &lt;/p&gt;&lt;p&gt;
但是，事情还没完。因为池化层变了，后面的卷积的感受野也对应的改变了，这样也不能进行fine-tune了。所以，Deeplab提出了一种新的卷积，带孔的卷积：Atrous Convolution.即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2ec8009452f89b7bbd9ecd519fc3e3ae.png" data-rawwidth="865" data-rawheight="543"&gt;&lt;p&gt;而具体的感受野变化如下： &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/766fc04b86b72f7e09d8f8ff6cb648e2.png" data-rawwidth="1147" data-rawheight="711"&gt;a为普通的池化的结果，b为“优雅”池化的结果。我们设想在a上进行卷积核尺寸为3的普通卷积，则对应的感受野大小为7.而在b上进行同样的操作，对应的感受野变为了5.感受野减小了。但是如果使用hole为1的Atrous Convolution则感受野依然为7.&lt;/p&gt;&lt;p&gt;所以，Atrous Convolution能够保证这样的池化后的感受野不变，从而可以fine tune，同时也能保证输出的结果更加精细。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/39577b54b8b53802020cab6da6f9e334.png" data-rawwidth="562" data-rawheight="325"&gt;&lt;p&gt;&lt;b&gt;总结 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;
这里介绍了三种结构：FCN, SegNet/DeconvNet，DeepLab。当然还有一些其他的结构方法，比如有用RNN来做的，还有更有实际意义的weakly-supervised方法等等。 &lt;/p&gt;&lt;h2&gt;后端 &lt;/h2&gt;&lt;p&gt;
终于到后端了，后端这里会讲几个场，涉及到一些数学的东西。我的理解也不是特别深刻，所以欢迎吐槽。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全连接条件随机场(DenseCRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于每个像素&lt;equation&gt;i&lt;/equation&gt;具有类别标签&lt;equation&gt;x_i&lt;/equation&gt;还有对应的观测值&lt;equation&gt;y_i&lt;/equation&gt;，这样每个像素点作为节点，像素与像素间的关系作为边，即构成了一个条件随机场。而且我们通过观测变量&lt;equation&gt;y_i&lt;/equation&gt;来推测像素&lt;equation&gt;i&lt;/equation&gt;对应的类别标签&lt;equation&gt;x_i&lt;/equation&gt;。条件随机场如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/eb0015ceb7aac30d571cd90a47d9e22d.png" data-rawwidth="282" data-rawheight="221"&gt;&lt;p&gt;条件随机场符合吉布斯分布：(此处的&lt;equation&gt;x&lt;/equation&gt;即上面说的观测值) &lt;/p&gt;&lt;equation&gt;P(\mathbf{X=x|I})=\frac{1}{Z(\mathbf{I})}\exp(-E(\mathbf{x|I}))&lt;/equation&gt;&lt;p&gt;其中的&lt;equation&gt;E(\mathbf{x|I})&lt;/equation&gt;是能量函数，为了简便，以下省略全局观测&lt;equation&gt;\mathbf{I}&lt;/equation&gt;： &lt;/p&gt;&lt;equation&gt;E(\mathbf{x})=\sum_i{\Psi_u(x_i)}+\sum_{i&amp;lt;j}\Psi_p(x_i, x_j)&lt;/equation&gt;&lt;p&gt; 其中的一元势函数&lt;equation&gt;\sum_i{\Psi_u(x_i)}&lt;/equation&gt;即来自于前端FCN的输出。而二元势函数如下： &lt;/p&gt;&lt;equation&gt;\Psi_p(x_i, x_j)=u(x_i, x_j)\sum_{m=1}^M{\omega^{(m)}k_G^{(m)}(\mathbf{f_i, f_j)}}&lt;/equation&gt;&lt;p&gt;
二元势函数就是描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片尽量在边界处分割。&lt;/p&gt;&lt;p&gt;而全连接条件随机场的不同就在于，二元势函数描述的是每一个像素与其他所有像素的关系，所以叫“全连接”。 &lt;/p&gt;&lt;p&gt;
关于这一堆公式大家随意理解一下吧... ...而直接计算这些公式是比较麻烦的（我想也麻烦），所以一般会使用平均场近似方法进行计算。而平均场近似又是一堆公式，这里我就不给出了（我想大家也不太愿意看），愿意了解的同学直接看论文吧。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;CRFasRNN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最开始使用DenseCRF是直接加在FCN的输出后面，可想这样是比较粗糙的。而且在深度学习中，我们都追求end-to-end的系统，所以CRFasRNN这篇文章将DenseCRF真正结合进了FCN中。&lt;/p&gt;&lt;p&gt;这篇文章也使用了平均场近似的方法，因为分解的每一步都是一些相乘相加的计算，和普通的加减（具体公式还是看论文吧），所以可以方便的把每一步描述成一层类似卷积的计算。这样即可结合进神经网络中，并且前后向传播也不存在问题。&lt;/p&gt;&lt;p&gt;当然，这里作者还将它进行了迭代，不同次数的迭代得到的结果优化程度也不同（一般取10以内的迭代次数），所以文章才说是as RNN。优化结果如下： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/40fb43fd1cc813501ed803224814f2ed.jpg" data-rawwidth="578" data-rawheight="323"&gt;&lt;p&gt;&lt;b&gt;马尔科夫随机场(MRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;在Deep Parsing Network中使用的是MRF，它的公式具体的定义和CRF类似，只不过作者对二元势函数进行了修改：&lt;/p&gt;&lt;equation&gt;\Psi(y_i^u, y_i^v)=\sum_{k=1}^K\lambda_ku_k(i, u, j, v)\sum_{\forall{z\in{N_j}}}d(j, z)p_z^v&lt;/equation&gt;&lt;p&gt;其中，作者加入的&lt;equation&gt;\lambda_k&lt;/equation&gt;为label context，因为&lt;equation&gt;u_k&lt;/equation&gt;只是定义了两个像素同时出现的频率，而&lt;equation&gt;\lambda_k&lt;/equation&gt;可以对一些情况进行惩罚，比如，人可能在桌子旁边，但是在桌子下面的可能性就更小一些。所以这个量可以学习不同情况出现的概率。而原来的距离&lt;equation&gt;d(i,j)&lt;/equation&gt;只定义了两个像素间的关系，作者在这儿加入了个triple penalty，即还引入了&lt;equation&gt;j&lt;/equation&gt;附近的&lt;equation&gt;z&lt;/equation&gt;，这样描述三方关系便于得到更充足的局部上下文。具体结构如下： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/fff5f41eacdcf624556b6ce0f069600d.jpg" data-rawwidth="641" data-rawheight="307"&gt;&lt;p&gt;这个结构的&lt;b&gt;优点&lt;/b&gt;在于： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;将平均场构造成了CNN &lt;/li&gt;&lt;li&gt;联合训练并且可以one-pass inference，而不用迭代&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;高斯条件随机场(G-CRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个结构使用CNN分别来学习一元势函数和二元势函数。这样的结构是我们更喜欢的： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/920bc48dd3702825b71b0cde4b1f9e5a.jpg" data-rawwidth="1132" data-rawheight="595"&gt;&lt;p&gt;而此中的能量函数又不同于之前： &lt;/p&gt;&lt;equation&gt;E(\mathbf{x})=\frac{1}{2}\mathbf{x}^T(\mathbf{A+\lambda I)x}-\mathbf{Bx}&lt;/equation&gt;&lt;p&gt;而当&lt;equation&gt;(\mathbf{A+\lambda I)}&lt;/equation&gt;是对称正定时，求&lt;equation&gt;E(\mathbf{x})&lt;/equation&gt;的最小值等于求解： &lt;/p&gt;&lt;equation&gt;(\mathbf{A+\lambda I)x}=\mathbf{B}&lt;/equation&gt;&lt;p&gt;而G-CRF的优点在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;二次能量有明确全局 &lt;/li&gt;&lt;li&gt;解线性简便很多 &lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;感悟 &lt;/h2&gt;&lt;ul&gt;&lt;li&gt;FCN更像一种技巧。随着基本网络（如VGG， ResNet）性能的提升而不断进步。 &lt;/li&gt;&lt;li&gt;深度学习+概率图模型（GPM）是一种趋势。其实DL说白了就是进行特征提取，而GPM能够从数学理论很好的解释事物本质间的联系。 &lt;/li&gt;&lt;li&gt;概率图模型的网络化。因为GPM通常不太方便加入DL的模型中，将GPM网络化后能够是GPM参数自学习，同时构成end-to-end的系统。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;


完结撒花&lt;/p&gt;&lt;p&gt;&lt;b&gt;转载须全文转载且注明作者和原文链接，否则保留维权权利&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;引用 &lt;/h2&gt;&lt;p&gt;[1]&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-editable="true" data-title="Fully Convolutional Networks for Semantic Segmentation"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2]&lt;a href="http://arxiv.org/abs/1505.04366)%20%5B3%5D%5BSegNet%5D(http://arxiv.org/abs/1511.00561?context=cs" data-title="Learning Deconvolution Network for Semantic Segmentation" class="" data-editable="true"&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3]&lt;a href="http://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" data-editable="true" data-title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"&gt;Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4]&lt;a href="http://arxiv.org/abs/1412.7062" data-title="Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs" class="" data-editable="true"&gt;Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5]&lt;a href="http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf" data-title="Conditional Random Fields as Recurrent Neural Networks" class="" data-editable="true"&gt;Conditional Random Fields as Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6]&lt;a href="http://liangchiehchen.com/projects/DeepLab.html" data-title="DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs" class="" data-editable="true"&gt;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7]&lt;a href="https://www.researchgate.net/publication/281670742_Semantic_Image_Segmentation_via_Deep_Parsing_Network" data-editable="true" data-title="Semantic Image Segmentation via Deep Parsing Network"&gt;Semantic Image Segmentation via Deep Parsing Network&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[8]&lt;a href="http://arxiv.org/abs/1603.08358v1" data-title="Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs" class="" data-editable="true"&gt;Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[9]&lt;a href="http://arxiv.org/abs/1511.00561?context=cs" data-editable="true" data-title="SegNet"&gt;SegNet&lt;/a&gt;&lt;/p&gt;</description><author>困兽</author><pubDate>Sun, 04 Sep 2016 21:55:08 GMT</pubDate></item><item><title>[原创翻译]循环神经网络惊人的有效性（上）</title><link>https://zhuanlan.zhihu.com/p/22107715</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/68ada38c72be590e4d3da1fbc7a26b65_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创翻译，禁止未授权转载。 &lt;/b&gt;&lt;blockquote&gt;&lt;b&gt;译者注：&lt;/b&gt;经知友推荐，将&lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-title="The Unreasonable Effectiveness of Recurrent Neural Networks" class="" data-editable="true"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;一文翻译作为CS231n课程无RNN和LSTM笔记的补充，感谢&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;的校对。&lt;/blockquote&gt;&lt;h2&gt;目录&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;循环神经网络&lt;/li&gt;&lt;li&gt;字母级别的语言模型&lt;/li&gt;&lt;li&gt;RNN的乐趣&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Paul Graham生成器&lt;/li&gt;&lt;li&gt;莎士比亚&lt;/li&gt;&lt;li&gt;维基百科&lt;/li&gt;&lt;li&gt;几何代数&lt;/li&gt;&lt;li&gt;Linux源码&lt;/li&gt;&lt;li&gt;生成婴儿姓名 &lt;i&gt;&lt;b&gt;译者注：上篇截止处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;理解训练过程&lt;/li&gt;&lt;ul&gt;&lt;li&gt;训练时输出文本的进化&lt;/li&gt;&lt;li&gt;RNN中的预测与神经元激活可视化&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;源代码&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;li&gt;结论&lt;/li&gt;&lt;li&gt;译者反馈&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;循环神经网络（RNN）简直像是魔法一样不可思议。我为&lt;a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-editable="true" data-title="图像标注"&gt;图像标注&lt;/a&gt;项目训练第一个循环网络时的情景到现在都还历历在目。当时才对第一个练手模型训练了十几分钟（超参数还都是随手设置的），它就开始生成一些对于图像的描述，描述内容看起来很不错，几乎让人感到语句是通顺的了。有时候你会遇到模型简单，结果的质量却远高预期的情况，这就是其中一次。当时这个结果让我非常惊讶是因为我本以为RNN是非常难以训练的（随着实践的增多，我的结论基本与之相反了）。让我们快进一年：即使现在我成天都在训练RNN，也常常看到它们的能力和鲁棒性，有时候它们那充满魔性的输出还是能够把我给逗乐。这篇博文就是来和你分享RNN中的一些魔法。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们将训练RNN，让它们生成一个又一个字母。同时好好思考这个问题：这怎么可能呢？&lt;/p&gt;&lt;/blockquote&gt;顺便说一句，和这篇博文一起，我在Github上发布了一个项目。项目基于多层的LSTM，使得你可以训练字母级别的语言模型。你可以输入一大段文本，然后它能学习并按照一次一个字母的方式生成文本。你也可以用它来复现我下面的实验。但是现在我们要超前一点：RNN到底是什么？&lt;h2&gt;循环神经网络&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;序列&lt;/strong&gt;。基于知识背景，你可能会思考：&lt;em&gt;是什么让RNN如此独特呢？&lt;/em&gt;普通神经网络和卷积神经网络的一个显而易见的局限就是他们的API都过于限制：他们接收一个固定尺寸的向量作为输入（比如一张图像），并且产生一个固定尺寸的向量作为输出（比如针对不同分类的概率）。不仅如此，这些模型甚至对于上述映射的演算操作的步骤也是固定的（比如模型中的层数）。RNN之所以如此让人兴奋，其核心原因在于其允许我们对向量的序列进行操作：输入可以是序列，输出也可以是序列，在最一般化的情况下输入输出都可以是序列。下面是一些直观的例子：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2a37bd4e9b12bcc19e045eaf22fea4e5.jpg" data-rawwidth="1329" data-rawheight="416"&gt;&lt;p&gt;上图中每个正方形代表一个向量，箭头代表函数（比如矩阵乘法）。输入向量是红色，输出向量是蓝色，绿色向量装的是RNN的状态（马上具体介绍）。从左至右为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;非RNN的普通过程，从固定尺寸的输入到固定尺寸的输出（比如图像分类）。&lt;/li&gt;&lt;li&gt;输出是序列（例如图像标注：输入是一张图像，输出是单词的序列）。&lt;/li&gt;&lt;li&gt;输入是序列（例如情绪分析：输入是一个句子，输出是对句子属于正面还是负面情绪的分类）。&lt;/li&gt;&lt;li&gt;输入输出都是序列（比如机器翻译：RNN输入一个英文句子输出一个法文句子）。&lt;/li&gt;&lt;li&gt;同步的输入输出序列（比如视频分类中，我们将对视频的每一帧都打标签）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;注意在每个案例中都没有对序列的长度做出预先规定，这是因为循环变换（绿色部分）是固定的，我们想用几次就用几次。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;如你期望的那样，相较于那些从一开始连计算步骤的都定下的固定网络，序列体制的操作要强大得多。并且对于那些和我们一样希望构建一个更加智能的系统的人来说，这样的网络也更有吸引力。我们后面还会看到，RNN将其输入向量、状态向量和一个固定（可学习的）函数结合起来生成一个新的状态向量。在程序的语境中，这可以理解为运行一个具有某些输入和内部变量的固定程序。从这个角度看，RNN本质上就是在描述程序。实际上RNN是具备&lt;a href="http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf" data-editable="true" data-title="图灵完备性"&gt;图灵完备性&lt;/a&gt;的，只要有合适的权重，它们可以模拟任意的程序。然而就像神经网络的通用近似理论一样，你不用过于关注其中细节。实际上，我建议你忘了我刚才说过的话。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;如果训练普通神经网络是对函数做最优化，那么训练循环网络就是针对程序做最优化。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;无序列也能进行序列化处理&lt;/strong&gt;。你可能会想，将序列作为输入或输出的情况是相对少见的，但是需要认识到的重要一点是：即使输入或输出是固定尺寸的向量，依然可以使用这个强大的形式体系以序列化的方式对它们进行处理。例如，下图来自于&lt;a href="http://deepmind.com/" data-editable="true" data-title="DeepMind"&gt;DeepMind&lt;/a&gt;的两篇非常不错的论文。左侧动图显示的是一个算法学习到了一个循环网络的策略，该策略能够引导它对图像进行观察；更具体一些，就是它学会了如何从左往右地阅读建筑的门牌号（&lt;a href="http://arxiv.org/abs/1412.7755" data-editable="true" data-title="Ba et al"&gt;Ba et al&lt;/a&gt;）。右边动图显示的是一个循环网络通过学习序列化地向画布上添加颜色，生成了写有数字的图片（&lt;a href="http://arxiv.org/abs/1502.04623" data-editable="true" data-title="Gregor et al"&gt;Gregor et al&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/45a48feeee79755aa02789f4bf437e77.png" data-rawwidth="1274" data-rawheight="828"&gt;左边：RNN学会如何阅读建筑物门牌号。右边：RNN学会绘出建筑门牌号。 &lt;em&gt;译者注：知乎专栏不支持动图，建议感兴趣读者前往原文查看。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;————————————————————————————————————————&lt;/em&gt;&lt;/p&gt;&lt;p&gt;必须理解到的一点就是：即使数据不是序列的形式，仍然可以构建并训练出能够进行序列化处理数据的强大模型。换句话说，你是要让模型学习到一个处理固定尺寸数据的分阶段程序。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RNN的计算&lt;/strong&gt;。那么RNN到底是如何工作的呢？在其核心，RNN有一个貌似简单的API：它接收输入向量&lt;strong&gt;x&lt;/strong&gt;，返回输出向量&lt;strong&gt;y&lt;/strong&gt;。然而这个输出向量的内容不仅被输入数据影响，而且会收到整个历史输入的影响。写成一个类的话，RNN的API只包含了一个&lt;strong&gt;step&lt;/strong&gt;方法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;rnn = RNN()
y = rnn.step(x) # x is an input vector, y is the RNN's output vector
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;每当&lt;strong&gt;step&lt;/strong&gt;方法被调用的时候，RNN的内部状态就被更新。在最简单情况下，该内部装着仅包含一个内部&lt;em&gt;隐向量&lt;/em&gt;&lt;i&gt;&lt;b&gt;h&lt;/b&gt;&lt;/i&gt;。下面是一个普通RNN的step方法的实现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;class RNN:
  # ...
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的代码详细说明了普通RNN的前向传播。该RNN的参数是三个矩阵：&lt;strong&gt;W_hh, W_xh, W_hy&lt;/strong&gt;。隐藏状态&lt;strong&gt;self.h&lt;/strong&gt;被初始化为零向量。&lt;strong&gt;np.tanh&lt;/strong&gt;函数是一个非线性函数，将激活数据挤压到[-1,1]之内。注意代码是如何工作的：在tanh内有两个部分。一个是基于前一个隐藏状态，另一个是基于当前的输入。在numpy中，&lt;strong&gt;np.dot&lt;/strong&gt;是进行矩阵乘法。两个中间变量相加，其结果被tanh处理为一个新的状态向量。如果你更喜欢用数学公式理解，那么公式是这样的：&lt;equation&gt;h_t=tanh(W_{hh}h_{t-1}+W_{hx}x_t)&lt;/equation&gt;。其中tanh是逐元素进行操作的。&lt;/p&gt;&lt;p&gt;我们使用随机数字来初始化RNN的矩阵，进行大量的训练工作来寻找那些能够产生描述行为的矩阵，使用一些损失函数来衡量描述的行为，这些损失函数代表了根据输入&lt;strong&gt;x&lt;/strong&gt;，你对于某些输出&lt;strong&gt;y&lt;/strong&gt;的偏好。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更深层网络&lt;/strong&gt;。RNN属于神经网络算法，如果你像叠薄饼一样开始对模型进行重叠来进行深度学习，那么算法的性能会单调上升（如果没出岔子的话）。例如，我们可以像下面代码一样构建一个2层的循环网络：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;y1 = rnn1.step(x)
y = rnn2.step(y1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;换句话说，我们分别有两个RNN：一个RNN接受输入向量，第二个RNN以第一个RNN的输出作为其输入。其实就RNN本身来说，它们并不在乎谁是谁的输入：都是向量的进进出出，都是在反向传播时梯度通过每个模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更好的网络&lt;/strong&gt;。需要简要指明的是在实践中通常使用的是一个稍有不同的算法，这就是我在前面提到过的&lt;em&gt;长短基记忆&lt;/em&gt;网络，简称LSTM。LSTM是循环网络的一种特别类型。由于其更加强大的更新方程和更好的动态反向传播机制，它在实践中效果要更好一些。本文不会进行细节介绍，但是在该算法中，所有本文介绍的关于RNN的内容都不会改变，唯一改变的是状态更新（就是&lt;strong&gt;self.h=...&lt;/strong&gt;那行代码）变得更加复杂。从这里开始，我会将术语RNN和LSTM混合使用，但是在本文中的所有实验都是用LSTM完成的。&lt;/p&gt;&lt;h2&gt;字母级别的语言模型&lt;/h2&gt;&lt;p&gt;现在我们已经理解了RNN是什么，它们何以令人兴奋，以及它们是如何工作的。现在通过一个有趣的应用来更深入地加以体会：我们将利用RNN训练一个字母级别的语言模型。也就是说，给RNN输入巨量的文本，然后让其建模并根据一个序列中的前一个字母，给出下一个字母的概率分布。这样就使得我们能够一个字母一个字母地生成新文本了。&lt;/p&gt;&lt;p&gt;在下面的例子中，假设我们的字母表只由4个字母组成“helo”，然后利用训练序列“hello”训练RNN。该训练序列实际上是由4个训练样本组成：1.当h为上文时，下文字母选择的概率应该是e最高。2.l应该是he的下文。3.l应该是hel文本的下文。4.o应该是hell文本的下文。&lt;/p&gt;&lt;p&gt;具体来说，我们将会把每个字母编码进一个1到k的向量（除对应字母为1外其余为0），然后利用&lt;strong&gt;step&lt;/strong&gt;方法一次一个地将其输入给RNN。随后将观察到4维向量的序列（一个字母一个维度）。我们将这些输出向量理解为RNN关于序列下一个字母预测的信心程度。下面是流程图：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/20c6a56f097aba3de796ac62c59605bc.jpg" data-rawwidth="902" data-rawheight="725"&gt;一个RNN的例子：输入输出是4维的层，隐层神经元数量是3个。该流程图展示了使用hell作为输入时，RNN中激活数据前向传播的过程。输出层包含的是RNN关于下一个字母选择的置信度（字母表是helo）。我们希望绿色数字大，红色数字小。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;举例如下：在第一步，RNN看到了字母h后，给出下一个字母的置信度分别是h为1，e为2.2，l为-3.0，o为4.1。因为在训练数据（字符串hello）中下一个正确的字母是e，所以我们希望提高它的置信度（绿色）并降低其他字母的置信度（红色）。类似的，在每一步都有一个目标字母，我们希望算法分配给该字母的置信度应该更大。因为RNN包含的整个操作都是可微分的，所以我们可以通过对算法进行反向传播（微积分中链式法则的递归使用）来求得权重调整的正确方向，在正确方向上可以提升正确目标字母的得分（绿色粗体数字）。然后进行&lt;em&gt;参数更新&lt;/em&gt;，即在该方向上轻微移动权重。如果我们将同样的数据输入给RNN，在参数更新后将会发现正确字母的得分（比如第一步中的e）将会变高（例如从2.2变成2.3），不正确字母的得分将会降低。重复进行一个过程很多次直到网络收敛，其预测与训练数据连贯一致，总是能正确预测下一个字母。&lt;/p&gt;&lt;p&gt;更技术派的解释是我们对输出向量同步使用标准的Softmax分类器（也叫作交叉熵损失）。使用小批量的随机梯度下降来训练RNN，使用&lt;a href="http://arxiv.org/abs/1502.04390" data-editable="true" data-title="RMSProp"&gt;RMSProp&lt;/a&gt;或Adam来让参数稳定更新。&lt;/p&gt;&lt;p&gt;注意当字母l第一次输入时，目标字母是l，但第二次的目标是o。因此RNN不能只靠输入数据，必须使用它的循环连接来保持对上下文的跟踪，以此来完成任务。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;测试&lt;/strong&gt;时，我们向RNN输入一个字母，得到其预测下一个字母的得分分布。我们根据这个分布取出得分最大的字母，然后将其输入给RNN以得到下一个字母。重复这个过程，我们就得到了文本！现在使用不同的数据集训练RNN，看看将会发生什么。&lt;/p&gt;&lt;p&gt;为了更好的进行介绍，我基于教学目的写了代码：&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="minimal character-level RNN language model in Python/numpy"&gt;minimal character-level RNN language model in Python/numpy&lt;/a&gt;，它只有100多行。如果你更喜欢读代码，那么希望它能给你一个更简洁直观的印象。我们下面介绍实验结果，这些实验是用更高效的Lua/Touch代码实现的。&lt;/p&gt;&lt;h2&gt;RNN的乐趣&lt;/h2&gt;&lt;p&gt;下面介绍的5个字母模型我都放在Github上的&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;里了。每个实验中的输入都是一个带有文本的文件，我们训练RNN让它能够预测序列中下一个字母。&lt;/p&gt;&lt;h3&gt;Paul Graham生成器&lt;/h3&gt;&lt;p&gt;译者注：中文名一般译为保罗•格雷厄姆，著有《黑客与画家》一书，中文版已面世。在康奈尔大学读完本科，在哈佛大学获得计算机科学博士学位。1995年，创办了Viaweb。1998年，Yahoo!收购了Viaweb，收购价约5000万美元。此后架起了个人网站paulgraham.com，在上面撰写关于软件和创业的文章，以深刻的见解和清晰的表达而著称。2005年，创建了风险投资公司Y Combinator，目前已经资助了80多家创业公司。现在，他是公认的互联网创业权威。&lt;/p&gt;&lt;p&gt;让我们先来试一个小的英文数据集来进行正确性检查。我最喜欢的数据集是&lt;a href="http://www.paulgraham.com/articles.html" data-editable="true" data-title="Paul Graham的文集"&gt;Paul Graham的文集&lt;/a&gt;。其基本思路是在这些文章中充满智慧，但Paul Graham的写作速度比较慢，要是能根据需求生成富于创业智慧的文章岂不美哉？那么就轮到RNN上场了。&lt;/p&gt;&lt;p&gt;将Paul Graham最近5年的文章收集起来，得到大小约1MB的文本文件，约有1百万个字符（这只算个很小的数据集）。&lt;em&gt;技术要点&lt;/em&gt;：训练一个2层的LSTM，各含512个隐节点（约350万个参数），每层之后使用0.5的dropout。每个数据批量中含100个样本，时间长度上截断了100个字符进行梯度的反向传播。按照上述设置，每个数据批量在TITAN GPU上的运算耗时为0.46秒（如果仅对50个字符进行BPTT，那么耗时会减半，性能的耗费几乎忽略不计）。&lt;em&gt;译者注：BPTT即Backpropagation Through Time&lt;/em&gt;。不在啰嗦，让我们看看RNN生成的文本：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;好吧，显然生成器暂时还无法替代Paul Graham，但是RNN可是完全从头开始学英语的（包括逗号，撇号和空格），而且数据集又如此的小。我还很喜欢它自己学会了如何进行引用（例如上文中的[2]）。有时候它甚至会说出一些充满智慧的洞见，比如“a company is a meeting to think to investors（公司就是一个琢磨如何让投资者打钱的会议）”。&lt;em&gt;译者注：RNN你瞎说什么大实话：）&lt;/em&gt;如果你想要查看更多细节，点击&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/pg.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;温度&lt;/strong&gt;。在生成文本的时候，我们可以调节Softmax的温度。将温度从1降到更低的数值（比如0.5）可以让RNN更加自信的同时变得更加保守。相反，如果将温度设置的更高，结果会更加多样化，但是代价就是可能会出现错误（比如更多的拼写错误）。如果将温度设置得非常接近于0，我们就会得到最像Paul Graham说的话：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;看来我们陷入到连续创业的无限循环中去了。&lt;/p&gt;&lt;h2&gt;莎士比亚&lt;/h2&gt;&lt;p&gt;看起来我们可以让RNN学会如何拼写单词，但是数据中有更多的结构和样式呢？为了回答这个问题，我下载了莎士比亚的所有作品，并将它们装进一个单独的文件中，大小约4.4MB。我们现在可以训练更大的网络，这次试用的是一个3层的RNN，每层的隐节点512个。当我们训练了几个小时候，我们得到了下面的文本输出：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain'd into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.

Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.

DUKE VINCENTIO:
Well, your wit is in the care of side and that.

Second Lord:
They would be ruled after this chamber, and
my fair nues begun out of the fact, to be conveyed,
Whose noble souls I'll have the heart of the wars.

Clown:
Come, sir, I will make did behold your worship.

VIOLA:
I'll drink it.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;记住，RNN只知道字符，所以它学会了说话者的名字和说话内容的格式，有时候我们还会得到类似独白的文字，比如：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;VIOLA:
Why, Salisbury must find his flesh and thought
That which I am not aps, not a man and in fire,
To show the reining of the raven and the wars
To grace my hand reproach within, and not a fair are hand,
That Caesar and my goodly father's world;
When I was heaven of presence and our fleets,
We spare with hours, but cut thy council I am great,
Murdered and by thy master's ready there
My power to give thee but so much as hell:
Some service in the noble bondman here,
Would show him to her wine.

KING LEAR:
O, if you were a feeble sight, the courtesy of your law,
Your sight and several breath, will wear the gods
With his heads, and my hands are wonder'd at the deeds,
So drop upon your lordship's head, and your opinion
Shall be against your honour.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我个人是很难把这段话从莎士比亚的原作中分辨出来啦：）如果你喜欢莎士比亚，那可以来看看这篇&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt" data-editable="true" data-title="长达1000000字符的生成文本"&gt;长达1000000字符的生成文本&lt;/a&gt;。当然，你可以使用我提供的代码，在不同的温度设置下来生成不同的文本。&lt;/p&gt;&lt;h2&gt;维基百科&lt;/h2&gt;&lt;p&gt;我们看见LSTM能够拼写单词，复现语法结构。那么现在就提高难度，使用markdown文本对它进行训练。我使用了&lt;a href="http://prize.hutter1.net/" data-editable="true" data-title="Hutter Prize"&gt;Hutter Prize&lt;/a&gt;的100MB的数据集，数据集内容是原始的维基百科内容，然后在LSTM上训练。根据&lt;a href="http://arxiv.org/abs/1308.0850" data-editable="true" data-title="Graves等的论文"&gt;Graves等的论文&lt;/a&gt;，我使用了其中96MB用于训练，剩下的用做验证集。模型跑了有一晚上，然后可以生成维基百科文章了。下面是一些有趣的文本片段。首先，一些基本的markdown输出：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;Naturalism and decision for the majority of Arab countries' capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated 
with Guangzham's sovereignty. His generals were the powerful ruler of the Portugal 
in the [[Protestant Immineners]], which could be said to be directly in Cantonese 
Communication, which followed a ceremony and set inspired prison, training. The 
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom 
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth's Dajoard]], known 
in western [[Scotland]], near Italy to the conquest of India with the conflict. 
Copyright was the succession of independence in the slop of Syrian influence that 
was a famous German movement based on a more popular servicious, non-doctrinal 
and sexual power post. Many governments recognize the military housing of the 
[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], 
that is sympathetic to be to the [[Punjab Resolution]]
(PJS)[http://www.humah.yahoo.com/guardian.
cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery 
was swear to advance to the resources for those Socialism's rule, 
was starting to signing a major tripad of aid exile.]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你注意到的话，yahoo的那个url是不存在的，是模型生造了它。还有，可以看见模型学会了对于圆括号要成对出现。模型还学会了很多markdown结构，比如标题，列表等：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;{ { cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none } }
''www.e-complete''.

'''See also''': [[List of ethical consent processing]]

== See also ==
*[[Iender dome of the ED]]
*[[Anti-autism]]

===[[Religion|Religion]]===
*[[French Writings]]
*[[Maria]]
*[[Revelation]]
*[[Mount Agamul]]

== External links==
* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]

==External links==
* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有时候模型也会生成一些随机但是合法的XML：&lt;/p&gt;&lt;pre&gt;&lt;code lang="html"&gt;&amp;lt;page&amp;gt;
  &amp;lt;title&amp;gt;Antichrist&amp;lt;/title&amp;gt;
  &amp;lt;id&amp;gt;865&amp;lt;/id&amp;gt;
  &amp;lt;revision&amp;gt;
    &amp;lt;id&amp;gt;15900676&amp;lt;/id&amp;gt;
    &amp;lt;timestamp&amp;gt;2002-08-03T18:14:12Z&amp;lt;/timestamp&amp;gt;
    &amp;lt;contributor&amp;gt;
      &amp;lt;username&amp;gt;Paris&amp;lt;/username&amp;gt;
      &amp;lt;id&amp;gt;23&amp;lt;/id&amp;gt;
    &amp;lt;/contributor&amp;gt;
    &amp;lt;minor /&amp;gt;
    &amp;lt;comment&amp;gt;Automated conversion&amp;lt;/comment&amp;gt;
    &amp;lt;text xml:space="preserve"&amp;gt;#REDIRECT [[Christianity]]&amp;lt;/text&amp;gt;
  &amp;lt;/revision&amp;gt;
&amp;lt;/page&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;模型生成了时间戳，id和其他一些东西。同时模型也能正确地让标示符成对出现，嵌套规则也合乎逻辑。如果你对文本感兴趣，点击&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;代数几何&lt;/h2&gt;&lt;p&gt;上面的结果表明模型确实比较擅长学习复杂的语法结构。收到这些结果的鼓舞，我和同伴&lt;a href="http://cs.stanford.edu/people/jcjohns/" data-editable="true" data-title="Justin Johnson"&gt;Justin Johnson&lt;/a&gt;决定在结构化这一块将研究更加推进一步。我们在网站Stacks上找到了这本关于代数几何的&lt;a href="http://stacks.math.columbia.edu/" data-editable="true" data-title="书"&gt;书&lt;/a&gt;，下载了latex源文件（16MB大小），然后用于训练一个多层的LSTM。令人惊喜的是，模型输出的结果几乎是可以编译的。我们手动解决了一些问题后，就得到了一个看起来像模像样的数学文档，看起来非常惊人：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/47e8530542991e18bdacb7c395d80b94.jpg" data-rawwidth="1343" data-rawheight="755"&gt;&lt;p&gt;生成的代数几何。这里是&lt;a href="http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf" data-editable="true" data-title="源文件"&gt;源文件&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;这是另一个例子：&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/14a67f3357d9f178e711b1c6f9e2ae32.jpg" data-rawwidth="1323" data-rawheight="748"&gt;&lt;p&gt;更像代数几何了，右边还出现了图表。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;由上可见，模型有时候尝试生成latex图表，但是没有成功。我个人还很喜欢它跳过证明的部分（“Proof omitted”，在顶部左边）。当然，需要注意的是latex是相对困难的结构化语法格式，我自己都还没有完全掌握呢。下面是模型生成的一个源文件：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;\begin{proof}
We may assume that $\mathcal{I}$ is an abelian sheaf on $\mathcal{C}$.
\item Given a morphism $\Delta : \mathcal{F} \to \mathcal{I}$
is an injective and let $\mathfrak q$ be an abelian sheaf on $X$.
Let $\mathcal{F}$ be a fibered complex. Let $\mathcal{F}$ be a category.
\begin{enumerate}
\item \hyperref[setain-construction-phantom]{Lemma}
\label{lemma-characterize-quasi-finite}
Let $\mathcal{F}$ be an abelian quasi-coherent sheaf on $\mathcal{C}$.
Let $\mathcal{F}$ be a coherent $\mathcal{O}_X$-module. Then
$\mathcal{F}$ is an abelian catenary over $\mathcal{C}$.
\item The following are equivalent
\begin{enumerate}
\item $\mathcal{F}$ is an $\mathcal{O}_X$-module.
\end{lemma}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这份模型输出的文本展示了一些常见错误。比如模型起了&lt;strong&gt;\begin{proof}&lt;/strong&gt;的头，结尾却是&lt;strong&gt;\end{lemma}&lt;/strong&gt;。这种错误我们就必须手动改正，错误产生的原因可能在于依赖关系过于长了：当模型完成证明后，它已经忘了自己的开头是proof还是lemma了。类似的，模型起了&lt;strong&gt;\begin{enumerate}&lt;/strong&gt;的头，却忘了结尾。我们观察到，在使用更大规模或更好的模型后，这些错误变少了。然而，以上这些错误总是存在出现的可能性的。&lt;/p&gt;&lt;h2&gt;Linux源码&lt;/h2&gt;&lt;p&gt;我想要把结构化数据的实验推到极限，所以最后一个实验我们决定使用代码。具体说来，就是从Linux在Github上的&lt;a href="https://github.com/torvalds/linux" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;提取了所有代码和能找到的头文件，把他们装进一个巨大的文件中（474MB的C代码）。我原本计划是只训练Linux的核心文件的，但是其大小只有约16MB。然后我在GPU上使用3层LSTM训练了好几天。这些模型有大约1000万的参数，这对于RNN模型来说还不算多的。实验结果超级有趣：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 * Increment the size file of the new incorrect UI_FILTER group information
 * of the size generatively.
 */
static int indicate_policy(void)
{
  int error;
  if (fd == MARN_EPT) {
    /*
     * The kernel blank will coeld it to userspace.
     */
    if (ss-&amp;gt;segment &amp;lt; mem_total)
      unblock_graph_and_set_blocked();
    else
      ret = 1;
    goto bail;
  }
  segaddr = in_SB(in.addr);
  selector = seg / 16;
  setup_works = true;
  for (i = 0; i &amp;lt; blocks; i++) {
    seq = buf[i++];
    bpf = bd-&amp;gt;bd.next + i * search;
    if (fd) {
      current = blocked;
    }
  }
  rw-&amp;gt;name = "Getjbbregs";
  bprm_self_clearl(&amp;amp;iv-&amp;gt;version);
  regs-&amp;gt;new = blocks[(BPF_STATS &amp;lt;&amp;lt; info-&amp;gt;historidac)] | PFMR_CLOBATHINC_SECONDS &amp;lt;&amp;lt; 12;
  return segtable;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这些代码看起来相当酷了。虽然我不认为这些代码能够编译，但是看着这些代码你会感觉是一个巨大的C代码库。注意RNN还不时的给自己的代码加上了注释。代码中也很少有语法错误。比如它合理地使用了字符串，指针标记等。它学会了让花括号和中括号成对出现，让代码有正确的缩进。一个常见的错误是它不能追中变量的名字：它常常使用未声明的变量（比如上面的&lt;strong&gt;rw&lt;/strong&gt;），或声明一些永不使用的变量（比如上面的&lt;strong&gt;int error&lt;/strong&gt;），或返回不存在的变量。让我们看看更多的例子，下面是一个代码片段，展示了RNN学习到的更多的操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 * If this error is set, we will need anything right after that BSD.
 */
static void action_new_function(struct s_stat_info *wb)
{
  unsigned long flags;
  int lel_idx_bit = e-&amp;gt;edd, *sys &amp;amp; ~((unsigned long) *FIRST_COMPAT);
  buf[0] = 0xFFFFFFFF &amp;amp; (bit &amp;lt;&amp;lt; 4);
  min(inc, slist-&amp;gt;bytes);
  printk(KERN_WARNING "Memory allocated %02x/%02x, "
    "original MLL instead\n"),
    min(min(multi_run - s-&amp;gt;len, max) * num_data_in),
    frame_pos, sz + first_seg);
  div_u64_w(val, inb_p);
  spin_unlock(&amp;amp;disk-&amp;gt;queue_lock);
  mutex_unlock(&amp;amp;s-&amp;gt;sock-&amp;gt;mutex);
  mutex_unlock(&amp;amp;func-&amp;gt;mutex);
  return disassemble(info-&amp;gt;pending_bh);
}

static void num_serial_settings(struct tty_struct *tty)
{
  if (tty == tty)
    disable_single_st_p(dev);
  pci_disable_spool(port);
  return 0;
}

static void do_command(struct seq_file *m, void *v)
{
  int column = 32 &amp;lt;&amp;lt; (cmd[2] &amp;amp; 0x80);
  if (state)
    cmd = (int)(int_state ^ (in_8(&amp;amp;ch-&amp;gt;ch_flags) &amp;amp; Cmd) ? 2 : 1);
  else
    seq = 1;
  for (i = 0; i &amp;lt; 16; i++) {
    if (k &amp;amp; (1 &amp;lt;&amp;lt; 1))
      pipe = (in_use &amp;amp; UMXTHREAD_UNCCA) +
        ((count &amp;amp; 0x00000000fffffff8) &amp;amp; 0x000000f) &amp;lt;&amp;lt; 8;
    if (count == 0)
      sub(pid, ppc_md.kexec_handle, 0x20000000);
    pipe_set_bytes(i, 0);
  }
  /* Free our user pages pointer to place camera if all dash */
  subsystem_info = &amp;amp;of_changes[PAGE_SIZE];
  rek_controls(offset, idx, &amp;amp;soffset);
  /* Now we want to deliberately put it to device */
  control_check_polarity(&amp;amp;context, val, 0);
  for (i = 0; i &amp;lt; COUNTER; i++)
    seq_puts(s, "policy ");
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意在第二个函数中，比较了&lt;strong&gt;tty == tty&lt;/strong&gt;，这永远为真。但这次至少参数名出现在了函数体中。在最后一个函数中，没有返回值，因为函数的声明有void，所以倒也正确。 然而，前面两个函数声明为void确有返回值，这又是一种因为依赖过长产生的常见错误。&lt;/p&gt;&lt;p&gt;有时候模型决定生成新的文件。这通常是非常让人惊喜的部分：模型首先引用了GUN许可，进行了一些导入操作，生成一些宏，然后开始进入代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 *  Copyright (c) 2006-2010, Intel Mobile Communications.  All rights reserved.
 *
 *   This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published by
 * the Free Software Foundation.
 *
 *        This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *
 *  GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software Foundation,
 *  Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

#include &amp;lt;linux/kexec.h&amp;gt;
#include &amp;lt;linux/errno.h&amp;gt;
#include &amp;lt;linux/io.h&amp;gt;
#include &amp;lt;linux/platform_device.h&amp;gt;
#include &amp;lt;linux/multi.h&amp;gt;
#include &amp;lt;linux/ckevent.h&amp;gt;

#include &amp;lt;asm/io.h&amp;gt;
#include &amp;lt;asm/prom.h&amp;gt;
#include &amp;lt;asm/e820.h&amp;gt;
#include &amp;lt;asm/system_info.h&amp;gt;
#include &amp;lt;asm/setew.h&amp;gt;
#include &amp;lt;asm/pgproto.h&amp;gt;

#define REG_PG    vesa_slot_addr_pack
#define PFM_NOCOMP  AFSR(0, load)
#define STACK_DDR(type)     (func)

#define SWAP_ALLOCATE(nr)     (e)
#define emulate_sigs()  arch_get_unaligned_child()
#define access_rw(TST)  asm volatile("movd %%esp, %0, %3" : : "r" (0));   \
  if (__type &amp;amp; DO_READ)

static void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \
          pC&amp;gt;[1]);

static void
os_prefix(unsigned long sys)
{
#ifdef CONFIG_PREEMPT
  PUT_PARAM_RAID(2, sel) = get_state_state();
  set_pid_sum((unsigned long)state, current_state_str(),
           (unsigned long)-1-&amp;gt;lr_full; low;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里面有太多有趣的地方可以讨论，我几乎可以写一整个博客，所以我现在还是暂停，感兴趣的可以查看&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/linux.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;生成婴儿姓名&lt;/h2&gt;&lt;p&gt;让我们再试一个。给RNN输入一个包含8000个小孩儿姓名的文本文件，一行只有一个名字。（名字是从&lt;a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;获得的）我们可以把这些输入RNN然后生成新的名字。下面是一些名字例子，只展示了那些没有在训练集中出现过的名字：&lt;/p&gt;&lt;p&gt;&lt;em&gt;Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina&lt;/em&gt;&lt;/p&gt;&lt;p&gt;点击&lt;a href="http://cs.stanford.edu/people/karpathy/namesGenUnique.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;可以查看更多。我个人最喜欢的名字包括“Baby” (哈)， “Killie”，“Char”，“R”，“More”，“Mars”，“Hi”，“Saddie”，“With”和“Ahbort”。这真的蛮有意思，你还可以畅想在写小说或者给创业公司起名字的时候，这个能给你灵感。&lt;/p&gt;&lt;p&gt;&lt;b&gt;上篇截止&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;翻译不到位的地方，欢迎知友们评论批评指正；&lt;/li&gt;&lt;li&gt;在计算机视觉方面，个人对于&lt;b&gt;图像标注（image caption）&lt;/b&gt;比较感兴趣，正在入坑。欢迎有同样兴趣的知友投稿讨论；&lt;/li&gt;&lt;li&gt;感谢&lt;a href="https://www.zhihu.com/people/157deec64cc5e062b2207aeece42f50f" data-hash="157deec64cc5e062b2207aeece42f50f" class="member_mention" data-hovercard="p$b$157deec64cc5e062b2207aeece42f50f"&gt;@七月&lt;/a&gt;的细节指正。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Tue, 06 Sep 2016 20:31:35 GMT</pubDate></item><item><title>Andrej Karpathy的回信和Quora活动邀请</title><link>https://zhuanlan.zhihu.com/p/22282421</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1b4d5e3761721c42438fbee54ccc8696_r.jpg"&gt;&lt;/p&gt;大家好：&lt;p&gt;关注我们CS231n翻译项目的知友应该知道，之所以开始这个翻译，也是因为得到了&lt;a href="http://cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;的授权。&lt;/p&gt;&lt;p&gt;这次翻译项目完结邮件了他，顺便问他有没有什么对学习CS231n的小伙伴们说的？邮件部分截取如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b3b88f5c66a44a82ec7ee541cc3aad2f.png" data-rawwidth="1590" data-rawheight="819"&gt;AK表示其实也没啥好说的，非常高兴能够分享这些知识，请&lt;b&gt;好好利用它来让这个世界变得更美好！&lt;/b&gt;最后表示感谢。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Session活动邀请&lt;/b&gt;：&lt;b&gt;下周四（9月8日）AK会在Quora上做一个类似知乎live的Session&lt;/b&gt;，貌似是基于文字的问答活动。活动地址&lt;a href="https://www.quora.com/session/Andrej-Karpathy/1" data-title="在这里" class=""&gt;在这里&lt;/a&gt;。感兴趣的同学可以去&lt;b&gt;提提问，捧捧场&lt;/b&gt;。这里也是义务为他做推广，因为我非常&lt;b&gt;佩服他坚持高质量分享知识的行为&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;注：Andrej Karpathy的个人情况点&lt;a href="http://cs.stanford.edu/people/karpathy/"&gt;这里&lt;/a&gt;。&lt;/p&gt;</description><author>杜客</author><pubDate>Sun, 04 Sep 2016 22:36:10 GMT</pubDate></item><item><title>知行合一码作业，深度学习真入门</title><link>https://zhuanlan.zhihu.com/p/22232836</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6e060188cc1571a89456af45670d51a9_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：文章为本人在&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;专栏原创首发，禁止未授权转载。&lt;/b&gt;&lt;h2&gt;前言&lt;/h2&gt;&lt;p&gt;这是一篇号召深度学习爱好者&lt;b&gt;共同学习、讨论和完成&lt;/b&gt;斯坦福深度学习课程&lt;b&gt;CS231n的3个编程大作业&lt;/b&gt;的召集令。&lt;/p&gt;&lt;p&gt;在过去的3个多月中，我们翻译完成了CS231n的官方课程笔记并汇总在《&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" data-editable="true" data-title="CS231n官方笔记授权翻译总集篇发布" class=""&gt;CS231n官方笔记授权翻译总集篇发布&lt;/a&gt;》一文中，得到了大家的支持。期间也有不少同学私信各种和编程作业相关的问题，这说明还是有不少的想要入门深度学习的同学希望通过完成编程作业来加深自己对于课程知识内容的理解。&lt;/p&gt;&lt;p&gt;同时很多同学也始终在私信是否有交流群，我也反复回答自己并没有建交流群。个人始终认为交流应该是“&lt;b&gt;慎思而明辨之&lt;/b&gt;”，学习主静，需要&lt;b&gt;沉下心来看课程、笔记和论文，完整地啃完一部分内容后，再和人交流，这样的交流才是有质量的，对人对己都有益&lt;/b&gt;。而这样有质量的交流，最好是写文章来互动。为了促进互动，我在“&lt;b&gt;学习互动&lt;/b&gt;”部分提出了一些思路。&lt;/p&gt;&lt;h2&gt;动机&lt;/h2&gt;&lt;p&gt;&lt;b&gt;为什么&lt;/b&gt;要完成这3个编程作业？&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;行之明觉精察处便是知，知之真切笃实处便是行。&lt;/b&gt;——王阳明&lt;/blockquote&gt;&lt;p&gt;对于想要深度学习入门的同学来说，CS231n的编程作业能够引导学习者&lt;b&gt;由浅入深地在实践中理解深度学习中常用的算法模型，训练与调参方法和常用的技巧套路&lt;/b&gt;等。&lt;/p&gt;&lt;p&gt;相较于直接阅读各大开源框架源码来学习，个人认为通过完成作业来入门有以下几个好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;难度循序渐进&lt;/b&gt;。小作业设置是从线性分类模型实现开始，到神经网络模型，再到循环神经网络模型。而基于Jupyter Notebook的作业脚本单个小作业中，也是将一个小作业切分成很多块，让学习者逐个顺序实现。&lt;/li&gt;&lt;li&gt;&lt;b&gt;实现实验并重&lt;/b&gt;。每个小作业就是一个实验：你需要初始化设置，读取数据集，然后实现算法模型，检查模型是否实现正确，训练模型，调参，最后分析评价算法性能。在这个过程中，你既要聚焦在核心的函数实现，又会跟着作业安排的顺序来完成实验，掌握算法实验的套路。&lt;/li&gt;&lt;li&gt;&lt;b&gt;细节设置平衡&lt;/b&gt;。作业使用Python语言，基于Numpy库来实现。一些常用的基础函数功能课程已经实现，学习者只需要聚焦实现最核心的函数，且不会陷入到特别底层的细节。这样就在工程实践与概念理解中得到了一个平衡。&lt;/li&gt;&lt;li&gt;&lt;b&gt;应用丰富有趣&lt;/b&gt;。目前相当热门的Prisma所使用的图像风格化，谷歌的DeepDream，以及图像标注等应用都在作业中有原型实现。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;作业概述&lt;/h2&gt;&lt;p&gt;CS231n一共有3个大作业，作业简介我们已经翻译并发布：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏" class=""&gt;斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 2简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 2简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;只需要&lt;b&gt;按照简介中的指导&lt;/b&gt;下载作业代码（文中有链接），安装完配置环境（非常简单！），就可以愉快地开始做作业了。&lt;/p&gt;&lt;p&gt;对于入门的同学来说，&lt;b&gt;作业的完成需要一些前置的知识学习，单就课程内资源来说&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;Assignment #1中的5个小作业包含了&lt;b&gt;knn、SVM、Softmax、一个两层的神经网络和特征提取&lt;/b&gt;。你需要学习课程视频1-4课，阅读&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" class="" data-editable="true" data-title="Python Numpy教程"&gt;Python Numpy教程&lt;/a&gt;，&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" class="" data-editable="true" data-title="图像分类笔记（上）"&gt;图像分类笔记（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，线性分类笔记&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" class="" data-editable="true" data-title="（中）"&gt;（中）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，最优化笔记&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，以及&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" class="" data-editable="true" data-title="反向传播笔记"&gt;反向传播笔记&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Assignment #2中的4个小作业包含了&lt;b&gt;全连接神经网络，卷及神经网络和一些最新的优化方法&lt;/b&gt;，你需要学习课程视频5-9课，阅读神经网络笔记1&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" class="" data-editable="true" data-title="神经网络笔记2"&gt;神经网络笔记2&lt;/a&gt;，神经网络笔记3&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，以及&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" class="" data-editable="true" data-title="卷积神经网络笔记"&gt;卷积神经网络笔记&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Assignment #3中的4个小作业包含了&lt;b&gt;RNN，LSTM，图像标注模型，图像生成模型&lt;/b&gt;等，你需要学习课程视频10-11课。这部分课程没有给出笔记，后续我们可能寻找一些较好的内容加以翻译补充。&lt;/p&gt;&lt;h2&gt;学习互动&lt;/h2&gt;&lt;p&gt;&lt;b&gt;推进进度&lt;/b&gt;：3个大作业共包含13个小作业。我们用一个比较宽松的进度来推进，今年还有将近4个月的时间，那么我们&lt;b&gt;从9月5日开始起算&lt;/b&gt;，平均一个月完成3个左右的小作业。也就是说&lt;b&gt;每个小作业1-2周的时间学习、讨论并完成&lt;/b&gt;。从给我个人的经验来看，这个速度即使是对于零基础的同学，也是足够的啦。&lt;/p&gt;&lt;p&gt;&lt;b&gt;互动方式&lt;/b&gt;：我们会按照小作业顺序发布完成作业所做的理论学习，具体实现解读文章。同样，&lt;b&gt;任何愿意按照上述进度进行作业学习的同学，都可以从自己对于理论的不同理解，核心函数不同的实现方式，遇到问题及解决问题的收获，对专栏进行投稿。而各种疑问，也可以在对应文章的评论中进行讨论&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;建不建群&lt;/b&gt;：我的个人意见是不太想建，因为自己精力有限，建了没办法良好管理与互动。但是大家也可以提意见，&lt;b&gt;如果大部分愿意一起学习的同学想要建一个群，同时有比较热心的同学能在群里面不时给大家介绍一下情况&lt;/b&gt;，那么还是可以考虑一下。ps：但是我可能基本是潜水。&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;最近读曾国藩家书，看到一个有趣的学习方法：&lt;/p&gt;&lt;blockquote&gt;予定&lt;b&gt;刚日读经&lt;/b&gt;，&lt;b&gt;柔日读史&lt;/b&gt;之法。——曾国藩&lt;/blockquote&gt;&lt;p&gt;所谓刚日，是说人的情绪亢阳激越的日子。柔日，是指人情绪卑幽忧昧的日子。而经主常，史生变。所以咱在感觉能打10个的状态时，就要读一些经书，让我们能够平和沉静。在我们感觉比较低落忧郁的日子，就看看史书，从波澜壮阔的历史中激扬斗志。&lt;/p&gt;&lt;p&gt;具体到现在，我看可以修改为“&lt;b&gt;刚日写码，柔日知乎&lt;/b&gt;”。&lt;b&gt;状态好的日子&lt;/b&gt;就看论文写代码，发现自己的不足从而提升自己，储才养望。&lt;b&gt;状态一般创造力不强时&lt;/b&gt;，可以对已经完成的事情做一个总结，以书面的形式发出来，对自己工作的总结积累和与别人探讨，一总结发现自己之前还是有很多收获的，所以不用心情那么低落；&lt;/p&gt;&lt;p&gt;最后说一句，预定是9月5日开始，大家有什么意见请尽情提吧。&lt;/p&gt;</description><author>杜客</author><pubDate>Wed, 31 Aug 2016 14:51:35 GMT</pubDate></item><item><title>最前沿：深度学习训练方法大革新，反向传播训练不再唯一</title><link>https://zhuanlan.zhihu.com/p/22143664</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;众所周知，在深度学习中我们使用反向传播算法进行训练。可能在任意一门深度学习课程中，反向传播都是必学的内容。我们使用反向传播计算每个参数的梯度，从而能够使用各种梯度下降方法SGD，Adam，RMSProp等来更新参数。基本上可以说反向传播算法是深度学习算法的基础。目前所有的深度学习应用，都基于反向传播算法进行训练。&lt;/p&gt;&lt;p&gt;但是，我们人类的大脑是这样学习的吗？&lt;/p&gt;&lt;p&gt;诚然现在的神经科学还无法告诉我们真正的答案，但我们凭我们的常识想想，我们大脑真正的神经网络会需要这样先前向传播一下，再反向传播一下然后更新神经元？这未免太不“科学”了。直观的想象我们大脑的神经元应该都是单独的个体，通过与周围的神经元交流来改变自己。但是对于反向传播算法，这种方法最大的缺点就是更新速度。前面的神经元需要等着后面的神经网络传回误差数据才能更新，要是以后搞个10000+层的神经网络，这显然就太慢了。所以，&lt;/p&gt;&lt;p&gt;能不能异步的更新参数？&lt;/p&gt;&lt;p&gt;甚至，每个参数能够同时更新？&lt;/p&gt;&lt;p&gt;或者差一点，只要前向传播一下就能更新参数？&lt;/p&gt;&lt;p&gt;这些问题要是能解决那就是game changing了。&lt;/p&gt;&lt;p&gt;那么现在DeepMind又开挂了，最新2016年8月18号出来的问题第一次解决了上面的问题。&lt;/p&gt;&lt;p&gt;文章题目：Decoupled Neural Interfaces using Synthetic Gradients &lt;/p&gt;&lt;p&gt;文章链接：&lt;a href="https://arxiv.org/pdf/1608.05343.pdf" data-editable="true" data-title="arxiv.org 的页面" class=""&gt;https://arxiv.org/pdf/1608.05343.pdf&lt;/a&gt; （本文图片都引用自文章）&lt;/p&gt;&lt;h2&gt;2 What is the idea?&lt;/h2&gt;&lt;p&gt;如果我们陷入在反向传播的思维中，我们就完全无法想象如果没有从后面传回来梯度误差，我们该怎么更新参数。DeepMind打破这种思路，如果不传回来，我们可以&lt;/p&gt;&lt;p&gt;&lt;b&gt;合成梯度！也就是Synthetic Gradients！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;也就是我们可以预测梯度。如果我们预测得准确，那么就可以直接更新了。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6bd8df82a8791c6e437263668d21b4e4.png" data-rawwidth="1990" data-rawheight="752"&gt;看上图，一般的反向传播如图b所示，从后到前依次传递梯度（绿色的线），然后更新参数。那么这里，我们使用一个M来预测梯度（蓝色的线），然后更新参数。我们传给M当前层的输出，然后M返回给我们梯度。&lt;/p&gt;&lt;p&gt;那么怎么预测梯度？&lt;/p&gt;&lt;p&gt;&lt;b&gt;就用神经网络来预测！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;也就是每一层的神经网络对应另一个神经网络M，每个M来调控每一层的神经网络更新！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;这套方法称为 Decoupled Neural Interfaces(DNI), 也就是将神经网络分解训练的意思。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;那么不管是MLP，CNN还是RNN或者其他各种结构的神经网络，因为都是以层为单位，都可以使用神经网络来合成梯度，也就是都可以使用这样的方法来实现训练。&lt;/p&gt;&lt;h2&gt;3 合成梯度的M神经网络是如何训练的？&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7223b3b3fd0b96029c8044ade57b5f0d.png" data-rawwidth="1778" data-rawheight="702"&gt;合成梯度的M神经网络用来输出估计的梯度误差。那么要训练M就需要有一个梯度误差来做目标，但是这里没有完全的反向传播，如何得到真实的梯度误差？作者采用一个tradeoff，利用下一层神经网络的估计梯度误差来计算本层的梯度误差，并利用这个误差作为目标训练M。如上图所示，&lt;equation&gt;f_i&lt;/equation&gt;输出&lt;equation&gt;h_i&lt;/equation&gt;到&lt;equation&gt;M_{i+1}&lt;/equation&gt;，然后&lt;equation&gt;M_{i+1}&lt;/equation&gt;输出估计的梯度误差&lt;equation&gt;\hat{\delta_i}&lt;/equation&gt;，接下来利用&lt;equation&gt;\hat{\delta_i}&lt;/equation&gt;来更新&lt;equation&gt;f_i&lt;/equation&gt;的参数。接下来&lt;equation&gt;h_i&lt;/equation&gt;输入到下一层神经网络&lt;equation&gt;f_{i+1}&lt;/equation&gt;中，同理得到该层的估计梯度误差&lt;equation&gt;\hat{\delta}_{i+1}&lt;/equation&gt;，然后利用&lt;equation&gt;\hat{\delta}_{i+1}&lt;/equation&gt;通过&lt;equation&gt;\delta_i = f^`_{i+1}(h_i)\hat{\delta}_{i+1}&lt;/equation&gt;也就是i+1层的梯度乘以梯度误差从而得到i层的梯度误差&lt;equation&gt;\delta_i&lt;/equation&gt;,然后就可以使用&lt;equation&gt;\delta_i&lt;/equation&gt;更新M了。所以M神经网络的训练需要BP。&lt;/p&gt;&lt;h2&gt;4 看结果&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c9edd09e655b6baa1aaa49025b146992.png" data-rawwidth="1956" data-rawheight="876"&gt;上图是MNIST的训练，采用全连接网络FCN或者CNN进行训练。从结果上可以看到，DNI特别是cDNI（就是将数据的标签作为神经网络M的输入）效果蛮好的（略低于反向传播），但是训练速度比原来采用反向传播的快，特别看上面的曲线橙色部分，比灰色的反向传播快了非常多。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/91fc6553e5d87aac17561dc550beffcb.png" data-rawwidth="2046" data-rawheight="774"&gt;上面这图是针对RNN的训练，一个是Repeat Copy复制任务，一个是语言模型的训练。因为RNN的梯度计算面临无穷的循环，所以一般采用一定的时间间隔来计算梯度。那么这里，DNI的效果远远超过了BPTT（Back Propagation Through Time). 速度两倍以上。&lt;/p&gt;&lt;p&gt;从上面的结果可以看出，采用DNI进行训练相比反向传播竟然速度快，效果好。要是预测梯度的神经网络能提前训练好，估计又能快不少吧！&lt;/p&gt;&lt;h2&gt;5 One More Thing&lt;/h2&gt;&lt;p&gt;DeepMind不仅仅做到不需要反向传播，甚至更进一步，连前向传播也不用，直接异步更新每一层的参数。怎么做的？&lt;/p&gt;&lt;p&gt;&lt;b&gt;不仅仅预测梯度，我们还预测输入！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/8826d3fcd18e292a08ddb3b84202649a.png" data-rawwidth="1996" data-rawheight="566"&gt;上图的I是每一层的输入预测神经网络，用来预测上一层的输出。&lt;/p&gt;&lt;p&gt;这样做对于MNIST的训练也能达到2%的误差，只是慢了一点。估计主要的慢是在I和M的模型训练上。这里4层隐藏层就有6个额外的神经网络了。&lt;/p&gt;&lt;h2&gt;6 这个成果意味着什么？&lt;/h2&gt;&lt;p&gt;&lt;b&gt;神经网络模块化了&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每一层网络都可以看成独立的一个模块，模块与模块之间相互通信，从而实现学习。而学习训练不再需要同步，可以异步。也就是说每个模块都可以独立训练。Paper中也做了异步训练的实验，可以随机的训练神经网络中的不同层，或者有两个神经网络需要相互配合的，都可以异步训练。这是这个成果最大的意义，将能够因此构建出完全不一样的神经网络模型，训练方式发生完全的改变。&lt;/p&gt;&lt;h2&gt;7 存在的问题&lt;/h2&gt;&lt;p&gt;大家都可以注意到，虽然这个idea能够使主神经网络不再使用反向传播算法，&lt;b&gt;但是I和M的神经网络都是依靠反向传播算法进行更新！也就是反而多了好多个小的神经网络。&lt;/b&gt;但是这个方法如果不考虑I和M（主要是M）的训练，那么显然将会非常的快。那么，I比较难，涉及到具体的输入，但是有没有可能能够预训练M呢？或者换一个角度思考，我们人类大脑的神经元是否是相互独立，每一个神经元都有自己的一套学习机制在里面，能够自主改变？这些很值得我们思考。&lt;/p&gt;&lt;h2&gt;8 一点感想&lt;/h2&gt;&lt;p&gt;因为神经网络什么都能学习，所以用神经网络来更新神经网络也不足为怪。之前的&lt;a href="https://zhuanlan.zhihu.com/p/21362413?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：让计算机学会学习Let Computers Learn to Learn - 智能单元 - 知乎专栏"&gt;最前沿：让计算机学会学习Let Computers Learn to Learn - 智能单元 - 知乎专栏&lt;/a&gt;就是使用神经网络来做梯度更新的工作。这里是使用神经网络来合成梯度。所以，如果把上一篇的成果结合进来，神经网络大部分都是神经网络自己在训练了！&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;这是深度学习基本学习机制的大变革，一步一步迈向人类的大脑！&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;补充：DeepMind官网给出了一个介绍DNI的博客：&lt;a href="https://deepmind.com/blog#decoupled-neural-interfaces-using-synthetic-gradients"&gt;https://deepmind.com/blog#decoupled-neural-interfaces-using-synthetic-gradients&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;版权声明：本文为原创文章，未经允许不得转载！&lt;/h2&gt;</description><author>Flood Sung</author><pubDate>Sat, 27 Aug 2016 15:59:24 GMT</pubDate></item><item><title>贺完结！CS231n官方笔记授权翻译总集篇发布</title><link>https://zhuanlan.zhihu.com/p/21930884</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/0dee7274beed25bd4abbcd76cb7d9576_r.jpg"&gt;&lt;/p&gt;&lt;blockquote&gt;哈哈哈！我们也是不谦虚，几个“业余水平”的网友，怎么就“零星”地把这件事给搞完了呢！&lt;b&gt;总之就是非常开心&lt;/b&gt;，废话不多说，进入正题吧！&lt;/blockquote&gt;&lt;h2&gt;CS231n简介&lt;/h2&gt;&lt;p&gt;CS231n的全称是&lt;a href="http://vision.stanford.edu/teaching/cs231n/index.html" data-editable="true" data-title="CS231n: Convolutional Neural Networks for Visual Recognition" class=""&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;，即&lt;b&gt;面向视觉识别的卷积神经网络&lt;/b&gt;。该课程是&lt;a href="http://vision.stanford.edu/index.html" data-editable="true" data-title="斯坦福大学计算机视觉实验室" class=""&gt;斯坦福大学计算机视觉实验室&lt;/a&gt;推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程描述&lt;/b&gt;：请允许我们引用课程主页上的&lt;b&gt;官方描述&lt;/b&gt;如下。&lt;/p&gt;&lt;blockquote&gt;计算机视觉在社会中已经逐渐普及，并广泛运用于搜索检索、图像理解、手机应用、地图导航、医疗制药、无人机和无人驾驶汽车等领域。而这些应用的核心技术就是图像分类、图像定位和图像探测等视觉识别任务。近期神经网络（也就是“深度学习”）方法上的进展极大地提升了这些代表当前发展水平的视觉识别系统的性能。本课程将深入讲解深度学习框架的细节问题，聚焦面向视觉识别任务（尤其是图像分类任务）的端到端学习模型。在10周的课程中，学生们将会学习如何实现、训练和调试他们自己的神经网络，并建立起对计算机视觉领域的前沿研究方向的细节理解。最终的作业将包括训练一个有几百万参数的卷积神经网络，并将其应用到最大的图像分类数据库（ImageNet）上。我们将会聚焦于教授如何确定图像识别问题，学习算法（比如反向传播算法），对网络的训练和精细调整（fine-tuning）中的工程实践技巧，指导学生动手完成课程作业和最终的课程项目。本课程的大部分背景知识和素材都来源于&lt;a href="http://image-net.org/challenges/LSVRC/2014/index" data-editable="true" data-title="ImageNet Challenge" class=""&gt;ImageNet Challenge&lt;/a&gt;竞赛。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;课程内容&lt;/b&gt;：官方课程安排及资源获取请点击&lt;a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" data-editable="true" data-title="这里" class=""&gt;这里&lt;/a&gt;，课程视频请在Youtube上查看&lt;a href="https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;创建的&lt;a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" data-editable="true" data-title="播放列表" class=""&gt;播放列表&lt;/a&gt;，也可私信我们获取云盘视频资源。通过查看官方课程表，我们可以看到：CS231n课程资源主要由&lt;b&gt;授课视频与PPT&lt;/b&gt;，&lt;b&gt;授课知识详解笔记&lt;/b&gt;和&lt;b&gt;课程作业&lt;/b&gt;三部分组成。其中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;授课视频15课&lt;/b&gt;。每节课时约1小时左右，每节课一份PPT。&lt;/li&gt;&lt;li&gt;&lt;b&gt;授课知识详解笔记共9份&lt;/b&gt;。光看课程视频是不够的，深入理解课程笔记才能比较扎实地学习到知识。&lt;/li&gt;&lt;li&gt;&lt;b&gt;课程作业3次&lt;/b&gt;。其中每次作业中又包含多个小作业，完成作业能确保对于课程关键知识的深入理解和实现。&lt;/li&gt;&lt;li&gt;&lt;b&gt;课程项目1个&lt;/b&gt;。这个更多是面向斯坦福的学生，组队实现课程项目。&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展阅读若干&lt;/b&gt;。课程推荐的拓展阅读大多是领域内的经典著作节选或论文，推荐想要深入学习的同学阅读。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;课程评价&lt;/b&gt;：我们觉得赞！很多人都觉得赞！当然也有人觉得不好。具体如何，大家搜搜CS231n在网络，在知乎上的评价不就好了嘛！&lt;b&gt;个人认为&lt;/b&gt;：入门深度学习的&lt;b&gt;一门良心课&lt;/b&gt;。&lt;b&gt;适合绝大多数&lt;/b&gt;想要学习深度学习知识的人。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程不足&lt;/b&gt;：课程后期从RCNN开始就没有课程笔记。&lt;/p&gt;&lt;h2&gt;课程学习方法&lt;/h2&gt;&lt;p&gt;三句话总结：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;看授课视频形成概念，发现个人感兴趣方向。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;读课程笔记理解细节，夯实工程实现的基础。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;码课程作业实现算法，积累实验技巧与经验。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;引用一下学习金字塔的图，意思大家都懂的：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/77465c8318e6c4d40df274b92602d83f.png" data-rawwidth="519" data-rawheight="423"&gt;&lt;h2&gt;我们的工作&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;完成了CS231n全部9篇课程知识详解笔记的翻译&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/python-numpy-tutorial" data-editable="true" data-title="[python/numpy tutorial]" class=""&gt;[python/numpy tutorial]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" data-editable="true" data-title="Python Numpy教程" class=""&gt;Python Numpy教程&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/classification" data-editable="true" data-title="[image classification notes]" class=""&gt;[image classification notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" data-editable="true" data-title="图像分类笔记（上）" class=""&gt;图像分类笔记（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" data-title="（下）" class="" data-editable="true"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法，内容列表：&lt;ul&gt;&lt;li&gt;图像分类、数据驱动方法和流程&lt;/li&gt;&lt;li&gt;Nearest Neighbor分类器&lt;/li&gt;&lt;ul&gt;&lt;li&gt;k-Nearest Neighbor &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;验证集、交叉验证集和超参数调参&lt;/li&gt;&lt;li&gt;Nearest Neighbor的优劣&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;li&gt;小结：应用kNN实践&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/linear-classify" data-editable="true" data-title="[linear classification notes]"&gt;[linear classification notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：线性分类笔记&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" data-title="（上）" class="" data-editable="true"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" data-editable="true" data-title="（中）"&gt;（中）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是&lt;b&gt;评分函数（score function）&lt;/b&gt;，它是原始图像数据到类别分值的映射。另一个是&lt;b&gt;损失函数（loss function）&lt;/b&gt;，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。内容列表：&lt;ul&gt;&lt;li&gt;线性分类器简介&lt;/li&gt;&lt;li&gt;线性评分函数&lt;/li&gt;&lt;li&gt;阐明线性分类器 &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;ul&gt;&lt;li&gt;多类SVM&lt;/li&gt;&lt;li&gt;Softmax分类器&lt;/li&gt;&lt;li&gt;SVM和Softmax的比较&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;基于Web的可交互线性分类器原型&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/optimization-1" data-editable="true" data-title="[optimization notes]"&gt;[optimization notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：最优化笔记&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" data-editable="true" data-title="（下）" class=""&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了图像分类任务的第三个关键部分：最优化。内容列表如下：&lt;ul&gt;&lt;li&gt;简介&lt;/li&gt;&lt;li&gt;损失函数可视化&lt;/li&gt;&lt;li&gt;最优化&lt;/li&gt;&lt;ul&gt;&lt;li&gt;策略#1：随机搜索&lt;/li&gt;&lt;li&gt;策略#2：随机局部搜索&lt;/li&gt;&lt;li&gt;策略#3：跟随梯度 &lt;i&gt;译者注：上篇截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;梯度计算&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用有限差值进行数值计算&lt;/li&gt;&lt;li&gt;微分计算梯度&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;梯度下降&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/optimization-2" data-editable="true" data-title="[backprop notes]"&gt;[backprop notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" data-editable="true" data-title="反向传播笔记"&gt;反向传播笔记&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记本将帮助读者&lt;b&gt;对反向传播形成直观而专业的理解&lt;/b&gt;。反向传播是利用链式法则递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。内容里列表如下：&lt;ul&gt;&lt;li&gt;简介&lt;/li&gt;&lt;li&gt;简单表达式和理解梯度&lt;/li&gt;&lt;li&gt;复合表达式，链式法则，反向传播&lt;/li&gt;&lt;li&gt;直观理解反向传播&lt;/li&gt;&lt;li&gt;模块：Sigmoid例子&lt;/li&gt;&lt;li&gt;反向传播实践：分段计算&lt;/li&gt;&lt;li&gt;回传流中的模式&lt;/li&gt;&lt;li&gt;用户向量化操作的梯度&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-1/" data-editable="true" data-title="Neural Nets notes 1"&gt;Neural Nets notes 1&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：神经网络笔记1&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" data-editable="true" data-title="（下）" class=""&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了神经网络的建模与结构，内容列表如下：&lt;ul&gt;&lt;li&gt;不用大脑做类比的快速简介&lt;/li&gt;&lt;li&gt;单个神经元建模&lt;ul&gt;&lt;li&gt;生物动机和连接&lt;/li&gt;&lt;li&gt;作为线性分类器的单个神经元&lt;/li&gt;&lt;li&gt;常用的激活函数 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;神经网络结构&lt;ul&gt;&lt;li&gt;层组织&lt;/li&gt;&lt;li&gt;前向传播计算例子&lt;/li&gt;&lt;li&gt;表达能力&lt;/li&gt;&lt;li&gt;设置层的数量和尺寸&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;小节&lt;/li&gt;&lt;li&gt;参考文献&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-2/" data-editable="true" data-title="Neural Nets notes 2"&gt;Neural Nets notes 2&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" data-editable="true" data-title="神经网络笔记2"&gt;神经网络笔记2&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了数据的预处理，正则化和损失函数，内容列表如下：&lt;ul&gt;&lt;li&gt;设置数据和模型&lt;ul&gt;&lt;li&gt;数据预处理&lt;/li&gt;&lt;li&gt;权重初始化&lt;/li&gt;&lt;li&gt;批量归一化（Batch Normalization）&lt;/li&gt;&lt;li&gt;正则化（L2/L1/Maxnorm/Dropout）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-3/" data-editable="true" data-title="Neural Nets notes 3" class=""&gt;Neural Nets notes 3&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：神经网络笔记3&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;该笔记讲解了神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。内容列表如下：&lt;/p&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练集与验证集准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="ConvNet notes" class=""&gt;ConvNet notes&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" data-editable="true" data-title="卷积神经网络笔记"&gt;卷积神经网络笔记&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;用来构建卷积神经网络的各种层&lt;/b&gt;&lt;ul&gt;&lt;li&gt;卷积层&lt;/li&gt;&lt;li&gt;汇聚层&lt;/li&gt;&lt;li&gt;归一化层&lt;/li&gt;&lt;li&gt;全连接层&lt;/li&gt;&lt;li&gt;将全连接层转化成卷积层&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;ul&gt;&lt;li&gt;层的排列规律&lt;/li&gt;&lt;li&gt;层的尺寸设置规律&lt;/li&gt;&lt;li&gt;案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）&lt;/li&gt;&lt;li&gt;计算上的考量&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;完成了3个课程作业页面的翻译&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment1/" data-editable="true" data-title="[Assignment #1]" class=""&gt;[Assignment #1]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="CS231n课程作业#1简介" class=""&gt;CS231n课程作业#1简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：实现k-NN，SVM分类器，Softmax分类器和两层神经网络，实践一个简单的图像分类流程。&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment2/" data-editable="true" data-title="[Assignment #2]"&gt;[Assignment #2]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" data-title="CS231n课程作业#2简介" class="" data-editable="true"&gt;CS231n课程作业#2简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：练习编写反向传播代码，训练神经网络和卷积神经网络。&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment3/" data-editable="true" data-title="[Assignment #3]" class=""&gt;[Assignment #3]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" data-editable="true" data-title="CS231n课程作业#3简介"&gt;CS231n课程作业#3简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。实现DeepDream等有趣应用。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;帮助知友&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;发起了在Youtube上合力翻译课程字幕的倡议&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" data-title="知友智靖远关于CS231n课程字幕翻译的倡议" class="" data-editable="true"&gt;知友智靖远关于CS231n课程字幕翻译的倡议&lt;/a&gt;。当时，&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;已经贡献了他对第一课字幕的翻译，目前这个翻译项目仍在进行中，欢迎各位知友积极参与。具体操作方式在倡议原文中有，请大家点击查看。&lt;/p&gt;&lt;p&gt;有很多知友私信我们，询问为何不做字幕。现在统一答复：&lt;b&gt;请大家积极参加&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;的字幕翻译项目。&lt;/b&gt;他先进行的字幕贡献与翻译，我们&lt;b&gt;不能夺人之美&lt;/b&gt;。&lt;b&gt;后续，我们也会向该翻译项目进行贡献&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;翻译团队&lt;/h2&gt;&lt;p&gt;CS231n课程笔记的翻译，始于&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-editable="true" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt;在一次回答问题“&lt;a href="https://www.zhihu.com/question/41907061" data-editable="true" data-title="应该选择TensorFlow还是Theano？" class=""&gt;应该选择TensorFlow还是Theano？&lt;/a&gt;”中的机缘巧合，在&lt;a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" data-editable="true" data-title="取得了授权"&gt;取得了授权&lt;/a&gt;后申请了知乎专栏&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元 - 知乎专栏"&gt;智能单元 - 知乎专栏&lt;/a&gt;独自翻译。随着翻译的进行，更多的知友参与进来。他们是&lt;a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" data-hash="584f06e4ed2edc6007e4793179e7cdc1" class="member_mention" data-title="@ShiqingFan" data-editable="true" data-hovercard="p$b$584f06e4ed2edc6007e4793179e7cdc1"&gt;@ShiqingFan&lt;/a&gt;，@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;，&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-editable="true" data-title="@李艺颖" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;大家因为认同这件事而聚集在一起&lt;/b&gt;，牺牲了很多个人的时间来进行翻译，校对和润色。而翻译的质量，我们不愿意自我表扬，还是&lt;b&gt;请各位知友自行阅读评价&lt;/b&gt;吧。现在笔记翻译告一段落，下面是&lt;b&gt;团队成员的简短感言&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" data-hash="584f06e4ed2edc6007e4793179e7cdc1" class="member_mention" data-editable="true" data-title="@ShiqingFan" data-hovercard="p$b$584f06e4ed2edc6007e4793179e7cdc1"&gt;@ShiqingFan&lt;/a&gt; ：一个偶然的机会让自己加入到这个翻译小队伍里来。CS231n给予了我知识的源泉和思考的灵感，前期的翻译工作也督促自己快速了学习了这门课程。虽然科研方向是大数据与并行计算，不过因为同时对深度学习比较感兴趣，于是乎现在的工作与两者都紧密相连。Merci!&lt;/p&gt;&lt;p&gt;@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;：在CS231n翻译小组工作的两个多月的时间非常难忘。我向杜客申请加入翻译小组的时候，才刚接触这门课不久，翻译和校对的工作让我对这门课的内容有了更深刻的理解。作为一个机器学习的初学者，我非常荣幸能和翻译小组一起工作并做一点贡献。希望以后能继续和翻译小组一起工作和学习。&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt; ：感谢组内各位成员的辛勤付出，很幸运能够参与这份十分有意义的工作，希望自己的微小工作能够帮助到大家，谢谢！&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-editable="true" data-title="@李艺颖" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt; ：当你真正沉下心来要做一件事情的时候才是学习和提高最好的状态；当你有热情做事时，并不会觉得是在牺牲时间，因为那是有意义并能带给你成就感和充实感的；不需要太过刻意地在乎大牛的巨大光芒，你只需像傻瓜一样坚持下去就好了，也许回头一看，你已前进了很多。就像老杜说的，我们就是每一步慢慢走，怎么就“零星”地把这件事给搞完了呢？&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-editable="true" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt; ：做了一点微小的工作，哈哈。&lt;/p&gt;&lt;h2&gt;未来工作&lt;/h2&gt;&lt;p&gt;目前通过大家的反馈，之后会有新的创作方向，会更多与大家互动，敬请期待吧！&lt;/p&gt;&lt;h2&gt;感谢&lt;/h2&gt;&lt;p&gt;感谢&lt;b&gt;所有给我们的翻译提出过批评指正的知友&lt;/b&gt;，每篇文章末尾处的译者反馈部分我们都列出了大家的具体指正与贡献；&lt;/p&gt;&lt;p&gt;感谢&lt;b&gt;所有给我们的翻译点赞的知友&lt;/b&gt;，你们的赞是我们的精神粮食；&lt;/p&gt;&lt;p&gt;感谢&lt;b&gt;给文章赞赏小钱钱的知友&lt;/b&gt;，谢谢老板们：）&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;&lt;b&gt;恳请大家点赞和分享到其他社交网络上&lt;/b&gt;，让更多&lt;b&gt;想要入门与系统学习深度学习&lt;/b&gt;的小伙伴能够看到这篇总集。同时，也欢迎大家在来专栏分享你的知识，发现志同道合的朋友！&lt;/p&gt;&lt;p&gt;&lt;b&gt;这个世界需要更多的英雄！&lt;/b&gt;&lt;/p&gt;</description><author>杜客</author><pubDate>Thu, 25 Aug 2016 15:47:00 GMT</pubDate></item><item><title>CS231n课程笔记翻译：卷积神经网络笔记</title><link>https://zhuanlan.zhihu.com/p/22038289</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5b83bc8331994f47fedb1459d1424872_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;译者注&lt;/b&gt;：本文翻译自斯坦福CS231n课程笔记&lt;a href="http://cs231n.github.io/convolutional-networks/" data-title="ConvNet notes" class="" data-editable="true"&gt;ConvNet notes&lt;/a&gt;，由课程教师&lt;a href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;授权进行翻译。本篇教程由&lt;a href="https://www.zhihu.com/people/du-ke" data-editable="true" data-title="杜客" class=""&gt;杜客&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/hmonkey" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;翻译完成，&lt;a href="https://www.zhihu.com/people/kun-kun-97-81" class="" data-editable="true" data-title="堃堃"&gt;堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" class="" data-editable="true" data-title="李艺颖"&gt;李艺颖&lt;/a&gt;进行校对修改。&lt;/p&gt;&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;用来构建卷积神经网络的各种层&lt;/b&gt;&lt;ul&gt;&lt;li&gt;卷积层&lt;/li&gt;&lt;li&gt;汇聚层&lt;/li&gt;&lt;li&gt;归一化层&lt;/li&gt;&lt;li&gt;全连接层&lt;/li&gt;&lt;li&gt;将全连接层转化成卷积层&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;ul&gt;&lt;li&gt;层的排列规律&lt;/li&gt;&lt;li&gt;层的尺寸设置规律&lt;/li&gt;&lt;li&gt;案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）&lt;/li&gt;&lt;li&gt;计算上的考量&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;卷积神经网络（CNNs / ConvNets）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。&lt;/p&gt;&lt;p&gt;那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;i&gt;回顾：常规神经网络&lt;/i&gt;。在上一章中，神经网络的输入是一个向量，然后在一系列的&lt;i&gt;隐层&lt;/i&gt;中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。&lt;/p&gt;&lt;p&gt;&lt;i&gt;常规神经网络对于大尺寸图像效果不尽人意&lt;/i&gt;。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。&lt;/p&gt;&lt;p&gt;&lt;i&gt;神经元的三维排列&lt;/i&gt;。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：&lt;b&gt;宽度&lt;/b&gt;、&lt;b&gt;高度&lt;/b&gt;和&lt;b&gt;深度&lt;/b&gt;（这里的&lt;b&gt;深度&lt;/b&gt;指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2ef08bb4cf60805d726b2d6db39dd985.jpg" data-rawwidth="1637" data-rawheight="379"&gt;左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;blockquote&gt;&lt;p&gt;卷积神经网络是由层组成的。每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据。&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;b&gt;用来构建卷积网络的各种层&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：&lt;b&gt;卷积层&lt;/b&gt;，&lt;b&gt;汇聚（Pooling）层&lt;/b&gt;和&lt;b&gt;全连接层&lt;/b&gt;（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。&lt;/p&gt;&lt;p&gt;&lt;i&gt;网络结构例子：&lt;/i&gt;这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。&lt;/li&gt;&lt;li&gt;卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。&lt;/li&gt;&lt;li&gt;ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的&lt;equation&gt;max(0,x)&lt;/equation&gt;作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。&lt;/li&gt;&lt;li&gt;汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。&lt;/li&gt;&lt;li&gt;全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。&lt;/li&gt;&lt;li&gt;卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。&lt;/li&gt;&lt;li&gt;每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。&lt;/li&gt;&lt;li&gt;有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。&lt;/li&gt;&lt;li&gt;有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d9259be829b1cdb3d98a399ebc56defa.jpg" data-rawwidth="1255" data-rawheight="601"&gt;一个卷积神经网络的激活输出例子。左边的输入层存有原始图像像素，右边的输出层存有类别分类评分。在处理流程中的每个激活数据体是铺成一列来展示的。因为对3D数据作图比较困难，我们就把每个数据体切成层，然后铺成一列显示。最后一层装的是针对不同类别的分类得分，这里只显示了得分最高的5个评分值和对应的类别。完整的&lt;a href="http://cs231n.stanford.edu/" data-editable="true" data-title="网页演示"&gt;网页演示&lt;/a&gt;在我们的课程主页。本例中的结构是一个小的VGG网络，VGG网络后面会有讨论。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;现在讲解不同的层，层的超参数和连接情况的细节。&lt;/p&gt;&lt;h4&gt;卷积层&lt;/h4&gt;&lt;p&gt;卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。&lt;/p&gt;&lt;p&gt;&lt;b&gt;概述和直观介绍&lt;/b&gt;：首先讨论的是，再没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。&lt;/p&gt;&lt;p&gt;在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以大脑做比喻&lt;/b&gt;：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;局部连接&lt;/b&gt;：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的&lt;b&gt;感受野（receptive field）&lt;/b&gt;，它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。&lt;/p&gt;&lt;p&gt;&lt;i&gt;例1&lt;/i&gt;：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。&lt;/p&gt;&lt;p&gt;&lt;i&gt;例2&lt;/i&gt;：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ba9dcfa847a71cb695c2653230ea9147.jpg" data-rawwidth="1031" data-rawheight="334"&gt;&lt;p&gt;&lt;b&gt;左边&lt;/b&gt;：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（&lt;b&gt;感受野&lt;/b&gt;相同）。至于深度列的讨论在下文中有。&lt;/p&gt;&lt;p&gt;&lt;b&gt;右边&lt;/b&gt;：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;空间排列&lt;/b&gt;：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：&lt;b&gt;深度（depth），步长（stride）&lt;/b&gt;和&lt;b&gt;零填充（zero-padding）&lt;/b&gt;。下面是对它们的讨论：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为&lt;b&gt;深度列（depth column）&lt;/b&gt;，也有人使用纤维（fibre）来称呼它们。&lt;/li&gt;&lt;li&gt;其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。&lt;/li&gt;&lt;li&gt;在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个&lt;b&gt;零填充（zero-padding）&lt;/b&gt;的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算。（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这里假设输入数组的空间形状是正方形，即高度和宽度相等&lt;/i&gt;）输出数据体的空间尺寸为(W-F +2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/90af0bd67ba498239688c81fd61bbc66.jpg" data-rawwidth="861" data-rawheight="172"&gt;&lt;p&gt;空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。&lt;/p&gt;&lt;p&gt;本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;i&gt;使用零填充&lt;/i&gt;：在上面左边例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般说来，当步长&lt;equation&gt;S=1&lt;/equation&gt;时，零填充的值是&lt;equation&gt;P=(F-1)/2&lt;/equation&gt;，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。&lt;/p&gt;&lt;p&gt;&lt;i&gt;步长的限制&lt;/i&gt;：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸&lt;equation&gt;W=10&lt;/equation&gt;，不使用零填充则&lt;equation&gt;P=0&lt;/equation&gt;，滤波器尺寸&lt;equation&gt;F=3&lt;/equation&gt;，这样步长&lt;equation&gt;S=2&lt;/equation&gt;就行不通，因为&lt;equation&gt;(W-F+2P)/S+1=(10-3+0)/2+1=4.5&lt;/equation&gt;，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。&lt;/p&gt;&lt;p&gt;&lt;i&gt;真实案例&lt;/i&gt;：&lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" data-editable="true" data-title="Krizhevsky"&gt;Krizhevsky&lt;/a&gt;构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸&lt;equation&gt;F=11&lt;/equation&gt;，步长&lt;equation&gt;S=4&lt;/equation&gt;，不使用零填充&lt;equation&gt;P=0&lt;/equation&gt;。因为(227-11)/4+1=55，卷积层的深度&lt;equation&gt;K=96&lt;/equation&gt;，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参数共享&lt;/b&gt;：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。&lt;/p&gt;&lt;p&gt;作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做&lt;b&gt;深度切片（depth slice）&lt;/b&gt;，比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。&lt;/p&gt;&lt;p&gt;注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的&lt;b&gt;卷积&lt;/b&gt;（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为&lt;b&gt;滤波器（filter）&lt;/b&gt;（或&lt;b&gt;卷积核（kernel）&lt;/b&gt;），因为它们和输入进行了卷积。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dd62e1d75bda9b592dabb91627d68aa6.jpg" data-rawwidth="627" data-rawheight="248"&gt;Krizhevsky等学习到的滤波器例子。这96个滤波器的尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为&lt;b&gt;局部连接层&lt;/b&gt;（Locally-Connected Layer）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Numpy例子&lt;/b&gt;：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组&lt;b&gt;X&lt;/b&gt;。那么：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个位于&lt;b&gt;(x,y)&lt;/b&gt;的深度列（或纤维）将会是&lt;b&gt;X[x,y,:]&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;在深度为&lt;b&gt;d&lt;/b&gt;处的深度切片，或激活图应该是&lt;b&gt;X[:,:,d]&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;卷积层例子&lt;/i&gt;：假设输入数据体&lt;b&gt;X&lt;/b&gt;的尺寸&lt;b&gt;X.shape:(11,11,4)&lt;/b&gt;，不使用零填充（&lt;equation&gt;P=0&lt;/equation&gt;），滤波器的尺寸是&lt;equation&gt;F=5&lt;/equation&gt;，步长&lt;equation&gt;S=2&lt;/equation&gt;。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为&lt;b&gt;V&lt;/b&gt;）看起来就是下面这样（在这个例子中，只有部分元素被计算）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在numpy中，&lt;b&gt;*&lt;/b&gt;操作是进行数组间的逐元素相乘。权重向量&lt;b&gt;W0&lt;/b&gt;是该神经元的权重，&lt;b&gt;b0&lt;/b&gt;是其偏差。在这里，&lt;b&gt;W0&lt;/b&gt;被假设尺寸是&lt;b&gt;W0.shape: (5,5,4)&lt;/b&gt;，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 &lt;/b&gt;（在y方向上）&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 &lt;/b&gt;（或两个方向上同时）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们访问的是&lt;b&gt;V&lt;/b&gt;的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是&lt;b&gt;W1&lt;/b&gt;了。在上面的例子中，为了简洁略去了卷积层对于输出数组&lt;b&gt;V&lt;/b&gt;中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结&lt;/b&gt;： 我们总结一下卷积层的性质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入数据体的尺寸为&lt;equation&gt;W_1\times H_1\times D_1&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;4个超参数：&lt;ul&gt;&lt;li&gt;滤波器的数量&lt;equation&gt;K&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;滤波器的空间尺寸&lt;equation&gt;F&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;步长&lt;equation&gt;S&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;零填充数量&lt;equation&gt;P&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;输出数据体的尺寸为&lt;equation&gt;W_2\times H_2\times D_2&lt;/equation&gt; ，其中：&lt;/li&gt;&lt;ul&gt;&lt;equation&gt;W_2=(W_1-F+2P)/S+1&lt;/equation&gt;&lt;li&gt;&lt;equation&gt;H_2=(H_1-F+2P)/S+1&lt;/equation&gt; （宽度和高度的计算方法相同）&lt;/li&gt;&lt;equation&gt;D_2=K&lt;/equation&gt;&lt;/ul&gt;&lt;li&gt;由于参数共享，每个滤波器包含&lt;equation&gt;F\cdot F\cdot D_1&lt;/equation&gt;个权重，卷积层一共有&lt;equation&gt;F\cdot F\cdot D_1\cdot K&lt;/equation&gt;个权重和&lt;equation&gt;K&lt;/equation&gt;个偏置。&lt;/li&gt;&lt;li&gt;在输出数据体中，第&lt;equation&gt;d&lt;/equation&gt;个深度切片（空间尺寸是&lt;equation&gt;W_2\times H_2&lt;/equation&gt;），用第&lt;equation&gt;d&lt;/equation&gt;个滤波器和输入数据进行有效卷积运算的结果（使用步长&lt;equation&gt;S&lt;/equation&gt;），最后在加上第&lt;equation&gt;d&lt;/equation&gt;个偏差。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对这些超参数，常见的设置是&lt;equation&gt;F=3&lt;/equation&gt;，&lt;equation&gt;S=1&lt;/equation&gt;，&lt;equation&gt;P=1&lt;/equation&gt;。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。&lt;/p&gt;&lt;p&gt;卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是&lt;equation&gt;W_1=5,H_1=5,D_1=3&lt;/equation&gt;，卷积层参数&lt;equation&gt;K=2,F=3,S=2,P=1&lt;/equation&gt;。就是说，有2个滤波器，滤波器的尺寸是&lt;equation&gt;3\cdot 3&lt;/equation&gt;，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充&lt;equation&gt;P=1&lt;/equation&gt;，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/333077b83ed421d6bd53eb7a44fd5799.jpg" data-rawwidth="734" data-rawheight="711"&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：请点击图片查看动画演示。如果gif不能正确播放，请读者前往&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="斯坦福课程官网" class=""&gt;斯坦福课程官网&lt;/a&gt;查看此演示。&lt;/i&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;用矩阵乘法实现&lt;/b&gt;：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;输入图像的局部区域被&lt;b&gt;im2col&lt;/b&gt;操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到&lt;i&gt;im2col&lt;/i&gt;操作的输出矩阵&lt;b&gt;X_col&lt;/b&gt;的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。&lt;/li&gt;&lt;li&gt;卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵&lt;b&gt;W_row&lt;/b&gt;，尺寸为[96x363]。&lt;/li&gt;&lt;li&gt;现在卷积的结果和进行一个大矩阵乘&lt;b&gt;np.dot(W_row, X_col)&lt;/b&gt;是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。&lt;/li&gt;&lt;li&gt;结果最后必须被重新变为合理的输出尺寸[55x55x96]。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在&lt;b&gt;X_col&lt;/b&gt;中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的&lt;a href="http://www.netlib.org/blas/" data-editable="true" data-title="BLAS" class=""&gt;BLAS&lt;/a&gt; API）。还有，同样的&lt;i&gt;im2col&lt;/i&gt;思路可以用在汇聚操作中。&lt;/p&gt;&lt;p&gt;反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1x1卷积&lt;/b&gt;：一些论文中使用了1x1的卷积，这个方法最早是在论文&lt;a href="http://arxiv.org/abs/1312.4400" data-editable="true" data-title="Network in Network"&gt;Network in Network&lt;/a&gt;中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;扩张卷积&lt;/b&gt;：最近一个研究（&lt;a href="https://arxiv.org/abs/1511.07122" data-title="Fisher Yu和Vladlen Koltun的论文" class="" data-editable="true"&gt;Fisher Yu和Vladlen Koltun的论文&lt;/a&gt;）给卷积层引入了一个新的叫&lt;i&gt;扩张（dilation）&lt;/i&gt;的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器&lt;b&gt;w&lt;/b&gt;的尺寸是3，那么计算输入&lt;b&gt;x&lt;/b&gt;的方式是：&lt;b&gt;w[0]*x[0] + w[1]*x[1] + w[2]*x[2]&lt;/b&gt;，此时扩张为0。如果扩张为1，那么计算为： &lt;b&gt;w[0]*x[0] + w[1]*x[2] + w[2]*x[4]&lt;/b&gt;。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的&lt;i&gt;有效感受野&lt;/i&gt;是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。&lt;/p&gt;&lt;h4&gt;汇聚层&lt;/h4&gt;&lt;p&gt;通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入数据体尺寸&lt;equation&gt;W_1\cdot H_1\cdot D_1&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;有两个超参数：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;空间大小&lt;equation&gt;F&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;步长&lt;equation&gt;S&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;输出数据体尺寸&lt;equation&gt;W_2\cdot H_2\cdot D_2&lt;/equation&gt;，其中&lt;/li&gt;&lt;equation&gt; W_2=(W_1-F)/S+1&lt;/equation&gt;&lt;equation&gt;H_2=(H_1-F)/S+1&lt;/equation&gt;&lt;equation&gt;D_2=D_1&lt;/equation&gt;&lt;li&gt;因为对输入进行的是固定函数计算，所以没有引入参数&lt;/li&gt;&lt;li&gt;在汇聚层中很少使用零填充&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在实践中，最大汇聚层通常只有两种形式：一种是&lt;equation&gt;F=3,S=2&lt;/equation&gt;，也叫重叠汇聚（overlapping pooling），另一个更常用的是&lt;equation&gt;F=2,S=2&lt;/equation&gt;。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;普通汇聚（General Pooling）&lt;/b&gt;：除了最大汇聚，汇聚单元还可以使用其他的函数，比如&lt;i&gt;平均&lt;/i&gt;汇聚&lt;i&gt;（average pooling）&lt;/i&gt;或&lt;i&gt;L-2范式&lt;/i&gt;汇聚&lt;i&gt;（L2-norm pooling）&lt;/i&gt;。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/641c8846abcb02d35938660cf96cef1b.jpg" data-rawwidth="1349" data-rawheight="406"&gt;汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;反向传播：&lt;/b&gt;回顾一下反向传播的内容，其中&lt;equation&gt;max(x,y)&lt;/equation&gt;函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作&lt;b&gt;道岔（switches）&lt;/b&gt;），这样在反向传播的时候梯度的路由就很高效。&lt;/p&gt;&lt;p&gt;&lt;b&gt;不使用汇聚层&lt;/b&gt;：很多人不喜欢汇聚操作，认为可以不使用它。比如在&lt;a href="http://arxiv.org/abs/1412.6806" data-editable="true" data-title="Striving for Simplicity: The All Convolutional Net" class=""&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt;一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无汇聚层的结构不太可能扮演重要的角色。&lt;/p&gt;&lt;h4&gt;归一化层&lt;/h4&gt;&lt;p&gt;在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于&lt;a href="https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)" data-title="cuda-convnet library API" class="" data-editable="true"&gt;cuda-convnet library API&lt;/a&gt;的讨论。&lt;/p&gt;&lt;h4&gt;全连接层&lt;/h4&gt;&lt;p&gt;在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看&lt;i&gt;神经网络&lt;/i&gt;章节。&lt;/p&gt;&lt;h2&gt;把全连接层转化成卷积层&lt;/h2&gt;&lt;p&gt;全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。&lt;/li&gt;&lt;li&gt;相反，任何全连接层都可以被转化为卷积层。比如，一个&lt;equation&gt;K=4096&lt;/equation&gt;的全连接层，输入数据体的尺寸是&lt;equation&gt;7\times 7\times 512&lt;/equation&gt;，这个全连接层可以被等效地看做一个&lt;equation&gt;F=7,P=0,S=1,K=4096&lt;/equation&gt;的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成&lt;equation&gt;1\times 1\times 4096&lt;/equation&gt;，这个结果就和使用初始的那个全连接层一样了。&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;全连接层转化为卷积层&lt;/b&gt;：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：&lt;ul&gt;&lt;li&gt;针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为&lt;equation&gt;F=7&lt;/equation&gt;，这样输出数据体就为[1x1x4096]了。&lt;/li&gt;&lt;li&gt;针对第二个全连接层，令其滤波器尺寸为&lt;equation&gt;F=1&lt;/equation&gt;，这样输出数据体为[1x1x4096]。&lt;/li&gt;&lt;li&gt;对最后一个全连接层也做类似的，令其&lt;equation&gt;F=1&lt;/equation&gt;，最终输出为[1x1x1000]&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分&lt;/i&gt;），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。&lt;/p&gt;&lt;p&gt;举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解&lt;/i&gt;）&lt;/p&gt;&lt;blockquote&gt;面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。&lt;/blockquote&gt;&lt;p&gt;自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。&lt;/p&gt;&lt;p&gt;最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解&lt;/i&gt;）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb" data-title="Net Surgery" class="" data-editable="true"&gt;Net Surgery&lt;/a&gt;上一个使用Caffe演示如何在进行变换的IPython Note教程。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。&lt;/p&gt;&lt;h4&gt;层的排列规律&lt;/h4&gt;&lt;p&gt;卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：&lt;/p&gt;&lt;p&gt;&lt;b&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其中&lt;b&gt;*&lt;/b&gt;指的是重复次数，&lt;b&gt;POOL?&lt;/b&gt;指的是一个可选的汇聚层。其中&lt;b&gt;N &amp;gt;=0&lt;/b&gt;,通常&lt;b&gt;N&amp;lt;=3&lt;/b&gt;,&lt;b&gt;M&amp;gt;=0&lt;/b&gt;,&lt;b&gt;K&amp;gt;=0&lt;/b&gt;,通常&lt;b&gt;K&amp;lt;3&lt;/b&gt;。例如，下面是一些常见的网络结构规律：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; FC&lt;/b&gt;,实现一个线性分类器，此处&lt;b&gt;N = M = K = 0&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; FC&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; POOL]*2 -&amp;gt; FC -&amp;gt; RELU -&amp;gt; FC&lt;/b&gt;。此处在每个汇聚层之间有一个卷积层。&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; POOL]*3 -&amp;gt; [FC -&amp;gt; RELU]*2 -&amp;gt; FC&lt;/b&gt;。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;几个小滤波器卷积层的组合比一个大滤波器卷积层好&lt;/i&gt;：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有&lt;equation&gt;C&lt;/equation&gt;个通道，那么单独的7x7卷积层将会包含&lt;equation&gt;C\times (7\times 7\times C)=49C^2&lt;/equation&gt;个参数，而3个3x3的卷积层的组合仅有&lt;equation&gt;3\times (C\times (3\times 3\times C))=27C^2&lt;/equation&gt;个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。&lt;/p&gt;&lt;p&gt;最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。&lt;/p&gt;&lt;h4&gt;层的尺寸设置规律&lt;/h4&gt;&lt;p&gt;到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：&lt;/p&gt;&lt;p&gt;&lt;b&gt;输入层&lt;/b&gt;（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。&lt;/p&gt;&lt;p&gt;&lt;b&gt;卷积层&lt;/b&gt;应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长&lt;equation&gt;S=1&lt;/equation&gt;。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当&lt;equation&gt;F=3&lt;/equation&gt;，那就使用&lt;equation&gt;P=1&lt;/equation&gt;来保持输入尺寸。当&lt;equation&gt;F=5,P=2&lt;/equation&gt;，一般对于任意&lt;equation&gt;F&lt;/equation&gt;，当&lt;equation&gt;P=(F-1)/2&lt;/equation&gt;的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。&lt;/p&gt;&lt;p&gt;&lt;b&gt;汇聚层&lt;/b&gt;负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即&lt;equation&gt;F=2&lt;/equation&gt;）的最大值汇聚，步长为2（&lt;equation&gt;S=2&lt;/equation&gt;）。注意这一操作将会报输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。&lt;/p&gt;&lt;p&gt;&lt;i&gt;减少尺寸设置的问题&lt;/i&gt;：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。&lt;/p&gt;&lt;p&gt;&lt;i&gt;为什么在卷积层使用1的步长&lt;/i&gt;？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。&lt;/p&gt;&lt;p&gt;&lt;i&gt;为何使用零填充&lt;/i&gt;？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。&lt;/p&gt;&lt;p&gt;&lt;i&gt;因为内存限制所做的妥协&lt;/i&gt;：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。&lt;/p&gt;&lt;h4&gt;案例学习&lt;/h4&gt;&lt;p&gt;下面是卷积神经网络领域中比较有名的几种结构：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;LeNet&lt;/b&gt;： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" data-editable="true" data-title="LeNet" class=""&gt;LeNet&lt;/a&gt;结构。&lt;/li&gt;&lt;li&gt;&lt;b&gt;AlexNet&lt;/b&gt;：&lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" data-editable="true" data-title="AlexNet" class=""&gt;AlexNet&lt;/a&gt;卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的&lt;a href="http://www.image-net.org/challenges/LSVRC/2014/" data-editable="true" data-title="ImageNet ILSVRC challenge" class=""&gt;ImageNet ILSVRC 竞赛&lt;/a&gt;中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。&lt;/li&gt;&lt;li&gt;&lt;b&gt;ZF Net&lt;/b&gt;：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 &lt;a href="http://arxiv.org/abs/1311.2901" data-editable="true" data-title="ZFNet" class=""&gt;ZFNet&lt;/a&gt;（Zeiler &amp;amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。&lt;/li&gt;&lt;li&gt;&lt;b&gt;GoogLeNet&lt;/b&gt;：ILSVRC 2014的胜利者是谷歌的&lt;a href="http://arxiv.org/abs/1409.4842" data-editable="true" data-title="Szeged等" class=""&gt;Szeged等&lt;/a&gt;实现的卷积神经网络。它主要的贡献就是实现了一个&lt;i&gt;奠基模块&lt;/i&gt;，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是&lt;a href="http://arxiv.org/abs/1602.07261" data-editable="true" data-title="Inception-v4" class=""&gt;Inception-v4&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;VGGNet&lt;/b&gt;：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="VGGNet" class=""&gt;VGGNet&lt;/a&gt;。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="预训练模型"&gt;预训练模型&lt;/a&gt;是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。&lt;/li&gt;&lt;li&gt;&lt;b&gt;ResNet&lt;/b&gt;：&lt;a href="http://arxiv.org/abs/1512.03385" data-editable="true" data-title="残余网络" class=""&gt;残差网络&lt;/a&gt;（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的&lt;i&gt;跳跃链接&lt;/i&gt;，大量使用了&lt;a href="http://arxiv.org/abs/1502.03167" data-title="批量归一化" class="" data-editable="true"&gt;批量归一化&lt;/a&gt;（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="视频"&gt;视频&lt;/a&gt;，&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="PPT"&gt;PPT&lt;/a&gt;），以及一些使用Torch重现网络的&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="实验"&gt;实验&lt;/a&gt;。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文&lt;a href="https://arxiv.org/abs/1603.05027" data-editable="true" data-title="Identity Mappings in Deep Residual Networks"&gt;Identity Mappings in Deep Residual Networks&lt;/a&gt;，2016年3月发表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;VGGNet的细节：&lt;/b&gt;我们进一步对&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="VGGNet"&gt;VGGNet&lt;/a&gt;的细节进行分析学习。整个VGGNet中的卷积层都是以步长为1进行3x3的卷积，使用了1的零填充，汇聚层都是以步长为2进行了2x2的最大值汇聚。可以写出处理过程中每一步数据体尺寸的变化，然后对数据尺寸和整体权重的数量进行查看：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意，大部分的内存和计算时间都被前面的卷积层占用，大部分的参数都用在后面的全连接层，这在卷积神经网络中是比较常见的。在这个例子中，全部参数有140M，但第一个全连接层就包含了100M的参数。&lt;/p&gt;&lt;h2&gt;计算上的考量&lt;/h2&gt;&lt;p&gt;在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。&lt;/li&gt;&lt;li&gt;来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。&lt;/li&gt;&lt;li&gt;卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;和实践相关的拓展资源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/soumith/convnet-benchmarks" data-editable="true" data-title="Soumith benchmarks for CONV performance" class=""&gt;Soumith benchmarks for CONV performance&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" data-editable="true" data-title="ConvNetJS CIFAR-10 demo" class=""&gt;ConvNetJS CIFAR-10 demo&lt;/a&gt; 可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。&lt;/li&gt;&lt;li&gt;&lt;a href="http://caffe.berkeleyvision.org/" data-editable="true" data-title="Caffe" class=""&gt;Caffe&lt;/a&gt;，一个流行的卷积神经网络库。&lt;/li&gt;&lt;li&gt;&lt;a href="http://torch.ch/blog/2016/02/04/resnets.html" data-editable="true" data-title="State of the art ResNets in Torch7" class=""&gt;State of the art ResNets in Torch7&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;卷积神经网络笔记&lt;/b&gt;结束。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;译者反馈&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接，否则保留维权权利&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;各位知友如果发现翻译不当的地方，欢迎通过评论和私信等方式批评指正，我们会在文中补充感谢贡献者；&lt;/li&gt;&lt;li&gt;CS231n的翻译进入尾声，&lt;b&gt;欢迎知友们建议后续的翻译方向&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;本文最初的译稿由杜客和我于五月底完成，为了保证质量，我们两人每人翻译了一个版本，然后结合两个译稿的长处来编辑最终的版本。本文的绝大部分都保留了杜客的版本，在少数段落和语句上采用了我的版本。整个翻译小组在校对工作上付出了很多努力，为译稿提出了近百条修改意见，使得译稿逐渐完善。杜客为了鼓励新手，让此文以我的ID投稿发表。这是我第一次发表文章，非常激动^_^；&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/chen-yi-91-27" class="" data-editable="true" data-title="陈一"&gt;陈一&lt;/a&gt; 和 @&lt;a href="https://www.zhihu.com/people/maxint" class="" data-editable="true" data-title="maxint"&gt;maxint&lt;/a&gt;对细节的指正；&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/zhong-xiao-qi-98" class="" data-editable="true" data-title="钟小祺"&gt;钟小祺&lt;/a&gt; 对翻译细节的建议。&lt;/li&gt;&lt;/ol&gt;</description><author>猴子</author><pubDate>Sat, 20 Aug 2016 12:24:22 GMT</pubDate></item><item><title>「无中生有」计算机视觉探奇</title><link>https://zhuanlan.zhihu.com/p/21341440</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3db4d60c513ffde3c66eb745bbf36f99_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;计算机视觉 (Computer Vision, CV) 是一门研究如何使机器“看”的科学。1963年来自MIT的Larry Roberts发表的该领域第一篇博士论文“&lt;a href="https://dspace.mit.edu/handle/1721.1/11589" class="" data-editable="true" data-title="Machine Perception of Three-Dimensional Solids"&gt;Machine Perception of Three-Dimensional Solids&lt;/a&gt;”，标志着CV作为一门新兴人工智能方向研究的开始。在发展了50多年后的今天，我们就来聊聊最近让计算机视觉拥有「无中生有」能力的几个有趣尝试：1）超分辨率重建；2）图像着色；3）看图说话；4）人像复原；5）图像自动生成。可以看出，这五个尝试层层递进，难度和趣味程度也逐步提升。（注：本文在此只谈视觉问题，不提太过具体的技术细节，若大家对某部分感兴趣，以后再来单独写文章讨论 :)&lt;/p&gt;&lt;h2&gt;超分辨率重建 (Image Super-Resolution)&lt;/h2&gt;&lt;p&gt;去年夏天，一款名为“&lt;a href="http://waifu2x.udp.jp/" class="" data-title="waifu 2x" data-editable="true"&gt;waifu 2x&lt;/a&gt;”的岛国应用在动画和计算机图形学中着实火了一把。waifu 2x借助深度「卷积神经网络」(Convolutional Neural Network, CNN) 可以将图像的分辨率提升2倍，同时还能对图像降噪。简单来说，就是让计算机「无中生有」的填充一些原图中并没有的像素，从而让漫画看起来更清晰真切。大家不妨看看下图，真想童年时候看的就是如此&lt;a href="http://www.bilibili.com/video/av2812282/" class="" data-title="高清的龙珠" data-editable="true"&gt;高清的龙珠&lt;/a&gt;啊！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/702440cec176c72f871d009a8568b3eb.jpg" data-rawwidth="1348" data-rawheight="864"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a50629abf235a9750a72aa65ffc06b5c.jpg" data-rawwidth="1344" data-rawheight="1044"&gt;&lt;p&gt;不过需要指出的是，图像超分辨率的研究始于2009年左右，只是得力于「深度学习」的发展，waifu 2x可以做出更好的效果。在具体训练CNN时，输入图像为原分辨率，

















而对应的超分辨率图像则作为目标，以此构成训练的“图像对”
(image pair)，经过模型训练便可得到超分辨率重建模型。waifu 2x的深度网络原型基于香港中文大学汤晓欧教授团队的工作&lt;a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" class="" data-title="[1]" data-editable="true"&gt;[1]&lt;/a&gt;。有趣的是，[1]中指出可以用传统方法给予深度模型以定性的解释。如下图，低分辨率图像通过CNN的卷积
(convolution) 和池化
(pooling) 操作后可以得到抽象后的特征图 (feature map)。基于低分辨率特征图，同样可以利用卷积和池化实现从低分辨率到高分辨率特征图的非线性映射 (non-linear mapping)。最后的步骤则是利用高分辨率特征图重建高分辨率图像。实际上，所述三个步骤与传统超分辨率重建方法的三个过程是一致的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/be6e82cc9984511924e111fd94684c00.jpg" data-rawwidth="1318" data-rawheight="480"&gt;&lt;h2&gt;图像着色 (Image Colorization)&lt;/h2&gt;&lt;p&gt;顾名思义，图像着色是将原本「没有」颜色的黑白图像进行彩色填充。图像着色同样借助卷积神经网络，输入为黑白和对应彩色图像的image pair，但是仅仅通过对比黑白像素和RGB像素来确定填充的颜色，效果欠佳。因为颜色填充的结果要符合我们的认知习惯，比如，把一条汪星人的毛涂成鲜绿色就会让人觉得很怪异。于是近期，早稻田大学发表在2016年计算机图形学国际顶级会议SIGGRAPH上的一项工作&lt;a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/" class="" data-editable="true" data-title="[2]"&gt;[2]&lt;/a&gt;就在原来深度模型的基础上，加入了「分类网络」来预先确定图像中物体的类别，以此为“依据”再做以颜色填充。下图分别是模型结构图和颜色恢复demo，其恢复效果还是颇为逼真的。另外，此类工作还可用于黑白电影的颜色恢复，操作时只需简单的将视频中每一帧拿出来作colorization即可。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e4dfc9b082694131d25be97bf97b3709.jpg" data-rawwidth="1640" data-rawheight="702"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a710c710a592ca7a6b723328207b280f.jpg" data-rawwidth="1602" data-rawheight="652"&gt;&lt;h2&gt;"看图说话" (Image Caption)&lt;/h2&gt;&lt;p&gt;常说“图文并茂”，文字是除图像外另一种描述世界的方式。

















近期，一项名为“image
caption”的研究逐渐升温起来，其主要任务是通过计算机视觉和机器学习的方法实现对一张图像自动地生成人类自然语言的描述，即“看图说话”。值得一提的是，在今年的CV国际顶会&lt;a href="http://cvpr2016.thecvf.com/" class="" data-editable="true" data-title="CVPR"&gt;CVPR&lt;/a&gt;上，image caption被列为了一个单独的session，其热度可见一斑。一般来讲在image caption中，CNN用来获取图像特征，接着将图像特征作为语言模型&lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="" data-editable="true" data-title="LSTM"&gt;LSTM&lt;/a&gt;（RNN的一种）的输入，整体作为一个「end-to-end」的结构进行联合训练，最终输出对图像的语言描述（见下图）。&lt;/p&gt;&lt;p&gt;目前image caption领域的最好结果&lt;a href="http://arxiv.org/abs/1506.01144" data-editable="true" data-title="[3]"&gt;[3]&lt;/a&gt;来自澳大利亚University of Adelaide的&lt;a href="http://cs.adelaide.edu.au/~chhshen/index.html" class="" data-editable="true" data-title="Chunhua Shen"&gt;Chunhua Shen&lt;/a&gt;教授团队。与之前image caption工作相比，他们的改进与刚才提到的颜色恢复简直有异曲同工之妙，同样是考虑利用图像中物体的类别作为较精准的“依据”来更好的生成自然语言描述，即下图中的红色框框圈起的部分。Image caption的急速发展不仅加速了CV和NLP在AI大领域内的交融，同时也为增强现实应用奠定了更加坚实的技术基础。另外，我们更乐于看到今后日趋成熟的image caption技术嵌入到穿戴式设备上，那一天盲人便可以间接的“看到光明”。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/dffe9d6d4757cece60ec8edd37d20f5a.jpg" data-rawwidth="1518" data-rawheight="1290"&gt;&lt;h2&gt;人像复原 (Sketch Inversion)&lt;/h2&gt;&lt;p&gt;就在六月初，荷兰科学家在&lt;a href="http://arxiv.org/" data-editable="true" data-title="arXiv"&gt;arXiv&lt;/a&gt;上发布了他们的最新研究成果&lt;a href="https://arxiv.org/abs/1606.03073" data-editable="true" data-title="[4]" class=""&gt;[4]&lt;/a&gt;——通过深度网络对人脸轮廓图进行「复原」。如下图所示，

















在模型训练阶段，首先对真实的人脸图像利用传统的线下边缘化方法获得对应人脸的轮廓图，并以原图和轮廓图组成的“图像对”作为深度网络的输入，进行类似超分辨率重建的模型训练。在预测阶段，输入为人脸轮廓（左二sketch），经过卷积神经网络的层层抽象和后续的“还原”可以逐步把相片般的人脸图像复原出来（右一），与最左边的人脸真实图像对比，足够以假乱真。在模型流程图下还另外展示了一些人像复原的结果，左侧一列为真实人像，中间列为画家手工描绘的人脸轮廓图，并以此作为网络输入进行人像复原，最终得到右侧一列的复原结果——目测以后刑侦警察再也不用苦练美术了    😂。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/03fde095d58a761d251f4d97529636ff.jpg" data-rawwidth="1840" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3f6c0451b50f638a9f89fd0823525862.jpg" data-rawwidth="1534" data-rawheight="502"&gt;&lt;h2&gt;图像自动生成&lt;/h2&gt;&lt;p&gt;回顾刚才的四个工作，其实他们的共同点是仍然需要依靠一些“素材”方可「无中生有」，例如“人像复原”还是需要一个轮廓画才可以恢复人像。接下来的这个工作则可以做到由任意一条随机向量生成一张逼近真实场景下的图像。&lt;/p&gt;&lt;p&gt;「无监督学习」可谓计算机视觉的圣杯。最近该方向的一项开创性工作是由Ian Goodfellow和 Yoshua Bengio等提出的「生成对抗网络」&lt;a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" data-title="(Generative Adversarial Nets, GAN)" class="" data-editable="true"&gt;(Generative Adversarial Nets, GAN)&lt;/a&gt;。该工作的灵感来自博弈论中的&lt;a href="https://en.wikipedia.org/wiki/Zero-sum_game" data-title="零和博弈" class="" data-editable="true"&gt;零和博弈&lt;/a&gt;。在二元零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。而GAN中的两位博弈方分别由一个「判别式网络」（图左）和一个「生成式网络」（图右下半部分）充当。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/32913217c610e3197d79685e6a03545a.jpg" data-rawwidth="1626" data-rawheight="1110"&gt;其中，「判别式网络」&lt;equation&gt;D&lt;/equation&gt;的输入为图像&lt;equation&gt;x&lt;/equation&gt;，其作用是判断&lt;equation&gt;x&lt;/equation&gt;是一张真实图像还是一张由计算机生成的图像；「生成式网络」&lt;equation&gt;G&lt;/equation&gt;的输入为一条随机向量&lt;equation&gt;z&lt;/equation&gt;，&lt;equation&gt;z&lt;/equation&gt;可以通过网络“生成”一张合成图像。这张合成图像亦可作为「判别式网络」&lt;equation&gt;D&lt;/equation&gt;的输入，只是此时，在理想情况下&lt;equation&gt;D&lt;/equation&gt;应能判断出它是由计算机生成的。&lt;/p&gt;&lt;p&gt;接下来，GAN中的零和博弈就发生在「判别式网络」和「生成式网络」上：「生成式网络」想方设法的让自己生成的图像逼近真实图像，从而可以“骗过”「判别式网络」；而「判别式网络」也时刻提高警惕，防止「生成式网络」蒙混过关……你来我往，如此迭代下去，颇有点“左右互搏”的意味。GAN整个过程的最终目标是习得一个可以逼近真实数据分布的「生成式网络」，从而掌握整体真实数据的分布情况，因此取名「生成对抗网络」。需要强调的是，GAN不再像传统的监督式深度学习那样需要海量带有类别标记的图像，GAN不需任何图像标记即可训练，也就是进行无监督条件下的深度学习。2016年初，在GAN的基础上，Indico Research和Facebook AI实验室将GAN用深度卷积神经网络进行实现（称作，&lt;a href="https://arxiv.org/abs/1511.06434" data-editable="true" data-title="DCGAN"&gt;DCGAN&lt;/a&gt;, Deep Convolutional GAN），工作发表在国际表示学习重要会议&lt;a href="http://www.iclr.cc/doku.php" data-editable="true" data-title="ICLR 2016"&gt;ICLR 2016&lt;/a&gt;上，并在无监督深度学习模型中取得了当时最好的效果。下图展示了一些由DCGAN生成的"bedroom"图像。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d0b5f3083c1985d8c121e5e787a137d9.jpg" data-rawwidth="1368" data-rawheight="688"&gt;更为interesting的是，DCGAN还可以像&lt;a href="https://en.wikipedia.org/wiki/Word2vec" data-editable="true" data-title="word2vec" class=""&gt;word2vec&lt;/a&gt;一样支持图像“语义”层面的加减。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7babca273fd052cd7a85664cca776edf.jpg" data-rawwidth="1274" data-rawheight="670"&gt;另外，前些天“生成式计算机视觉”研究领域大牛UCLA的Song-Chun Zhu教授团队发布了他们基于生成式卷积网络的最新工作&lt;a href="http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html" class="" data-title="STGConvNet" data-editable="true"&gt;STGConvNet&lt;/a&gt;：不仅可以自动合成动态纹理，同时还可以合成声音，可以说将无监督计算机视觉又向前推进了一大步。（下图是两个demo GIF，左侧是真实动态纹理，右侧是STGConvNet的合成纹理。不动&lt;a href="http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html" data-editable="true" data-title="戳我" class=""&gt;戳我&lt;/a&gt;～）&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/272674a266f12f970a963b28d4da50f6.jpg" data-rawwidth="463" data-rawheight="224"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/080497cf13089e6b1403823527ef5524.jpg" data-rawwidth="463" data-rawheight="224"&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;如今借着「深度学习」的东风，计算机视觉中绝大多数任务的performance都被“刷”上了新高，甚至连“人像复原”，“图像生成”类似「无中生有」的“奇谈”都能以较高质量地实现，着实让人们激动不已。不过尽管如此，事实上距离所谓的颠覆人类的AI“奇点”还相当遥远，并且可以预见，现阶段甚至相当长的一段时间内，计算机视觉或人工智能还不可能做到真正意义上的「无中生有」，即“自我开创”或称为“自我意识”。&lt;/p&gt;&lt;p&gt;然而，也非常庆幸我们可以目睹并且亲身经历这次计算机视觉乃至是整个人工智能的革命浪潮，相信今后一定还会有更多「无中生有」般的奇迹发生。此刻，我们站在浪潮之巅，因此我们兴奋不已、彻夜难眠。&lt;/p&gt;&lt;p&gt;6月18日于南京&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多精彩内容，还请关注&lt;a href="https://www.zhihu.com/people/wei-xiu-shen"&gt;我&lt;/a&gt;的专栏&lt;a href="https://zhuanlan.zhihu.com/furthersight" class="" data-editable="true" data-title="「欲穷千里目」"&gt;「欲穷千里目」&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;（声明：原文版权属于CSDN《程序员》杂志原创，作者为本人，发表于2016年7月刊。欢迎订阅！）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5c3e27c7bd10db36c4db98aeec0f0715.jpg" data-rawwidth="591" data-rawheight="756"&gt;&lt;h2&gt;&lt;b&gt;References:&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;[1] Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. &lt;b&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/b&gt;, &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, Preprint, 2015.&lt;/p&gt;&lt;p&gt;[2] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. &lt;b&gt;Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification&lt;/b&gt;, &lt;i&gt;In Proc. of SIGGRAPH 2016&lt;/i&gt;, to appear.&lt;/p&gt;&lt;p&gt;[3] Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel. &lt;b&gt;What value do explicit high level concepts have in vision to language problems, &lt;/b&gt;&lt;i&gt;In Proc. of CVPR 2016&lt;/i&gt;, to appear.&lt;/p&gt;&lt;p&gt;[4] Yağmur Güçlütürk, Umut Güçlü, Rob van Lier, Marcel A. J. van Gerven. &lt;b&gt;Convolutional Sketch Inversion&lt;/b&gt;, &lt;i&gt;arXiv:1606.03073&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. &lt;b&gt;Generative Adversarial Nets&lt;/b&gt;, &lt;i&gt;In Proc. of NIPS 2014.&lt;/i&gt;&lt;/p&gt;[6] Jianwen Xie, Song-Chun Zhu, Ying Nian Wu. &lt;b&gt;Synthesizing Dynamic Textures and Sounds by Spatial-Temporal Generative ConvNet&lt;/b&gt;, &lt;i&gt;arXiv:1606.00972.&lt;/i&gt;</description><author>魏秀参</author><pubDate>Mon, 04 Jul 2016 20:32:07 GMT</pubDate></item><item><title>智能单元专栏投稿说明</title><link>https://zhuanlan.zhihu.com/p/21917736</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7d2d39eb9ae721a13607713c73193172_r.jpg"&gt;&lt;/p&gt;首先感谢各位知友对于本专栏的支持！发布本说明是因为收到知友的投稿和建议，促使我和&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 重新思考专栏的定位和意义。&lt;h2&gt;来龙去脉&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原本定位&lt;/b&gt;：这个专栏&lt;b&gt;原本只是我们整理记录深度学习笔记和思考的地方&lt;/b&gt;，立足相互促进学习，自产自销，没考虑过有人投稿。由于各位知友抬爱，也收到了一些投稿及意见。&lt;/li&gt;&lt;li&gt;&lt;b&gt;主要疑虑&lt;/b&gt;：接受投稿，就要负起审稿甚至查询是否原创等相关责任，而我们时间有限，刚开始存在多一事不如少一事的心理。自己写的文章，自己把关。面对知友的投稿，我们可能方向不同，或水平有限，也难以做出权威的判断。&lt;/li&gt;&lt;li&gt;&lt;b&gt;主要动力&lt;/b&gt;：知友的支持，增进交流的初心和把专栏做好的小小成就感。最终，我们还是决定开放投稿，并讨论出一些向本专栏投稿的原则性思路。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;重要说明&lt;/b&gt;：我们只是凭借自己的知识背景进行讨论，缺乏更多法律背景上的相关知识，&lt;u&gt;&lt;b&gt;如果知友发现我们的投稿要求有不妥的地方，敬请评论或私信指正&lt;/b&gt;！&lt;/u&gt;我们也&lt;b&gt;会根据大家的反馈持续对下面的投稿要求进行修改&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;向专栏投稿&lt;/h2&gt;&lt;p&gt;&lt;b&gt;投稿原则&lt;/b&gt;：本专栏接受的投稿应满足：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原创&lt;/b&gt;：我们更推崇原创的实践与思考，以及由此而激起的理性探讨交流。不强求首发，如果您想投稿自己以前的文章，也是可以的。&lt;/li&gt;&lt;li&gt;&lt;b&gt;题材&lt;/b&gt;：深度学习相关的技术内容。比如前沿的新技术原创实践与思考，重要的资料翻译，和自己的实践与思考总结等。&lt;/li&gt;&lt;li&gt;&lt;b&gt;排版&lt;/b&gt;：请合理使用知乎的文章的编辑功能，让排版简约大方。&lt;/li&gt;&lt;li&gt;&lt;b&gt;版权&lt;/b&gt;：&lt;b&gt;版权归原作者所有&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;互动&lt;/b&gt;：&lt;b&gt;倡导&lt;/b&gt;投稿者尽可能与读者评论进行优质互动，维护专栏良好的讨论氛围。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;赞赏所得&lt;/b&gt;：本专栏已经开通赞赏功能，&lt;b&gt;赞赏所得归投稿者所有&lt;/b&gt;。具体操作请参考知乎产品专栏中的说明：&lt;a href="https://zhuanlan.zhihu.com/p/21268480?refer=zhihu-product" data-title="知乎专栏文章开始内测" class=""&gt;知乎专栏文章开始内测&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;审稿原则&lt;/b&gt;：我们会在认真阅读投稿后和你进行讨论。如果在投稿题材上不是很有把握，也可以直接私信&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt; 或&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 提前交流。 &lt;/p&gt;&lt;h2&gt;最后的话&lt;/h2&gt;&lt;p&gt;期待更多小伙伴的投稿、交流和拍砖的同时，我和&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 依旧会坚持原创与翻译，保证高质量的输出。&lt;/p&gt;&lt;h2&gt;修正记录&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;经知友建议，增加投稿原则中关于与读者互动的建议性原则。2016.08.22；&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Sun, 14 Aug 2016 21:30:21 GMT</pubDate></item><item><title>斯坦福CS231n课程作业# 3简介</title><link>https://zhuanlan.zhihu.com/p/21946525</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/89758df295a0927d26da59356338a2ff_r.jpg"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，由@&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译自斯坦福CS231n课程作业1介绍页面&lt;a href="http://cs231n.github.io/assignments2016/assignment2/" class="" data-editable="true" data-title="[Assignment #2]"&gt;[Assignment #2]&lt;/a&gt;。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;在本作业中，你将实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。我们还会介绍TinyImageNet数据集，然后在这个数据集使用一个预训练的模型来查看图像梯度的不同应用。本作业的目标如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;理解&lt;i&gt;循环神经网络（RNN）&lt;/i&gt;的结构，知道它们是如何随时间共享权重来对序列进行操作的。&lt;/li&gt;&lt;li&gt;理解普通循环神经网络和长短基记忆（Long-Short Term Memory）循环神经网络之间的差异。&lt;/li&gt;&lt;li&gt;理解在测试时如何从RNN生成序列。&lt;/li&gt;&lt;li&gt;理解如何将卷积神经网络和循环神经网络结合在一起来实现图像标注。&lt;/li&gt;&lt;li&gt;理解一个训练过的卷积神经网络是如何用来从输入图像中计算梯度的。&lt;/li&gt;&lt;li&gt;进行高效的交叉验证并为神经网络结构找到最好的超参数。&lt;/li&gt;&lt;li&gt;实现图像梯度的不同应用，比如显著图，搞笑图像，类别可视化，特征反演和DeepDream。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;安装&lt;/h2&gt;&lt;p&gt;有两种方法来完成作业：在本地使用自己的机器，或者使用&lt;a href="https://link.zhihu.com/?target=http%3A//Terminal.com" class="" data-editable="true" data-title="http://Terminal.com"&gt;http://Terminal.com&lt;/a&gt;的虚拟机。&lt;/p&gt;&lt;h3&gt;云端作业&lt;/h3&gt;&lt;p&gt;Terminal公司为我们的课程创建了一个单独的子域名：&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/" class="" data-editable="true" data-title="www.stanfordterminalcloud.com"&gt;www.stanfordterminalcloud.com&lt;/a&gt;。在该域名下注册。作业2的快照可以在&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/snapshot/49f5a1ea15dc424aec19155b3398784d57c55045435315ce4f8b96b62819ef65" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;找到。如果你注册到了本课程，就可以联系上助教（更多信息请上Piazza）来得到用来做作业的点数。一旦你启动了快照，所有的环境都是为你配置好的，马上就可以开始作业。我们在Terminal上写了一个简明&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/terminal-tutorial/" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;本地作业&lt;/h3&gt;&lt;p&gt;点击&lt;a href="http://cs231n.stanford.edu/winter1516_assignment3.zip" class="" data-editable="true" data-title="此处"&gt;此处&lt;/a&gt;下载代码压缩文件。初次之外还有些库间依赖的配置：&lt;/p&gt;&lt;p&gt;&lt;b&gt;[选项1]使用Anaconda&lt;/b&gt;：推荐方法是安装&lt;a href="https://link.zhihu.com/?target=https%3A//www.continuum.io/downloads" class="" data-editable="true" data-title="Anaconda"&gt;Anaconda&lt;/a&gt;，它是Python的一个发布版，包含了最流行的科研、数学、工程和数据分析Python包。一旦安装了它，下面的提示就都可略过，准备直接开始写作业吧。&lt;i&gt;译者注：推荐。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[选项2]手动安装，虚拟环境&lt;/b&gt;：如果你不想用Anaconda，想要走一个充满风险的手动安装路径，那么可能就要为项目创建一个&lt;a href="https://link.zhihu.com/?target=http%3A//docs.python-guide.org/en/latest/dev/virtualenvs/" class="" data-editable="true" data-title="虚拟环境"&gt;虚拟环境&lt;/a&gt;了。如果你不想用虚拟环境，那么你的确保所有代码需要的依赖关系都是景在你的机器上被安装了。要建立虚拟环境，运行下面代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd assignment3
sudo pip install virtualenv      # This may already be installed
virtualenv .env                  # Create a virtual environment
source .env/bin/activate         # Activate the virtual environment
pip install -r requirements.txt  # Install dependencies
# Work on the assignment for a while ...
deactivate                       # Exit the virtual environment
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;下载数据&lt;/b&gt;：一旦得到作业初始代码，你就需要下载CIFAR-10数据集，然后在assignment1目录下运行下面代码：&lt;i&gt;译者注：也可手动下载解压后放到&lt;/i&gt;&lt;i&gt;cs231n/datasets目录&lt;/i&gt;&lt;i&gt;。&lt;/i&gt;&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd cs231n/datasets 
./get_coco_captioning.sh
./get_tiny_imagenet_a.sh
./get_pretrained_model.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;编译Cython扩展包&lt;/b&gt;：卷积神经网络需要一个高效的实现。我们使用&lt;a href="http://cython.org/" data-editable="true" data-title="Cython" class=""&gt;Cython&lt;/a&gt;实现了一些函数。在运行代码前，你需要编译Cython扩展包。在cs231n目录下，运行下面命令：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;python setup.py build_ext --inplace
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;启用IPython&lt;/b&gt;：得到了CIFAR-10数据集之后，你应该在作业assignment1目录中启用IPython notebook的服务器，如果对IPython notebook不熟悉，可以阅读&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/ipython-tutorial" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意&lt;/b&gt;：如果你是在OSX上的虚拟环境中工作，可能会遇到一个由matplotlib导致的错误，原因在&lt;a href="https://link.zhihu.com/?target=http%3A//matplotlib.org/faq/virtualenv_faq.html" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。你可以通过在assignment2目录中运行start_ipython_osx.sh脚本来解决问题。&lt;/p&gt;&lt;h2&gt;提交作业&lt;/h2&gt;&lt;p&gt;无论你是在云终端还是在本地完成作业，一旦完成作业，就运行collectSubmission.sh脚本；这样将会产生一个assignment3.zip的文件，然后将这个文件上传到你的dropbox中这门课的&lt;a href="https://coursework.stanford.edu/portal/site/W15-CS-231N-01/" class="" data-editable="true" data-title="作业页面"&gt;作业页面&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;Q1：使用普通RNN进行图像标注（40分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;RNN_Captioning.ipynb&lt;/b&gt;将会带你使用普通RNN实现一个在微软COCO数据集上的图像标注系统。&lt;/p&gt;&lt;h3&gt;Q2：使用LSTM进行图像标注（35分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;LSTM_Captioning.ipynb&lt;/b&gt;将会带你实现LSTM，并应用于在微软COCO数据集上进行图像标注。&lt;/p&gt;&lt;h3&gt;Q3：图像梯度：显著图和高效图像（10分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;ImageGradients.ipynb&lt;/b&gt;将会介绍TinyImageNet数据集。你将使用一个训练好的模型在这个数据集上计算梯度，然后将其用于生成显著图和高效图像。&lt;/p&gt;&lt;h3&gt;Q4：图像生成：类别，反演和DeepDream（30分）&lt;/h3&gt;&lt;p&gt;在IPython Notebook文件&lt;b&gt;ImageGeneration.ipynb&lt;/b&gt;中，你将使用一个训练好的TinyImageNet模型来生成图像。具体说来，你将生成类别可视化，实现特征反演和DeepDream。&lt;/p&gt;&lt;h3&gt;Q5：做点儿其他的！（+10分）&lt;/h3&gt;&lt;p&gt;根据作业内容，做点够酷的事儿。比如作业中没有讲过的其他生成图像的方式？&lt;/p&gt;&lt;p&gt;&lt;b&gt;全文完。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;译者反馈：&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;转载须全文转载并注明原文链接，否则保留维权权利；&lt;/li&gt;&lt;li&gt;如对翻译有意见建议，请通过评论批评指正，贡献者均会补充提及；&lt;/li&gt;&lt;li&gt;后续将根据作业内容和自己的学习笔记原创教程；&lt;/li&gt;&lt;li&gt;感谢&lt;a href="https://www.zhihu.com/people/f38193bc2f7f3c10c55042d009b411a5" data-hash="f38193bc2f7f3c10c55042d009b411a5" class="member_mention" data-hovercard="p$b$f38193bc2f7f3c10c55042d009b411a5"&gt;@Frankenstein&lt;/a&gt;的纠错。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Thu, 11 Aug 2016 06:30:36 GMT</pubDate></item></channel></rss>