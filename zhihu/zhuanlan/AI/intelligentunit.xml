<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>斯坦福CS231n官方教程笔记翻译连载。

深度增强学习领域论文和项目的原创思考和Demo复现。

领域内其他感兴趣论文和项目的原创思考解读。</description><lastBuildDate>Fri, 23 Sep 2016 22:15:08 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>看图说话的AI小朋友——图像标注趣谈（下）</title><link>https://zhuanlan.zhihu.com/p/22520434</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7c2ef2d9f4d9244473a201b0b19318ec_r.jpeg"&gt;&lt;/p&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;blockquote&gt;前言：近来&lt;b&gt;图像标注（Image Caption）&lt;/b&gt;问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文通俗有趣，让非专业读者也能一窥其妙。&lt;/blockquote&gt;&lt;h2&gt;内容列表：&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;图像标注问题简介&lt;/li&gt;&lt;ul&gt;&lt;li&gt;图像标注是什么&lt;/li&gt;&lt;li&gt;当前水平&lt;/li&gt;&lt;li&gt;价值和意义&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注数据集&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MSCOCO&lt;/li&gt;&lt;li&gt;Flickr8K和Flickr30K&lt;/li&gt;&lt;li&gt;PASCAL 1K&lt;/li&gt;&lt;li&gt;创建一个守望先锋数据集？&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注评价标准&lt;/li&gt;&lt;ul&gt;&lt;li&gt;人类判断与自动评价标准&lt;/li&gt;&lt;li&gt;Perplexity&lt;/li&gt;&lt;li&gt;BLEU&lt;/li&gt;&lt;li&gt;ROUGE&lt;/li&gt;&lt;li&gt;METEOR&lt;/li&gt;&lt;li&gt;CIDEr &lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注模型发展 &lt;i&gt;&lt;b&gt;注：下篇起始处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;百度的m-RNN&lt;/li&gt;&lt;li&gt;谷歌的NIC&lt;/li&gt;&lt;li&gt;目前最高水平模型&lt;/li&gt;&lt;li&gt;模型的比较思考&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;代码实践&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CS231n的LSTM_Captioning&lt;/li&gt;&lt;li&gt;基于Numpy的NerualTalk&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注问题展望&lt;/li&gt;&lt;ul&gt;&lt;li&gt;模型的更新&lt;/li&gt;&lt;li&gt;自动评价标注的更新&lt;/li&gt;&lt;li&gt;数据集的更新&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;上篇回顾&lt;/h2&gt;&lt;p&gt;在上篇中，我们介绍了研究图像标注问题常用的&lt;b&gt;数据集&lt;/b&gt;和评价算法常用的&lt;b&gt;自动评价标准&lt;/b&gt;，并脑洞了一个&lt;b&gt;开源创建基于守望先锋游戏画面的中文图像标注数据集&lt;/b&gt;的想法。文章发布后，有不少感兴趣的知友表示愿意参与，并提出了意见和建议。对此我将单独分出一篇文章介绍相关情况，请感兴趣的知友注意。在下篇中，我们将对比较有代表性的图像标注方法进行介绍，展示一些代码实践。&lt;/p&gt;&lt;h2&gt;图像标注模型的发展&lt;/h2&gt;&lt;p&gt;说是发展，其实时间也并不长，将CNN和RNN结合的模型用于解决图像标注问题的研究最早也就从2014开始提出，在2015年开始对模型各部分组成上进行更多尝试与优化，到2016年CVPR上成为一个热门的专题。&lt;/p&gt;&lt;p&gt;在这个发展中，将RNN和CNN结合的核心思路没变，变化的是使用了更好更复杂的CNN模型，效果更好的LSTM，图像特征输入到RNN中的方式，以及更复合的特征输入等。正由于其发展时间跨度较短，通过阅读该领域的一些重要文章，可以相对轻松地理出大牛们攻城拔寨的思路脉络，这对我们自己从事研究的思路也会有所启发。&lt;/p&gt;&lt;h2&gt;m-RNN模型&lt;/h2&gt;&lt;p&gt;2014年10月，百度研究院的Junhua Mao和Wei Xu等人在arXiv上发布论文《&lt;a href="http://arxiv.org/abs/1410.1090" data-editable="true" data-title="Explain Images with Multimodal Recurrent Neural Networks"&gt;Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;》，提出了&lt;b&gt;multimodal Recurrent Neural Network（即m-RNN）&lt;/b&gt;模型，创造性地将深度卷积神经网络CNN和深度循环神经网络RNN结合起来，用于解决图像标注和图像和语句检索等问题。通过14年的相关新闻可知，Wei Xu应该是百度研究院的徐伟，Junhua Mao是徐伟团队中的毛俊华，此外还有杨亿，王江等。&lt;/p&gt;&lt;p&gt;这篇论文是首先抓住这个想法并实现的文章，作者们在文中也当仁不让地说：&lt;/p&gt;&lt;blockquote&gt;To the best of our knowledge, this is the first work that incorporates the Recurrent
Neural Network in a deep multimodal architecture.&lt;/blockquote&gt;&lt;p&gt;在后续的几篇优秀论文中，m-RNN都被作为一个基准方法用于比较和超越。因此，&lt;b&gt;首先介绍百度研究院的m-RNN模型，在于其创造性工作。知友切莫遇百度即黑&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文中，在对原始RNN结构进行简要说明后，提出了m-RNN模型如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-32d251cf7486c257c030e3e5eaad93c3.png" data-rawwidth="1566" data-rawheight="352"&gt;其结构特点可以归纳如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型的&lt;b&gt;输入&lt;/b&gt;是图像和与图像对应的标注语句（比如在上图中，这个语句就可能是a man at a giant tree in the jungle）。其&lt;b&gt;输出&lt;/b&gt;是对于下一个单词的可能性的分布；&lt;/li&gt;&lt;li&gt;模型在&lt;b&gt;每个时间帧&lt;/b&gt;都有6层：分别是输入层、2个单词嵌入层，循环层，多模型层和最后的Softmax层；&lt;/li&gt;&lt;li&gt;输入单词本来是以独热码（one-hot）方式编码，但是经过两个单词嵌入层后，最终变换为稠密单词表达。在该文中，单词表达层是随机初始化并在训练过程中自己学习的。第二个嵌入层输出的激活数据，作为输入直接进入到多模型层（蓝色线条）；&lt;/li&gt;&lt;li&gt;循环层的维度是256维，在其中进行的是对&lt;b&gt;t&lt;/b&gt;时刻的单词表达向量&lt;equation&gt;w(t)&lt;/equation&gt;和&lt;b&gt;t-1&lt;/b&gt;时刻的循环层激活数据&lt;equation&gt;r(t-1)&lt;/equation&gt;的变换和计算，具体计算公式是：&lt;equation&gt;r(t)=f_2(U_r\cdot r(t-1)+w(t))&lt;/equation&gt;。其中，函数&lt;equation&gt;f_2(.)&lt;/equation&gt;是&lt;b&gt;ReLU&lt;/b&gt;，这个非线性激活函数在&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" data-editable="true" data-title="本专栏的CS231n笔记系列"&gt;本专栏的CS231n笔记系列&lt;/a&gt;中已经详细介绍过，这里略过。而&lt;equation&gt;U_r&lt;/equation&gt;是为了将&lt;equation&gt;r(t-1)&lt;/equation&gt;映射到和&lt;equation&gt;w(t)&lt;/equation&gt;同样的向量空间中所做的变换；&lt;/li&gt;&lt;li&gt;512维的&lt;b&gt;多模型层&lt;/b&gt;连接着模型的&lt;b&gt;语言部分&lt;/b&gt;和&lt;b&gt;图像部分&lt;/b&gt;。&lt;b&gt;图像部分&lt;/b&gt;就是上图中绿色虚线包围的部分，其本质是利用深度卷积神经网络来提取图像的特征。在该文中，使用的是大名鼎鼎的AlexNet的&lt;b&gt;第七层的激活数据&lt;/b&gt;作为特征数据输入到多模型层，如此就得到了图像特征向量&lt;equation&gt;I&lt;/equation&gt;。而&lt;b&gt;语言部分&lt;/b&gt;就是包含了单词嵌入层和循环层；&lt;/li&gt;&lt;li&gt;多模型层中所做的计算是：&lt;equation&gt;m(t)=g_2(V_w\cdot w(t)+V_r\cdot r(t)+I)&lt;/equation&gt;。其中，&lt;b&gt;m&lt;/b&gt;表示的是多模型层的特征向量，&lt;b&gt;I&lt;/b&gt;表示的是图像部分输入的特征向量，&lt;b&gt;w(t)&lt;/b&gt;和&lt;b&gt;r(t)&lt;/b&gt;的解释同上。至于&lt;equation&gt;V_w&lt;/equation&gt;和&lt;equation&gt;V_r&lt;/equation&gt;，依旧是一个矩阵变换。在这个公式中，&lt;b&gt;需要特！别！注！意！的&lt;/b&gt;是：&lt;b&gt;在每个t时刻，图像特征&lt;equation&gt;I&lt;/equation&gt;都作为输入进入了计算&lt;/b&gt;。这里向大家提问：&lt;b&gt;这样做好不好呢&lt;/b&gt;？先思考一下。后面会给出答案。最后，&lt;equation&gt;g_2(.)&lt;/equation&gt;函数是一个带参数的tanh函数：&lt;equation&gt;g_2(x)=1.7159\cdot tanh(\frac{2}{3}x)&lt;/equation&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该网络在训练的时候，设计的&lt;b&gt;代价函数是基于语句的困惑度（Perplexity）&lt;/b&gt;的。关于困惑度，我们在&lt;a href="https://zhuanlan.zhihu.com/p/22408033?refer=intelligentunit" data-title="上篇中已经介绍" class="" data-editable="true"&gt;上篇中已经介绍&lt;/a&gt;，这里就不重复了。论文设计的代价函数为：&lt;/p&gt;&lt;equation&gt;C=\frac{1}{N}\sum^N_{i=1}L\cdot log_2PPL(w^{(i)}_{1:L}|I^{(i)})+||\theta||^2_2&lt;/equation&gt;&lt;p&gt;其中N是训练集中单词的数量，&lt;equation&gt;\theta&lt;/equation&gt;是模型的参数。所以&lt;equation&gt;||\theta||^2_2&lt;/equation&gt;实际上是一个正则化部分。而L是单词序列的长度。&lt;b&gt;训练的目标&lt;/b&gt;就是最小化代价函数值。可以看见，上述代价函数是可导的，由此就可以用反向传播来求梯度，而后用随机梯度下降方法来学习参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型的语句生成&lt;/b&gt;：模型从一个特殊的开始符号“##START##”或者任意个参考单词（这里的意思是，作者们可以输入参考语句中的前K个单词作为开始）开始，然后模型开始计算下一个单词的概率分布&lt;equation&gt;P(w|w_{1:n-1}|I)&lt;/equation&gt;。然后取概率最大的一个单词作为选取的单词，同时再把这个单词作为输入，预测下一个单词，循环往复，直到生成结束符号##END##。&lt;/p&gt;&lt;p&gt;&lt;b&gt;实验数据集和标注&lt;/b&gt;：该论文发表较早，使用的数据集有我们在上篇中介绍的&lt;b&gt;Flickr8K和30K&lt;/b&gt;，也有我们没有介绍的&lt;b&gt;IAPR TC-12&lt;/b&gt;。使用的自动评价标准也较少，有&lt;b&gt;Perplexity，BLUE1-3，没有BLUE4，其余评价都没有。&lt;/b&gt;与该方法对比的，也是一些相对传统的方法。因此在这里，就对其实验结果略过了，感兴趣的知友可以自行阅读论文。&lt;/p&gt;&lt;p&gt;&lt;b&gt;综上&lt;/b&gt;：该论文的主要贡献就是提出了将RNN和CNN结合起来的模型。模型中有一些设计在后续中被证明不是良好的设计，后续的论文在这个模型的基础上逐渐优化。&lt;/p&gt;&lt;h2&gt;NIC模型&lt;/h2&gt;&lt;p&gt;2014年11月，谷歌的Vinyals等人发布了论文《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》，推出了&lt;b&gt;NIC（Neural Image Caption）模型&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;相较于百度的m-RNN模型，NIC模型的主要不同点在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;抛弃RNN，使用了&lt;b&gt;LSTM&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;CNN部分使用了一个比AlexNet&lt;b&gt;更好的卷积神经网络&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;CNN提取的&lt;b&gt;图像特征数据只在开始输入一次&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;而从论文角度来看，该论文使用的图像标注数据集较为丰富，有Pascal VOC 2008，Flickr8K和30K，MSCOCO，SBU。其采用的自动评价标准也较为齐全，有BLEU-1，BLEU-4，METEOR和CIDEr。同时，就像我在上篇中提到的那样，论文还用人工方法客观地对NIC模型生成的标注语句进行了分级评价，展示了得分和实际效果之间的距离。下面我们主要对NIC模型本身进行一些讲解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;NIC模型结构&lt;/b&gt;如下图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a5051f70a403c8284dae4fede8131bba.png" data-rawwidth="1056" data-rawheight="586"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;图像特征部分&lt;/b&gt;是换汤不换药：我们可以看见，图像经过卷积神经网络，最终还是变成了特征数据（就是特征向量）出来了。唯一的不同就是这次试用的CNN不一样了，取得第几层的激活数据不一样了，归根结底，出来的还是特征向量；&lt;/li&gt;&lt;li&gt;&lt;b&gt;但是！&lt;/b&gt;图像特征只在刚开始的时候输入了LSTM，后续没有输入，这点和m-RNN模型是不同的！&lt;/li&gt;&lt;li&gt;&lt;b&gt;单词输入部分&lt;/b&gt;还是老思路：和m-RNN模型一样，每个单词采取了独热（one-hot）编码，用来表示单词的是一个维度是词汇表数量的向量。向量和矩阵&lt;equation&gt;W_e&lt;/equation&gt;相乘后，作为输入进入到LSTM中。&lt;/li&gt;&lt;li&gt;&lt;b&gt;使用LSTM来替换了RNN&lt;/b&gt;。LSTM是什么东西呢，简单地来说，可以把它看成是效果更好RNN吧。为什么效果更好呢？因为它的公式更复杂哈哈😝（并不是）。如果知友对LSTM的细节感兴趣，想要理解LSTM。&lt;b&gt;建议观看CS231n的视频课程第10课：Recurrent Neural Networks, Image Captioning, LSTM&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上模型所示的流程，可以用下列公式来概括：&lt;/p&gt;&lt;equation&gt;x_{-1}=CNN(I)&lt;/equation&gt;&lt;equation&gt;x_t=W_eS_t,\quad t\in\{0...N-1\}&lt;/equation&gt;&lt;equation&gt;p_{t+1}=LSTM(x_t),\quad t\in\{0...N-1\}&lt;/equation&gt;&lt;p&gt;那么，为什么在NIC模型中，只在第一次输入图像特征数据，而不是每次都输入了呢？论文中说：&lt;/p&gt;&lt;blockquote&gt;We empirically verified
that feeding the image at each time step as an extra input
yields inferior results, as the network can explicitly exploit
noise in the image and overfits more easily.我们从实践经验上证实如果在每一个时间点都输入图像数据，将会导致较差的结果。网络可能会放大图像数据中的噪音，并且更容易过拟合。&lt;/blockquote&gt;&lt;p&gt;后续的论文中，基本上都是采取在初始时输入一次图像特征数据，不再使用m-RNN每次都输入的方法了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型的训练&lt;/b&gt;：NIC模型的损失函数和m-RNN模型却有不同，但基本思路还是一样的：一个可求导的损失函数，利用反向传播来求梯度，然后利用随机梯度下降来学习到最优的参数。其损失函数为：&lt;/p&gt;&lt;equation&gt;L(I,S)=-\sum^N_{t=1}logp_t(s_t)&lt;/equation&gt;&lt;p&gt;&lt;b&gt;实验结果&lt;/b&gt;：经过了以上这些改进后，NIC模型比起m-RNN模型还是有了较大进步：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a51e0c551ffce8b739942a3fb6a822a9.png" data-rawwidth="1100" data-rawheight="526"&gt;上图是不同算法在不同数据集上的BLEU-1得分的比较。可以看到NIC比起m-RNN还是有较大的进步的。当然，该论文值得深入学习的地方还有很多，但是本文主要聚焦于图像标注方面，对论文中其他任务（如图像检索等）的结果，在这里就不过多介绍了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;综上&lt;/b&gt;：NIC模型相较于m-RNN模型，其重要的改进在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，在语言模型部分将RNN替换为了实践证明在NLP方面效果更好的&lt;b&gt;LSTM&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;其次，在图像模型部分使用了效果&lt;b&gt;更好的卷积神经网络模型&lt;/b&gt;来做图像特征数据的提取。&lt;/li&gt;&lt;li&gt;最后，改变了图像特征数据的输入方式，从m-RNN的每个时间点都输入变成了&lt;b&gt;只在初始时输入1次&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;目前最高水平模型&lt;/h2&gt;&lt;p&gt;在介绍完前面两个模型后，仍然有一些论文继续做出了更好水平的方法，但是没有选择介绍是因为他们的思路其实和NIC模型相较于m-RNN模型做出的改进思路是雷同的：更好的卷积神经网络模型，更好的语言模型，不同的图像输入方式，不同的单词嵌入方式等等。&lt;/p&gt;&lt;p&gt;那么为什么要选择这个模型呢？大家会说：当然咯，因为这是目前最好的嘛。其实并不完全是这个原因。关于如何看待论文，不久之前我看到了清华大学的&lt;a href="https://www.zhihu.com/people/45e79aed2c8b69623a57b4889414afe0" data-hash="45e79aed2c8b69623a57b4889414afe0" class="member_mention" data-editable="true" data-title="@肖寒" data-hovercard="p$b$45e79aed2c8b69623a57b4889414afe0"&gt;@肖寒&lt;/a&gt;博士在&lt;a href="https://www.zhihu.com/question/50508148/answer/121343083?from=profile_answer_card" data-editable="true" data-title="某个问题"&gt;某个问题&lt;/a&gt;下的回答，&lt;b&gt;个人认为说得非常好&lt;/b&gt;，这里&lt;b&gt;强力推荐&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;不过，一般注水的作者相对而言都是新手，因为比较有经验的研究者都知道：&lt;b&gt;&lt;i&gt;“论文的一切都在于贡献，不在于结果”&lt;/i&gt;&lt;/b&gt;你的结果只是一个说明你贡献的例证，多那么点少那么点，&lt;b&gt;大家看了毫无区别&lt;/b&gt;。你注水除了恶心我们这些后来实验的人，就没什么别的用处了。有那些&lt;b&gt;疯狂调参和使劲弄技巧&lt;/b&gt;的时间，真不如&lt;b&gt;拿来整理好你自己的思路，把论文的论述过程做到有理有据&lt;/b&gt;！&lt;i&gt;因为 80.2 和 80.3 正常人都没法记住其间区别，但你&lt;b&gt;循循善诱的精致论述会让所有人印象深刻&lt;/b&gt;。我希望新手不要本末倒置！&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;所以，选择《&lt;a href="http://arxiv.org/abs/1506.01144" data-editable="true" data-title="What Value Do Explicit High Level Concepts Have in Vision to Language Problems?"&gt;What Value Do Explicit High Level Concepts Have in Vision to Language Problems?&lt;/a&gt;》这篇论文中的模型来讲，不仅仅是因为它效果好，还因为它的贡献：通过实验回答了论文题目本身提出的这个问题：&lt;b&gt;在视觉到语言问题（比如图像标注）中，明确的高等级概念到底有没有价值？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个问题一旦我们对于现在流行的CNN+RNN模型比较熟练了，就会自然而然地产生疑问：话说这图像特征也不知道是啥，反正我卷积神经网络几个层一过，变成了激活数据，变成了一堆浮点数构成的向量，然后就往RNN初始状态里面一丢，诶，效果还可以。但是&lt;b&gt;为啥呢？！&lt;/b&gt;为啥效果会不错呢？这明明就是一堆说不清楚的特征啊啊！图像的信息并没有用更高级的语义信息表达，就这么稀里糊涂的扔进去了。&lt;/p&gt;&lt;p&gt;该论文在摘要中就一针见血地指明了这个问题：&lt;/p&gt;&lt;blockquote&gt;Much recent progress in Vision-to-Language (V2L) prob-
lems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Net-
works (RNNs). This approach &lt;b&gt;does not explicitly represent
high-level semantic concepts, but rather seeks to progress
directly from image features to text&lt;/b&gt;. &lt;/blockquote&gt;这种直接把用CNN提取的图像特征数据扔进RNN的方法&lt;b&gt;寻求的是从图像特征直接到文本，而不是先将其用更高等级的语义概念进行表达&lt;/b&gt;。那么面对这个情况，作者们做了什么（也就是贡献）呢？&lt;blockquote&gt;In this paper we&lt;b&gt; investigate&lt;/b&gt; whether this direct approach succeeds due to, or
despite, the fact that it &lt;b&gt;avoids the explicit representation of
high-level information&lt;/b&gt;. We propose a method of &lt;b&gt;incorporating high-level concepts into the successful CNN-RNN approach&lt;/b&gt;, and show that it achieves a &lt;b&gt;significant improvement&lt;/b&gt;
on the state-of-the-art in both image captioning and visual
question answering. &lt;/blockquote&gt;&lt;p&gt;作者们说，我们就来调查一下，当前流行的这个方法它成功，到底是不是因为它就是避免了将图像信息表达为高等级的语义信息呢？于是作者们在当前的CNN+RNN模型中，增加了一个高等级的语义概念表达，结果发现这么一改，结果很好，出现了很大的提升。这就说明，&lt;b&gt;之前稀里糊涂地把图像特征直接扔进RNN并不是一个好办法，将图像特征用高等级的语义概念表达后再输入RNN会更好&lt;/b&gt;！&lt;/p&gt;&lt;p&gt;这篇论文解答了我同样存在的疑惑，并且通过改进和实验证明，我们存在疑惑的地方是可以有所作为的，改进后的方法有了较大提升。这就是我选择这篇论文的最主要原因。总之，看完摘要我就非常高兴，迫不及待地就开始跳进去想看看人家到底是怎么来做的了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型结构&lt;/b&gt;：如下图所示，需要注意的特点有：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-40533b56d6cf861e3d96396cd15baba0.png" data-rawwidth="1122" data-rawheight="796"&gt;&lt;ul&gt;&lt;li&gt;在语言模型部分使用的是LSTM，这一点和之前的模型&lt;b&gt;没有太大区别&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;针对3各不同的任务（图像标注、单个单词问答，语句问答）分别实际了3个语言模型部分，这里我们&lt;b&gt;只关注第一个图像标注任务&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;改进重在视觉部分&lt;/b&gt;：请知友们往上看看之前的m-RNN和NIC模型，在他们的视觉部分，图像的处理是相对简单的：图像输入CNN，然后从CNN靠后的层中取出激活数据，输入到RNN即可。然而在这里，我们看到情况变复杂了。&lt;/li&gt;&lt;li&gt;首先预训练一个的单标签的CNN（蓝色虚线中），然后把该CNN的参数迁移到下方多标签的CNN中（红色虚线中），并对多标签的CNN做精细调整（fine-tune）。&lt;/li&gt;&lt;li&gt;图像输入到红色虚线中的CNN，输出的是一个&lt;b&gt;有高等级语义概念和对应概率的向量&lt;/b&gt;，并将这个向量作为语言部分LSTM的输入。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，&lt;b&gt;输入LSTM的不是一个不知道到底是什么的浮点数向量了，而是我们可以理解的语义概念的概率的向量&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文介绍模型的时候说：我们的模型还是由图像分析和语句生成两个部分构成。&lt;b&gt;在图像分析部分，&lt;/b&gt;我们使用有监督学习来预测一个属性的集合，这些属性实际上就是图像的标注语句中常见的单词。这一步是如何做到的呢？我们把这一步&lt;b&gt;看做是一个多标签分类问题&lt;/b&gt;，训练了一个对应的深度卷积神经网络来实现。&lt;/p&gt;&lt;p&gt;图像经过模型的图像分析部分，输出的就是&lt;equation&gt;V_{att}(I)&lt;/equation&gt;，它是一个向量，其长度等于标签集合中标签的数量（也就是词汇表的数量），&lt;b&gt;每个维度上装的是某个标签对应的预测概率&lt;/b&gt;。然后这个&lt;equation&gt;V_{att}(I)&lt;/equation&gt;就要作为输入进入到LSTM，也就是语言生成部分了。&lt;/p&gt;&lt;p&gt;在针对图像标注问题的语言模型部分，该论文中简明扼要地说，我们就是按照《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》论文中的方法来进行语句生成的，喏，就上面的NIC模型，所以这里也就不更多逼逼啦。&lt;/p&gt;&lt;p&gt;&lt;b&gt;属性预测部分&lt;/b&gt;：该论文，我个人感到最有价值的部分，还是在它的图像分析部分中&lt;b&gt;如何从图像到属性的实现，这是它的核心创新点，&lt;/b&gt;所以对该部分做一个比较细节的介绍。需要注意的要点有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;属性词汇表的构建&lt;/b&gt;：语义属性是从训练集标注语句中提取出来的，可以是句子中的任何部分：物体名称（名词），动作（动词）或者性质（形容词）。使用了&lt;equation&gt;c&lt;/equation&gt;个最常用的单词来构建属性词汇表。在构建的时候，对复数和时态不区分，比如ride和riding，bag和bags被看做一个单词。这样就有效地缩小了词汇表数量，最后得到一个包含256个单词的属性词汇表；&lt;/li&gt;&lt;li&gt;&lt;b&gt;属性预测器的实现&lt;/b&gt;：有了词汇表，就希望给出一张图片，能够得到多个对应的在词汇表中的属性单词。将这个需求，&lt;b&gt;看做是一个多标签分类问题来解决&lt;/b&gt;。具体怎么做呢？如下图所示：&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9c5eee8af0ab6d2abd8e1bc0de6d20c5.png" data-rawwidth="1212" data-rawheight="676"&gt;&lt;ul&gt;&lt;li&gt;首先拿一个用ImageNet&lt;b&gt;预训练好的VGGNet模型作为初始模型&lt;/b&gt;。然后再用MS COCO这样的有&lt;b&gt;多标签的数据集来对这个VGGNet做精细调整&lt;/b&gt;（fine-tune）。精细调整具体怎么做呢？就是将最后一个全连接层的输出输入到c分类的softmax中。c=256代表的是词汇表的数量。然后使用逐元素的逻辑回归作为损失函数：&lt;/li&gt;&lt;li&gt;&lt;b&gt;损失函数&lt;/b&gt;：假设有&lt;equation&gt;N&lt;/equation&gt;个训练样例，&lt;equation&gt;y_i=[y_{i1},y_{i2},...,y_{ic}]&lt;/equation&gt;是第i个图像对应的标签向量，如果&lt;equation&gt;y_{ij}&lt;/equation&gt;=1，表示图像中有该标签，反之则没有。&lt;equation&gt;p_i=[p_{i1},p_{i2},...,p_{ic}]&lt;/equation&gt;是对应的预测概率向量，则损失函数为：&lt;equation&gt;J=\frac{1}{N}\sum^N_{i=1}\sum^c_{j=1}log(1+exp(-y_{ij}p_{ij}))&lt;/equation&gt;。在精细调整的训练过程中只需要最小化这个损失函数值即可；&lt;/li&gt;&lt;li&gt;然后对于一张输入的图像，要将其分割成不同的局部。刚开始的时候是计划分割出上百个局部窗口，后来感到计算起来太耗费时间，就采取了归一化剪枝的算法将所有的方框分从m个簇，然后每个簇中保留最好的k个方框，最后加上原图，得到m*k+1个建议方框，将对应的局部输入到网络中。在使用中，作者令m=10，k=5；&lt;/li&gt;&lt;li&gt;对于局部方框的生成，作者们使用的方法是Multiscale Combinatorial Grouping (MCG)方法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实验结果：通过将图像信息表达为高等级的语义信息输入LSTM，该方法得到了一个比较显著的性能提升：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-505bc9a994886250e68e9f22e67a4fb3.png" data-rawwidth="1220" data-rawheight="736"&gt;这张表是论文方法在MSCOCO数据集上和其他很多方法，以及自己设定的基准模型之间的得分比较。注意除了困惑度（P）外，得分都是越高越好。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-5c67701be4a5dae18ab97658c661ca7d.png" data-rawwidth="1192" data-rawheight="546"&gt;这张表示论文方法在MSCOCO的5个标注和40个标注语句测试集上与如m-RNN等方法和人类得分的比较，注意论文方法在14个对比中，有13个的得分都是超过人类得分，拿到最高。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意&lt;/b&gt;：就如同我在上篇中所说，自动评价标准得分高于人类的分，并不代表实际标注语句就比人类标注语句水平高。&lt;b&gt;该论文没有如同谷歌NIC模型论文中一样，设置人工对生成的标注语句的分级评价，是一大遗憾&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;模型的比较思考&lt;/h2&gt;至此，3个模型及其论文介绍完毕。原本还有LRCN模型和斯坦福的NeuralTalk模型，后将其精简掉了。从这三个模型的进化我们可以看到一个比较清晰的脉络：一个创新之后，对这个创新中的局部进行优化，对局部之间的协作方式进行优化，对创新中说得不清晰或者不合理的部分敢于反思并探索，往往大的提升就在这些模糊的区域中了。&lt;h2&gt;代码实践&lt;/h2&gt;&lt;p&gt;&lt;b&gt;光说不练假把式&lt;/b&gt;。机器学习本来就是一个实践性很强的领域，工程能力是非常重要的一环。因此在我们专栏里面，比较推崇的就是知行合一的协作理念。在这个小节，会对CS231n课程的第三个大作业中的RNN图像标注作业部分内容进行解析，并简要介绍开源的NeuralTalk项目。&lt;/p&gt;&lt;h2&gt;CS231n #A3 RNN_Captioning&lt;/h2&gt;&lt;p&gt;首先，需要说一下的关于CS231n的几个课程作业，个人都非常推荐。希望入门深度学习的同学如能完成，则入门扎实了。我后续也会在专栏进行相关的解析。作业3相关情况请看我们的介绍：&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏" class=""&gt;斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏&lt;/a&gt;。由于本文主要是介绍图像标注问题，所以作业相关背景就不多说了。&lt;/p&gt;&lt;p&gt;完成RNN_Captioning作业首先需要运行jupyter notebook，然后打开&lt;b&gt;RNN_Captioning.ipynb&lt;/b&gt;文件。然后就可以看到整个作业文件是由Markdown文字说明块（Cell）和python代码块组成的，基本上是手把手教你完成该实验，在某些代码块之前，实验要求你要实现某些核心函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码块1-3：&lt;/b&gt;这3个代码块都不需要我们做什么，他们依次是在进行一些文件导入，初始化设置，导入MS COCO数据，显示数据集中的某些图像和标注语句。在代码块-1的下方，有对MSCOCO数据集的介绍：&lt;/p&gt;&lt;blockquote&gt;在本练习中，我们将使用微软COCO数据集2014版，该数据集已经成为图像标注的标准数据库。数据集包含80000张训练图像，40000张验证图像，每张图都有5个描述句子，句子是利用亚马逊的土耳其机器人招募人工做的。要下载数据集，到cs231n/datasets目录中运行get_coco_captioning.sh脚本。我们已经为你对数据进行了预处理并从中提取出了特征。使用在ImageNet上预训练的VGG-16网络，我们从网络的 fc7层提取出了所有图像的特征。这些特征被分别存储在train2014_vgg16_fc7.h5 和val2014_vgg16_fc7.h5两个文件中。为了减少处理时间和内存需求，还使用PCA将维度从4096减少到了512，这些数据存在train2014_vgg16_fc7_pca.h5val201h和4_vgg16_fc7_pca.h5l两个文件中。原始图像有20G，所以没有包含在这次的下载中。然而所有的图像都是从Flickr中获取，训练和验证图像的url都存在train2014_urls.txt和val2014_urls.txt，这样你就可以通过网络下载图像了。处理字符串是很低效的，所以练习中使用的是编码版的标注。每个单词都分配了一个整数ID，这样就能用数字序列来表示标注语句了。单词和ID之间的映射在文件coco2014_vocab.json中。你可以使用cs231n/coco_utils.py中的decode_captions来将装着整数ID的numpy数组转化为字符串。我们向字符表中加入了一些特殊的标记，在每个标注的开头加入&amp;lt;START&amp;gt; 结尾加入&amp;lt;END&amp;gt;，很少见的单词用 &amp;lt;UNK&amp;gt;替换。还有，因为小批量数据中的标注句子长度不同，所有在短的句子结束 &amp;lt;END&amp;gt; 后后面加上了 &amp;lt;NULL&amp;gt;标记，并且对 &amp;lt;NULL&amp;gt;标记不计算损失值和梯度。因为处理起来有点痛苦，所以我们已经帮你搞定了这些特殊标记的实现细节。使用load_coco_datah函数将所有的COCO数据进行加载。&lt;/blockquote&gt;&lt;p&gt;从上面的说明中我们可以知道几个要点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;助教在设计实验的时候，已经将图像输入到卷积神经网络中，提取出了特征数据并将其文件化，我们可以直接用了；&lt;/li&gt;&lt;li&gt;单词都是用整数ID来表示的。打开json文件我们可以看到：&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-487ddc0cafce573205084ef7218d1329.png" data-rawwidth="604" data-rawheight="276"&gt;&lt;ul&gt;&lt;li&gt;一些特殊的符号用来表示句子的开始和结束，以及不常见的单词。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来，我们需要开始在作业文件夹中的&lt;b&gt;cs231n/rnn_layers.py&lt;/b&gt;文件中实现一些核心函数&lt;b&gt;rnn_step_forward&lt;/b&gt;，不然代码块4运行是会报错的。rnn_step_forward函数实现的就是RNN模型一个时间戳的前向传播。我们先来看看代码块4。&lt;/p&gt;&lt;p&gt;代码块4：其实就是在&lt;b&gt;检验rnn_step_forward函数有没有正确实现&lt;/b&gt;。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;N, D, H = 3, 10, 4

x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)
prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)
Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)
Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)
b = np.linspace(-0.2, 0.4, num=H)

next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)
expected_next_h = np.asarray([
  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],
  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],
  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])

print 'next_h error: ', rel_error(expected_next_h, next_h)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在让我们用sublime text打开cs231n/rnn_layers.py文件找到rnn_step_forward函数：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rnn_step_forward(x, prev_h, Wx, Wh, b):
  """
  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh
  activation function.

  The input data has dimension D, the hidden state has dimension H, and we use
  a minibatch size of N.

  Inputs:
  - x: Input data for this timestep, of shape (N, D).
  - prev_h: Hidden state from previous timestep, of shape (N, H)
  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
  - b: Biases of shape (H,)

  Returns a tuple of:
  - next_h: Next hidden state, of shape (N, H)
  - cache: Tuple of values needed for the backward pass.
  """
  next_h, cache = None, None
  #######################################################################
  # TODO: Implement a single forward step for the vanilla RNN. Store the next  #
  # hidden state and any values you need for the backward pass in the next_h   #
  # and cache variables respectively.                                          
#######################################################################
# implemente the function   #######################################################################
 #                          END OF YOUR CODE                      #######################################################################
  return next_h, cache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;回顾课程，该函数主要要实现的就是RNN模型的下面计算：&lt;/p&gt;&lt;equation&gt;h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)&lt;/equation&gt;&lt;p&gt;在上述函数定义中，x就是该时间点的输入。prev_h就是RNN上一个隐藏状态，即&lt;equation&gt;h_{t-1}&lt;/equation&gt;。Wx对应的就是&lt;equation&gt;W_{xh}&lt;/equation&gt;，Wh对应的就是&lt;equation&gt;W_{hh}&lt;/equation&gt;，b是偏置量。于是实现如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# stage computation
  # Step 1. mul1: Wh(H, H) dot prev_h(N, H) -&amp;gt; (N, H)
  mul1 = np.dot(prev_h, Wh)

  # Step 2. mul2: Wx(D, H) dot x(N, D) -&amp;gt; (N, H)
  mul2 = np.dot(x, Wx)

  # Step 3. add1: mul1 + mul2 -&amp;gt; (N, H)
  add1 = mul1 + mul2

  # Step 4. add2: add1(N, H) + b(H,) Broadcasting -&amp;gt; (N, H)
  add2 = add1 + b

  # Step 5. tanhed: apply tanh to add2 -&amp;gt; (N, H)
  tanhed = np.tanh(add2)
  next_h = tanhed

  # cache
  cache = (mul1, mul2, add1, add2, tanhed, x, Wx, Wh, prev_h.copy()) 
  # .copy is important!!!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;大家看到这段实现可能会很奇怪，明明一行代码就能实现的事情，&lt;b&gt;为什么分成这么多步，还用了这么多中间变量&lt;/b&gt;？实际上，这种分段式的实现，是为了能够方便实现反向传播。为了说明这一点，接下来展示一下实现该步骤的反向传播函数rnn_step_backward。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rnn_step_backward(dnext_h, cache):
  """
  Backward pass for a single timestep of a vanilla RNN.
  
  Inputs:
  - dnext_h: Gradient of loss with respect to next hidden state
  - cache: Cache object from the forward pass
  
  Returns a tuple of:
  - dx: Gradients of input data, of shape (N, D)
  - dprev_h: Gradients of previous hidden state, of shape (N, H)
  - dWx: Gradients of input-to-hidden weights, of shape (N, H) ? maybe wrong -&amp;gt; (D, H)
  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
  - db: Gradients of bias vector, of shape (H,)
  """
  dx, dprev_h, dWx, dWh, db = None, None, None, None, None
  #######################################################################
  # TODO: Implement the backward pass for a single step of a vanilla RNN.      #
  # HINT: For the tanh function, you can compute the local derivative in terms of the output value from tanh. 
#######################################################################
# implemente the function  
#######################################################################
  #                      END OF YOUR CODE                    
#######################################################################
  return dx, dprev_h, dWx, dWh, db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我对该函数的实现如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# get the cache
  mul1, mul2, add1, add2, tanhed, x, Wx, Wh, prev_h = cache

  # backward pass
  # Back to step 5: backprop through tanh. dnext_h(N, H) taned(N, H)
  # d/dx (tanh x)^2 = 1 - (tanh x)^2
  dadd2 = (1.0 - tanhed * tanhed) * dnext_h

  # Back to step 4: z=x+y -&amp;gt; dz/dx(writted in dx) = 1, dz/dy(writted in dy) = 1
  dadd1 = 1.0 * dadd2 # -&amp;gt; (N, H)
  db = 1.0 * np.sum(dadd2, axis=0) # since db shape:(H,)

  # Back to step 3: z=x+y -&amp;gt; dz/dx(writted in dx) = 1, dz/dy(writted in dy) = 1
  dmul1 = 1.0 * dadd1 # -&amp;gt; (N, H)
  dmul2 = 1.0 * dadd1 # -&amp;gt; (N, H)

  # Back to step 2: x * y = z -&amp;gt; dx = y, dy = x
  # dWx = x * dmul2. x(N, D) dmul2(N, H) dWx should be (D, H)
  dWx = np.dot(x.T, dmul2)
  # dx = Wx * dmul2. Wx(D, H), dmul2(N, H) -&amp;gt; (N, D)
  dx = np.dot(dmul2, Wx.T)

  # Back to step 1: x * y = z -&amp;gt; dx = y, dy = x
  # dWh = prev_h * dmul1. prev_h(N, H), dmul1(N, H) -&amp;gt; (H, H)
  # which H is row ?
  dWh = np.dot(prev_h.T, dmul1)
  # dprev_h = Wh * dmul1. Wh(H, H), dmul1(N, H) -&amp;gt; (N, H)
  dprev_h = np.dot(dmul1, Wh.T)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看见，这个反向传播求梯度和刚才前向传播的分步是一致的。&lt;/p&gt;&lt;p&gt;以上，就是CS231n作业3中用简单RNN来实现图像标注实验中RNN前向步进的实现，只是整个作业的一小部分，鉴于这个作业较长，且后续我会连载CS231n作业解析，所以就不全部讲解了。总体说来，完成作业后对于学习者对深度学习的理解，是很有帮助的。&lt;/p&gt;&lt;h2&gt;NeuralTalk项目&lt;/h2&gt;&lt;p&gt;在斯坦福计算机视觉实验室的论文《&lt;a href="http://arxiv.org/abs/1412.2306" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions" class=""&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;》中，作者&lt;a href="http://cs.stanford.edu/people/karpathy/" data-editable="true" data-title="Andrej Karpathy" class=""&gt;Andrej Karpathy&lt;/a&gt;给出了实验数据、代码和介绍：&lt;a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions"&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;。现在去看页面，发现最初的基于Numpy的NerualTalk项目已经被废弃停止维护了，新的基于Torch的NerualTalk2已经发布。&lt;/p&gt;&lt;p&gt;鉴于&lt;a href="http://cs.stanford.edu/people/karpathy/" data-editable="true" data-title="Andrej Karpathy" class=""&gt;Andrej Karpathy&lt;/a&gt;已经从斯坦福计算机视觉实验室毕业，去了OpenAI，开始搞深度增强学习和tensorflow，所以这个项目是否会继续维护下去，还不得而知。&lt;/p&gt;&lt;p&gt;对于完成了CS231n作业的同学，建议可以先从老的基于Numpy的&lt;a href="https://github.com/karpathy/neuraltalk" data-editable="true" data-title="NerualTalk项目"&gt;NerualTalk项目&lt;/a&gt;入手学习。因为实际上作者自己也说：&lt;/p&gt;&lt;blockquote&gt; I am leaving it on Github for educational purposes.&lt;/blockquote&gt;&lt;p&gt;等对于老的NerualTalk项目比较熟悉了，可以考虑去阅读基于Torch的NerualTalk2，或者自己使用tensorflow来进行实现。&lt;/p&gt;&lt;h2&gt;图像标注问题展望&lt;/h2&gt;写到这里，其实我个人关于图像标注问题的看法已经比较清晰地包含在前面的文中了。想要更好地解决图像标注问题，需要：&lt;ul&gt;&lt;li&gt;更好的自动评价标准。这里个更好，是指的能够和人类评价相关性更高；&lt;/li&gt;&lt;li&gt;更大的数据集。图像更多，图像对应标注句子更多；&lt;/li&gt;&lt;li&gt;在图像分析部分，语言生成部分，或者两个部分的连接方式上出现新的模型或思路。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总而言之，我个人对于图像标注问题比较乐观，在前面图像分类问题的深厚基础上，图像标注问题应该能够在1-2年内拿出一个接近或者达到人类标注水平的方法。&lt;/p&gt;&lt;h2&gt;作者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;欢迎对文中不妥之处批评指正；&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22520434&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Thu, 22 Sep 2016 16:40:31 GMT</pubDate></item><item><title>最前沿：深度增强学习再发力，家用机器人已近在眼前</title><link>https://zhuanlan.zhihu.com/p/22523121</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpeg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;如果问你“家用机器人真正实用化还需要多久？”我说的是那种可以端茶倒水，帮你干这干那的机器人，可能你的回答是10年，15年。但是现在，深度增强学习的不断发展，很可能将时间缩短到5年。&lt;/p&gt;&lt;p&gt;2016年9月16号，&lt;b&gt;Li Feifei&lt;/b&gt;组（了解ImageNet的知友们肯定都熟悉）放出了最新的Paper：&lt;/p&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/1609.05143" data-title="Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning " class="" data-editable="true"&gt;Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning &lt;/a&gt;&lt;/p&gt;&lt;p&gt;做了什么事呢？使用深度增强学习实现目标驱动的视觉导航。说的简单一点就是机器人找东西，有一个地面机器人，让机器人去找一本书，或者去冰箱，机器人就自己去了，然后能找到物体停下。大家先看一下官方的视频：&lt;/p&gt;&lt;p&gt;&lt;video id="69269" data-swfurl="" poster="" data-sourceurl="http://v.youku.com/v_show/id_XMTczMTM5Mzk4OA==.html" data-name='Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learni—在线播放—优酷网，视频高清在线观看" &amp;gt;&amp;lt;/a&amp;gt;&amp;lt;meta name="irTitle" conte'&gt;&lt;/video&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;实现机器人找东西有多困难呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;very hard! 下文我们会说一下传统机器人学的方法。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;采用深度增强学习实现又有多大的意义？&lt;/b&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;先说结论。简单的说就是以前为了实现机器人找东西这个事情，我们需要做大量的工作，大量的hand-engineering，但是现在，采用深度增强学习，我们只需要使用神经网络。就是机器人只根据实时看到的画面还有目标选择动作，这和人类的行为很像。并且整个过程都是学习来的。这种方法具有革命性的意义。&lt;/p&gt;&lt;p&gt;还记得之前那篇文章吗？&lt;a href="https://zhuanlan.zhihu.com/p/21470871?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！ - 智能单元 - 知乎专栏"&gt;最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！ - 智能单元 - 知乎专栏&lt;/a&gt; 那篇Paper作者研究如何通过迁移学习将不同场景学到的知识移植过来。特别是从虚拟到现实的迁移。但是，这篇文章想了个更简单的做法-------我们造一个非常仿真的环境不就完了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-92b55a9f3ef3e25426edcba6ab5ef6ff.png" data-rawwidth="1014" data-rawheight="662"&gt;&lt;p&gt;因此，这篇文章中，作者构建了一个非常好的仿真环境（如上图），并且通过在高度仿真的环境中训练，然后迁移到真实场景中。这种方法被证明是有效的。那么想象一下，如果构建了一个更复杂更真实的场景，然后让机器人在里面无限次的训练学习，掌握技能之后，再移植到现实世界。这将是game-changing的事情。让家用机器人能够端茶倒水将不再那么困难。&lt;/p&gt;&lt;p&gt;下面，我们来好好分析一下这篇文章的意义。最重要的是深度增强学习的意义。&lt;/p&gt;&lt;h2&gt;2 机器人找东西，传统的方法怎么做？&lt;/h2&gt;&lt;p&gt;假设我们面前有一台轮式机器人，有摄像头，可控制，上面还有一个机械臂。我们希望能够跟机器人说去帮我拿一个杯子过来。然后机器人能够自主的去厨房找到一个杯子，用机械臂拿起来，最后送到我手上。那么为了实现这么一个任务，我们首先需要实现的就是去找到杯子这个任务。我们再进一步假设房间里的东西都是静止的，然后要求机器人能够去找房间中的各种物品。&lt;/p&gt;&lt;p&gt;OK，在这种假设下，传统的机器人学告诉我们要怎么实现这个系统呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 1：建图 Mapping和定位Localization也就是SLAM&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-05310424cd5a51deeb9a97d6484a8f1c.jpeg" data-rawwidth="448" data-rawheight="324"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-e08704b6dc925b575c2f1fe15e92be84.png" data-rawwidth="1024" data-rawheight="619"&gt;&lt;p&gt;要想找东西，总得先知道自己在哪，还要知道房间的结构好规划路线。因此，我们得先利用机器人上面的传感器比如摄像头、雷达等构建整个场景，因为要找东西，有的高，有的低，所以最好还是3d场景。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 2：构建语义地图Semantic Map&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7b91f9c057908d1b0f311266978921d2.jpeg" data-rawwidth="600" data-rawheight="424"&gt;&lt;p&gt;要找东西，当然要知道东西在哪里了。因此，这一步需要我们在上一步得到的地图上添加语义，也就是各种东西的位置信息。或者反过来也可以给3d地图里面的每个东西贴标签，这是冰箱，我就贴上冰箱的标志，那是门，那我就贴上门的标签。大家可以看到，这需要计算机视觉的物体检测（Object Detection）技术，比如我们可以使用YOLO或者Fast R-CNN算法在SLAM的同时进行物体检测，为物体贴上标签。比如上图所示，我们知道桌子，椅子，柜子的具体位置。做完这一步，我们不但知道整个房屋的构造，机器人自己的位置，也知道每一个物体的相对位置。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 3：路径规划（Path Planning）&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-6d2c266ed5d12dfd50c77b13bce14fb7.png" data-rawwidth="938" data-rawheight="526"&gt;&lt;p&gt;有了地图和定位，再加上目标位置，ok，万事俱备了。我们只要根据这个设计出一条最佳的路径，然后让机器人走就行了。这就是路径规划需要干的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结一下&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在传统的机器人学中，要实现机器人找东西这件事，需要完成上面三大任务。真是太复杂。单单SLAM都是一个很大的研究课题了。而这中间需要的计算量，需要的人力工程，都是非常多的。那么，这篇Paper是怎么做的呢？&lt;/p&gt;&lt;h2&gt;3 深度增强学习是怎么做的？&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-062f4429783928c286e33d62ddd488d4.png" data-rawwidth="1350" data-rawheight="700"&gt;深度增强学习的思路就完全不是机器人学的思路了，而是人的思路。上一节我们说又要建图，又要路径规划的。问题是：我们人需要这么做吗？不得不承认，我们人具备自动在大脑中构建3d场景的能力，但是对于找东西我们并不需要这么做。我们人就是大致知道东西的位置，然后往那个方向上走就行了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因此，深度增强学习要仅使用2D的视觉信息来完成找东西的任务！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上图所示，机器人找东西的任务变成：看到图像，选择行走动作，直到找到想要的物体，停下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这篇文章的创新点在哪呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;就是将目标图像作为输入！老实说这个创新点非常简单，也蛮低级的，而且仅针对这种目标能够具化的任务有效。但是对于机器人找东西而言，等于和人一样给个目标图像，然后让你去找到它。这样做的好处就是&lt;b&gt;神经网络具有通用性。&lt;/b&gt;之前，如果只用当前看到的图像作为输入，那么每找一个东西，都要单独训练一个网络。现在，把目标图像作为输入进行训练，那么就可以使这个网络不管输入什么目标都ok，也就具备了通用性。&lt;/p&gt;&lt;p&gt;这篇文章采用了&lt;b&gt;&lt;a href="https://arxiv.org/abs/1602.01783" data-title="A3C算法" class=""&gt;A3C算法&lt;/a&gt;&lt;/b&gt;，也就是Deepmind提出的当前深度增强学习最强的算法来训练，主要就是神经网络的模型变了，还有多线程训练的方式做了一点改变（也就是同时开启多个Agent训练多个场景多个任务）。甚至，还根据不同场景搞不同的神经网络来输出，这算不算通用我得打个问号。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-dd93e3c95dcb22ec636baf710e73d287.png" data-rawwidth="1356" data-rawheight="1014"&gt;为了让观察的图像输入和目标图像输入能够和在一起，为两个图像采用了相同的预训练好的神经网络（使用ImageNet训练的深度残差网络）。&lt;/p&gt;&lt;p&gt;要理解整个训练过程，需要熟悉A3C算法，鉴于篇幅，这里就不细讲了。本专栏将陆续发布更多文章介绍深度增强学习的各种算法。&lt;/p&gt;&lt;p&gt;对于这个成果我们应该思考一个问题是：&lt;b&gt;神经网络是如何让机器人找东西的？&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4 深度神经网络通过学习学到了什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;它并没有记住物体的位置，更不知道房屋的结构。但它记住了在每一个位置，通向各个物体的行为方法。&lt;/p&gt;&lt;p&gt;比如说，冰箱在左前方5米，那么机器人在这个位置的时候根据看到的图像做出向左前方走的动作。它只是知道要这么走，它知道这个轨迹，但是并不知道其他。&lt;/p&gt;&lt;p&gt;反过来想，传统的机器人学方法可以通过记录轨迹来控制吗？&lt;/p&gt;&lt;p&gt;当然是可以，但是同样的，SLAM和语义地图还有路径规划都是避开不了的工程。而采用深度增强学习，我们是让机器人通过不断试错的方法来找到这条路径，而不是计算。&lt;/p&gt;&lt;h2&gt;5 小结与展望&lt;/h2&gt;&lt;p&gt;深度增强学习确实是一种非常模仿人类行为的思路。在真实环境中，我们确实很难让机器人通过试错来找到正确的行为方式，但是如果我们能够构建非常逼真的仿真环境的话，我们就很有希望在仿真环境中训练机器人，然后迁移到真实场景中。这样的话，让机器人通过自学习来掌握各种技能就真不是说说而已了。我怀疑DeepMind已经在构建非常好的仿真环境训练他们的机器人，也许DeepMind的下一个重量级工作就是深度增强学习的机器人了！&lt;/p&gt;&lt;h2&gt;声明：本文为原创文章，未经作者允许不得转载！&lt;/h2&gt;&lt;p&gt;本文图片来源于Paper及网络。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22523121&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Wed, 21 Sep 2016 16:09:25 GMT</pubDate></item><item><title>看图说话的AI小朋友——图像标注趣谈（上）</title><link>https://zhuanlan.zhihu.com/p/22408033</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7c2ef2d9f4d9244473a201b0b19318ec_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;blockquote&gt;&lt;b&gt;前言&lt;/b&gt;：近来&lt;b&gt;图像标注（Image Caption）&lt;/b&gt;问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文&lt;b&gt;通俗有趣&lt;/b&gt;，&lt;b&gt;让非专业读者也能一窥其妙&lt;/b&gt;。文中还提出了&lt;b&gt;开源构建一个基于守望先锋的中文图像标注数据集的构想&lt;/b&gt;，欢迎知友讨论参与。&lt;/blockquote&gt;&lt;h2&gt;内容列表：&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;图像标注问题简介&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;图像标注是什么&lt;/li&gt;&lt;li&gt;当前水平&lt;/li&gt;&lt;li&gt;价值和意义&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注数据集&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MSCOCO&lt;/li&gt;&lt;li&gt;Flickr8K和Flickr30K&lt;/li&gt;&lt;li&gt;PASCAL 1K&lt;/li&gt;&lt;li&gt;&lt;b&gt;创建一个守望先锋数据集&lt;/b&gt;？&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注评价标准&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;人类判断与自动评价标准&lt;/li&gt;&lt;li&gt;Perplexity&lt;/li&gt;&lt;li&gt;BLEU&lt;/li&gt;&lt;li&gt;ROUGE&lt;/li&gt;&lt;li&gt;METEOR&lt;/li&gt;&lt;li&gt;CIDEr &lt;b&gt;&lt;i&gt;注：上篇截止处&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注模型发展&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;百度的m-RNN&lt;/li&gt;&lt;li&gt;谷歌的NIC&lt;/li&gt;&lt;li&gt;斯坦福的NeuralTalk&lt;/li&gt;&lt;li&gt;目前的State of art&lt;/li&gt;&lt;li&gt;对几个模型的比较&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;代码实践&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CS231n的LSTM_Captioning&lt;/li&gt;&lt;li&gt;基于Numpy的NerualTalk&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注问题展望&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;模型图像部分、语言部分和连接方式上的更新&lt;/li&gt;&lt;li&gt;自动评价标注的更新&lt;/li&gt;&lt;li&gt;数据集的更新&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;问题简介&lt;/h2&gt;&lt;p&gt;图像标注问题其本质是视觉到语言（Visual-to-Language，即V2L）的问题，解释起来很简单，就是四个字：&lt;b&gt;看图说话&lt;/b&gt;。就像老师要求小朋友们在看图说话作业中完成的任务一样，我们也希望算法能够&lt;b&gt;根据图像给出能够描述图像内容的自然语言语句&lt;/b&gt;。然而这种对于人类实在是小事一桩的小儿科级任务，在计算机视觉领域却不能不说是一个挑战：因为图像标注问题需要在两种不同形式的信息（图像信息到文本信息）之间进行“翻译”。&lt;/p&gt;&lt;p&gt;随着深度学习领域的发展，一种将深度卷积神经网络（Deep Convolutional Neural Network）和循环神经网络（Recurrent Neural Network）结合起来的方法在图像标注问题上取得了显著的进步。由于该方法的成功，使得基于该方法的对图像标注问题研究迅速地火热起来，在2016年的&lt;a href="http://cvpr2016.thecvf.com" data-title="IEEE国际计算机视觉与模式识别会议" class="" data-editable="true"&gt;IEEE国际计算机视觉与模式识别会议&lt;/a&gt;（即IEEE Conference on Computer Vision and Pattern Recognition，缩写为&lt;b&gt;CVPR&lt;/b&gt;）上专门有一个小型会议（session）的主题就是图像标注。&lt;/p&gt;&lt;p&gt;然而在“人工智能领域取得进展”这个问题上，显然大众和专业研究者们敏感程度是大不相同的，吐槽如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d2195427d901f96541ef96446961233f.jpg" data-rawwidth="990" data-rawheight="300"&gt;&lt;b&gt;研究者&lt;/b&gt;：看我们根据图像生成了描述语句咯，不是以前的标签咯，厉不厉害？！~\(≧▽≦)/~&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4906d07619a0b44d3f7fe726207df965.jpg" data-rawwidth="400" data-rawheight="297"&gt;&lt;b&gt;热心AI的大众&lt;/b&gt;：&lt;b&gt;→_→&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e29fa599072b844a5cb31f542d90a3e9.jpg" data-rawwidth="600" data-rawheight="312"&gt;&lt;b&gt;暴雪&lt;/b&gt;：在“智能危机”结束之后，一群被放逐的&lt;b&gt;智能机器人&lt;/b&gt;感受到了被其称为“灵魂觉醒”的升华之道。他们在&lt;b&gt;冥思其存在本质和意义多年&lt;/b&gt;之后，渐渐相信他们不止是人工智能而已，和人类一样，他们&lt;b&gt;也有灵魂&lt;/b&gt;。——摘自&lt;a href="goog_1712963362" data-editable="true" data-title="守望先锋官网英雄"&gt;守望先锋官网英雄&lt;/a&gt;&lt;a href="http://ow.blizzard.cn/heroes/zenyatta" data-editable="true" data-title="禅雅塔简介"&gt;禅雅塔简介&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/aec9b9d222cfd8d2881c3df70db50455.jpg" data-rawwidth="400" data-rawheight="300"&gt;&lt;b&gt;热心AI的大众&lt;/b&gt;：和尚放大了！&lt;b&gt;恁死对面的源氏&lt;/b&gt;！世界需要更多的英雄！&lt;/p&gt;&lt;p&gt;之所以有这种反差，是大众对于人工智能认识的起点，几乎是人工智能研究者奋斗的终极目标：）&lt;/p&gt;&lt;p&gt;回到图像标注问题，那么现在该领域最厉害的方法，到底达到了什么样的水平？我的看法是：&lt;b&gt;貌似接近人类，其实差距尚大&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;为什么说貌似接近人类呢？这里引用几篇论文中表格：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/899efcd8d9b02c2c7f7c7bd5b1038468.png" data-rawwidth="782" data-rawheight="374"&gt;上面这个表格来自于目前图像标注领域结果最好的论文：《&lt;a href="http://arxiv.org/abs/1506.01144" data-title="What Value Do Explicit High Level Concepts Havein Vision to Language Problems?" class="" data-editable="true"&gt;What Value Do Explicit High Level Concepts Havein Vision to Language Problems?&lt;/a&gt;》，作者们来自澳大利亚Adelaide大学的计算机学院。上面表格是论文中作者们展示了自己的算法和其他算法在微软的COCO数据库的测试集上取得的测试结果。其中，5-Refs和40-Refs表示的是测试集中有两个数据集，一个数据集每张图像有5个参考标注（也就是人类输入的正确语句），一个数据集每张图像有40个参考标注。&lt;/p&gt;&lt;p&gt;B-N（N=1，2，3，4），M，R和CIDEr代表的是4中不同的对于算法的自动评价标准，后文会详细介绍，这里只需要&lt;b&gt;知道得分越高越好&lt;/b&gt;。参与比较的有包含论文方法在内的4种图像标注算法和人类水平。可以看见在&lt;b&gt;14个得分中，论文方法有13个得分都超过了人类得分&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;那么是不是论文方法就已经超越人类水平了呢？在我看来，答案是否定的&lt;/b&gt;。至于为什么，原因将在后文关于&lt;b&gt;图像标注算法评价标准的小节&lt;/b&gt;中揭晓。&lt;/p&gt;&lt;p&gt;虽说我个人认为最高水平的算法也尚未超过人类表现，但是其进展是毋庸置疑的，各种算法已经能够让人类评价者觉得非常不错的图像标注了。比如在上表中提到的LRCN模型，就能根据图像生成这样的描述：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/fc97868a916e972e0d36baefb8fa9df5.png" data-rawwidth="1580" data-rawheight="454"&gt;大家看了是不是觉得已经非常惊艳了？不要太激动，依旧存在一些惨不忍睹的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4c01fe630c5e0c43fab8f461add7c2e5.png" data-rawwidth="1424" data-rawheight="396"&gt;这些错误的图像标注案例是来自斯坦福视觉实验室的论文《&lt;a href="http://arxiv.org/abs/1412.2306" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions" class=""&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;》，第一作者是Andrej Karpathy，学习了斯坦福深度学习课程CS231n 2016的同学对作为讲师的他应该很熟悉了。我们可以看见最右边的图像标注语句尤其离谱，“在路中间站着一匹马”，大兄弟，感情这马🐴是透明的咯？&lt;/p&gt;&lt;p&gt;图像标注问题如果能够得到很好的解决，那么价值是显而易见的，可以应用到&lt;b&gt;图像检索，儿童教育和视力受损人士的生活辅助等方面&lt;/b&gt;。而从学术的角度来看，当前图像标注问题的研究，促使人工智能领域的两大领域，计算机视觉和自然语言处理很好地结合，这种跨子领域的结合能够催生出更让人惊艳的方法吗？我个人表示很期待。&lt;/p&gt;&lt;h2&gt;图像标注数据集&lt;/h2&gt;&lt;p&gt;到目前为止，深度学习依旧是一种需要大量数据来进行驱动的方法。小样本学习尚未有突破性的进展，所以数据对于基于深度学习的算法依旧非常重要。在图像标准问题研究的过程中，研究者们对于基准数据库的选择偏好也在发生变化，一些数据集运用的越来越广泛，而一些数据集则越来越少地被使用。本小节将基于图像标注问题，对这些数据集做简要的介绍和对比。&lt;/p&gt;&lt;h2&gt;Microsoft COCO Caption数据集&lt;/h2&gt;&lt;p&gt;Microsoft COCO Caption数据集的推出，是建立在Microsoft Common Objects in COntext
(COCO)数据集的工作基础上的。在论文《&lt;a href="http://arxiv.org/abs/1504.00325" data-editable="true" data-title="Microsoft COCO Captions: Data Collection and Evaluation Server"&gt;Microsoft COCO Captions: Data Collection and Evaluation Server&lt;/a&gt;》中，作者们详细介绍了他们基于MS COCO数据集构建MS COCO Caption数据集的工作。&lt;/p&gt;&lt;p&gt;简要地来说，就是对于原COCO数据集中约330,000张图像，使用亚马逊公司的“&lt;b&gt;土耳其机器人（Mechanical
Turk）&lt;/b&gt;”服务，&lt;b&gt;人工地&lt;/b&gt;为每张图像都生成了至少5句标注，标注语句总共超过了约150万句。至于亚马逊的“土耳其机器人”服务，其实也就是另一种形式的雇人拿钱干活而已。&lt;/p&gt;&lt;p&gt;实际上，COCO Caption数据集包含了两个数据集：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一个数据集是MS COCO c5。它包含的训练集、验证集合测试集图像和原始的MS COCO数据库是一致的，只不过每个图像都带有5个人工生成的标注语句。&lt;/li&gt;&lt;li&gt;第二个数据集是MS COCO c40。它只包含5000张图片，而且这些图像是从MS COCO数据集的测试集中随机选出的。和c5不同的是，它的每张图像都有用40个人工生成的标注语句。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;之所以要做MS COCO c40数据集，是因为如果有更多的参考标注语句，很多对于算法生成的标注的自动计算标准能够和人类判断有更高的相关性。下一步可能将MS COCO验证集中所有的图像都加上40个人工生成的标注语句。&lt;/p&gt;&lt;p&gt;作者们的另一个主要工作就是搭建了一个评价服务器，实现了当前最流行的评价标准（BLEU, METEOR, ROUGE and CIDEr）。使用MS COCO Caption数据集训练并用验证机调参后，研究者可以按照固定的JSON格式：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[{
"image_id":int,
"caption" :str,
}]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;向服务器上传自己算法对于测试集图像生成的标注语句，服务器将自动地给出各种评价标准的得分。要上传结果，需要在&lt;a href="https://www.codalab.org/competitions/3221" data-editable="true" data-title="CodaLab"&gt;CodaLab&lt;/a&gt;注册账号，且每个账号能够提交结果的次数是有限的。微软在Github上也提供了能够在本地对验证集数据生成标注进行评价的代码，地址&lt;a href="https://github.com/tylin/coco-caption" data-editable="true" data-title="在这里"&gt;在这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;简言之，MS COCO Caption数据集就是针对图像标注问题创建的，图像及其标注数量大，提供了现成的评价标准计算服务器和代码。就目前发表的高水平论文来看，MS COCO Caption数据集已经越来越成为研究者的首选。&lt;/p&gt;&lt;h2&gt;Flickr8K和30K&lt;/h2&gt;&lt;p&gt;Flickr8K和Flickr30K数据集的特性从它们的命名就能很方便地猜测出来：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;图像数据来源是雅虎的相册网站Flickr；&lt;/li&gt;&lt;li&gt;数据集中图像的数量分别是8,000张和30,000张（确切地说是31,783）；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个数据库中的图像大多展示的是人类在参与到某项活动中的情景。每张图像的对应人工标注依旧是5句话。这两个数据库本是同根生，所以其标注的语法比较类似。数据库也是按照标准的训练集、验证集合测试集来进行分块的。&lt;/p&gt;&lt;p&gt;相较于MS COCO Caption数据集，Flickr8K和Flickr30K数据集的明显劣势就在于其数据量不足。我个人“不乏恶意”地揣度：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;微软：他们搞图像标注都用什么数据库呀？
员工：Flickr数据集居多吧。
微软：数量多少呀？
员工：开始是8k，后来出了个30k，感觉够用了。
微软：搞个大新闻，让他们只有我们一个零头！做个330k的！
员工：好的老板，是的老板，微软大法好！&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在论文《&lt;a href="http://arxiv.org/abs/1411.5654" data-editable="true" data-title="Learning a Recurrent Visual Representation for Image Caption Generation"&gt;Learning a Recurrent Visual Representation for Image Caption Generation&lt;/a&gt;》中作者指出：&lt;/p&gt;&lt;blockquote&gt;We observed this fine-
tuning strategy is particularly helpful for MS COCO, but
does not give much performance gain on Flickr Datasets before it overfits. The Flickr datasets may not provide enough
training data to avoid overfitting. 翻译：我们观察到这个精细调整策略在使用MS COCO数据集训练的时候效果很好，但是在Flickr数据集上算法却没有很明显的提升。可能是因为Flickr数据集没有提供足够多的数据来防止算法过拟合吧。&lt;/blockquote&gt;&lt;p&gt;眼尖的知友会发现，上面这篇论文的作者中，有一位是微软的......于是我又忍不住“邪恶”脑洞一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;员工：老板，论文发了！
微软：不错，黑得有水平，不留痕迹。
员工：那您看......
微软：好说好说。&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上虽然是玩笑话，但是数据量上的劣势，确实使Flickr数据集正逐渐失宠，14年论文中几乎都使用，现在一些高水平论文仅在补充文档中展示甚至不采用。个人还是希望Flickr数据集能够有一个比较好的更新，比如来个333k的？&lt;/p&gt;&lt;h2&gt;PASCAL 1K&lt;/h2&gt;&lt;p&gt;该数据集的图像是大名鼎鼎的PASCAL VOC challenge图像数据集的一个子集，对于其20个分类，随机选出了50张图像，共1,000张图像。然后同样适用亚马逊公司的土耳其机器人服务为每张图像人工标注了5个描述语句。一般说来，这个数据集只是用来测试的。&lt;/p&gt;&lt;p&gt;在其他论文中，还有一些诸如&lt;b&gt;IAPR TC-12&lt;/b&gt;，&lt;b&gt;SBU&lt;/b&gt;等数据集，数据采集思路都大同小异，这里就不一一介绍了。&lt;/p&gt;&lt;h2&gt;创建基于守望先锋的图像标注数据集？&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/6ee0e745fcdc2659f6a54f5409351e4b.jpg" data-rawwidth="1920" data-rawheight="1030"&gt;&lt;blockquote&gt;标注：来自东方的戴眼镜的某组织领导者正在殴打来自东方某半岛的女性直播。&lt;/blockquote&gt;&lt;p&gt;脑洞简言之：&lt;b&gt;开源搭建一个简单的图像标注页面，接受玩家的游戏截图投稿和对图像的中文标注，当数据收集达到目标数量后，数据集对所有参与贡献的人开放&lt;/b&gt;。做这件事的价值有两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;目前为止，个人&lt;b&gt;没有看到一个基于中文的公开图像标注数据集&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;基于守望先锋的故事背景，能够&lt;b&gt;吸引更多的年轻人从对游戏中人工智能的兴趣进而对真正的人工智能研究感兴趣&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为啥我对守望先锋情有独钟，主要是游戏基于后人工智能危机时代的世界观设定和故事吸引。看着网易乐呵呵地说守望先锋的销量超过了暗黑破坏神3，那么守望的销量也该有300万了吧？基于这个&lt;b&gt;庞大的玩家群体&lt;/b&gt;，如果这个数据集创建项目能够在玩家中有一定影响力，那么数据集的图像和标注数量应该比较客观。&lt;/p&gt;&lt;p&gt;当然，这只是我的一个小脑洞，&lt;b&gt;欢迎认可这个脑洞的知友在评论中留言讨论&lt;/b&gt;！我个人是真心希望推动这个脑洞成真。对了，个人倾向于&lt;b&gt;使用Ruby On Rails&lt;/b&gt;来搭建网站及后台。&lt;/p&gt;&lt;h2&gt;图像标注评价标准&lt;/h2&gt;&lt;p&gt;在简介中，提到虽然在多个评价标准的得分中，最新的方法的得分已经超过人类得分，但是我仍然不认为算法的真实水平超过人类，其原因就在于这些&lt;b&gt;自动评价标准（automatic evaluation metric）&lt;/b&gt;上！&lt;/p&gt;&lt;h2&gt;人类判断与自动评价标准&lt;/h2&gt;&lt;p&gt;简而言之，算法根据图像生成出来的标注语句质量高不高？和图像内容是不是相符？语法上有没有错误？&lt;b&gt;评价这些最靠谱最权威的还是咱们人类老爷&lt;/b&gt;！而各类的&lt;b&gt;自动评价标准的目前都是尽量让自己的计算结果能够和人类判断结果相关&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文《&lt;a href="http://www.aclweb.org/anthology/P/P14/P14-2074.xhtml" data-editable="true" data-title="Comparing Automatic Evaluation Measures for Image Description"&gt;Comparing Automatic Evaluation Measures for Image Description&lt;/a&gt;》关于这个问题就有很细致的分析。论文基于Flickr8K和E&amp;amp;K数据集，对BLEU、ROUGE、METEOR，TER几个评价标准与人类判断的相关性进行了研究，结果显示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c848d3b9934448272355b67eaf6ed21c.png" data-rawwidth="724" data-rawheight="482"&gt;论文中指出：与人类判断的相关性co-efficient在0.0–0.1是不相关，0.11–0.4是弱相关，0.41–0.7是中等相关，0.71–0.90是强相关，如果在0.91–1.0那就很完美了。所以论文的结论是首先推荐METEOR，或者使用ROUGE SU-4和Smoothed BLEU。PS：由于CIDEr标准是2015发布，所以这篇论文中没有体现。&lt;/p&gt;&lt;p&gt;而在谷歌2015年的论文《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》中，更是三图胜千言：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/cda54a4f425a16b865032c9dcba879c5.png" data-rawwidth="772" data-rawheight="734"&gt;我们可以看到：在表1中，实验使用了微软的COCO数据集，3中评价标准的得分，谷歌NIC模型的得分和人类（Human）得分是不相仲伯的。在表2中，基于不同数据集统一计算BLEU-1得分，NIC的得分和人类得分也比较接近。是不是很牛了？&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;然而难能可贵的是人家马上就自己打自己的脸！&lt;/u&gt;&lt;/b&gt;在装了逼后，作者们&lt;b&gt;马上开始说实话&lt;/b&gt;。补充了一个基于人类判断的实验，邀请人类对于自己生成的标准语句进行评级，一共分成4个等级：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/79db143aa3a04a40d8f9b33078690b0e.png" data-rawwidth="1912" data-rawheight="458"&gt;如上图所示，分成“描述没有错误”、“描述中有点小错误”、“多少还是和图像相关”和“和图像无关”4个等级，分别得分从4到1。那么，真实的对比就来了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a9ba6674deaa9b1e05d971d4584854ca.png" data-rawwidth="884" data-rawheight="664"&gt;上图中，x坐标是BLEU得分，y坐标是表示积累分布（也就是说，输出的描述语句集合中，有百分只多少的得分大于当前的x）。其中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Flickr-8k：NIC表示的是使用NIC模型在Flick8k测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;Pascal：NIC表示的是是使用NIC模型在Pascal测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;COCO-1k：NIC表示的是是使用NIC模型在COCO-1k测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;Flickr-8k：ref表示的是另一篇论文的结果的得分曲线，这里作为一个基准；&lt;/li&gt;&lt;li&gt;Flickr-8k：GT表示的是对Flickr-8k图像的人工标注语句同样进项人工分等级评价的结果。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由此可见：虽然自动裁判员BLEU-4认为NIC模型的得分超出了人类得分，但是如果让人类来当裁判员，NIC还差得远哪！这一结果也印证了上一篇论文中对于不同自动评价标准的分析。&lt;/p&gt;&lt;p&gt;当然，自动评价标准也在持续发展着，2015年发布的专门面向图像标注问题的自动评价标准CIDEr就做的更好些。&lt;/p&gt;&lt;p&gt;在刚才的人类判断与自动评价标准讨论中，几个标准的名称大家都应该比较熟悉了，接下来我就简单地介绍一下，并帮助大家理解它们是如何进行计算的：&lt;/p&gt;&lt;h2&gt;Perplexity&lt;/h2&gt;&lt;p&gt;首先，&lt;b&gt;这个perplexity该翻译成中文的哪个词&lt;/b&gt;就让我反复琢磨了好些天。查阅资料的过程中，大家要么就是不翻译（非我所认同），或者翻译为&lt;b&gt;复杂度&lt;/b&gt;、&lt;b&gt;混乱度&lt;/b&gt;或者&lt;b&gt;困惑度&lt;/b&gt;等。如何翻译先按下不表，所谓“信达雅”，第一是要把事情说明白，那么我们就先来把Perplexity理解了，再来选择译名不迟：&lt;/p&gt;&lt;p&gt;关于perplexity，在维基百科上有&lt;a href="https://en.wikipedia.org/wiki/Perplexity" data-editable="true" data-title="详细的解释"&gt;详细的解释&lt;/a&gt;，但是这里我想引用的是2014年百度的论文《&lt;a href="http://arxiv.org/abs/1410.1090" data-title="Explain Images with Multimodal Recurrent Neural Networks" class="" data-editable="true"&gt;Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;》中对perplexity的定义公式。为什么引用这篇呢？这篇论文是我读的论文中最早提出将RNN和CNN结合起来用于图像标注的，以此表示敬意。作者们在简介中也是这么说的：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;To the best of our knowledge, this is the first work that incorporates the Recurrent
Neural Network in a deep multimodal architecture.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;他们提出的&lt;b&gt;m-RNN&lt;/b&gt;模型也是后续论文方法的常用比较方法之一。在论文中，作者们结合图像标注任务，将perplexity定义为：&lt;/p&gt;&lt;equation&gt;log_2PPL(w_{1:L}|I)=-\frac{1}{L}\sum^L_{n=1}log_2P(w_n|w_{1:n-1},I)&lt;/equation&gt;&lt;p&gt;其中，&lt;equation&gt;L&lt;/equation&gt;是句子的长度，&lt;equation&gt;PPL(w_{1:L}|I)&lt;/equation&gt;就是根据图像&lt;equation&gt;I&lt;/equation&gt;给出的描述句子&lt;equation&gt;w_{1:L}&lt;/equation&gt;的perplexity。而&lt;equation&gt;P(w_n|w_{1:n-1},I)&lt;/equation&gt;是根据图像&lt;equation&gt;I&lt;/equation&gt;和前面的单词序列&lt;equation&gt;w_{1:n-1}&lt;/equation&gt;生成下一个单词&lt;equation&gt;w_n&lt;/equation&gt;的概率。对于此前没有接触过自然语言处理的同学（包括我）来说，此刻的感觉就是：&lt;b&gt;what fxxk ?!&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;不要惊慌。——《&lt;a href="https://book.douban.com/subject/1394364/" data-editable="true" data-title="银河系漫游指南"&gt;银河系漫游指南&lt;/a&gt;》&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;举例子&lt;/b&gt;：下面我们用守望先锋的游戏画面来举个例子。假设知友们和玩家们很给力，前面提到的守望先锋图像标注数据集已经有了足够的数据量了，我们也成功地训练了一个图像标注模型，恩，就叫她&lt;b&gt;ATHENA&lt;/b&gt;吧！Athena是&lt;b&gt;Advanced Tactical Heroine Assistance&lt;/b&gt;的缩写！（劳资真是缩写拼凑小王子！）&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/eaa7e55ded2c3bdc3c9e44fb1f5492ac.jpg" data-rawwidth="699" data-rawheight="293"&gt;&lt;p&gt;我们将上面这张图输入模型，假设模型给出了图像标注句子：&lt;/p&gt;&lt;blockquote&gt;there are two screens on the table.&lt;/blockquote&gt;&lt;p&gt;恩，语句和图像内容相关度还不错，那么我们就计算一下PPL来评价下这个句子。首先我们应该将句子补充一下，&lt;b&gt;添加一个特殊的开始和结束符号&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;lt;STA&amp;gt;&lt;/b&gt;there are two screens on the table&lt;b&gt;&amp;lt;END&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们知道，RNN生成句子的方式是采取一个单词一个单词预测的方式，那么假设咱们的词汇表里面有100个单词吧（挺少的），好了，现在从开始符号开始：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;模型根据图像，从&amp;lt;STA&amp;gt;符号开始，对词汇表中100个单词分别给出了每个单词可能为&amp;lt;STA&amp;gt;下一个单词的可能性，其中由于there单词的可能性为0.6，高于其他单词，所以最终就选择了there；&lt;/li&gt;&lt;li&gt;模型根据图像和&amp;lt;STA&amp;gt;there序列，再次对词汇表中100个单词分别给出了每个单词可能为下一个单词的可能性，其中are的可能性为0.8，高于其他单词，所以选择了are；&lt;/li&gt;&lt;li&gt;以此类推，假设two的可能性为0.3，screens的可能性为0.4，on的可能性为0.3，the的可能性为0.5，table的可能性为0.6；&lt;/li&gt;&lt;li&gt;当table出现后，模型预测下一个为&amp;lt;END&amp;gt;结束符号，句子生成结束。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么根据上面例子，&lt;equation&gt;L=7&lt;/equation&gt;，上面公式的右部就变成了：&lt;/p&gt;&lt;equation&gt;-\frac{1}{L}\sum^L_{n=1}log_2P(w_n|w_{1:n-1},I)=-\frac{1}{7}(log_2(0.6)+log_2(0.8)+log_2(0.3)+log_2(0.4)+log_2(0.3)+log_2(0.5)+log_2(0.6))&lt;/equation&gt;&lt;p&gt;由此可得：&lt;/p&gt;&lt;equation&gt;log_2PPL(w_{1:L}|I)=1.0845&lt;/equation&gt;&lt;p&gt;于是：&lt;/p&gt;&lt;equation&gt;PPL(w_{1:7}|I)=2^{1.0845}=2.12&lt;/equation&gt;&lt;p&gt;于是我们就得到了这个根据图像得出的标注语句的Perplexity值为2.12。那么我们的Athena的perplexity水平如何呢？这里引用一下百度这篇论文的结果表格：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6adb35fe9216103573556319ada4e870.png" data-rawwidth="920" data-rawheight="424"&gt;可以看见，m-RNN的PPL（即perplexity的缩写）值是6.92。那么这个分数值是低好一些还是高好一些呢？&lt;/p&gt;&lt;p&gt;我们利用刚才的例子重新算一遍，将每个单词的可能性都降低一些，就会发现perplexity值会升高。这就说明：&lt;b&gt;当模型对于下一个生成单词的确信程度降低时，perplexity值反而升高。我们当然是期望一个模型对于它预测的单词能比较有把握啊，所以perplexity值是越低越好&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;到了这里，我们就可以讨论perplexity到底可以怎么翻译了，其实我个人比较赞同&lt;a href="https://www.zhihu.com/people/4735cce127addcedc38470543c4ff409" data-hash="4735cce127addcedc38470543c4ff409" class="member_mention" data-editable="true" data-title="@硅谷王川" data-hovercard="p$b$4735cce127addcedc38470543c4ff409"&gt;@硅谷王川&lt;/a&gt;在文章中的一句话：&lt;/p&gt;&lt;blockquote&gt;换言之, 聊天机器人使用的语言模型, 如果&lt;b&gt;困惑度&lt;/b&gt;足够低,那么它就能够写出流利通顺和逻辑清晰的语句。借用韩愈老师在&amp;lt;师说&amp;gt;里的话:“&lt;b&gt;机器非生而知之者，孰能无惑？&lt;/b&gt;". 语言模型里进一步解惑的工具,则来自更多的数据和更精巧的算法。&lt;/blockquote&gt;&lt;p&gt;机器非生而知之者，孰能无惑？妙！所以我&lt;b&gt;个人认为将perplexity翻译为困惑度比较好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;BLEU&lt;/h2&gt;&lt;p&gt;BLEU是&lt;b&gt;B&lt;/b&gt;i&lt;b&gt;l&lt;/b&gt;ingual &lt;b&gt;E&lt;/b&gt;valuation &lt;b&gt;U&lt;/b&gt;nderstudy的缩写。这个计算标准在图像标注结果评价中使用是很广泛的，但是它的设计初衷并不是针对图像标注问题，而是针对机器翻译问题，它是用于分析待评价的翻译语句和参考翻译语句之间n元组的相关性的。直白地来说，它的核心思想就是：&lt;b&gt;机器翻译语句与人类的专业翻译语句越接近就越好&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一些需要说明的符号含义&lt;/b&gt;：对于图像&lt;equation&gt;I_i&lt;/equation&gt;，模型会生成对应的标注语句&lt;equation&gt;c_i&lt;/equation&gt;，自动评价标准能够根据参考标注语句（也就是人工标注的语句）的一个集合&lt;equation&gt;S_i=\{s_{i1},...,s_{im} \}\in S&lt;/equation&gt;，对待评价的标准语句&lt;equation&gt;c_i&lt;/equation&gt;的质量做出评价。标注语句都是用&lt;b&gt;n元组（n-gram）&lt;/b&gt;来表示的，一个n元组&lt;equation&gt;w_k\in \Omega&lt;/equation&gt;是一个由一个或者多个有顺序单词组成的序列。现在一般只探索n元组从1个单词到4个单词的情况。n元组&lt;equation&gt;w_k&lt;/equation&gt;在语句&lt;equation&gt;s_{ij}&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt;，n元组&lt;equation&gt;w_k&lt;/equation&gt;在待评价语句&lt;equation&gt;c_i\in C&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;明确上述符号含义后，BLEU的计算公式如下：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先计算的是全局的&lt;b&gt;n元组&lt;/b&gt;精度：其中&lt;equation&gt;k&lt;/equation&gt;指的是长度为n的可能的n元组的集合数。&lt;/p&gt;&lt;equation&gt;CP_n(C,S)=\frac{\sum_i \sum_kmin(h_k(c_i),max_{j\in m}h_k(s_{ij}))}{\sum_i \sum_kh_k(c_i)}&lt;/equation&gt;&lt;p&gt;然后计算的是简洁性惩罚值：其中&lt;equation&gt;l_C&lt;/equation&gt;是待评价语句&lt;equation&gt;c_i&lt;/equation&gt;们的总长，&lt;equation&gt;l_S&lt;/equation&gt;是全局级别的有效参考句子的总长度。如果对于一个待评价语句有多个参考语句，那么就选择让简洁性惩罚最小的那个。&lt;/p&gt;&lt;equation&gt;b(C,S)=\left\{
\begin{array}{rcl}
1           &amp;amp;      &amp;amp; {if \qquad l_C      &amp;gt;      l_S}\\
e^{1-l_S/l_C}       &amp;amp;      &amp;amp; {if \qquad l_C \leq l_S}
\end{array} \right. &lt;/equation&gt;&lt;p&gt;最终计算BLEU分数：其中&lt;equation&gt;N=1,2,3,4&lt;/equation&gt;，对于所有的&lt;equation&gt;n&lt;/equation&gt;，&lt;equation&gt;w_n&lt;/equation&gt;都是常量。&lt;/p&gt;&lt;equation&gt;BLEU_N(C,S)=b(C,S)exp\left(\sum^N_{n=1}w_nlogCP_n(C,S) \right)&lt;/equation&gt;&lt;p&gt;怎么算？同学你看懂了吗？反正我刚开始是没看懂:(，直到我看了&lt;a href="https://en.wikipedia.org/wiki/BLEU" data-editable="true" data-title="维基百科上的例子" class=""&gt;维基百科上的例子&lt;/a&gt;如下：&lt;/p&gt;&lt;p&gt;&lt;b&gt;举例子&lt;/b&gt;：假设我们现在有1个模型生成的待评价句子和2个参考句子如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1bf11b221e188a5f1bbedb83422960d6.png" data-rawwidth="762" data-rawheight="178"&gt;如果我们先来计算1元组the的精度的话，根据精度的公式：&lt;equation&gt;P=\frac{m}{w_t}=\frac{7}{7}=1&lt;/equation&gt;其中，&lt;equation&gt;m&lt;/equation&gt;怎么理解呢？就是如果一个单词是待评价句子中的，同时在在参考句子中也能找到的这个单词，那么这个单词在待评价语句中出现的次数就是&lt;equation&gt;m&lt;/equation&gt;，在现在例子中the在待评价语句中出现了，也在参考语句中出现了，所以符合条件，而在待评价语句中，the出现了7次，所以这里m=7。而&lt;equation&gt;w_t&lt;/equation&gt;是待评价句子总的单词数量，很显然&lt;equation&gt;w_t&lt;/equation&gt;的值也是7。现在就看到这个例子的奇葩之处了，看起来精度很完美，但是实际上翻译效果很差。&lt;b&gt;BLEU就是要解决这种问题&lt;/b&gt;：所以对于待评价句子中的任意一个单词，算法计算其在参考句子中出现的最大次数&lt;equation&gt;max_{j\in m}h_k(s_{ij})&lt;/equation&gt;，比如，the在参考1中出现了2次，在参考2中出现了1次，那么&lt;equation&gt;max_{j\in m}h_k(s_{ij})=2&lt;/equation&gt;。对于待评价句子，其中每个单词的出现次数&lt;equation&gt;h_k(c_i)&lt;/equation&gt;将被记为该单词的最大出现次数，比如对于the，值为7。而又因为&lt;equation&gt;min(h_k(c_i),max_{j\in m}h_k(s_{ij}))&lt;/equation&gt;，要在这两个值之间取最小值，所以值就是2了。于是，1元组the的精度分数&lt;equation&gt;CP_n(C,S)&lt;/equation&gt;就是&lt;b&gt;2/7&lt;/b&gt;了。然而在实际中，使用单个单词来比较并不是最理想的，所以BLEU使用n元组来计算，n值最高为4。1元组分数对于评价翻译并不足够。更长的元组得分对应的是语言的流畅性。接下来计算简洁性惩罚，&lt;b&gt;为啥还要引入一个简洁性惩罚呢&lt;/b&gt;？这是因为BLEU倾向于更短的句子，这样精度分数就会很高。为了解决这个问题，使用了乘以一个简洁性惩罚来防止很短的句子获得很高的分数。令&lt;equation&gt;l_S&lt;/equation&gt;为参考句子的总长度，&lt;equation&gt;l_C&lt;/equation&gt;是待评价句子的总长度，如果&lt;equation&gt;l_C&lt;/equation&gt;小于等于&lt;equation&gt;l_S&lt;/equation&gt;，那么惩罚生效，计算&lt;equation&gt;e^{1-l_S/l_C}&lt;/equation&gt;。反之，简洁性惩罚值为1。&lt;b&gt;如果有多个参考句子，那么就选取长度和待评价句子长度最接近的那个参考句子的长度&lt;/b&gt;。最后一步的计算相对比较清晰，就不过多解释了。&lt;p&gt;&lt;b&gt;一句话&lt;/b&gt;：&lt;b&gt;BLEU得分越高越好。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;ROUGE&lt;/h2&gt;&lt;p&gt;ROUGE是一个设计&lt;b&gt;用来评价文本摘要算法&lt;/b&gt;的自动评价标准集，其中有3个评价标准，分别是&lt;b&gt;ROUGE-N，ROUGE-L和ROUGE-S&lt;/b&gt;。下面逐个进行介绍：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_N&lt;/equation&gt;是第一个ROUGE标准。根据给出的待评价句子，它对所有的参考摘要计算一个简单的n元组召回：&lt;/p&gt;&lt;equation&gt;ROUGE_n(c_i,S_i)=\frac{\sum_j \sum_kmin(h_k(c_i),h_k(s_{ij}))}{\sum_i \sum_kh_k(s_{ij})}&lt;/equation&gt;&lt;p&gt;回顾一下：n元组&lt;equation&gt;w_k&lt;/equation&gt;在语句&lt;equation&gt;s_{ij}&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt;，n元组&lt;equation&gt;w_k&lt;/equation&gt;在待评价语句&lt;equation&gt;c_i\in C&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。所以上面&lt;equation&gt;ROUGE_N&lt;/equation&gt;的计算还是挺容易理解的。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_L&lt;/equation&gt;是基于longest common subsequence（LCS）的一种测量方法。所谓LCS，就是一个同时出现在两个句子中的单词集合，且单词出现的顺序也是相同的。和n元组不同的是，在单词之间可能还存在能够创建出LCS的单词。将比较的两个句子间的LCS的长度记为：&lt;equation&gt;l(c_i,s_{ij})&lt;/equation&gt;。ROUGE-L通过计算F-meansure（&lt;a href="https://en.wikipedia.org/wiki/F1_score" data-editable="true" data-title="F1 score"&gt;F1 score&lt;/a&gt;）来求得：&lt;/p&gt;&lt;equation&gt;R_l=max_j\frac{l(c_i,s_{ij})}{|s_{ij}|}&lt;/equation&gt;&lt;equation&gt;P_l=max_j\frac{l(c_i,s_{ij})}{|c_i|}&lt;/equation&gt;&lt;equation&gt;ROUGE_L(c_i,S_i)=\frac{(1+\beta^2)R_lP_l}{R_l+\beta^2P_l}&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;R_l&lt;/equation&gt;是召回，&lt;equation&gt;P_l&lt;/equation&gt;是精度，&lt;equation&gt;\beta&lt;/equation&gt;一般等于1.2，在这个计算中不需要管n元组。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_S&lt;/equation&gt;是最后一个标准，没有使用LCS或n元组，使用的是&lt;b&gt;跳跃二元组（skip bigram）&lt;/b&gt;。跳跃二元组是句子中有序的单词对，和LCS类似，在单词对之间，单词可能被跳过。比如一句有4个单词的句子，按照排列组合就可能有6种跳跃二元组。再次使用精度和召回率来计算F，将句子&lt;equation&gt;s_{ij}&lt;/equation&gt;中跳跃二元组的个数记为&lt;equation&gt;f_k(s_{ij})&lt;/equation&gt;，则计算公式如下：&lt;/p&gt;&lt;equation&gt;R_s=max_j\frac{\sum_kmin(f_k(c_i),f_k(s_{ij}))}{\sum_kf_k(s_{ij})}&lt;/equation&gt;&lt;equation&gt;P_s=max_j\frac{\sum_kmin(f_k(c_i),f_k(s_{ij}))}{\sum_kf_k(c_i)}&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;GOUGE_S(c_i,S_i)=\frac{(1+\beta^2)R_sP_s}{R_s+\beta^2P_s}&lt;/equation&gt;跳跃二元组能够捕获到长距离的句子结构。在实践中，跳跃二元组计算的时候单词间最长距离为4。ROUGE-SU是在跳跃二元组基础上增加使用了1元组。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话：ROUGE得分越高越好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;METEOR&lt;/h2&gt;&lt;p&gt;&lt;b&gt;METEOR是用来评价机器翻译输出的标准&lt;/b&gt;。该方法基于一元组的精度和召回的调和平均（&lt;a href="https://en.wikipedia.org/wiki/Harmonic_mean" data-editable="true" data-title="Harmonic mean"&gt;Harmonic mean&lt;/a&gt;），召回的权重比精度要高一点。这个标准还有一些其他标准没有的特性，设计它是为了解决BLEU存在的一些问题。它与人类判断相关性高，而且和BLEU不同，它不仅在整个集合，而且在句子和分段级别，也能和人类判断的相关性高。在全集级别，它的相关性是0.964，BLEU是0.817。在句子级别，它的相关性最高到了0.403。&lt;/p&gt;&lt;p&gt;&lt;b&gt;METEOR的计算公式&lt;/b&gt;：其中m是&lt;b&gt;平面图（alignments）&lt;/b&gt;的集合，ch是&lt;b&gt;块（chunk）&lt;/b&gt;的数量，&lt;equation&gt;P_m&lt;/equation&gt;是精度，&lt;equation&gt;R_m&lt;/equation&gt;是召回率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;Pen=\gamma\left(\frac{ch}{m}  \right)^{\theta}&lt;/equation&gt;&lt;equation&gt;F_{mean}=\frac{P_mR_m}{\alpha P_m+(1-\alpha)R_m}&lt;/equation&gt;&lt;equation&gt;P_m=\frac{|m|}{\sum_k h_k(c_i)}&lt;/equation&gt;&lt;equation&gt;R_m=\frac{|m|}{\sum_k h_k(s_{ij})}&lt;/equation&gt;&lt;equation&gt;METEOR=(1-Pen)F_{mean}&lt;/equation&gt;&lt;b&gt;理解&lt;/b&gt;：看公式总是挺抽象的，下面我们还是看看来自维基百科的例子吧。计算的最基本单元是句子。算法首先从待评价字符串和参考字符串之间创建一个平面图如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/96242ff6cfe2cbb92b23f755a78c28f7.png" data-rawwidth="1096" data-rawheight="174"&gt;所谓平面图，就是1元组之间的映射集。平面图有如下的一些限制：在待评价翻译中的每个1元组必须映射到参考翻译中的1个或0个一元组，然后根据这个定义创建平面图。&lt;b&gt;如果有两个平面图的映射数量相同，那么选择映射交叉数目较少的那个&lt;/b&gt;。也就是说，上面左侧平面图会被选择。状态会持续运行，在每个状态下只会向平面图加入那些在前一个状态中尚未匹配的1元组。一旦最终的平面图计算完毕，就开始计算METEOR得分：&lt;/p&gt;&lt;p&gt;1元组精度：&lt;equation&gt;P=\frac{m}{w_t}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;其中m是在参考句子中同样存在的，待评价句子中的一元组的数量。&lt;equation&gt;w_t&lt;/equation&gt;是待评价翻译中一元组的数量。&lt;/p&gt;&lt;p&gt;1元组召回率：&lt;equation&gt;R=\frac{m}{w_r}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;m同上，&lt;equation&gt;w_r&lt;/equation&gt;是参考翻译中一元组的数量。然后使用调和平均来计算F-mean，且召回的权重是精度的9倍。&lt;equation&gt;F_{mean}=\frac{10PR}{R+9P}&lt;/equation&gt;到目前为止，这个方法只对单个单词的一致性进行了衡量，却没有对参考翻译和待评价翻译中更大的分段进行衡量。为了将其计算在内，使用更长的n元组来计算对于平面图的惩罚p。在参考和待评价句子中的没有毗连的映射越多，惩罚就越高。为了计算惩罚，1元组被分组成最少可能的&lt;b&gt;块（chunks）&lt;/b&gt;。块的定义是在待评价语句和参考语句中毗邻的一元组集合。在待评价语句和参考语句之间的毗邻映射越长，块的数量就越少。一个待评价翻译如果和参考翻译相同，那么就只有一个块。惩罚p的计算如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p=0.5\left(\frac{c}{u_m} \right)^3&lt;/equation&gt;其中c就是块的数量，&lt;equation&gt;u_m&lt;/equation&gt;是被映射的一元组的数量。p可以减少F-mean的值。最后&lt;equation&gt;M=(1-p)F_{mean}&lt;/equation&gt;。&lt;b&gt;计算例子&lt;/b&gt;：这里偷个懒，直接截图维基百科，可以结合这个例子对照看自己的计算是否正确。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0c33f88db9521eab9c82afe1cf794ea3.png" data-rawwidth="1736" data-rawheight="1162"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话：METEOR得分越高越好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;CIDEr&lt;/h2&gt;CIDEr是专门设计出来用于图像标注问题的，它是通过对每个n元组进行&lt;b&gt;Term Frequency Inverse Document Frequency (TF-IDF) &lt;/b&gt;权重计算，来衡量图像标注的一致性的。一个n元组wk在出现在参考句子sij中的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt; ，如果出现在待评价句子中，则被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。CIDEr为每个n元组wk都计算TF-IDF权重&lt;equation&gt;g_k(s_{ij})&lt;/equation&gt;：&lt;equation&gt;g_k(s_{ij})=\frac{h_k(s_{ij})}{\sum_{w_l\in \Omega}}log \left(\frac{|I|}{\sum_{I_p \in I}min(1,\sum_qh_k(s_{pq}))} \right)&lt;/equation&gt;其中&lt;equation&gt;\Omega&lt;/equation&gt;是所有n元组的词汇表，I是数据集中所有图像的集合。公式第一个部分计算的是每个n元组wk的TF，公式第二部分是使用IDF来计算&lt;equation&gt;w_k&lt;/equation&gt;的稀有程度。从直观上来说，如果一些n元组频繁地出现在描述图像的参考标注中，TF对于这些n元组将给出更高的权重，而IDF则降低那些在所有描述语句中都常常出现的n元组的权重。也就是说，IDF提供了一种测量单词显著性的方法，这就是将那些容易常常出现，但是对于视觉内容信息没有多大帮助的单词的重要性打折。IDF的计算方法是：分子为数据集中图像的数量I，分母为一些图像的数量，这些图像是其任意一个描述中出现了n元组&lt;equation&gt;w_k&lt;/equation&gt;的图像，然后再对着个分数求对数。对于长度为n的n元组的CIDEr-n分数是使用待评价句子和参考句子之间的平均相似性来计算的，其中精度和召回率都要占比例：&lt;equation&gt;CIDEr_n(c_i,S_i)=\frac{1}{m}\sum_j\frac{g^n(c_i)\cdot g^n(s_{ij})}{||g^n(c_i)|||| g^n(s_{ij})||}&lt;/equation&gt;其中，&lt;equation&gt;g^n(c_i)&lt;/equation&gt;是一个由&lt;equation&gt;g_k(c_i)&lt;/equation&gt;生成的向量，对应的是所有长度为n的n元组。&lt;equation&gt;||g^n(c_i)||&lt;/equation&gt;是向量的大小。而&lt;equation&gt;g^n(s_{ij})&lt;/equation&gt;的情况类似。更长的n元组是用来获取语法性质和更丰富的语义信息的。不同长度的n元组的得分计算如下：&lt;equation&gt;CIDEr(c_i,S_i)=\sum^N_{n=1}w_nCIDEr_n(c_i,S_i)&lt;/equation&gt;&lt;p&gt;标准权重&lt;equation&gt;w_n=1/N&lt;/equation&gt;，N=4比较常用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;CIDEr-D&lt;/b&gt;是修改版本，为的是让CIDEr对于gaming问题更加鲁棒。什么是Gaming问题？它是一种现象，就是一个句子经过人工判断得分很低，但是在自动计算标准中却得分很高的情况。为了避免这种情况，CIDEr-D增加了&lt;b&gt;截断（clipping）和基于长度的高斯惩罚&lt;/b&gt;：&lt;/p&gt;&lt;equation&gt;CIDEr\text{-}D_n(c_i,S_i)=\frac{10}{m}\sum_j e^{\frac{-(l(c_i)-l(s_{ij}))^2}{2\sigma^2}} \frac{min(g^n(c_i),g^n(s_{ij}))\cdot g^n(s_{ij})}{||g^n(c_i)|||| g^n(s_{ij})||}&lt;/equation&gt;其中，&lt;equation&gt;l(c_i)&lt;/equation&gt;和&lt;equation&gt;l(s_{ij})&lt;/equation&gt;分别表示的是待评价句子和参考句子的长度，&lt;equation&gt;\sigma&lt;/equation&gt;=6，分子为10是为了让得分和其他标准比较相似。最终：&lt;equation&gt;CIDEr\text{-}D(c_i,S_i)=\sum^N_{n=1}w_nCIDEr\text{-}D_n(c_i,S_i)&lt;/equation&gt;&lt;b&gt;一句话：CIDEr得分越高越好&lt;/b&gt;。&lt;h2&gt;下篇预告&lt;/h2&gt;&lt;p&gt;由于文章篇幅较长，顾分为上下篇发布。在上篇中，&lt;b&gt;介绍了图像标准问题，以及研究问题的数据集，人工和自动评价标准，并脑洞了开源创建守望先锋图像标注数据集的想法&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;在下篇中，将&lt;b&gt;介绍比较几个有代表性的图像标注方法，一些代码实践，和对于图像标准问题的思考&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;作者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;欢迎大家对文中的不当之处留言批评指正，共同学习提高；&lt;/li&gt;&lt;li&gt;由于希望写得有趣和通俗些，所以解释和比喻较多，行文稍显冗余，不知阅读效果如何？请批评；&lt;/li&gt;&lt;li&gt;真心想做一个基于守望先锋游戏画面的图像标注数据集，为什么特别指明用Ruby呢？因为朋友天天安利我说Ruby on rails做网站好，我对网站这一块不太懂，就信任小伙伴咯，不希望因为这个问题引战:)&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22408033&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 20 Sep 2016 12:04:02 GMT</pubDate></item><item><title>[原创翻译]循环神经网络惊人的有效性（下）</title><link>https://zhuanlan.zhihu.com/p/22230074</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2029a1ea020883b4d065592d14acc96e_r.jpg"&gt;&lt;/p&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创翻译，禁止未授权转载。 &lt;blockquote&gt;译者注：在CS231n课程笔记止步于CNN，没有循环神经网络（RNN和LSTM），实为憾事。经知友推荐，将&lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-title="The Unreasonable Effectiveness of Recurrent Neural Networks" class="" data-editable="true"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;一文翻译完毕，作为补充。感谢@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;，&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt;的校对。&lt;/blockquote&gt;&lt;h2&gt;目录&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;循环神经网络&lt;/li&gt;&lt;li&gt;字母级别的语言模型&lt;/li&gt;&lt;li&gt;RNN的乐趣&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Paul Graham生成器&lt;/li&gt;&lt;li&gt;莎士比亚&lt;/li&gt;&lt;li&gt;维基百科&lt;/li&gt;&lt;li&gt;几何代数&lt;/li&gt;&lt;li&gt;Linux源码&lt;/li&gt;&lt;li&gt;生成婴儿姓名&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;理解训练过程 &lt;i&gt;&lt;b&gt;译者注：下篇起始处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;训练时输出文本的进化&lt;/li&gt;&lt;li&gt;RNN中的预测与神经元激活可视化&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;源代码&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;li&gt;结论&lt;/li&gt;&lt;li&gt;译者反馈&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;理解训练过程&lt;/h2&gt;&lt;p&gt;我们已经看见训练结束后的结果令人印象深刻，但是它到底是如何运作的呢？现在跑两个小实验来一探究竟。&lt;/p&gt;&lt;h3&gt;训练时输出文本的进化&lt;/h3&gt;&lt;p&gt;首先，观察模型在训练时输出文本的不断进化是很有意思的。例如，我使用托尔斯泰的《战争与和平》来训练LSTM，并在训练过程中每迭代100次就输出一段文本。在第100次迭代时，模型输出的文本是随机排列的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;tyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne 'nhthnee e 
plia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是至少可以看到它学会了单词是被空格所分割的，只是有时候它使用了两个连续空格。它还没学到逗号后面总是有个空格。在迭代到第300次的时候，可以看到模型学会使用引号和句号。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Tmont thithey" fomesscerliund
Keushey. Thom here
sheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome
coaniogennc Phe lism thond hon at. MeiDimorotion in ther thize."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;单词被空格所分割，模型开始知道在句子末尾使用句号。在第500次迭代时：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;we counter. He stutn co des. His stanted out one ofler that concossions and was 
to gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;模型开始学会使用最短和最常用的单词，比如“we”、“He”、“His”、“Which”、“and”等。从第700次迭代开始，可以看见更多和英语单词形似的文本：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Aftair fall unsuch that the hall for Prince Velzonski's that me of
her hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort 
how, and Gogition is so overelical and ofter.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在第1200次迭代，我们可以看见使用引号、问好和感叹号，更长的单词也出现了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Kite vouch!" he repeated by her
door. "But I would be done and quarts, feeling, then, son is people...."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在迭代到2000次的时候，模型开始正确的拼写单词，引用句子和人名。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Why do what that day," replied Natasha, and wishing to himself the fact the
princess, Princess Mary was easier, fed in had oftened him.
Pierre aking his soul came to the packs and drove up his father-in-law women.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上述结果中可见，模型首先发现的是一般的单词加空格结构，然后开始学习单词；从短单词开始，然后学习更长的单词。由多个单词组成的话题和主题词要到训练后期才会出现。&lt;/p&gt;&lt;h2&gt;RNN中的预测与神经元激活可视化&lt;/h2&gt;&lt;p&gt;另一个有趣的实验内容就是将模型对于字符的预测可视化。下面的图示是我们对用维基百科内容训练的RNN模型输入验证集数据（蓝色和绿色的行）。在每个字母下面我们列举了模型预测的概率最高的5个字母，并用深浅不同的红色着色。深红代表模型认为概率很高，白色代表模型认为概率较低。注意有时候模型对于预测的字母是非常有信心的。比如在&lt;a href="http://www/" data-editable="true" data-title="www 的页面"&gt;http://www&lt;/a&gt;. 序列中就是。&lt;/p&gt;&lt;p&gt;输入字母序列也被着以蓝色或者绿色，这代表的是RNN隐层表达中的某个随机挑选的神经元是否被&lt;em&gt;激活&lt;/em&gt;。绿色代表非常兴奋，蓝色代表不怎么兴奋。LSTM中细节也与此类似，隐藏状态向量中的值是[-1, 1]，这就是经过各种操作并使用tanh计算后的LSTM细胞状态。直观地说，这就是当RNN阅读输入序列时，它的“大脑”中的某些神经元的激活率。不同的神经元关注的是不同的模式。在下面我们会看到4种不同的神经元，我认为比较有趣和能够直观理解（当然也有很多不能直观理解）。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2ae42139518681b8cbbe4f854fdef515.jpg" data-rawwidth="1639" data-rawheight="825"&gt;本图中高亮的神经元看起来对于URL的开始与结束非常敏感。LSTM看起来是用这个神经元来记忆自己是不是在一个URL中。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/63ac60c5e0fc60017e45360516c95682.jpg" data-rawwidth="1688" data-rawheight="827"&gt;高亮的神经元看起来对于markdown符号[[]]的开始与结束非常敏感。有趣的是，一个[符号不足以激活神经元，必须等到两个[[同时出现。而判断有几个[的任务看起来是由另一个神经元完成的。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c41f6b3d378e74b45698950a95b32d29.jpg" data-rawwidth="1685" data-rawheight="398"&gt;这是一个在[[]]中线性变化的神经元。换句话说，在[[]]中，它的激活是为RNN提供了一个以时间为准的坐标系。RNN可以使用该信息来根据字符在[[]]中出现的早晚来决定其出现的频率（也许？）。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/27c8e4245a2e3726f6faa3af58c15603.jpg" data-rawwidth="1668" data-rawheight="185"&gt;这是一个进行局部动作的神经元：它大部分时候都很安静，直到出现www序列中的第一个w后，就突然关闭了。RNN可能是使用这个神经元来计算www序列有多长，这样它就知道是该输出有一个w呢，还是开始输出URL了。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;当然，由于RNN的隐藏状态是一个巨大且分散的高维度表达，所以上面这些结论多少有一点手动调整。上面的这些可视化图片是用定制的HTML/CSS/Javascript实现的，如果你想实现类似的，可以查看&lt;a href="http://cs.stanford.edu/people/karpathy/viscode.zip" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们可以进一步简化可视化效果：不显示预测字符仅仅显示文本，文本的着色代表神经元的激活情况。可以看到大部分的细胞做的事情不是那么直观能理解，但是其中5%看起来是学到了一些有趣并且能理解的算法：&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d86db8d5322c96a366e59572e01a5685.png" data-rawwidth="990" data-rawheight="865"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/25eea1bd327845168fab51e66c78d77d.png" data-rawwidth="766" data-rawheight="865"&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;在预测下个字符的过程中优雅的一点是：我们不用进行任何的硬编码。比如，不用去实现判断我们到底是不是在一个引号之中。我们只是使用原始数据训练LSTM，然后它自己决定这是个有用的东西于是开始跟踪。换句话说，其中一个单元自己在训练中变成了引号探测单元，只因为这样有助于完成最终任务。这也是深度学习模型（更一般化地说是端到端训练）强大能力的一个简洁有力的证据。&lt;/p&gt;&lt;h2&gt;源代码&lt;/h2&gt;&lt;p&gt;我想这篇博文能够让你认为训练一个字符级别的语言模型是一件有趣的事儿。你可以使用我在Github上的&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="char rnn代码"&gt;char rnn代码&lt;/a&gt;训练一个自己的模型。它使用一个大文本文件训练一个字符级别的模型，可以输出文本。如果你有GPU，那么会在比CPU上训练快10倍。如果你训练结束得到了有意思的结果，请联系我。如果你看Torch/Lua代码看的头疼，别忘了它们只不过是这个&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="100行项目"&gt;100行项目&lt;/a&gt;的高端版。&lt;/p&gt;&lt;p&gt;&lt;em&gt;题外话&lt;/em&gt;。代码是用&lt;a href="http://torch.ch/" data-editable="true" data-title="Torch7"&gt;Torch7&lt;/a&gt;写的，它最近变成我最爱的深度学习框架了。我开始学习Torch/LUA有几个月了，这并不简单（花了很多时间学习Github上的原始Torch代码，向项目创建者提问来解决问题），但是一旦你搞懂了，它就会给你带来很大的弹性和加速。之前我使用的是Caffe和Theano，虽然Torch虽然还不完美，但是我相信它的抽象和哲学层次比前两个高。在我看来，一个高效的框架应有以下特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;有丰富函数（例如切片，数组/矩阵操作等）的，对底层CPU/GPU透明的张量库。&lt;/li&gt;&lt;li&gt;一整个基于脚本语言（比如Python）的分离的代码库，能够对张量进行操作，实现所有深度学习内容（前向、反向传播，计算图等）。&lt;/li&gt;&lt;li&gt;分享预训练模型非常容易（Caffe做得很好，其他的不行）。&lt;/li&gt;&lt;li&gt;最关键的：没有编译过程！或者至少不要像Theano现在这样！深度学习的趋势是更大更复杂的网络，这些网络都有随着时间展开的复杂计算流程。编译时间不能太长，不然开发过程将充满痛苦。其次，编译导致开发者放弃解释能力，不能高效地进行调试。如果在流程开发完成后有个&lt;em&gt;选项&lt;/em&gt;能进行编译，那也可以。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;拓展阅读&lt;/h2&gt;&lt;p&gt;在结束本篇博文前，我想把RNN放到更广的背景中，提供一些当前的研究方向。RNN现在在深度学习领域引起了不小的兴奋。和卷积神经网络一样，它出现已经有十多年了，但是直到最近它的潜力才被逐渐发掘出来，这是因为我们的计算能力日益强大。下面是当前的一些进展（肯定不完整，而且很多工作可以追溯的1990年）：&lt;/p&gt;&lt;p&gt;在NLP/语音领域，RNN将&lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" data-editable="true" data-title="语音转化为文字"&gt;语音转化为文字&lt;/a&gt;，进行&lt;a href="http://arxiv.org/abs/1409.3215" data-editable="true" data-title="机器翻译"&gt;机器翻译&lt;/a&gt;，生成&lt;a href="http://www.cs.toronto.edu/~graves/handwriting.html" data-editable="true" data-title="手写文本"&gt;手写文本&lt;/a&gt;，当然也是强大的语言模型 (Sutskever等) (Graves) (Mikolov等)。字符级别和单词级别的模型都有，目前看来是单词级别的模型更领先，但是这只是暂时的。&lt;/p&gt;&lt;p&gt;计算机视觉。RNN迅速地在计算机视觉领域中被广泛运用。比如，使用RNN用于&lt;a href="http://arxiv.org/abs/1411.4389" data-editable="true" data-title="视频分类"&gt;视频分类&lt;/a&gt;，&lt;a href="http://arxiv.org/abs/1411.4555" data-editable="true" data-title="图像标注"&gt;图像标注&lt;/a&gt;（其中有我自己的工作和其他一些），&lt;a href="http://arxiv.org/abs/1505.00487" data-editable="true" data-title="视频标注"&gt;视频标注&lt;/a&gt;和最近的&lt;a href="http://arxiv.org/abs/1505.02074" data-editable="true" data-title="视觉问答"&gt;视觉问答&lt;/a&gt;。在计算机视觉领域，我个人最喜欢的RNN论文是《&lt;a href="http://arxiv.org/abs/1406.6247" data-editable="true" data-title="Recurrent Models of Visual Attention"&gt;Recurrent Models of Visual Attention&lt;/a&gt;》，之所以推荐它，是因为它高层上的指导方向和底层的建模方法（对图像短时间观察后的序列化处理），和建模难度低（REINFORCE算法规则是增强学习里面策略梯度方法中的一个特例，使得能够用非微分的计算来训练模型（在该文中是对图像四周进行快速查看））。我相信这种用CNN做原始数据感知，RNN在顶层做快速观察策略的混合模型将会在感知领域变得越来越流行，尤其是在那些不单单是对物体简单分类的复杂任务中将更加广泛运用。&lt;/p&gt;&lt;p&gt;归纳推理，记忆和注意力（Inductive Reasoning, Memories and Attention）。另一个令人激动的研究方向是要解决普通循环网络自身的局限。RNN的一个问题是它不具有归纳性：它能够很好地记忆序列，但是从其表现上来看，它不能很好地在正确的方向上对其进行归纳（一会儿会举例让这个更加具体一些）。另一个问题是RNN在运算的每一步都将表达数据的尺寸和计算量联系起来，而这并非必要。比如，假设将隐藏状态向量尺寸扩大为2倍，那么由于矩阵乘法操作，在每一步的浮点运算量就要变成4倍。理想状态下，我们希望保持大量的表达和记忆（比如存储全部维基百科或者很多中间变量），但同时每一步的运算量不变。&lt;/p&gt;&lt;p&gt;在该方向上第一个具有说服力的例子来自于DeepMind的&lt;a href="http://arxiv.org/abs/1410.5401" data-editable="true" data-title="神经图灵机（Neural Turing Machines）"&gt;神经图灵机（Neural Turing Machines）&lt;/a&gt;论文。该论文展示了一条路径：模型可以在巨大的外部存储数组和较小的存储寄存器集（将其看做工作的存储器）之间进行读写操作，而运算是在存储寄存器集中进行。更关键的一点是，神经图灵机论文提出了一个非常有意思的存储解决机制，该机制是通过一个（soft和全部可微分的）注意力模型来实现的。&lt;em&gt;译者注：这里的soft取自softmax&lt;/em&gt;。基于概率的“软”注意力机制（soft attention）是一个强有力的建模特性，已经在面向机器翻译的《&lt;a href="http://arxiv.org/abs/1409.0473" data-editable="true" data-title="Neural Machine Translation by Jointly Learning to Align and Translate"&gt; Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;》一文和面向问答的《&lt;a href="http://arxiv.org/abs/1503.08895" data-editable="true" data-title="Memory Networks"&gt;Memory Networks&lt;/a&gt;》中得以应用。实际上，我想说的是：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;注意力概念是近期神经网络领域中最有意思的创新。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;现在我不想更多地介绍细节，但是软注意力机制存储器寻址是非常方便的，因为它让模型是完全可微的。不好的一点就是牺牲了效率，因为每一个可以关注的地方都被关注了（虽然是“软”式的）。想象一个C指针并不指向一个特定的地址，而是对内存中所有的地址定义一个分布，然后间接引用指针，返回一个与指向内容的权重和（这将非常耗费计算资源）。这让很多研究者都从软注意力模式转向硬注意力模式，而硬注意力模式是指对某一个区域内的内容固定关注（比如，对某些单元进行读写操作而不是所有单元进行读写操作）。这个模型从设计哲学上来说肯定更有吸引力，可扩展且高效，但不幸的是模型就不是可微分的了。这就导致了对于增强学习领域技术的引入（比如REINFORCE算法），因为增强学习领域中的研究者们非常熟悉不可微交互的概念。这项工作现在还在进展中，但是硬注意力模型已经被发展出来了，在《&lt;a href="http://arxiv.org/abs/1503.01007" data-editable="true" data-title="Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"&gt; Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets&lt;/a&gt;》，《&lt;a href="http://arxiv.org/abs/1505.00521" data-editable="true" data-title="Reinforcement Learning Neural Turing Machines"&gt; Reinforcement Learning Neural Turing Machines&lt;/a&gt;》，《&lt;a href="http://arxiv.org/abs/1502.03044" data-editable="true" data-title="Show Attend and Tell"&gt;Show Attend and Tell&lt;/a&gt;》三篇文章中均有介绍。&lt;/p&gt;&lt;p&gt;研究者。如果你想在RNN方面继续研究，我推荐&lt;a href="http://www.cs.toronto.edu/~graves/" data-editable="true" data-title="Alex Graves"&gt;Alex Graves&lt;/a&gt;，&lt;a href="http://www.cs.toronto.edu/~ilya/" data-editable="true" data-title="Ilya Sutskever"&gt;Ilya Sutskever&lt;/a&gt;和&lt;a href="http://www.rnnlm.org/" data-editable="true" data-title="Tomas Mikolov"&gt;Tomas Mikolov&lt;/a&gt;三位研究者。想要知道更多增强学习和策略梯度方法（REINFORCE算法是其中一个特例），可以学习&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html" data-editable="true" data-title="David Silver的课程"&gt;David Silver的课程&lt;/a&gt;，或&lt;a href="http://www.cs.berkeley.edu/~pabbeel/" data-editable="true" data-title="Pieter Abbeel的课程"&gt;Pieter Abbeel的课程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;代码。如果你想要继续训练RNN，我听说Theano上的&lt;a href="https://github.com/fchollet/keras" data-editable="true" data-title="keras"&gt;keras&lt;/a&gt;或&lt;a href="https://github.com/IndicoDataSolutions/Passage" data-editable="true" data-title="passage"&gt;passage&lt;/a&gt;还不错。我使用Torch写了一个&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;，也用numpy实现了一个可以前向和后向传播的LSTM。你还可以在Github上看看我的&lt;a href="https://github.com/karpathy/neuraltalk" data-editable="true" data-title="NeuralTalk" class=""&gt;NeuralTalk&lt;/a&gt;项目，是用RNN/LSTM来进行图像标注。或者看看Jeff Donahue用&lt;a href="http://jeffdonahue.com/lrcn/" data-editable="true" data-title="Caffe"&gt;Caffe&lt;/a&gt;实现的项目。&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;我们已经学习了RNN，知道了它如何工作，以及为什么它如此重要。我们还利用不同的数据集将RNN训练成字母级别的语言模型，观察了它是如何进行这个过程的。可以预见，在未来将会出现对RNN的巨大创新，我个人认为它们将成为智能系统的关键组成部分。&lt;/p&gt;&lt;p&gt;最后，为了给文章增添一点格调，我使用本篇博文对RNN进行了训练。然而由于博文的长度很短，不足以很好地训练RNN。但是返回的一段文本如下（使用低的温度设置来返回更典型的样本）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;I've the RNN with and works, but the computed with program of the 
RNN with and the computed of the RNN with with and the code
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;是的，这篇博文就是讲RNN和它如何工作的，所以显然模型是有用的：）下次见！&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;翻译不到位的地方，欢迎知友们评论批评指正；&lt;/li&gt;&lt;li&gt;关于Torch和TensorFlow，AK本人现在在OpenAI工作主要是在用TF了，但是他对于Torch还是有很强的倾向性。这在他最新的博文中可以看到；&lt;/li&gt;&lt;li&gt;在计算机视觉方面，个人对于&lt;b&gt;图像标注&lt;/b&gt;比较感兴趣，正在入坑。欢迎有同样兴趣的知友投稿讨论；&lt;/li&gt;&lt;li&gt;想要加入翻译小组的同学，&lt;b&gt;请连续3次在评论中对我们最新的翻译做出认真的批评和指正，而后我们会小组内投票决定是否吸纳新成员：）&lt;/b&gt;这个小小的门槛是为了方便我们找到真正喜爱机器学习和翻译的同学。&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22230074&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 13 Sep 2016 19:09:57 GMT</pubDate></item><item><title>智能单元专栏目录</title><link>https://zhuanlan.zhihu.com/p/22339097</link><description>&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元 - 知乎专栏"&gt;智能单元 - 知乎专栏&lt;/a&gt;长期原创和翻译&lt;b&gt;深度学习和深度增强学习&lt;/b&gt;等领域高质量文章，并接受知友投稿。&lt;h2&gt;最前沿系列&lt;/h2&gt;&lt;blockquote&gt;最前沿系列解读我们认为的深度学习领域有巨大影响的论文和成果。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21320865?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：史蒂夫的人工智能大挑战 - 智能单元 - 知乎专栏"&gt;最前沿：史蒂夫的人工智能大挑战&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21362413?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：让计算机学会学习Let Computers Learn to Learn - 智能单元 - 知乎专栏"&gt;最前沿：让计算机学会学习Let Computers Learn to Learn&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21470871?refer=intelligentunit" data-editable="true" data-title="最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能" class=""&gt;最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能&lt;/a&gt;！&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22143664?refer=intelligentunit" data-editable="true" data-title="最前沿：深度学习训练方法大革新，反向传播训练不再唯一 - 智能单元 - 知乎专栏"&gt;最前沿：深度学习训练方法大革新，反向传播训练不再唯一 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22523121?refer=intelligentunit" data-editable="true" data-title="最前沿：深度增强学习再发力，家用机器人已近在眼前 - 智能单元 - 知乎专栏"&gt;最前沿：深度增强学习再发力，家用机器人已近在眼前&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;CS231n课程翻译系列&lt;/h2&gt;&lt;blockquote&gt;斯坦度CS231n：面向视觉识别的卷积神经网络，入门深度学习利器。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" class="" data-editable="true" data-title="贺完结！CS231n官方笔记授权翻译总集篇发布"&gt;贺完结！CS231n官方笔记授权翻译总集篇发布&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" class="" data-editable="true" data-title="获得授权翻译斯坦福CS231n课程笔记系列"&gt;获得授权翻译斯坦福CS231n课程笔记系列&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：Python Numpy教程" class=""&gt;CS231n课程笔记翻译：Python Numpy教程&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：图像分类笔记（上）" class=""&gt;CS231n课程笔记翻译：图像分类笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：图像分类笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：图像分类笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（上） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：线性分类笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（中）" class=""&gt;CS231n课程笔记翻译：线性分类笔记（中）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：线性分类笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" class="" data-editable="true" data-title="知友智靖远关于CS231n课程字幕翻译的倡议 "&gt;知友智靖远关于CS231n课程字幕翻译的倡议 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：最优化笔记（上）" class=""&gt;CS231n课程笔记翻译：最优化笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：最优化笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：最优化笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：反向传播笔记 - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：反向传播笔记 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 1简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记1（上）" class=""&gt;CS231n课程笔记翻译：神经网络笔记1（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记1（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：神经网络笔记1（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记 2 - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：神经网络笔记 2 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记3（上）" class=""&gt;CS231n课程笔记翻译：神经网络笔记3（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记3（下）" class=""&gt;CS231n课程笔记翻译：神经网络笔记3（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 2简介 " class=""&gt;斯坦福CS231n课程作业# 2简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：卷积神经网络笔记 - 猴子的文章 - 知乎专栏"&gt;CS231n课程笔记翻译：卷积神经网络笔记 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 3简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22282421?refer=intelligentunit" data-editable="true" data-title="Andrej Karpathy的回信和Quora活动邀请 - 智能单元 - 知乎专栏"&gt;Andrej Karpathy的回信和Quora活动邀请&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22232836?refer=intelligentunit" data-editable="true" data-title="知行合一码作业，深度学习真入门 - 智能单元 - 知乎专栏"&gt;知行合一码作业，深度学习真入门 &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;深度增强学习系列&lt;/h2&gt;&lt;blockquote&gt;分享深度增强学习相关的资料，文章，代码及教程。&lt;/blockquote&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20885568?refer=intelligentunit" class="" data-editable="true" data-title="Deep Reinforcement Learning 深度增强学习资源 (持续更新） - 智能单元 - 知乎专栏"&gt;Deep Reinforcement Learning 深度增强学习资源 (持续更新） &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20893777?refer=intelligentunit" data-editable="true" data-title="深度解读AlphaGo - 智能单元 - 知乎专栏"&gt;深度解读AlphaGo &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20924929?refer=intelligentunit" data-editable="true" data-title="从OpenAI看深度学习研究前沿 - 智能单元 - 知乎专栏"&gt;从OpenAI看深度学习研究前沿 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃1 DQN与增强学习 - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃1 DQN与增强学习&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21263408?refer=intelligentunit" data-editable="true" data-title="你是这样获取人工智能AI前沿信息的吗？ - 智能单元 - 知乎专栏"&gt;你是这样获取人工智能AI前沿信息的吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21296798?refer=intelligentunit" class="" data-editable="true" data-title="解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？ - 智能单元 - 知乎专栏"&gt;解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃2 增强学习与MDP - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃2 增强学习与MDP &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21340755?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃3 价值函数与Bellman方程 " class=""&gt;DQN 从入门到放弃3 价值函数与Bellman方程 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit" class="" data-editable="true" data-title="DQN 从入门到放弃4 动态规划与Q-Learning - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃4 动态规划与Q-Learning &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21421729?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃5 深度解读DQN算法 - 智能单元 - 知乎专栏"&gt;DQN从入门到放弃5 深度解读DQN算法 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21434933?refer=intelligentunit" data-editable="true" data-title="DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym - 智能单元 - 知乎专栏"&gt;DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21477488?refer=intelligentunit" data-editable="true" data-title="150行代码实现DQN算法玩CartPole " class=""&gt;150行代码实现DQN算法玩CartPole &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21547911?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃6 DQN的各种改进 - 智能单元 - 知乎专栏"&gt;DQN从入门到放弃6 DQN的各种改进 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21609472?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃7 连续控制DQN算法-NAF " class=""&gt;DQN从入门到放弃7  连续控制DQN算法-NAF &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21725498?refer=intelligentunit" data-editable="true" data-title="深度增强学习之Policy Gradient方法1 - 智能单元 - 知乎专栏"&gt;深度增强学习之Policy Gradient方法1 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21369441?refer=intelligentunit" data-editable="true" data-title="深度增强学习暑期学校 PPT 详解1 - 智能单元 - 知乎专栏"&gt;深度增强学习暑期学校 PPT 详解1 &lt;/a&gt;&lt;/li&gt;&lt;h2&gt;原创系列&lt;/h2&gt;&lt;blockquote&gt;共同学习的翻译小组奉上值得一翻的文章。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit" data-editable="true" data-title="[原创翻译]循环神经网络惊人的有效性（上）" class=""&gt;[原创翻译]循环神经网络惊人的有效性（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit" data-editable="true" data-title="[原创翻译]循环神经网络惊人的有效性（下）" class=""&gt;[原创翻译]循环神经网络惊人的有效性（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22408033?refer=intelligentunit" data-editable="true" data-title="看图说话的AI小朋友——图像标注趣谈（上） - 智能单元 - 知乎专栏"&gt;看图说话的AI小朋友——图像标注趣谈（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22520434?refer=intelligentunit" data-editable="true" data-title="看图说话的AI小朋友——图像标注趣谈（下） - 智能单元 - 知乎专栏" class=""&gt;看图说话的AI小朋友——图像标注趣谈（下）&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;知友投稿系列&lt;/h2&gt;&lt;blockquote&gt;欢迎知友投稿，请看投稿说明。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21917736?refer=intelligentunit" data-editable="true" data-title="智能单元专栏投稿说明 - 智能单元 - 知乎专栏"&gt;智能单元专栏投稿说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21341440?refer=intelligentunit" class="" data-editable="true" data-title="「无中生有」计算机视觉探奇 - 欲穷千里目 - 知乎专栏"&gt;「无中生有」计算机视觉探奇 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22308032?refer=intelligentunit" data-editable="true" data-title="关于图像语义分割的总结和感悟 - 困兽的文章 - 知乎专栏"&gt;关于图像语义分割的总结和感悟 &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;END&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22339097&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Sun, 11 Sep 2016 21:13:02 GMT</pubDate></item><item><title>深度增强学习暑期学校 PPT 详解1</title><link>https://zhuanlan.zhihu.com/p/21369441</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e6bbfe6ccf2febd9efdd67ab1ebf9b8f.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;p&gt;这是2016年机器学习暑期学校的课程，OpenAI的John Schulman做了4次Deep Reinforcement Learning的讲座，讲座的具体链接如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=aUrX-rP_ss4" data-editable="true" data-title="Lecture 1"&gt;Lecture 1&lt;/a&gt;: intro, derivative free optimization&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=oPGVsoBonLM" data-editable="true" data-title="Lecture 2"&gt;Lecture 2&lt;/a&gt;: score function gradient estimation and policy gradients&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=rO7Dx8pSJQw" data-editable="true" data-title="Lecture 3"&gt;Lecture 3&lt;/a&gt;: actor critic methods&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=gb5Q2XL5c8A" data-editable="true" data-title="Lecture 4"&gt;Lecture 4&lt;/a&gt;: trust region and natural gradient methods, open problems&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文基于其讲座的&lt;a href="https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf" data-editable="true" data-title="ppt"&gt;ppt&lt;/a&gt;及视频，对讲座的内容按照ppt的顺序做一些翻译，分析和理解。&lt;/p&gt;&lt;p&gt;课程的实验材料在这里：&lt;a href="https://goo.gl/5wsgbJ"&gt;https://goo.gl/5wsgbJ&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/28f14d7ec3989c6db1061721a191cd20.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;这个课程偏向于介绍Policy Gradient方法，从Andrej Karparthy的blog &lt;a href="http://karpathy.github.io/2016/05/31/rl/" class="" data-editable="true" data-title="Deep Reinforcement Learning: Pong from Pixels"&gt;Deep Reinforcement Learning: Pong from Pixels&lt;/a&gt;上说，Policy Gradient是目前更侧重的深度增强学习的方法。那么本身John Schulman最重要的工作就是TPRO，一个基于Policy Gradient的改进算法。这个算法目前在OpenAI Gym的表现非常好。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2c3e9b1ea2e51e37dc4280c87218cc48.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;首先是整体介绍部分&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2c8117dfc0b1805a390e728588651cc5.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;什么是增强学习？&lt;/h2&gt;&lt;p&gt;关于什么是增强学习在我们专栏中已经有详细的介绍&lt;a href="https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit" class=""&gt;https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit&lt;/a&gt;&lt;/p&gt;&lt;p&gt;增强学习是机器学习的一个分支，不同于监督学习和无监督学习，它主要侧重于如何输出序列动作。也就是增强学习关注决策与控制。那么最基本的问题描述就是一个智能体（Agent）在未知的环境中，如何根据观察（Observation)和反馈（Reward）来调整自己行为（Action），从而使累积的反馈（Reward）最大化。注意这里说的是累积的反馈，而不是单一行为之后的反馈。这很好理解。举炒股票为例，我们一个买入卖出动作之后，当天的收益也就是Reward，但是我们看重的是比如一周之后的收益是怎样的。这就需要累积每一天的Reward。我们的目标就是希望未来的最终收益最大化。这就是增强学习要研究的问题。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/699c37bd7b0aad686021bee62c7a62e9.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;增强学习有什么用？&lt;/h2&gt;&lt;p&gt;因为增强学习的重要在于解决决策与控制，因此机器人的控制显然是首当其冲了。那么怎么用增强学习的模型来描述机器人控制的问题呢&lt;/p&gt;&lt;ul&gt;&lt;li&gt;观察Observations：摄像头的图像，机械臂关节的角度，，&lt;/li&gt;&lt;li&gt;动作Actions：就是每个电机的输出扭矩，如果是机械臂那么就是对应每个关节的扭矩啦&lt;/li&gt;&lt;li&gt;反馈Rewards：有多种形式，主要就是看是什么任务了。比如保持平衡，移动到某个特定位置，或者服务及保护人类等等&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2cc0a9440ef8aea3174cac9cdb9dae75.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;增强学习在商业上的问题&lt;/h2&gt;&lt;p&gt;既然涉及到决策，那么商业决策是显然值得研究的。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;库存管理。对于一个超市来说，如何管理库存是很需要研究的，最好的情况就是所有的货物都不会积压，都正常卖出，不出现损失。这就需要考虑每个时间段如何根据当前库存购买新的货物的问题。在这个问题上，观察就是当前的库存情况，动作就是购买的每一种货物的数量，反馈就是最后的收益。&lt;/li&gt;&lt;li&gt;资源分配。比如当前有一定量的资金，那么有很多投资选择，如何分配这些投资才能使投资收益最大化呢？观察就是当前的投资及资金分配情况，动作就是分配资金，反馈就是收益。&lt;/li&gt;&lt;li&gt;运输问题。比如滴滴的算法大赛问题，就是希望能够估计出不同地区的顾客需求，从而更好的调配出租车的位置。比赛的问题只是一个预测问题。但是如何直接调配出租车那么就是一个增强学习问题了。&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/827f9d3f0efc75c00633cf33abe89eb9.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;游戏上的应用&lt;/h2&gt;&lt;p&gt;因为AlphaGo的出现，大家对此就比较了解了。很多游戏都是RL的问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;围棋。完全的信息，确定性决策（就是每次只有一个确定性的选择）。&lt;/li&gt;&lt;li&gt;Backgammon, 西洋双陆棋：图片引用自&lt;a href="http://imag.juegosdb.com/blog/images/2013/51/backgammon.jpg"&gt;http://imag.juegosdb.com/blog/images/2013/51/backgammon.jpg&lt;/a&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0982f2efcb811acbf6e6c6b5eb6367ed.jpg" data-rawwidth="537" data-rawheight="302"&gt;这种游戏也是完全信息的，只不过需要掷骰子，因此存在随机性Stochastic。这个游戏有个著名的算法叫TD-Gammon，很早了，用了简单的神经网络实现。也就是深度增强学习其实也很有历史啦。&lt;/li&gt;&lt;li&gt;Stragego 西洋陆军棋：图片引用自&lt;a href="https://staticdelivery.nexusmods.com/mods/461/images/152-1-1398654075.jpg" class=""&gt;https://staticdelivery.nexusmods.com/mods/461/images/152-1-1398654075.jpg&lt;/a&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/36953ccb54e20f5b772517e998eaa772.jpg" data-rawwidth="500" data-rawheight="193"&gt;这种棋和我们的军棋差不多，也是下暗棋，所以是不完全信息博弈，但它的每一步是确定性的。&lt;/li&gt;&lt;li&gt;Poker 扑克，特别是德州扑克：图片引用自&lt;a href="https://i.ytimg.com/vi/oa109hO4ZxQ/maxresdefault.jpg"&gt;https://i.ytimg.com/vi/oa109hO4ZxQ/maxresdefault.jpg&lt;/a&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2aa16c2358ed22727b0981914ccb40b9.jpg" data-rawwidth="1280" data-rawheight="720"&gt;对于这种游戏，显然是不完全信息博弈，并且每一个回合都存在随机性，大家都不知道发的什么牌。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9e674e4536155881c111beb5c06aa374.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;增强学习的解决方法&lt;/h2&gt;&lt;p&gt;那么从上面的ppt中可以看到基于RL的解决办法可以分成两大类：&lt;/p&gt;&lt;p&gt;1）策略优化Policy Optimization。就是直接优化策略&lt;equation&gt;\pi(s)&lt;/equation&gt;。这种方法又分为两种：一种称为进化Evolution方法，就是不断调整策略的参数，选择更好的参数。另一种就是策略梯度Policy Gradient。通过计算策略的梯度方向，通过梯度下降的方式来优化策略。&lt;/p&gt;&lt;p&gt;2）动态规划Dynamic Programming。这个方法就是利用价值函数Value Function来实现曲线救国。有策略迭代Policy Iteration和值迭代Value Iteration。最有名的就是Q-Learning了，DQN就是Deep Q-Learning。&lt;/p&gt;&lt;p&gt;除了上面的两种方法，第三种就综合上面两种方法形成所谓的Actor-Critic方法，也就是行动-监督方法。&lt;/p&gt;&lt;p&gt;其实，对于增强学习的问题，还有一个方法就是只使用监督学习，只是这种方法需要有现有的方法提供监督学习的样本。UC Berkerley其实这方面的成果是最多的，Guided Policy Search就属于这类监督学习的方法。如果你不是很明白，但是了解AlphaGo。AlphaGo一开始的监督学习就是利用人类棋谱做训练，这也可以达到不错的效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/9a8b778b70e8de7a30d99d73a62d450a.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;什么是深度增强学习Deep RL？&lt;/h2&gt;&lt;p&gt;深度增强学习就是将增强学习中的策略Policy或者价值函数Value Function用非线性函数来近似，然后显然用深度神经网络是最好的非线性函数。那么通常就是使用随机梯度下降的方式来更新里面的参数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/585fada39cb030ac8bdb4ee937be4e6a.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;这里将Deep RL研究的问题与人类大脑皮层的功能做对比。Deep RL类比为人类的前半部分大脑皮层，用于处理决策与控制。而大脑的后半部分则处理感知类比计算机视觉处理的功能。从这里的类比可见Deep RL研究的重要性。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/16f25e5b2a24860097e37d4c480bee30.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;接下来介绍马尔科夫决策过程&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c470410c8e52cf359c4b515d45689ebc.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;马尔科夫决策过程最基本的定义就是（S,A,P)，其中&lt;/p&gt;&lt;ul&gt;&lt;li&gt;S：状态空间&lt;/li&gt;&lt;li&gt;A：动作空间&lt;/li&gt;&lt;li&gt;&lt;equation&gt;P(r,s^`|s,a)&lt;/equation&gt;：状态转移的概率分布。也就是在当前s和动作a下，下一个时间片的状态s和反馈r会是怎样的，这通常用概率分布来表示。对于一些完全可观察的问题比如围棋，那么下一步的情况则是确定性的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;根据问题的设定常常会定义额外的对象：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;equation&gt;\mu &lt;/equation&gt;：初始的状态分布&lt;/li&gt;&lt;li&gt;&lt;equation&gt;\lambda &lt;/equation&gt;：衰减系数。就是反馈Reward一般根据时间作用越来越小，用该系数来表示。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/23416bdaeab43f925e7d1e09cdbf6310.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;篇章式设定&lt;/h2&gt;&lt;p&gt;在很多RL问题中，问题有一个欲达成的目标，也就是一个任务的时间长度是有限制的，是可以结束的，对于这种可以结束的问题，就是每运行一次实验就称为一次episode，依旧是从初始状态开始，直到最终的结束状态到达。比如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;的士机器人到达目的地（结束状态是好的）&lt;/li&gt;&lt;li&gt;侍者机器人完成一个移动（在有限时间内）&lt;/li&gt;&lt;li&gt;移动机器人摔倒（结束状态是坏的）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;还有其他的比如围棋就是到一盘棋下完，而玩Atari游戏就是到游戏结束。&lt;/p&gt;&lt;p&gt;那么这种问题的目标是比较好确定的，就是到任务结束时得到最大的累加反馈值。那么有没有一些问题是没有结束的呢？只能说可以有，但都可以转化为可以结束的情况。比如说机器蚂蚁走路，目标就是向前走。这个时候不管怎么样可以设定一个时间让其强制结束如果没有走到某个特定位置。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c8ef45d5301ade8e70a3f8289587a638.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;策略&lt;/h2&gt;&lt;p&gt;策略有两种方式表示，这个我们在之前的专栏文章中也说明过：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一种就是确定性策略：&lt;equation&gt;a=\pi(s)&lt;/equation&gt;，也就是有一个状态s，就对应一个动作a&lt;/li&gt;&lt;li&gt;另一种就是随机分布：&lt;equation&gt;a\sim \pi(a|s)&lt;/equation&gt;，也就是即使面对同一个状态s，也存在多种动作选择可能都是较优的，因此用概率来表示选择某一个动作的可能性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;那么深度增强学习就是要研究如何更新策略的参数来优化策略。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/163deaf9001b372d53d63f3a76b3802f.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;那么对于篇章式设定的问题，目标是非常简单可以设定的，就是累加反馈的期望值，如上面的公式表示。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/f55d107d818aa9ea3a36c297eaab1d5c.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;那么上面这个图可以更清楚的表示这个过程。大家要记住这一点。就是在MDP的框架下，时间可分割，因此就是分割成一个一个的时间片。因此可以形成&lt;equation&gt;\{s_0,a_0,s_1,r_0,a_1,s_2,r_1....\}&lt;/equation&gt;这样的时间序列数据。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/65bfc3f75b5a8ef194b3c8985f3e4bde.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;参数化策略&lt;/h2&gt;&lt;p&gt;就是用参数来表示一个策略，比如用一个线性方程来表示策略，那么线性方程的参数就影响了这个策略。如果用&lt;equation&gt;\theta&lt;/equation&gt;来表示参数，那么参数化的策略就如下表示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;确定性策略：&lt;equation&gt;a=\pi(s,\theta)&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;随机性策略：&lt;equation&gt;\pi(a|s,\theta)&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;很多不是非常理解这个带参数的策略应该是如何构建，特别是使用神经网络。神经网络的输出可是分类Classification也可以是回归Regression，取决于动作是连续还是离散。比如Atari游戏，输出就是几个离散的动作，那么就可以使用分类的神经网络。输入的是图像，输出的是几个动作的概率，这就可以用softmax来输出。那么如果是连续的动作空间，比如机器人控制基本都是连续动作，那么可以使用regression回归神经网络。网络输出可以是高斯分布的平均值及对角协方差。关于这一点在后面的CEM算法中可以有清晰的理解。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8c6ca3d85bff3a391d6c0c85436b94a2.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;接下来就介绍通过黑盒优化来求解增强学习问题&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/0db14a0d35f9cccd994c1efd1167fd02.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;无需导数的优化方法&lt;/h2&gt;&lt;p&gt;说到优化我们首先会想到梯度下降，但是其实有很多其他的优化的方法并不使用梯度下降。比如模拟退火，爬山法，遗传算法等等。那么这里我们就把策略Policy看做是一个黑盒，我们的目标很明确，就是让期望的反馈最大化。除了R之外，其他的信息我们都不管。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d386a8e117d4d71871cd8cd7e563e212.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;交叉熵方法Cross Entropy Method&lt;/h2&gt;&lt;p&gt;一种进化算法。但是却取得了很不错的效果。ppt列出了较早的将CEM方法应用到增强学习的论文。用一种带噪声的交叉熵方法。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/88973c5434a0c6e78b81360166170168.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;一个和交叉熵类似的方法叫做Covariance Matrix Adaption，简称CMA。在图形学上已经成为了一种标准算法。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/02dec73f0fd570dd5b171813536802e5.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;交叉熵方法&lt;/h2&gt;&lt;p&gt;这里具体介绍交叉熵的方法。可以用很简单的一句话来说明：&lt;b&gt;就是每次根据当前的策略参数采样多组带噪声的新的参数，然后进行试验，选择反馈R最好的前几组参数，然后取平均作为新的参数&lt;/b&gt;。这种方法其实就是一种贪婪算法，通过纯随机的方式来寻找最佳的参数。按道理这种方法不应该和梯度下降的方法相提并论，但没想到CEM方法出奇的好。就是即使这个策略用巨大的神经网络来表示，也同样能够取得非常不错的效果。&lt;/p&gt;&lt;p&gt;有些知友可能看不明白上面的算法中的高斯分布是啥意思，其实就是选择中的几组参数的平均值就是高斯分布的平均值，而方差就是几组参数的方差。方差和平均值都用于之后再产生新的参数。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/524e1b9750dacd213f8d5923e1957683.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;CEM方法的分析&lt;/p&gt;&lt;p&gt;这边作者也没有深入的分析，只是提了几句相关的算法。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/aeb4af1189a6016f930b83a4c229a5fa.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;策略梯度方法&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/aad9dced603a37b65bce6b8abbb8712c.jpg" data-rawwidth="1511" data-rawheight="1133"&gt;&lt;h2&gt;策略梯度方法&lt;/h2&gt;&lt;p&gt;策略梯度方法的目标是和上面的CEM方法是一样的，就是最大化期望的反馈R。那么一个很直观的做法就是不断试验，采集各种状态动作数据，也就是所谓的轨迹trajectory。那么有了样本，就有得做了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一种是让好的轨迹出现可能性增大&lt;/li&gt;&lt;li&gt;第二种是让好的动作出现可能性增大（actor-critic方法，GAE方法）&lt;/li&gt;&lt;li&gt;第三种是让动作趋向于好的动作（DPG，SVG方法）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;那么这三种有什么不一样呢？第一种就是要求一系列的动作都好，比如某一次实验比较好，那么这次实验的每一个动作的可能性都增大。第二种就是只针对单一的动作。如果某一个动作的评估比较好，那么让其出现可能性增大。第三种的目标是让动作趋向好的动作，也就是有一个好的动作的目标，比如DPG使用Q值，目标就是让动作的Q值增大。那就得到好的动作了。&lt;/p&gt;&lt;p&gt;第一部分先到这里。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21369441&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Sun, 11 Sep 2016 15:25:05 GMT</pubDate></item><item><title>关于图像语义分割的总结和感悟</title><link>https://zhuanlan.zhihu.com/p/22308032</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/cb5e078e5008907cb04b300369b7d621_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;前言 &lt;/h2&gt;&lt;p&gt;
(呕血制作啊！)前几天刚好做了个图像语义分割的汇报，把最近看的论文和一些想法讲了一下。所以今天就把它总结成文章啦，方便大家一起讨论讨论。本文只是展示了一些比较经典和自己觉得比较不错的结构，毕竟这方面还是有挺多的结构方法了。&lt;/p&gt;&lt;h2&gt;介绍 &lt;/h2&gt;&lt;blockquote&gt;&lt;b&gt;图像语义分割&lt;/b&gt;，简单而言就是给定一张图片，对图片上的每一个像素点分类&lt;/blockquote&gt;

从图像上来看，就是我们需要将实际的场景图分割成下面的分割图： &lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/cb5e078e5008907cb04b300369b7d621.jpg" data-rawwidth="738" data-rawheight="472"&gt;不同颜色代表不同类别。

经过我阅读“大量”论文（羞涩）和查看&lt;a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;amp;compid=6" data-editable="true" data-title="PASCAL VOC Challenge performance evaluation server" class=""&gt;PASCAL VOC Challenge performance evaluation server&lt;/a&gt;，我发现图像语义分割从深度学习引入这个任务（FCN）到现在而言，一个通用的框架已经大概确定了。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/3adeadf2a20b0cc9cd68553a95f00552.png" data-rawwidth="1963" data-rawheight="781"&gt;&lt;ul&gt;&lt;li&gt;FCN-全卷积网络 &lt;/li&gt;&lt;li&gt;CRF-条件随机场 &lt;/li&gt;&lt;li&gt;MRF-马尔科夫随机场 &lt;/li&gt;&lt;/ul&gt;
前端使用FCN进行特征粗提取，后端使用CRF/MRF优化前端的输出，最后得到分割图。 &lt;p&gt;
接下来，我会从前端和后端两部分进行总结。&lt;/p&gt;&lt;h2&gt;前端 &lt;/h2&gt;&lt;h2&gt;为什么需要FCN？&lt;/h2&gt;&lt;p&gt;我们分类使用的网络通常会在最后连接几层全连接层，它会将原来二维的矩阵（图片）压扁成一维的，从而丢失了空间信息，最后训练输出一个标量，这就是我们的分类标签。 &lt;/p&gt;&lt;p&gt;
而图像语义分割的输出需要是个分割图，且不论尺寸大小，但是至少是二维的。所以，我们需要丢弃全连接层，换上全卷积层，而这就是全卷积网络了。具体定义请参看论文：&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-title="Fully Convolutional Networks for Semantic Segmentation" class="" data-editable="true"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;前端结构&lt;/h2&gt;&lt;h2&gt;FCN &lt;/h2&gt;&lt;p&gt;此处的FCN特指&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-title="Fully Convolutional Networks for Semantic Segmentation" class="" data-editable="true"&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;论文中提出的结构，而非广义的全卷积网络。 &lt;/p&gt;&lt;p&gt;
作者的FCN主要使用了三种技术：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;卷积化（Convolutional） &lt;/li&gt;&lt;li&gt;上采样（Upsample） &lt;/li&gt;&lt;li&gt;跳跃结构（Skip Layer） &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;卷积化 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/42d85c5f7ddcb3f527666b250f62f5d6.png" data-rawwidth="630" data-rawheight="355"&gt;&lt;p&gt;&lt;b&gt;上采样 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;此处的上采样即是反卷积（Deconvolution）。当然关于这个名字不同框架不同，Caffe和Kera里叫Deconvolution，而tensorflow里叫conv_transpose。CS231n这门课中说，叫conv_transpose更为合适。 &lt;/p&gt;&lt;p&gt;
众所诸知，普通的池化（为什么这儿是普通的池化请看后文）会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。 &lt;/p&gt;&lt;p&gt;
反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。图解如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/c18522f52e930a3f83748a73a829f0ad.jpg" data-rawwidth="403" data-rawheight="138"&gt;&lt;p&gt;但是，虽然文中说是可学习的反卷积，但是作者实际代码并没有让它学习，可能正是因为这个一对多的逻辑关系。代码如下： &lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;layer {
  name: "upscore"
  type: "Deconvolution"
  bottom: "score_fr"
  top: "upscore"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 21
    bias_term: false
    kernel_size: 64
    stride: 32
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到lr_mult被设置为了0. &lt;/p&gt;&lt;p&gt;&lt;b&gt;跳跃结构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;（这个奇怪的名字是我翻译的，好像一般叫忽略连接结构）这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。具体结构如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/ccb6dd0a7f207134ae7690974c3e88a5.png" data-rawwidth="1254" data-rawheight="864"&gt;&lt;p&gt;而不同上采样结构得到的结果对比如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5df8b118a7f77a2222d343a852b46034.png" data-rawwidth="594" data-rawheight="250"&gt;&lt;p&gt;当然，你也可以将pool1， pool2的输出再上采样输出。不过，作者说了这样得到的结果提升并不大。 &lt;/p&gt;&lt;p&gt;这是第一种结构，也是深度学习应用于图像语义分割的开山之作，所以得了CVPR2015的最佳论文。但是，还是有一些处理比较粗糙的地方，具体和后面对比就知道了。 &lt;/p&gt;&lt;h2&gt;SegNet/DeconvNet &lt;/h2&gt;&lt;p&gt;这样的结构总结在这儿，只是我觉得结构上比较优雅，它得到的结果不一定比上一种好。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;SegNet &lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/6cab0e3643d16ccab0a1bf1909813484.png" data-rawwidth="877" data-rawheight="250"&gt;&lt;p&gt;&lt;b&gt;DeconvNet&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/99f62dbfe0e39aea5674deeaa2d8363d.jpg" data-rawwidth="828" data-rawheight="313"&gt;&lt;p&gt;这样的对称结构有种自编码器的感觉在里面，先编码再解码。这样的结构主要使用了反卷积和上池化。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/c18522f52e930a3f83748a73a829f0ad.jpg" data-rawwidth="403" data-rawheight="138"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f2da827523ecaa6b8c96b73464ba4c5c.jpg" data-rawwidth="411" data-rawheight="130"&gt;&lt;p&gt;
反卷积如上。而上池化的实现主要在于池化时记住输出值的位置，在上池化时再将这个值填回原来的位置，其他位置填0即OK。 &lt;/p&gt;&lt;h2&gt;DeepLab &lt;/h2&gt;&lt;p&gt;
接下来介绍一个很成熟优雅的结构，以至于现在的很多改进是基于这个网络结构的进行的。&lt;/p&gt;&lt;p&gt;

首先这里我们将指出一个第一个结构FCN的粗糙之处：为了保证之后输出的尺寸不至于太小，FCN的作者在第一层直接对原图加了100的padding，可想而知，这会引入噪声。 &lt;/p&gt;&lt;p&gt;
而怎样才能保证输出的尺寸不会太小而又不会产生加100 padding这样的做法呢？可能有人会说减少池化层不就行了，这样理论上是可以的，但是这样直接就改变了原先可用的结构了，而且最重要的一点是就不能用以前的结构参数进行fine-tune了。所以，Deeplab这里使用了一个非常优雅的做法：将pooling的stride改为1，再加上 1 padding。这样池化后的图片尺寸并未减小，并且依然保留了池化整合特征的特性。 &lt;/p&gt;&lt;p&gt;
但是，事情还没完。因为池化层变了，后面的卷积的感受野也对应的改变了，这样也不能进行fine-tune了。所以，Deeplab提出了一种新的卷积，带孔的卷积：Atrous Convolution.即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2ec8009452f89b7bbd9ecd519fc3e3ae.png" data-rawwidth="865" data-rawheight="543"&gt;&lt;p&gt;而具体的感受野变化如下： &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/766fc04b86b72f7e09d8f8ff6cb648e2.png" data-rawwidth="1147" data-rawheight="711"&gt;a为普通的池化的结果，b为“优雅”池化的结果。我们设想在a上进行卷积核尺寸为3的普通卷积，则对应的感受野大小为7.而在b上进行同样的操作，对应的感受野变为了5.感受野减小了。但是如果使用hole为1的Atrous Convolution则感受野依然为7.&lt;/p&gt;&lt;p&gt;所以，Atrous Convolution能够保证这样的池化后的感受野不变，从而可以fine tune，同时也能保证输出的结果更加精细。即： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/39577b54b8b53802020cab6da6f9e334.png" data-rawwidth="562" data-rawheight="325"&gt;&lt;p&gt;&lt;b&gt;总结 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;
这里介绍了三种结构：FCN, SegNet/DeconvNet，DeepLab。当然还有一些其他的结构方法，比如有用RNN来做的，还有更有实际意义的weakly-supervised方法等等。 &lt;/p&gt;&lt;h2&gt;后端 &lt;/h2&gt;&lt;p&gt;
终于到后端了，后端这里会讲几个场，涉及到一些数学的东西。我的理解也不是特别深刻，所以欢迎吐槽。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全连接条件随机场(DenseCRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于每个像素&lt;equation&gt;i&lt;/equation&gt;具有类别标签&lt;equation&gt;x_i&lt;/equation&gt;还有对应的观测值&lt;equation&gt;y_i&lt;/equation&gt;，这样每个像素点作为节点，像素与像素间的关系作为边，即构成了一个条件随机场。而且我们通过观测变量&lt;equation&gt;y_i&lt;/equation&gt;来推测像素&lt;equation&gt;i&lt;/equation&gt;对应的类别标签&lt;equation&gt;x_i&lt;/equation&gt;。条件随机场如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/eb0015ceb7aac30d571cd90a47d9e22d.png" data-rawwidth="282" data-rawheight="221"&gt;&lt;p&gt;条件随机场符合吉布斯分布：(此处的&lt;equation&gt;x&lt;/equation&gt;即上面说的观测值) &lt;/p&gt;&lt;equation&gt;P(\mathbf{X=x|I})=\frac{1}{Z(\mathbf{I})}\exp(-E(\mathbf{x|I}))&lt;/equation&gt;&lt;p&gt;其中的&lt;equation&gt;E(\mathbf{x|I})&lt;/equation&gt;是能量函数，为了简便，以下省略全局观测&lt;equation&gt;\mathbf{I}&lt;/equation&gt;： &lt;/p&gt;&lt;equation&gt;E(\mathbf{x})=\sum_i{\Psi_u(x_i)}+\sum_{i&amp;lt;j}\Psi_p(x_i, x_j)&lt;/equation&gt;&lt;p&gt; 其中的一元势函数&lt;equation&gt;\sum_i{\Psi_u(x_i)}&lt;/equation&gt;即来自于前端FCN的输出。而二元势函数如下： &lt;/p&gt;&lt;equation&gt;\Psi_p(x_i, x_j)=u(x_i, x_j)\sum_{m=1}^M{\omega^{(m)}k_G^{(m)}(\mathbf{f_i, f_j)}}&lt;/equation&gt;&lt;p&gt;
二元势函数就是描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片尽量在边界处分割。&lt;/p&gt;&lt;p&gt;而全连接条件随机场的不同就在于，二元势函数描述的是每一个像素与其他所有像素的关系，所以叫“全连接”。 &lt;/p&gt;&lt;p&gt;
关于这一堆公式大家随意理解一下吧... ...而直接计算这些公式是比较麻烦的（我想也麻烦），所以一般会使用平均场近似方法进行计算。而平均场近似又是一堆公式，这里我就不给出了（我想大家也不太愿意看），愿意了解的同学直接看论文吧。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;CRFasRNN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最开始使用DenseCRF是直接加在FCN的输出后面，可想这样是比较粗糙的。而且在深度学习中，我们都追求end-to-end的系统，所以CRFasRNN这篇文章将DenseCRF真正结合进了FCN中。&lt;/p&gt;&lt;p&gt;这篇文章也使用了平均场近似的方法，因为分解的每一步都是一些相乘相加的计算，和普通的加减（具体公式还是看论文吧），所以可以方便的把每一步描述成一层类似卷积的计算。这样即可结合进神经网络中，并且前后向传播也不存在问题。&lt;/p&gt;&lt;p&gt;当然，这里作者还将它进行了迭代，不同次数的迭代得到的结果优化程度也不同（一般取10以内的迭代次数），所以文章才说是as RNN。优化结果如下： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/40fb43fd1cc813501ed803224814f2ed.jpg" data-rawwidth="578" data-rawheight="323"&gt;&lt;p&gt;&lt;b&gt;马尔科夫随机场(MRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;在Deep Parsing Network中使用的是MRF，它的公式具体的定义和CRF类似，只不过作者对二元势函数进行了修改：&lt;/p&gt;&lt;equation&gt;\Psi(y_i^u, y_i^v)=\sum_{k=1}^K\lambda_ku_k(i, u, j, v)\sum_{\forall{z\in{N_j}}}d(j, z)p_z^v&lt;/equation&gt;&lt;p&gt;其中，作者加入的&lt;equation&gt;\lambda_k&lt;/equation&gt;为label context，因为&lt;equation&gt;u_k&lt;/equation&gt;只是定义了两个像素同时出现的频率，而&lt;equation&gt;\lambda_k&lt;/equation&gt;可以对一些情况进行惩罚，比如，人可能在桌子旁边，但是在桌子下面的可能性就更小一些。所以这个量可以学习不同情况出现的概率。而原来的距离&lt;equation&gt;d(i,j)&lt;/equation&gt;只定义了两个像素间的关系，作者在这儿加入了个triple penalty，即还引入了&lt;equation&gt;j&lt;/equation&gt;附近的&lt;equation&gt;z&lt;/equation&gt;，这样描述三方关系便于得到更充足的局部上下文。具体结构如下： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/fff5f41eacdcf624556b6ce0f069600d.jpg" data-rawwidth="641" data-rawheight="307"&gt;&lt;p&gt;这个结构的&lt;b&gt;优点&lt;/b&gt;在于： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;将平均场构造成了CNN &lt;/li&gt;&lt;li&gt;联合训练并且可以one-pass inference，而不用迭代&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;高斯条件随机场(G-CRF) &lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个结构使用CNN分别来学习一元势函数和二元势函数。这样的结构是我们更喜欢的： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/920bc48dd3702825b71b0cde4b1f9e5a.jpg" data-rawwidth="1132" data-rawheight="595"&gt;&lt;p&gt;而此中的能量函数又不同于之前： &lt;/p&gt;&lt;equation&gt;E(\mathbf{x})=\frac{1}{2}\mathbf{x}^T(\mathbf{A+\lambda I)x}-\mathbf{Bx}&lt;/equation&gt;&lt;p&gt;而当&lt;equation&gt;(\mathbf{A+\lambda I)}&lt;/equation&gt;是对称正定时，求&lt;equation&gt;E(\mathbf{x})&lt;/equation&gt;的最小值等于求解： &lt;/p&gt;&lt;equation&gt;(\mathbf{A+\lambda I)x}=\mathbf{B}&lt;/equation&gt;&lt;p&gt;而G-CRF的优点在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;二次能量有明确全局 &lt;/li&gt;&lt;li&gt;解线性简便很多 &lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;感悟 &lt;/h2&gt;&lt;ul&gt;&lt;li&gt;FCN更像一种技巧。随着基本网络（如VGG， ResNet）性能的提升而不断进步。 &lt;/li&gt;&lt;li&gt;深度学习+概率图模型（PGM）是一种趋势。其实DL说白了就是进行特征提取，而PGM能够从数学理论很好的解释事物本质间的联系。 &lt;/li&gt;&lt;li&gt;概率图模型的网络化。因为PGM通常不太方便加入DL的模型中，将PGM网络化后能够是PGM参数自学习，同时构成end-to-end的系统。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;


完结撒花&lt;/p&gt;&lt;h2&gt;引用 &lt;/h2&gt;&lt;p&gt;[1]&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" data-editable="true" data-title="Fully Convolutional Networks for Semantic Segmentation" class=""&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2]&lt;a href="http://arxiv.org/abs/1505.04366)%20%5B3%5D%5BSegNet%5D(http://arxiv.org/abs/1511.00561?context=cs" data-title="Learning Deconvolution Network for Semantic Segmentation" class="" data-editable="true"&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3]&lt;a href="http://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf" data-editable="true" data-title="Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"&gt;Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4]&lt;a href="http://arxiv.org/abs/1412.7062" data-title="Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs" class="" data-editable="true"&gt;Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5]&lt;a href="http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf" data-title="Conditional Random Fields as Recurrent Neural Networks" class="" data-editable="true"&gt;Conditional Random Fields as Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6]&lt;a href="http://liangchiehchen.com/projects/DeepLab.html" data-title="DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs" class="" data-editable="true"&gt;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7]&lt;a href="https://www.researchgate.net/publication/281670742_Semantic_Image_Segmentation_via_Deep_Parsing_Network" data-editable="true" data-title="Semantic Image Segmentation via Deep Parsing Network"&gt;Semantic Image Segmentation via Deep Parsing Network&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[8]&lt;a href="http://arxiv.org/abs/1603.08358v1" data-title="Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs" class="" data-editable="true"&gt;Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[9]&lt;a href="http://arxiv.org/abs/1511.00561?context=cs" data-editable="true" data-title="SegNet"&gt;SegNet&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;转载须全文转载且注明作者和原文链接，否则保留维权权利&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22308032&amp;pixel&amp;useReferer"/&gt;</description><author>困兽</author><pubDate>Sun, 04 Sep 2016 21:55:08 GMT</pubDate></item><item><title>[原创翻译]循环神经网络惊人的有效性（上）</title><link>https://zhuanlan.zhihu.com/p/22107715</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/68ada38c72be590e4d3da1fbc7a26b65_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创翻译，禁止未授权转载。 &lt;/b&gt;&lt;blockquote&gt;&lt;b&gt;译者注：&lt;/b&gt;经知友推荐，将&lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-title="The Unreasonable Effectiveness of Recurrent Neural Networks" class="" data-editable="true"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;一文翻译作为CS231n课程无RNN和LSTM笔记的补充，感谢&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;的校对。&lt;/blockquote&gt;&lt;h2&gt;目录&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;循环神经网络&lt;/li&gt;&lt;li&gt;字母级别的语言模型&lt;/li&gt;&lt;li&gt;RNN的乐趣&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Paul Graham生成器&lt;/li&gt;&lt;li&gt;莎士比亚&lt;/li&gt;&lt;li&gt;维基百科&lt;/li&gt;&lt;li&gt;几何代数&lt;/li&gt;&lt;li&gt;Linux源码&lt;/li&gt;&lt;li&gt;生成婴儿姓名 &lt;i&gt;&lt;b&gt;译者注：上篇截止处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;理解训练过程&lt;/li&gt;&lt;ul&gt;&lt;li&gt;训练时输出文本的进化&lt;/li&gt;&lt;li&gt;RNN中的预测与神经元激活可视化&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;源代码&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;li&gt;结论&lt;/li&gt;&lt;li&gt;译者反馈&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;循环神经网络（RNN）简直像是魔法一样不可思议。我为&lt;a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-editable="true" data-title="图像标注"&gt;图像标注&lt;/a&gt;项目训练第一个循环网络时的情景到现在都还历历在目。当时才对第一个练手模型训练了十几分钟（超参数还都是随手设置的），它就开始生成一些对于图像的描述，描述内容看起来很不错，几乎让人感到语句是通顺的了。有时候你会遇到模型简单，结果的质量却远高预期的情况，这就是其中一次。当时这个结果让我非常惊讶是因为我本以为RNN是非常难以训练的（随着实践的增多，我的结论基本与之相反了）。让我们快进一年：即使现在我成天都在训练RNN，也常常看到它们的能力和鲁棒性，有时候它们那充满魔性的输出还是能够把我给逗乐。这篇博文就是来和你分享RNN中的一些魔法。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们将训练RNN，让它们生成一个又一个字母。同时好好思考这个问题：这怎么可能呢？&lt;/p&gt;&lt;/blockquote&gt;顺便说一句，和这篇博文一起，我在Github上发布了一个项目。项目基于多层的LSTM，使得你可以训练字母级别的语言模型。你可以输入一大段文本，然后它能学习并按照一次一个字母的方式生成文本。你也可以用它来复现我下面的实验。但是现在我们要超前一点：RNN到底是什么？&lt;h2&gt;循环神经网络&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;序列&lt;/strong&gt;。基于知识背景，你可能会思考：&lt;em&gt;是什么让RNN如此独特呢？&lt;/em&gt;普通神经网络和卷积神经网络的一个显而易见的局限就是他们的API都过于限制：他们接收一个固定尺寸的向量作为输入（比如一张图像），并且产生一个固定尺寸的向量作为输出（比如针对不同分类的概率）。不仅如此，这些模型甚至对于上述映射的演算操作的步骤也是固定的（比如模型中的层数）。RNN之所以如此让人兴奋，其核心原因在于其允许我们对向量的序列进行操作：输入可以是序列，输出也可以是序列，在最一般化的情况下输入输出都可以是序列。下面是一些直观的例子：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/2a37bd4e9b12bcc19e045eaf22fea4e5.jpg" data-rawwidth="1329" data-rawheight="416"&gt;&lt;p&gt;上图中每个正方形代表一个向量，箭头代表函数（比如矩阵乘法）。输入向量是红色，输出向量是蓝色，绿色向量装的是RNN的状态（马上具体介绍）。从左至右为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;非RNN的普通过程，从固定尺寸的输入到固定尺寸的输出（比如图像分类）。&lt;/li&gt;&lt;li&gt;输出是序列（例如图像标注：输入是一张图像，输出是单词的序列）。&lt;/li&gt;&lt;li&gt;输入是序列（例如情绪分析：输入是一个句子，输出是对句子属于正面还是负面情绪的分类）。&lt;/li&gt;&lt;li&gt;输入输出都是序列（比如机器翻译：RNN输入一个英文句子输出一个法文句子）。&lt;/li&gt;&lt;li&gt;同步的输入输出序列（比如视频分类中，我们将对视频的每一帧都打标签）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;注意在每个案例中都没有对序列的长度做出预先规定，这是因为循环变换（绿色部分）是固定的，我们想用几次就用几次。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;如你期望的那样，相较于那些从一开始连计算步骤的都定下的固定网络，序列体制的操作要强大得多。并且对于那些和我们一样希望构建一个更加智能的系统的人来说，这样的网络也更有吸引力。我们后面还会看到，RNN将其输入向量、状态向量和一个固定（可学习的）函数结合起来生成一个新的状态向量。在程序的语境中，这可以理解为运行一个具有某些输入和内部变量的固定程序。从这个角度看，RNN本质上就是在描述程序。实际上RNN是具备&lt;a href="http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf" data-editable="true" data-title="图灵完备性"&gt;图灵完备性&lt;/a&gt;的，只要有合适的权重，它们可以模拟任意的程序。然而就像神经网络的通用近似理论一样，你不用过于关注其中细节。实际上，我建议你忘了我刚才说过的话。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;如果训练普通神经网络是对函数做最优化，那么训练循环网络就是针对程序做最优化。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;无序列也能进行序列化处理&lt;/strong&gt;。你可能会想，将序列作为输入或输出的情况是相对少见的，但是需要认识到的重要一点是：即使输入或输出是固定尺寸的向量，依然可以使用这个强大的形式体系以序列化的方式对它们进行处理。例如，下图来自于&lt;a href="http://deepmind.com/" data-editable="true" data-title="DeepMind"&gt;DeepMind&lt;/a&gt;的两篇非常不错的论文。左侧动图显示的是一个算法学习到了一个循环网络的策略，该策略能够引导它对图像进行观察；更具体一些，就是它学会了如何从左往右地阅读建筑的门牌号（&lt;a href="http://arxiv.org/abs/1412.7755" data-editable="true" data-title="Ba et al"&gt;Ba et al&lt;/a&gt;）。右边动图显示的是一个循环网络通过学习序列化地向画布上添加颜色，生成了写有数字的图片（&lt;a href="http://arxiv.org/abs/1502.04623" data-editable="true" data-title="Gregor et al"&gt;Gregor et al&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/45a48feeee79755aa02789f4bf437e77.png" data-rawwidth="1274" data-rawheight="828"&gt;左边：RNN学会如何阅读建筑物门牌号。右边：RNN学会绘出建筑门牌号。 &lt;em&gt;译者注：知乎专栏不支持动图，建议感兴趣读者前往原文查看。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;————————————————————————————————————————&lt;/em&gt;&lt;/p&gt;&lt;p&gt;必须理解到的一点就是：即使数据不是序列的形式，仍然可以构建并训练出能够进行序列化处理数据的强大模型。换句话说，你是要让模型学习到一个处理固定尺寸数据的分阶段程序。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RNN的计算&lt;/strong&gt;。那么RNN到底是如何工作的呢？在其核心，RNN有一个貌似简单的API：它接收输入向量&lt;strong&gt;x&lt;/strong&gt;，返回输出向量&lt;strong&gt;y&lt;/strong&gt;。然而这个输出向量的内容不仅被输入数据影响，而且会收到整个历史输入的影响。写成一个类的话，RNN的API只包含了一个&lt;strong&gt;step&lt;/strong&gt;方法：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;rnn = RNN()
y = rnn.step(x) # x is an input vector, y is the RNN's output vector
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;每当&lt;strong&gt;step&lt;/strong&gt;方法被调用的时候，RNN的内部状态就被更新。在最简单情况下，该内部装着仅包含一个内部&lt;em&gt;隐向量&lt;/em&gt;&lt;i&gt;&lt;b&gt;h&lt;/b&gt;&lt;/i&gt;。下面是一个普通RNN的step方法的实现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;class RNN:
  # ...
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的代码详细说明了普通RNN的前向传播。该RNN的参数是三个矩阵：&lt;strong&gt;W_hh, W_xh, W_hy&lt;/strong&gt;。隐藏状态&lt;strong&gt;self.h&lt;/strong&gt;被初始化为零向量。&lt;strong&gt;np.tanh&lt;/strong&gt;函数是一个非线性函数，将激活数据挤压到[-1,1]之内。注意代码是如何工作的：在tanh内有两个部分。一个是基于前一个隐藏状态，另一个是基于当前的输入。在numpy中，&lt;strong&gt;np.dot&lt;/strong&gt;是进行矩阵乘法。两个中间变量相加，其结果被tanh处理为一个新的状态向量。如果你更喜欢用数学公式理解，那么公式是这样的：&lt;equation&gt;h_t=tanh(W_{hh}h_{t-1}+W_{hx}x_t)&lt;/equation&gt;。其中tanh是逐元素进行操作的。&lt;/p&gt;&lt;p&gt;我们使用随机数字来初始化RNN的矩阵，进行大量的训练工作来寻找那些能够产生描述行为的矩阵，使用一些损失函数来衡量描述的行为，这些损失函数代表了根据输入&lt;strong&gt;x&lt;/strong&gt;，你对于某些输出&lt;strong&gt;y&lt;/strong&gt;的偏好。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更深层网络&lt;/strong&gt;。RNN属于神经网络算法，如果你像叠薄饼一样开始对模型进行重叠来进行深度学习，那么算法的性能会单调上升（如果没出岔子的话）。例如，我们可以像下面代码一样构建一个2层的循环网络：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;y1 = rnn1.step(x)
y = rnn2.step(y1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;换句话说，我们分别有两个RNN：一个RNN接受输入向量，第二个RNN以第一个RNN的输出作为其输入。其实就RNN本身来说，它们并不在乎谁是谁的输入：都是向量的进进出出，都是在反向传播时梯度通过每个模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更好的网络&lt;/strong&gt;。需要简要指明的是在实践中通常使用的是一个稍有不同的算法，这就是我在前面提到过的&lt;em&gt;长短基记忆&lt;/em&gt;网络，简称LSTM。LSTM是循环网络的一种特别类型。由于其更加强大的更新方程和更好的动态反向传播机制，它在实践中效果要更好一些。本文不会进行细节介绍，但是在该算法中，所有本文介绍的关于RNN的内容都不会改变，唯一改变的是状态更新（就是&lt;strong&gt;self.h=...&lt;/strong&gt;那行代码）变得更加复杂。从这里开始，我会将术语RNN和LSTM混合使用，但是在本文中的所有实验都是用LSTM完成的。&lt;/p&gt;&lt;h2&gt;字母级别的语言模型&lt;/h2&gt;&lt;p&gt;现在我们已经理解了RNN是什么，它们何以令人兴奋，以及它们是如何工作的。现在通过一个有趣的应用来更深入地加以体会：我们将利用RNN训练一个字母级别的语言模型。也就是说，给RNN输入巨量的文本，然后让其建模并根据一个序列中的前一个字母，给出下一个字母的概率分布。这样就使得我们能够一个字母一个字母地生成新文本了。&lt;/p&gt;&lt;p&gt;在下面的例子中，假设我们的字母表只由4个字母组成“helo”，然后利用训练序列“hello”训练RNN。该训练序列实际上是由4个训练样本组成：1.当h为上文时，下文字母选择的概率应该是e最高。2.l应该是he的下文。3.l应该是hel文本的下文。4.o应该是hell文本的下文。&lt;/p&gt;&lt;p&gt;具体来说，我们将会把每个字母编码进一个1到k的向量（除对应字母为1外其余为0），然后利用&lt;strong&gt;step&lt;/strong&gt;方法一次一个地将其输入给RNN。随后将观察到4维向量的序列（一个字母一个维度）。我们将这些输出向量理解为RNN关于序列下一个字母预测的信心程度。下面是流程图：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/20c6a56f097aba3de796ac62c59605bc.jpg" data-rawwidth="902" data-rawheight="725"&gt;一个RNN的例子：输入输出是4维的层，隐层神经元数量是3个。该流程图展示了使用hell作为输入时，RNN中激活数据前向传播的过程。输出层包含的是RNN关于下一个字母选择的置信度（字母表是helo）。我们希望绿色数字大，红色数字小。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;举例如下：在第一步，RNN看到了字母h后，给出下一个字母的置信度分别是h为1，e为2.2，l为-3.0，o为4.1。因为在训练数据（字符串hello）中下一个正确的字母是e，所以我们希望提高它的置信度（绿色）并降低其他字母的置信度（红色）。类似的，在每一步都有一个目标字母，我们希望算法分配给该字母的置信度应该更大。因为RNN包含的整个操作都是可微分的，所以我们可以通过对算法进行反向传播（微积分中链式法则的递归使用）来求得权重调整的正确方向，在正确方向上可以提升正确目标字母的得分（绿色粗体数字）。然后进行&lt;em&gt;参数更新&lt;/em&gt;，即在该方向上轻微移动权重。如果我们将同样的数据输入给RNN，在参数更新后将会发现正确字母的得分（比如第一步中的e）将会变高（例如从2.2变成2.3），不正确字母的得分将会降低。重复进行一个过程很多次直到网络收敛，其预测与训练数据连贯一致，总是能正确预测下一个字母。&lt;/p&gt;&lt;p&gt;更技术派的解释是我们对输出向量同步使用标准的Softmax分类器（也叫作交叉熵损失）。使用小批量的随机梯度下降来训练RNN，使用&lt;a href="http://arxiv.org/abs/1502.04390" data-editable="true" data-title="RMSProp"&gt;RMSProp&lt;/a&gt;或Adam来让参数稳定更新。&lt;/p&gt;&lt;p&gt;注意当字母l第一次输入时，目标字母是l，但第二次的目标是o。因此RNN不能只靠输入数据，必须使用它的循环连接来保持对上下文的跟踪，以此来完成任务。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;测试&lt;/strong&gt;时，我们向RNN输入一个字母，得到其预测下一个字母的得分分布。我们根据这个分布取出得分最大的字母，然后将其输入给RNN以得到下一个字母。重复这个过程，我们就得到了文本！现在使用不同的数据集训练RNN，看看将会发生什么。&lt;/p&gt;&lt;p&gt;为了更好的进行介绍，我基于教学目的写了代码：&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="minimal character-level RNN language model in Python/numpy"&gt;minimal character-level RNN language model in Python/numpy&lt;/a&gt;，它只有100多行。如果你更喜欢读代码，那么希望它能给你一个更简洁直观的印象。我们下面介绍实验结果，这些实验是用更高效的Lua/Torch代码实现的。&lt;/p&gt;&lt;h2&gt;RNN的乐趣&lt;/h2&gt;&lt;p&gt;下面介绍的5个字母模型我都放在Github上的&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;里了。每个实验中的输入都是一个带有文本的文件，我们训练RNN让它能够预测序列中下一个字母。&lt;/p&gt;&lt;h3&gt;Paul Graham生成器&lt;/h3&gt;&lt;p&gt;译者注：中文名一般译为保罗•格雷厄姆，著有《黑客与画家》一书，中文版已面世。在康奈尔大学读完本科，在哈佛大学获得计算机科学博士学位。1995年，创办了Viaweb。1998年，Yahoo!收购了Viaweb，收购价约5000万美元。此后架起了个人网站paulgraham.com，在上面撰写关于软件和创业的文章，以深刻的见解和清晰的表达而著称。2005年，创建了风险投资公司Y Combinator，目前已经资助了80多家创业公司。现在，他是公认的互联网创业权威。&lt;/p&gt;&lt;p&gt;让我们先来试一个小的英文数据集来进行正确性检查。我最喜欢的数据集是&lt;a href="http://www.paulgraham.com/articles.html" data-editable="true" data-title="Paul Graham的文集"&gt;Paul Graham的文集&lt;/a&gt;。其基本思路是在这些文章中充满智慧，但Paul Graham的写作速度比较慢，要是能根据需求生成富于创业智慧的文章岂不美哉？那么就轮到RNN上场了。&lt;/p&gt;&lt;p&gt;将Paul Graham最近5年的文章收集起来，得到大小约1MB的文本文件，约有1百万个字符（这只算个很小的数据集）。&lt;em&gt;技术要点&lt;/em&gt;：训练一个2层的LSTM，各含512个隐节点（约350万个参数），每层之后使用0.5的dropout。每个数据批量中含100个样本，时间长度上截断了100个字符进行梯度的反向传播。按照上述设置，每个数据批量在TITAN GPU上的运算耗时为0.46秒（如果仅对50个字符进行BPTT，那么耗时会减半，性能的耗费几乎忽略不计）。&lt;em&gt;译者注：BPTT即Backpropagation Through Time&lt;/em&gt;。不在啰嗦，让我们看看RNN生成的文本：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;好吧，显然生成器暂时还无法替代Paul Graham，但是RNN可是完全从头开始学英语的（包括逗号，撇号和空格），而且数据集又如此的小。我还很喜欢它自己学会了如何进行引用（例如上文中的[2]）。有时候它甚至会说出一些充满智慧的洞见，比如“a company is a meeting to think to investors（公司就是一个琢磨如何让投资者打钱的会议）”。&lt;em&gt;译者注：RNN你瞎说什么大实话：）&lt;/em&gt;如果你想要查看更多细节，点击&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/pg.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;温度&lt;/strong&gt;。在生成文本的时候，我们可以调节Softmax的温度。将温度从1降到更低的数值（比如0.5）可以让RNN更加自信的同时变得更加保守。相反，如果将温度设置的更高，结果会更加多样化，但是代价就是可能会出现错误（比如更多的拼写错误）。如果将温度设置得非常接近于0，我们就会得到最像Paul Graham说的话：&lt;/p&gt;&lt;p&gt;&lt;em&gt;“is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”&lt;/em&gt;&lt;/p&gt;&lt;p&gt;看来我们陷入到连续创业的无限循环中去了。&lt;/p&gt;&lt;h2&gt;莎士比亚&lt;/h2&gt;&lt;p&gt;看起来我们可以让RNN学会如何拼写单词，但是数据中有更多的结构和样式呢？为了回答这个问题，我下载了莎士比亚的所有作品，并将它们装进一个单独的文件中，大小约4.4MB。我们现在可以训练更大的网络，这次试用的是一个3层的RNN，每层的隐节点512个。当我们训练了几个小时候，我们得到了下面的文本输出：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain'd into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.

Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.

DUKE VINCENTIO:
Well, your wit is in the care of side and that.

Second Lord:
They would be ruled after this chamber, and
my fair nues begun out of the fact, to be conveyed,
Whose noble souls I'll have the heart of the wars.

Clown:
Come, sir, I will make did behold your worship.

VIOLA:
I'll drink it.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;记住，RNN只知道字符，所以它学会了说话者的名字和说话内容的格式，有时候我们还会得到类似独白的文字，比如：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;VIOLA:
Why, Salisbury must find his flesh and thought
That which I am not aps, not a man and in fire,
To show the reining of the raven and the wars
To grace my hand reproach within, and not a fair are hand,
That Caesar and my goodly father's world;
When I was heaven of presence and our fleets,
We spare with hours, but cut thy council I am great,
Murdered and by thy master's ready there
My power to give thee but so much as hell:
Some service in the noble bondman here,
Would show him to her wine.

KING LEAR:
O, if you were a feeble sight, the courtesy of your law,
Your sight and several breath, will wear the gods
With his heads, and my hands are wonder'd at the deeds,
So drop upon your lordship's head, and your opinion
Shall be against your honour.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我个人是很难把这段话从莎士比亚的原作中分辨出来啦：）如果你喜欢莎士比亚，那可以来看看这篇&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt" data-editable="true" data-title="长达1000000字符的生成文本"&gt;长达1000000字符的生成文本&lt;/a&gt;。当然，你可以使用我提供的代码，在不同的温度设置下来生成不同的文本。&lt;/p&gt;&lt;h2&gt;维基百科&lt;/h2&gt;&lt;p&gt;我们看见LSTM能够拼写单词，复现语法结构。那么现在就提高难度，使用markdown文本对它进行训练。我使用了&lt;a href="http://prize.hutter1.net/" data-editable="true" data-title="Hutter Prize"&gt;Hutter Prize&lt;/a&gt;的100MB的数据集，数据集内容是原始的维基百科内容，然后在LSTM上训练。根据&lt;a href="http://arxiv.org/abs/1308.0850" data-editable="true" data-title="Graves等的论文"&gt;Graves等的论文&lt;/a&gt;，我使用了其中96MB用于训练，剩下的用做验证集。模型跑了有一晚上，然后可以生成维基百科文章了。下面是一些有趣的文本片段。首先，一些基本的markdown输出：&lt;/p&gt;&lt;pre&gt;&lt;code lang="pytb"&gt;Naturalism and decision for the majority of Arab countries' capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated 
with Guangzham's sovereignty. His generals were the powerful ruler of the Portugal 
in the [[Protestant Immineners]], which could be said to be directly in Cantonese 
Communication, which followed a ceremony and set inspired prison, training. The 
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom 
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth's Dajoard]], known 
in western [[Scotland]], near Italy to the conquest of India with the conflict. 
Copyright was the succession of independence in the slop of Syrian influence that 
was a famous German movement based on a more popular servicious, non-doctrinal 
and sexual power post. Many governments recognize the military housing of the 
[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], 
that is sympathetic to be to the [[Punjab Resolution]]
(PJS)[http://www.humah.yahoo.com/guardian.
cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery 
was swear to advance to the resources for those Socialism's rule, 
was starting to signing a major tripad of aid exile.]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你注意到的话，yahoo的那个url是不存在的，是模型生造了它。还有，可以看见模型学会了对于圆括号要成对出现。模型还学会了很多markdown结构，比如标题，列表等：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;{ { cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none } }
''www.e-complete''.

'''See also''': [[List of ethical consent processing]]

== See also ==
*[[Iender dome of the ED]]
*[[Anti-autism]]

===[[Religion|Religion]]===
*[[French Writings]]
*[[Maria]]
*[[Revelation]]
*[[Mount Agamul]]

== External links==
* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]

==External links==
* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有时候模型也会生成一些随机但是合法的XML：&lt;/p&gt;&lt;pre&gt;&lt;code lang="html"&gt;&amp;lt;page&amp;gt;
  &amp;lt;title&amp;gt;Antichrist&amp;lt;/title&amp;gt;
  &amp;lt;id&amp;gt;865&amp;lt;/id&amp;gt;
  &amp;lt;revision&amp;gt;
    &amp;lt;id&amp;gt;15900676&amp;lt;/id&amp;gt;
    &amp;lt;timestamp&amp;gt;2002-08-03T18:14:12Z&amp;lt;/timestamp&amp;gt;
    &amp;lt;contributor&amp;gt;
      &amp;lt;username&amp;gt;Paris&amp;lt;/username&amp;gt;
      &amp;lt;id&amp;gt;23&amp;lt;/id&amp;gt;
    &amp;lt;/contributor&amp;gt;
    &amp;lt;minor /&amp;gt;
    &amp;lt;comment&amp;gt;Automated conversion&amp;lt;/comment&amp;gt;
    &amp;lt;text xml:space="preserve"&amp;gt;#REDIRECT [[Christianity]]&amp;lt;/text&amp;gt;
  &amp;lt;/revision&amp;gt;
&amp;lt;/page&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;模型生成了时间戳，id和其他一些东西。同时模型也能正确地让标示符成对出现，嵌套规则也合乎逻辑。如果你对文本感兴趣，点击&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;代数几何&lt;/h2&gt;&lt;p&gt;上面的结果表明模型确实比较擅长学习复杂的语法结构。收到这些结果的鼓舞，我和同伴&lt;a href="http://cs.stanford.edu/people/jcjohns/" data-editable="true" data-title="Justin Johnson"&gt;Justin Johnson&lt;/a&gt;决定在结构化这一块将研究更加推进一步。我们在网站Stacks上找到了这本关于代数几何的&lt;a href="http://stacks.math.columbia.edu/" data-editable="true" data-title="书"&gt;书&lt;/a&gt;，下载了latex源文件（16MB大小），然后用于训练一个多层的LSTM。令人惊喜的是，模型输出的结果几乎是可以编译的。我们手动解决了一些问题后，就得到了一个看起来像模像样的数学文档，看起来非常惊人：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/47e8530542991e18bdacb7c395d80b94.jpg" data-rawwidth="1343" data-rawheight="755"&gt;&lt;p&gt;生成的代数几何。这里是&lt;a href="http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf" data-editable="true" data-title="源文件"&gt;源文件&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;这是另一个例子：&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/14a67f3357d9f178e711b1c6f9e2ae32.jpg" data-rawwidth="1323" data-rawheight="748"&gt;&lt;p&gt;更像代数几何了，右边还出现了图表。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;由上可见，模型有时候尝试生成latex图表，但是没有成功。我个人还很喜欢它跳过证明的部分（“Proof omitted”，在顶部左边）。当然，需要注意的是latex是相对困难的结构化语法格式，我自己都还没有完全掌握呢。下面是模型生成的一个源文件：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;\begin{proof}
We may assume that $\mathcal{I}$ is an abelian sheaf on $\mathcal{C}$.
\item Given a morphism $\Delta : \mathcal{F} \to \mathcal{I}$
is an injective and let $\mathfrak q$ be an abelian sheaf on $X$.
Let $\mathcal{F}$ be a fibered complex. Let $\mathcal{F}$ be a category.
\begin{enumerate}
\item \hyperref[setain-construction-phantom]{Lemma}
\label{lemma-characterize-quasi-finite}
Let $\mathcal{F}$ be an abelian quasi-coherent sheaf on $\mathcal{C}$.
Let $\mathcal{F}$ be a coherent $\mathcal{O}_X$-module. Then
$\mathcal{F}$ is an abelian catenary over $\mathcal{C}$.
\item The following are equivalent
\begin{enumerate}
\item $\mathcal{F}$ is an $\mathcal{O}_X$-module.
\end{lemma}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这份模型输出的文本展示了一些常见错误。比如模型起了&lt;strong&gt;\begin{proof}&lt;/strong&gt;的头，结尾却是&lt;strong&gt;\end{lemma}&lt;/strong&gt;。这种错误我们就必须手动改正，错误产生的原因可能在于依赖关系过于长了：当模型完成证明后，它已经忘了自己的开头是proof还是lemma了。类似的，模型起了&lt;strong&gt;\begin{enumerate}&lt;/strong&gt;的头，却忘了结尾。我们观察到，在使用更大规模或更好的模型后，这些错误变少了。然而，以上这些错误总是存在出现的可能性的。&lt;/p&gt;&lt;h2&gt;Linux源码&lt;/h2&gt;&lt;p&gt;我想要把结构化数据的实验推到极限，所以最后一个实验我们决定使用代码。具体说来，就是从Linux在Github上的&lt;a href="https://github.com/torvalds/linux" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;提取了所有代码和能找到的头文件，把他们装进一个巨大的文件中（474MB的C代码）。我原本计划是只训练Linux的核心文件的，但是其大小只有约16MB。然后我在GPU上使用3层LSTM训练了好几天。这些模型有大约1000万的参数，这对于RNN模型来说还不算多的。实验结果超级有趣：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 * Increment the size file of the new incorrect UI_FILTER group information
 * of the size generatively.
 */
static int indicate_policy(void)
{
  int error;
  if (fd == MARN_EPT) {
    /*
     * The kernel blank will coeld it to userspace.
     */
    if (ss-&amp;gt;segment &amp;lt; mem_total)
      unblock_graph_and_set_blocked();
    else
      ret = 1;
    goto bail;
  }
  segaddr = in_SB(in.addr);
  selector = seg / 16;
  setup_works = true;
  for (i = 0; i &amp;lt; blocks; i++) {
    seq = buf[i++];
    bpf = bd-&amp;gt;bd.next + i * search;
    if (fd) {
      current = blocked;
    }
  }
  rw-&amp;gt;name = "Getjbbregs";
  bprm_self_clearl(&amp;amp;iv-&amp;gt;version);
  regs-&amp;gt;new = blocks[(BPF_STATS &amp;lt;&amp;lt; info-&amp;gt;historidac)] | PFMR_CLOBATHINC_SECONDS &amp;lt;&amp;lt; 12;
  return segtable;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这些代码看起来相当酷了。虽然我不认为这些代码能够编译，但是看着这些代码你会感觉是一个巨大的C代码库。注意RNN还不时的给自己的代码加上了注释。代码中也很少有语法错误。比如它合理地使用了字符串，指针标记等。它学会了让花括号和中括号成对出现，让代码有正确的缩进。一个常见的错误是它不能追中变量的名字：它常常使用未声明的变量（比如上面的&lt;strong&gt;rw&lt;/strong&gt;），或声明一些永不使用的变量（比如上面的&lt;strong&gt;int error&lt;/strong&gt;），或返回不存在的变量。让我们看看更多的例子，下面是一个代码片段，展示了RNN学习到的更多的操作：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 * If this error is set, we will need anything right after that BSD.
 */
static void action_new_function(struct s_stat_info *wb)
{
  unsigned long flags;
  int lel_idx_bit = e-&amp;gt;edd, *sys &amp;amp; ~((unsigned long) *FIRST_COMPAT);
  buf[0] = 0xFFFFFFFF &amp;amp; (bit &amp;lt;&amp;lt; 4);
  min(inc, slist-&amp;gt;bytes);
  printk(KERN_WARNING "Memory allocated %02x/%02x, "
    "original MLL instead\n"),
    min(min(multi_run - s-&amp;gt;len, max) * num_data_in),
    frame_pos, sz + first_seg);
  div_u64_w(val, inb_p);
  spin_unlock(&amp;amp;disk-&amp;gt;queue_lock);
  mutex_unlock(&amp;amp;s-&amp;gt;sock-&amp;gt;mutex);
  mutex_unlock(&amp;amp;func-&amp;gt;mutex);
  return disassemble(info-&amp;gt;pending_bh);
}

static void num_serial_settings(struct tty_struct *tty)
{
  if (tty == tty)
    disable_single_st_p(dev);
  pci_disable_spool(port);
  return 0;
}

static void do_command(struct seq_file *m, void *v)
{
  int column = 32 &amp;lt;&amp;lt; (cmd[2] &amp;amp; 0x80);
  if (state)
    cmd = (int)(int_state ^ (in_8(&amp;amp;ch-&amp;gt;ch_flags) &amp;amp; Cmd) ? 2 : 1);
  else
    seq = 1;
  for (i = 0; i &amp;lt; 16; i++) {
    if (k &amp;amp; (1 &amp;lt;&amp;lt; 1))
      pipe = (in_use &amp;amp; UMXTHREAD_UNCCA) +
        ((count &amp;amp; 0x00000000fffffff8) &amp;amp; 0x000000f) &amp;lt;&amp;lt; 8;
    if (count == 0)
      sub(pid, ppc_md.kexec_handle, 0x20000000);
    pipe_set_bytes(i, 0);
  }
  /* Free our user pages pointer to place camera if all dash */
  subsystem_info = &amp;amp;of_changes[PAGE_SIZE];
  rek_controls(offset, idx, &amp;amp;soffset);
  /* Now we want to deliberately put it to device */
  control_check_polarity(&amp;amp;context, val, 0);
  for (i = 0; i &amp;lt; COUNTER; i++)
    seq_puts(s, "policy ");
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意在第二个函数中，比较了&lt;strong&gt;tty == tty&lt;/strong&gt;，这永远为真。但这次至少参数名出现在了函数体中。在最后一个函数中，没有返回值，因为函数的声明有void，所以倒也正确。 然而，前面两个函数声明为void确有返回值，这又是一种因为依赖过长产生的常见错误。&lt;/p&gt;&lt;p&gt;有时候模型决定生成新的文件。这通常是非常让人惊喜的部分：模型首先引用了GUN许可，进行了一些导入操作，生成一些宏，然后开始进入代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="c"&gt;/*
 *  Copyright (c) 2006-2010, Intel Mobile Communications.  All rights reserved.
 *
 *   This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published by
 * the Free Software Foundation.
 *
 *        This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *
 *  GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software Foundation,
 *  Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

#include &amp;lt;linux/kexec.h&amp;gt;
#include &amp;lt;linux/errno.h&amp;gt;
#include &amp;lt;linux/io.h&amp;gt;
#include &amp;lt;linux/platform_device.h&amp;gt;
#include &amp;lt;linux/multi.h&amp;gt;
#include &amp;lt;linux/ckevent.h&amp;gt;

#include &amp;lt;asm/io.h&amp;gt;
#include &amp;lt;asm/prom.h&amp;gt;
#include &amp;lt;asm/e820.h&amp;gt;
#include &amp;lt;asm/system_info.h&amp;gt;
#include &amp;lt;asm/setew.h&amp;gt;
#include &amp;lt;asm/pgproto.h&amp;gt;

#define REG_PG    vesa_slot_addr_pack
#define PFM_NOCOMP  AFSR(0, load)
#define STACK_DDR(type)     (func)

#define SWAP_ALLOCATE(nr)     (e)
#define emulate_sigs()  arch_get_unaligned_child()
#define access_rw(TST)  asm volatile("movd %%esp, %0, %3" : : "r" (0));   \
  if (__type &amp;amp; DO_READ)

static void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \
          pC&amp;gt;[1]);

static void
os_prefix(unsigned long sys)
{
#ifdef CONFIG_PREEMPT
  PUT_PARAM_RAID(2, sel) = get_state_state();
  set_pid_sum((unsigned long)state, current_state_str(),
           (unsigned long)-1-&amp;gt;lr_full; low;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里面有太多有趣的地方可以讨论，我几乎可以写一整个博客，所以我现在还是暂停，感兴趣的可以查看&lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/linux.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;生成婴儿姓名&lt;/h2&gt;&lt;p&gt;让我们再试一个。给RNN输入一个包含8000个小孩儿姓名的文本文件，一行只有一个名字。（名字是从&lt;a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;获得的）我们可以把这些输入RNN然后生成新的名字。下面是一些名字例子，只展示了那些没有在训练集中出现过的名字：&lt;/p&gt;&lt;p&gt;&lt;em&gt;Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina&lt;/em&gt;&lt;/p&gt;&lt;p&gt;点击&lt;a href="http://cs.stanford.edu/people/karpathy/namesGenUnique.txt" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;可以查看更多。我个人最喜欢的名字包括“Baby” (哈)， “Killie”，“Char”，“R”，“More”，“Mars”，“Hi”，“Saddie”，“With”和“Ahbort”。这真的蛮有意思，你还可以畅想在写小说或者给创业公司起名字的时候，这个能给你灵感。&lt;/p&gt;&lt;p&gt;&lt;b&gt;上篇截止&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;翻译不到位的地方，欢迎知友们评论批评指正；&lt;/li&gt;&lt;li&gt;在计算机视觉方面，个人对于&lt;b&gt;图像标注（image caption）&lt;/b&gt;比较感兴趣，正在入坑。欢迎有同样兴趣的知友投稿讨论；&lt;/li&gt;&lt;li&gt;感谢&lt;a href="https://www.zhihu.com/people/157deec64cc5e062b2207aeece42f50f" data-hash="157deec64cc5e062b2207aeece42f50f" class="member_mention" data-hovercard="p$b$157deec64cc5e062b2207aeece42f50f"&gt;@七月&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/39ed720d0b87cdc8c5dbe25618c51646" data-hash="39ed720d0b87cdc8c5dbe25618c51646" class="member_mention" data-hovercard="p$b$39ed720d0b87cdc8c5dbe25618c51646"&gt;@陈子服&lt;/a&gt; 的细节指正。&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22107715&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 06 Sep 2016 20:31:35 GMT</pubDate></item><item><title>Andrej Karpathy的回信和Quora活动邀请</title><link>https://zhuanlan.zhihu.com/p/22282421</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1b4d5e3761721c42438fbee54ccc8696_r.jpg"&gt;&lt;/p&gt;大家好：&lt;p&gt;关注我们CS231n翻译项目的知友应该知道，之所以开始这个翻译，也是因为得到了&lt;a href="http://cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;的授权。&lt;/p&gt;&lt;p&gt;这次翻译项目完结邮件了他，顺便问他有没有什么对学习CS231n的小伙伴们说的？邮件部分截取如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/b3b88f5c66a44a82ec7ee541cc3aad2f.png" data-rawwidth="1590" data-rawheight="819"&gt;AK表示其实也没啥好说的，非常高兴能够分享这些知识，请&lt;b&gt;好好利用它来让这个世界变得更美好！&lt;/b&gt;最后表示感谢。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Session活动邀请&lt;/b&gt;：&lt;b&gt;下周四（9月8日）AK会在Quora上做一个类似知乎live的Session&lt;/b&gt;，貌似是基于文字的问答活动。活动地址&lt;a href="https://www.quora.com/session/Andrej-Karpathy/1" data-title="在这里" class=""&gt;在这里&lt;/a&gt;。感兴趣的同学可以去&lt;b&gt;提提问，捧捧场&lt;/b&gt;。这里也是义务为他做推广，因为我非常&lt;b&gt;佩服他坚持高质量分享知识的行为&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;注：Andrej Karpathy的个人情况点&lt;a href="http://cs.stanford.edu/people/karpathy/"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22282421&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Sun, 04 Sep 2016 22:36:10 GMT</pubDate></item><item><title>知行合一码作业，深度学习真入门</title><link>https://zhuanlan.zhihu.com/p/22232836</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6e060188cc1571a89456af45670d51a9_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：文章为本人在&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;专栏原创首发，禁止未授权转载。&lt;/b&gt;&lt;h2&gt;前言&lt;/h2&gt;&lt;p&gt;这是一篇号召深度学习爱好者&lt;b&gt;共同学习、讨论和完成&lt;/b&gt;斯坦福深度学习课程&lt;b&gt;CS231n的3个编程大作业&lt;/b&gt;的召集令。&lt;/p&gt;&lt;p&gt;在过去的3个多月中，我们翻译完成了CS231n的官方课程笔记并汇总在《&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" data-editable="true" data-title="CS231n官方笔记授权翻译总集篇发布" class=""&gt;CS231n官方笔记授权翻译总集篇发布&lt;/a&gt;》一文中，得到了大家的支持。期间也有不少同学私信各种和编程作业相关的问题，这说明还是有不少的想要入门深度学习的同学希望通过完成编程作业来加深自己对于课程知识内容的理解。&lt;/p&gt;&lt;p&gt;同时很多同学也始终在私信是否有交流群，我也反复回答自己并没有建交流群。个人始终认为交流应该是“&lt;b&gt;慎思而明辨之&lt;/b&gt;”，学习主静，需要&lt;b&gt;沉下心来看课程、笔记和论文，完整地啃完一部分内容后，再和人交流，这样的交流才是有质量的，对人对己都有益&lt;/b&gt;。而这样有质量的交流，最好是写文章来互动。为了促进互动，我在“&lt;b&gt;学习互动&lt;/b&gt;”部分提出了一些思路。&lt;/p&gt;&lt;h2&gt;动机&lt;/h2&gt;&lt;p&gt;&lt;b&gt;为什么&lt;/b&gt;要完成这3个编程作业？&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;行之明觉精察处便是知，知之真切笃实处便是行。&lt;/b&gt;——王阳明&lt;/blockquote&gt;&lt;p&gt;对于想要深度学习入门的同学来说，CS231n的编程作业能够引导学习者&lt;b&gt;由浅入深地在实践中理解深度学习中常用的算法模型，训练与调参方法和常用的技巧套路&lt;/b&gt;等。&lt;/p&gt;&lt;p&gt;相较于直接阅读各大开源框架源码来学习，个人认为通过完成作业来入门有以下几个好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;难度循序渐进&lt;/b&gt;。小作业设置是从线性分类模型实现开始，到神经网络模型，再到循环神经网络模型。而基于Jupyter Notebook的作业脚本单个小作业中，也是将一个小作业切分成很多块，让学习者逐个顺序实现。&lt;/li&gt;&lt;li&gt;&lt;b&gt;实现实验并重&lt;/b&gt;。每个小作业就是一个实验：你需要初始化设置，读取数据集，然后实现算法模型，检查模型是否实现正确，训练模型，调参，最后分析评价算法性能。在这个过程中，你既要聚焦在核心的函数实现，又会跟着作业安排的顺序来完成实验，掌握算法实验的套路。&lt;/li&gt;&lt;li&gt;&lt;b&gt;细节设置平衡&lt;/b&gt;。作业使用Python语言，基于Numpy库来实现。一些常用的基础函数功能课程已经实现，学习者只需要聚焦实现最核心的函数，且不会陷入到特别底层的细节。这样就在工程实践与概念理解中得到了一个平衡。&lt;/li&gt;&lt;li&gt;&lt;b&gt;应用丰富有趣&lt;/b&gt;。目前相当热门的Prisma所使用的图像风格化，谷歌的DeepDream，以及图像标注等应用都在作业中有原型实现。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;作业概述&lt;/h2&gt;&lt;p&gt;CS231n一共有3个大作业，作业简介我们已经翻译并发布：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏" class=""&gt;斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 2简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 2简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;只需要&lt;b&gt;按照简介中的指导&lt;/b&gt;下载作业代码（文中有链接），安装完配置环境（非常简单！），就可以愉快地开始做作业了。&lt;/p&gt;&lt;p&gt;对于入门的同学来说，&lt;b&gt;作业的完成需要一些前置的知识学习，单就课程内资源来说&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;Assignment #1中的5个小作业包含了&lt;b&gt;knn、SVM、Softmax、一个两层的神经网络和特征提取&lt;/b&gt;。你需要学习课程视频1-4课，阅读&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" class="" data-editable="true" data-title="Python Numpy教程"&gt;Python Numpy教程&lt;/a&gt;，&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" class="" data-editable="true" data-title="图像分类笔记（上）"&gt;图像分类笔记（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，线性分类笔记&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" class="" data-editable="true" data-title="（中）"&gt;（中）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，最优化笔记&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，以及&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" class="" data-editable="true" data-title="反向传播笔记"&gt;反向传播笔记&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Assignment #2中的4个小作业包含了&lt;b&gt;全连接神经网络，卷及神经网络和一些最新的优化方法&lt;/b&gt;，你需要学习课程视频5-9课，阅读神经网络笔记1&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" class="" data-editable="true" data-title="神经网络笔记2"&gt;神经网络笔记2&lt;/a&gt;，神经网络笔记3&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" class="" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" class="" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;，以及&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" class="" data-editable="true" data-title="卷积神经网络笔记"&gt;卷积神经网络笔记&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Assignment #3中的4个小作业包含了&lt;b&gt;RNN，LSTM，图像标注模型，图像生成模型&lt;/b&gt;等，你需要学习课程视频10-11课。这部分课程没有给出笔记，后续我们可能寻找一些较好的内容加以翻译补充。&lt;/p&gt;&lt;h2&gt;学习互动&lt;/h2&gt;&lt;p&gt;&lt;b&gt;推进进度&lt;/b&gt;：3个大作业共包含13个小作业。我们用一个比较宽松的进度来推进，今年还有将近4个月的时间，那么我们&lt;b&gt;从9月5日开始起算&lt;/b&gt;，平均一个月完成3个左右的小作业。也就是说&lt;b&gt;每个小作业1-2周的时间学习、讨论并完成&lt;/b&gt;。从给我个人的经验来看，这个速度即使是对于零基础的同学，也是足够的啦。&lt;/p&gt;&lt;p&gt;&lt;b&gt;互动方式&lt;/b&gt;：我们会按照小作业顺序发布完成作业所做的理论学习，具体实现解读文章。同样，&lt;b&gt;任何愿意按照上述进度进行作业学习的同学，都可以从自己对于理论的不同理解，核心函数不同的实现方式，遇到问题及解决问题的收获，对专栏进行投稿。而各种疑问，也可以在对应文章的评论中进行讨论&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;建不建群&lt;/b&gt;：我的个人意见是不太想建，因为自己精力有限，建了没办法良好管理与互动。但是大家也可以提意见，&lt;b&gt;如果大部分愿意一起学习的同学想要建一个群，同时有比较热心的同学能在群里面不时给大家介绍一下情况&lt;/b&gt;，那么还是可以考虑一下。ps：但是我可能基本是潜水。&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;最近读曾国藩家书，看到一个有趣的学习方法：&lt;/p&gt;&lt;blockquote&gt;予定&lt;b&gt;刚日读经&lt;/b&gt;，&lt;b&gt;柔日读史&lt;/b&gt;之法。——曾国藩&lt;/blockquote&gt;&lt;p&gt;所谓刚日，是说人的情绪亢阳激越的日子。柔日，是指人情绪卑幽忧昧的日子。而经主常，史生变。所以咱在感觉能打10个的状态时，就要读一些经书，让我们能够平和沉静。在我们感觉比较低落忧郁的日子，就看看史书，从波澜壮阔的历史中激扬斗志。&lt;/p&gt;&lt;p&gt;具体到现在，我看可以修改为“&lt;b&gt;刚日写码，柔日知乎&lt;/b&gt;”。&lt;b&gt;状态好的日子&lt;/b&gt;就看论文写代码，发现自己的不足从而提升自己，储才养望。&lt;b&gt;状态一般创造力不强时&lt;/b&gt;，可以对已经完成的事情做一个总结，以书面的形式发出来，对自己工作的总结积累和与别人探讨，一总结发现自己之前还是有很多收获的，所以不用心情那么低落；&lt;/p&gt;&lt;p&gt;最后说一句，预定是9月5日开始，大家有什么意见请尽情提吧。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22232836&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Wed, 31 Aug 2016 14:51:35 GMT</pubDate></item></channel></rss>