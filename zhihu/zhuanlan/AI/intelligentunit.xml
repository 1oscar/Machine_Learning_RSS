<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>斯坦福CS231n官方教程笔记翻译连载。

深度增强学习领域论文和项目的原创思考和Demo复现。

领域内其他感兴趣论文和项目的原创思考解读。</description><lastBuildDate>Fri, 26 Aug 2016 19:15:10 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>贺完结！CS231n官方笔记授权翻译总集篇发布</title><link>https://zhuanlan.zhihu.com/p/21930884</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/0dee7274beed25bd4abbcd76cb7d9576_r.jpg"&gt;&lt;/p&gt;&lt;blockquote&gt;哈哈哈！我们也是不谦虚，几个“业余水平”的网友，怎么就“零星”地把这件事给搞完了呢！&lt;b&gt;总之就是非常开心&lt;/b&gt;，废话不多说，进入正题吧！&lt;/blockquote&gt;&lt;h2&gt;CS231n简介&lt;/h2&gt;&lt;p&gt;CS231n的全称是&lt;a href="http://vision.stanford.edu/teaching/cs231n/index.html" data-editable="true" data-title="CS231n: Convolutional Neural Networks for Visual Recognition" class=""&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;，即&lt;b&gt;面向视觉识别的卷积神经网络&lt;/b&gt;。该课程是&lt;a href="http://vision.stanford.edu/index.html" data-editable="true" data-title="斯坦福大学计算机视觉实验室" class=""&gt;斯坦福大学计算机视觉实验室&lt;/a&gt;推出的课程。需要注意的是，目前大家说CS231n，大都指的是2016年冬季学期（一月到三月）的最新版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程描述&lt;/b&gt;：请允许我们引用课程主页上的&lt;b&gt;官方描述&lt;/b&gt;如下。&lt;/p&gt;&lt;blockquote&gt;计算机视觉在社会中已经逐渐普及，并广泛运用于搜索检索、图像理解、手机应用、地图导航、医疗制药、无人机和无人驾驶汽车等领域。而这些应用的核心技术就是图像分类、图像定位和图像探测等视觉识别任务。近期神经网络（也就是“深度学习”）方法上的进展极大地提升了这些代表当前发展水平的视觉识别系统的性能。本课程将深入讲解深度学习框架的细节问题，聚焦面向视觉识别任务（尤其是图像分类任务）的端到端学习模型。在10周的课程中，学生们将会学习如何实现、训练和调试他们自己的神经网络，并建立起对计算机视觉领域的前沿研究方向的细节理解。最终的作业将包括训练一个有几百万参数的卷积神经网络，并将其应用到最大的图像分类数据库（ImageNet）上。我们将会聚焦于教授如何确定图像识别问题，学习算法（比如反向传播算法），对网络的训练和精细调整（fine-tuning）中的工程实践技巧，指导学生动手完成课程作业和最终的课程项目。本课程的大部分背景知识和素材都来源于&lt;a href="http://image-net.org/challenges/LSVRC/2014/index" data-editable="true" data-title="ImageNet Challenge" class=""&gt;ImageNet Challenge&lt;/a&gt;竞赛。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;课程内容&lt;/b&gt;：官方课程安排及资源获取请点击&lt;a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" data-editable="true" data-title="这里" class=""&gt;这里&lt;/a&gt;，课程视频请在Youtube上查看&lt;a href="https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;创建的&lt;a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" data-editable="true" data-title="播放列表" class=""&gt;播放列表&lt;/a&gt;，也可私信我们获取云盘视频资源。通过查看官方课程表，我们可以看到：CS231n课程资源主要由&lt;b&gt;授课视频与PPT&lt;/b&gt;，&lt;b&gt;授课知识详解笔记&lt;/b&gt;和&lt;b&gt;课程作业&lt;/b&gt;三部分组成。其中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;授课视频15课&lt;/b&gt;。每节课时约1小时左右，每节课一份PPT。&lt;/li&gt;&lt;li&gt;&lt;b&gt;授课知识详解笔记共9份&lt;/b&gt;。光看课程视频是不够的，深入理解课程笔记才能比较扎实地学习到知识。&lt;/li&gt;&lt;li&gt;&lt;b&gt;课程作业3次&lt;/b&gt;。其中每次作业中又包含多个小作业，完成作业能确保对于课程关键知识的深入理解和实现。&lt;/li&gt;&lt;li&gt;&lt;b&gt;课程项目1个&lt;/b&gt;。这个更多是面向斯坦福的学生，组队实现课程项目。&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展阅读若干&lt;/b&gt;。课程推荐的拓展阅读大多是领域内的经典著作节选或论文，推荐想要深入学习的同学阅读。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;课程评价&lt;/b&gt;：我们觉得赞！很多人都觉得赞！当然也有人觉得不好。具体如何，大家搜搜CS231n在网络，在知乎上的评价不就好了嘛！&lt;b&gt;个人认为&lt;/b&gt;：入门深度学习的&lt;b&gt;一门良心课&lt;/b&gt;。&lt;b&gt;适合绝大多数&lt;/b&gt;想要学习深度学习知识的人。&lt;/p&gt;&lt;p&gt;&lt;b&gt;课程不足&lt;/b&gt;：课程后期从RCNN开始就没有课程笔记。&lt;/p&gt;&lt;h2&gt;课程学习方法&lt;/h2&gt;&lt;p&gt;三句话总结：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;看授课视频形成概念，发现个人感兴趣方向。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;读课程笔记理解细节，夯实工程实现的基础。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;码课程作业实现算法，积累实验技巧与经验。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;引用一下学习金字塔的图，意思大家都懂的：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/77465c8318e6c4d40df274b92602d83f.png" data-rawwidth="519" data-rawheight="423"&gt;&lt;h2&gt;我们的工作&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;完成了CS231n全部9篇课程知识详解笔记的翻译&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/python-numpy-tutorial" data-editable="true" data-title="[python/numpy tutorial]" class=""&gt;[python/numpy tutorial]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" data-editable="true" data-title="Python Numpy教程" class=""&gt;Python Numpy教程&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/classification" data-editable="true" data-title="[image classification notes]" class=""&gt;[image classification notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" data-editable="true" data-title="图像分类笔记（上）" class=""&gt;图像分类笔记（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" data-title="（下）" class="" data-editable="true"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法，内容列表：&lt;ul&gt;&lt;li&gt;图像分类、数据驱动方法和流程&lt;/li&gt;&lt;li&gt;Nearest Neighbor分类器&lt;/li&gt;&lt;ul&gt;&lt;li&gt;k-Nearest Neighbor &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;验证集、交叉验证集和超参数调参&lt;/li&gt;&lt;li&gt;Nearest Neighbor的优劣&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;li&gt;小结：应用kNN实践&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/linear-classify" data-editable="true" data-title="[linear classification notes]"&gt;[linear classification notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：线性分类笔记&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" data-title="（上）" class="" data-editable="true"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" data-editable="true" data-title="（中）"&gt;（中）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是&lt;b&gt;评分函数（score function）&lt;/b&gt;，它是原始图像数据到类别分值的映射。另一个是&lt;b&gt;损失函数（loss function）&lt;/b&gt;，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。内容列表：&lt;ul&gt;&lt;li&gt;线性分类器简介&lt;/li&gt;&lt;li&gt;线性评分函数&lt;/li&gt;&lt;li&gt;阐明线性分类器 &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;ul&gt;&lt;li&gt;多类SVM&lt;/li&gt;&lt;li&gt;Softmax分类器&lt;/li&gt;&lt;li&gt;SVM和Softmax的比较&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;基于Web的可交互线性分类器原型&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/optimization-1" data-editable="true" data-title="[optimization notes]"&gt;[optimization notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：最优化笔记&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" data-editable="true" data-title="（下）" class=""&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了图像分类任务的第三个关键部分：最优化。内容列表如下：&lt;ul&gt;&lt;li&gt;简介&lt;/li&gt;&lt;li&gt;损失函数可视化&lt;/li&gt;&lt;li&gt;最优化&lt;/li&gt;&lt;ul&gt;&lt;li&gt;策略#1：随机搜索&lt;/li&gt;&lt;li&gt;策略#2：随机局部搜索&lt;/li&gt;&lt;li&gt;策略#3：跟随梯度 &lt;i&gt;译者注：上篇截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;梯度计算&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用有限差值进行数值计算&lt;/li&gt;&lt;li&gt;微分计算梯度&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;梯度下降&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/optimization-2" data-editable="true" data-title="[backprop notes]"&gt;[backprop notes]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" data-editable="true" data-title="反向传播笔记"&gt;反向传播笔记&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记本将帮助读者&lt;b&gt;对反向传播形成直观而专业的理解&lt;/b&gt;。反向传播是利用链式法则递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。内容里列表如下：&lt;ul&gt;&lt;li&gt;简介&lt;/li&gt;&lt;li&gt;简单表达式和理解梯度&lt;/li&gt;&lt;li&gt;复合表达式，链式法则，反向传播&lt;/li&gt;&lt;li&gt;直观理解反向传播&lt;/li&gt;&lt;li&gt;模块：Sigmoid例子&lt;/li&gt;&lt;li&gt;反向传播实践：分段计算&lt;/li&gt;&lt;li&gt;回传流中的模式&lt;/li&gt;&lt;li&gt;用户向量化操作的梯度&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-1/" data-editable="true" data-title="Neural Nets notes 1"&gt;Neural Nets notes 1&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：神经网络笔记1&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" data-editable="true" data-title="（下）" class=""&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了神经网络的建模与结构，内容列表如下：&lt;ul&gt;&lt;li&gt;不用大脑做类比的快速简介&lt;/li&gt;&lt;li&gt;单个神经元建模&lt;ul&gt;&lt;li&gt;生物动机和连接&lt;/li&gt;&lt;li&gt;作为线性分类器的单个神经元&lt;/li&gt;&lt;li&gt;常用的激活函数 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;神经网络结构&lt;ul&gt;&lt;li&gt;层组织&lt;/li&gt;&lt;li&gt;前向传播计算例子&lt;/li&gt;&lt;li&gt;表达能力&lt;/li&gt;&lt;li&gt;设置层的数量和尺寸&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;小节&lt;/li&gt;&lt;li&gt;参考文献&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-2/" data-editable="true" data-title="Neural Nets notes 2"&gt;Neural Nets notes 2&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" data-editable="true" data-title="神经网络笔记2"&gt;神经网络笔记2&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;该笔记介绍了数据的预处理，正则化和损失函数，内容列表如下：&lt;ul&gt;&lt;li&gt;设置数据和模型&lt;ul&gt;&lt;li&gt;数据预处理&lt;/li&gt;&lt;li&gt;权重初始化&lt;/li&gt;&lt;li&gt;批量归一化（Batch Normalization）&lt;/li&gt;&lt;li&gt;正则化（L2/L1/Maxnorm/Dropout）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/neural-networks-3/" data-editable="true" data-title="Neural Nets notes 3" class=""&gt;Neural Nets notes 3&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：神经网络笔记3&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" data-editable="true" data-title="（上）"&gt;（上）&lt;/a&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" data-editable="true" data-title="（下）"&gt;（下）&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;该笔记讲解了神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。内容列表如下：&lt;/p&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练集与验证集准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="ConvNet notes" class=""&gt;ConvNet notes&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" data-editable="true" data-title="卷积神经网络笔记"&gt;卷积神经网络笔记&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;用来构建卷积神经网络的各种层&lt;/b&gt;&lt;ul&gt;&lt;li&gt;卷积层&lt;/li&gt;&lt;li&gt;汇聚层&lt;/li&gt;&lt;li&gt;归一化层&lt;/li&gt;&lt;li&gt;全连接层&lt;/li&gt;&lt;li&gt;将全连接层转化成卷积层&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;ul&gt;&lt;li&gt;层的排列规律&lt;/li&gt;&lt;li&gt;层的尺寸设置规律&lt;/li&gt;&lt;li&gt;案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）&lt;/li&gt;&lt;li&gt;计算上的考量&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;完成了3个课程作业页面的翻译&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment1/" data-editable="true" data-title="[Assignment #1]" class=""&gt;[Assignment #1]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="CS231n课程作业#1简介" class=""&gt;CS231n课程作业#1简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：实现k-NN，SVM分类器，Softmax分类器和两层神经网络，实践一个简单的图像分类流程。&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment2/" data-editable="true" data-title="[Assignment #2]"&gt;[Assignment #2]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" data-title="CS231n课程作业#2简介" class="" data-editable="true"&gt;CS231n课程作业#2简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：练习编写反向传播代码，训练神经网络和卷积神经网络。&lt;/blockquote&gt;&lt;p&gt;原文：&lt;a href="http://cs231n.github.io/assignments2016/assignment3/" data-editable="true" data-title="[Assignment #3]" class=""&gt;[Assignment #3]&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;翻译：&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" data-editable="true" data-title="CS231n课程作业#3简介"&gt;CS231n课程作业#3简介&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;作业内容：实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。实现DeepDream等有趣应用。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;帮助知友&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;发起了在Youtube上合力翻译课程字幕的倡议&lt;/b&gt;：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" data-title="知友智靖远关于CS231n课程字幕翻译的倡议" class="" data-editable="true"&gt;知友智靖远关于CS231n课程字幕翻译的倡议&lt;/a&gt;。当时，&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;已经贡献了他对第一课字幕的翻译，目前这个翻译项目仍在进行中，欢迎各位知友积极参与。具体操作方式在倡议原文中有，请大家点击查看。&lt;/p&gt;&lt;p&gt;有很多知友私信我们，询问为何不做字幕。现在统一答复：&lt;b&gt;请大家积极参加&lt;a href="https://www.zhihu.com/people/313544833f1060900fcb4f6a75c9f6b6" data-hash="313544833f1060900fcb4f6a75c9f6b6" class="member_mention" data-title="@智靖远" data-editable="true" data-hovercard="p$b$313544833f1060900fcb4f6a75c9f6b6"&gt;@智靖远&lt;/a&gt;的字幕翻译项目。&lt;/b&gt;他先进行的字幕贡献与翻译，我们&lt;b&gt;不能夺人之美&lt;/b&gt;。&lt;b&gt;后续，我们也会向该翻译项目进行贡献&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;翻译团队&lt;/h2&gt;&lt;p&gt;CS231n课程笔记的翻译，始于&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-editable="true" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt;在一次回答问题“&lt;a href="https://www.zhihu.com/question/41907061" data-editable="true" data-title="应该选择TensorFlow还是Theano？" class=""&gt;应该选择TensorFlow还是Theano？&lt;/a&gt;”中的机缘巧合，在&lt;a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" data-editable="true" data-title="取得了授权"&gt;取得了授权&lt;/a&gt;后申请了知乎专栏&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元 - 知乎专栏"&gt;智能单元 - 知乎专栏&lt;/a&gt;独自翻译。随着翻译的进行，更多的知友参与进来。他们是&lt;a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" data-hash="584f06e4ed2edc6007e4793179e7cdc1" class="member_mention" data-title="@ShiqingFan" data-editable="true" data-hovercard="p$b$584f06e4ed2edc6007e4793179e7cdc1"&gt;@ShiqingFan&lt;/a&gt;，@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;，&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-editable="true" data-title="@李艺颖" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;大家因为认同这件事而聚集在一起&lt;/b&gt;，牺牲了很多个人的时间来进行翻译，校对和润色。而翻译的质量，我们不愿意自我表扬，还是&lt;b&gt;请各位知友自行阅读评价&lt;/b&gt;吧。现在笔记翻译告一段落，下面是&lt;b&gt;团队成员的简短感言&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" data-hash="584f06e4ed2edc6007e4793179e7cdc1" class="member_mention" data-editable="true" data-title="@ShiqingFan" data-hovercard="p$b$584f06e4ed2edc6007e4793179e7cdc1"&gt;@ShiqingFan&lt;/a&gt; ：一个偶然的机会让自己加入到这个翻译小队伍里来。CS231n给予了我知识的源泉和思考的灵感，前期的翻译工作也督促自己快速了学习了这门课程。虽然科研方向是大数据与并行计算，不过因为同时对深度学习比较感兴趣，于是乎现在的工作与两者都紧密相连。Merci!&lt;/p&gt;&lt;p&gt;@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;：在CS231n翻译小组工作的两个多月的时间非常难忘。我向杜客申请加入翻译小组的时候，才刚接触这门课不久，翻译和校对的工作让我对这门课的内容有了更深刻的理解。作为一个机器学习的初学者，我非常荣幸能和翻译小组一起工作并做一点贡献。希望以后能继续和翻译小组一起工作和学习。&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-editable="true" data-title="@堃堃" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt; ：感谢组内各位成员的辛勤付出，很幸运能够参与这份十分有意义的工作，希望自己的微小工作能够帮助到大家，谢谢！&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-editable="true" data-title="@李艺颖" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt; ：当你真正沉下心来要做一件事情的时候才是学习和提高最好的状态；当你有热情做事时，并不会觉得是在牺牲时间，因为那是有意义并能带给你成就感和充实感的；不需要太过刻意地在乎大牛的巨大光芒，你只需像傻瓜一样坚持下去就好了，也许回头一看，你已前进了很多。就像老杜说的，我们就是每一步慢慢走，怎么就“零星”地把这件事给搞完了呢？&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-editable="true" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt; ：做了一点微小的工作，哈哈。&lt;/p&gt;&lt;h2&gt;未来工作&lt;/h2&gt;&lt;p&gt;目前通过大家的反馈，之后会有新的创作方向，会更多与大家互动，敬请期待吧！&lt;/p&gt;&lt;h2&gt;感谢&lt;/h2&gt;&lt;p&gt;感谢&lt;b&gt;所有给我们的翻译提出过批评指正的知友&lt;/b&gt;，每篇文章末尾处的译者反馈部分我们都列出了大家的具体指正与贡献；&lt;/p&gt;&lt;p&gt;感谢&lt;b&gt;所有给我们的翻译点赞的知友&lt;/b&gt;，你们的赞是我们的精神粮食；&lt;/p&gt;&lt;p&gt;感谢&lt;b&gt;给文章赞赏小钱钱的知友&lt;/b&gt;，谢谢老板们：）&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;&lt;b&gt;恳请大家点赞和分享到其他社交网络上&lt;/b&gt;，让更多&lt;b&gt;想要入门与系统学习深度学习&lt;/b&gt;的小伙伴能够看到这篇总集。同时，也欢迎大家在来专栏分享你的知识，发现志同道合的朋友！&lt;/p&gt;&lt;p&gt;&lt;b&gt;这个世界需要更多的英雄！&lt;/b&gt;&lt;/p&gt;</description><author>杜客</author><pubDate>Thu, 25 Aug 2016 15:47:00 GMT</pubDate></item><item><title>CS231n课程笔记翻译：卷积神经网络笔记</title><link>https://zhuanlan.zhihu.com/p/22038289</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5b83bc8331994f47fedb1459d1424872_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;译者注&lt;/b&gt;：本文翻译自斯坦福CS231n课程笔记&lt;a href="http://cs231n.github.io/convolutional-networks/" data-title="ConvNet notes" class="" data-editable="true"&gt;ConvNet notes&lt;/a&gt;，由课程教师&lt;a href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;授权进行翻译。本篇教程由&lt;a href="https://www.zhihu.com/people/du-ke" data-editable="true" data-title="杜客" class=""&gt;杜客&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/hmonkey" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;翻译完成，&lt;a href="https://www.zhihu.com/people/kun-kun-97-81" class="" data-editable="true" data-title="堃堃"&gt;堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" class="" data-editable="true" data-title="李艺颖"&gt;李艺颖&lt;/a&gt;进行校对修改。&lt;/p&gt;&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;用来构建卷积神经网络的各种层&lt;/b&gt;&lt;ul&gt;&lt;li&gt;卷积层&lt;/li&gt;&lt;li&gt;汇聚层&lt;/li&gt;&lt;li&gt;归一化层&lt;/li&gt;&lt;li&gt;全连接层&lt;/li&gt;&lt;li&gt;将全连接层转化成卷积层&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;ul&gt;&lt;li&gt;层的排列规律&lt;/li&gt;&lt;li&gt;层的尺寸设置规律&lt;/li&gt;&lt;li&gt;案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）&lt;/li&gt;&lt;li&gt;计算上的考量&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;卷积神经网络（CNNs / ConvNets）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。&lt;/p&gt;&lt;p&gt;那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结构概述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;i&gt;回顾：常规神经网络&lt;/i&gt;。在上一章中，神经网络的输入是一个向量，然后在一系列的&lt;i&gt;隐层&lt;/i&gt;中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。&lt;/p&gt;&lt;p&gt;&lt;i&gt;常规神经网络对于大尺寸图像效果不尽人意&lt;/i&gt;。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。&lt;/p&gt;&lt;p&gt;&lt;i&gt;神经元的三维排列&lt;/i&gt;。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：&lt;b&gt;宽度&lt;/b&gt;、&lt;b&gt;高度&lt;/b&gt;和&lt;b&gt;深度&lt;/b&gt;（这里的&lt;b&gt;深度&lt;/b&gt;指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2ef08bb4cf60805d726b2d6db39dd985.jpg" data-rawwidth="1637" data-rawheight="379"&gt;左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;blockquote&gt;&lt;p&gt;卷积神经网络是由层组成的。每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据。&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;b&gt;用来构建卷积网络的各种层&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：&lt;b&gt;卷积层&lt;/b&gt;，&lt;b&gt;汇聚（Pooling）层&lt;/b&gt;和&lt;b&gt;全连接层&lt;/b&gt;（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。&lt;/p&gt;&lt;p&gt;&lt;i&gt;网络结构例子：&lt;/i&gt;这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。&lt;/li&gt;&lt;li&gt;卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。&lt;/li&gt;&lt;li&gt;ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的&lt;equation&gt;max(0,x)&lt;/equation&gt;作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。&lt;/li&gt;&lt;li&gt;汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。&lt;/li&gt;&lt;li&gt;全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。&lt;/li&gt;&lt;li&gt;卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。&lt;/li&gt;&lt;li&gt;每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。&lt;/li&gt;&lt;li&gt;有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。&lt;/li&gt;&lt;li&gt;有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d9259be829b1cdb3d98a399ebc56defa.jpg" data-rawwidth="1255" data-rawheight="601"&gt;一个卷积神经网络的激活输出例子。左边的输入层存有原始图像像素，右边的输出层存有类别分类评分。在处理流程中的每个激活数据体是铺成一列来展示的。因为对3D数据作图比较困难，我们就把每个数据体切成层，然后铺成一列显示。最后一层装的是针对不同类别的分类得分，这里只显示了得分最高的5个评分值和对应的类别。完整的&lt;a href="http://cs231n.stanford.edu/" data-editable="true" data-title="网页演示"&gt;网页演示&lt;/a&gt;在我们的课程主页。本例中的结构是一个小的VGG网络，VGG网络后面会有讨论。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;现在讲解不同的层，层的超参数和连接情况的细节。&lt;/p&gt;&lt;h4&gt;卷积层&lt;/h4&gt;&lt;p&gt;卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。&lt;/p&gt;&lt;p&gt;&lt;b&gt;概述和直观介绍&lt;/b&gt;：首先讨论的是，再没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。&lt;/p&gt;&lt;p&gt;在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以大脑做比喻&lt;/b&gt;：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;局部连接&lt;/b&gt;：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的&lt;b&gt;接受区域（receptive field）&lt;/b&gt;，它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。&lt;/p&gt;&lt;p&gt;&lt;i&gt;例1&lt;/i&gt;：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果接受区域（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。&lt;/p&gt;&lt;p&gt;&lt;i&gt;例2&lt;/i&gt;：假设输入数据体的尺寸是[16x16x20]，接受区域尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/ba9dcfa847a71cb695c2653230ea9147.jpg" data-rawwidth="1031" data-rawheight="334"&gt;&lt;p&gt;&lt;b&gt;左边&lt;/b&gt;：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（&lt;b&gt;接受区域&lt;/b&gt;相同）。至于深度列的讨论在下文中有。&lt;/p&gt;&lt;p&gt;&lt;b&gt;右边&lt;/b&gt;：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;空间排列&lt;/b&gt;：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：&lt;b&gt;深度（depth），步长（stride）&lt;/b&gt;和&lt;b&gt;零填充（zero-padding）&lt;/b&gt;。下面是对它们的讨论：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、接受区域相同的神经元集合称为&lt;b&gt;深度列（depth column）&lt;/b&gt;，也有人使用纤维（fibre）来称呼它们。&lt;/li&gt;&lt;li&gt;其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。&lt;/li&gt;&lt;li&gt;在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个&lt;b&gt;零填充（zero-padding）&lt;/b&gt;的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的接受区域尺寸（F），步长（S）和零填充的数量（P）的函数来计算。（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这里假设输入数组的空间形状是正方形，即高度和宽度相等&lt;/i&gt;）输出数据体的空间尺寸为(W-F +2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/90af0bd67ba498239688c81fd61bbc66.jpg" data-rawwidth="861" data-rawheight="172"&gt;&lt;p&gt;空间排列的图示。在本例中只有一个空间维度（x轴），神经元的接受区域尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。&lt;/p&gt;&lt;p&gt;本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;i&gt;使用零填充&lt;/i&gt;：在上面左边例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为接受区域是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般说来，当步长&lt;equation&gt;S=1&lt;/equation&gt;时，零填充的值是&lt;equation&gt;P=(F-1)/2&lt;/equation&gt;，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。&lt;/p&gt;&lt;p&gt;&lt;i&gt;步长的限制&lt;/i&gt;：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸&lt;equation&gt;W=10&lt;/equation&gt;，不使用零填充则&lt;equation&gt;P=0&lt;/equation&gt;，滤波器尺寸&lt;equation&gt;F=3&lt;/equation&gt;，这样步长&lt;equation&gt;S=2&lt;/equation&gt;就行不通，因为&lt;equation&gt;(W-F+2P)/S+1=(10-3+0)/2+1=4.5&lt;/equation&gt;，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。&lt;/p&gt;&lt;p&gt;&lt;i&gt;真实案例&lt;/i&gt;：&lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" data-editable="true" data-title="Krizhevsky"&gt;Krizhevsky&lt;/a&gt;构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的接受区域尺寸&lt;equation&gt;F=11&lt;/equation&gt;，步长&lt;equation&gt;S=4&lt;/equation&gt;，不使用零填充&lt;equation&gt;P=0&lt;/equation&gt;。因为(227-11)/4+1=55，卷积层的深度&lt;equation&gt;K=96&lt;/equation&gt;，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。&lt;/p&gt;&lt;p&gt;&lt;b&gt;参数共享&lt;/b&gt;：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。&lt;/p&gt;&lt;p&gt;作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做&lt;b&gt;深度切片（depth slice）&lt;/b&gt;，比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。&lt;/p&gt;&lt;p&gt;注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的&lt;b&gt;卷积&lt;/b&gt;（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为&lt;b&gt;滤波器（filter）&lt;/b&gt;（或&lt;b&gt;卷积核（kernel）&lt;/b&gt;），因为它们和输入进行了卷积。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dd62e1d75bda9b592dabb91627d68aa6.jpg" data-rawwidth="627" data-rawheight="248"&gt;Krizhevsky等学习到的滤波器例子。这96个滤波器的尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为&lt;b&gt;局部连接层&lt;/b&gt;（Locally-Connected Layer）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Numpy例子&lt;/b&gt;：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组&lt;b&gt;X&lt;/b&gt;。那么：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个位于&lt;b&gt;(x,y)&lt;/b&gt;的深度列（或纤维）将会是&lt;b&gt;X[x,y,:]&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;在深度为&lt;b&gt;d&lt;/b&gt;处的深度切片，或激活图应该是&lt;b&gt;X[:,:,d]&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;卷积层例子&lt;/i&gt;：假设输入数据体&lt;b&gt;X&lt;/b&gt;的尺寸&lt;b&gt;X.shape:(11,11,4)&lt;/b&gt;，不使用零填充（&lt;equation&gt;P=0&lt;/equation&gt;），滤波器的尺寸是&lt;equation&gt;F=5&lt;/equation&gt;，步长&lt;equation&gt;S=2&lt;/equation&gt;。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为&lt;b&gt;V&lt;/b&gt;）看起来就是下面这样（在这个例子中，只有部分元素被计算）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在numpy中，&lt;b&gt;*&lt;/b&gt;操作是进行数组间的逐元素相乘。权重向量&lt;b&gt;W0&lt;/b&gt;是该神经元的权重，&lt;b&gt;b0&lt;/b&gt;是其偏差。在这里，&lt;b&gt;W0&lt;/b&gt;被假设尺寸是&lt;b&gt;W0.shape: (5,5,4)&lt;/b&gt;，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 &lt;/b&gt;（在y方向上）&lt;/li&gt;&lt;li&gt;&lt;b&gt;V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 &lt;/b&gt;（或两个方向上同时）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们访问的是&lt;b&gt;V&lt;/b&gt;的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是&lt;b&gt;W1&lt;/b&gt;了。在上面的例子中，为了简洁略去了卷积层对于输出数组&lt;b&gt;V&lt;/b&gt;中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结&lt;/b&gt;： 我们总结一下卷积层的性质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入数据体的尺寸为&lt;equation&gt;W_1\times H_1\times D_1&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;4个超参数：&lt;ul&gt;&lt;li&gt;滤波器的数量&lt;equation&gt;K&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;滤波器的空间尺寸&lt;equation&gt;F&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;步长&lt;equation&gt;S&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;零填充数量&lt;equation&gt;P&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;输出数据体的尺寸为&lt;equation&gt;W_2\times H_2\times D_2&lt;/equation&gt; ，其中：&lt;/li&gt;&lt;ul&gt;&lt;equation&gt;W_2=(W_1-F+2P)/S+1&lt;/equation&gt;&lt;li&gt;&lt;equation&gt;H_2=(H_1-F+2P)/S+1&lt;/equation&gt; （宽度和高度的计算方法相同）&lt;/li&gt;&lt;equation&gt;D_2=K&lt;/equation&gt;&lt;/ul&gt;&lt;li&gt;由于参数共享，每个滤波器包含&lt;equation&gt;F\cdot F\cdot D_1&lt;/equation&gt;个权重，卷积层一共有&lt;equation&gt;F\cdot F\cdot D_1\cdot K&lt;/equation&gt;个权重和&lt;equation&gt;K&lt;/equation&gt;个偏置。&lt;/li&gt;&lt;li&gt;在输出数据体中，第&lt;equation&gt;d&lt;/equation&gt;个深度切片（空间尺寸是&lt;equation&gt;W_2\times H_2&lt;/equation&gt;），用第&lt;equation&gt;d&lt;/equation&gt;个滤波器和输入数据进行有效卷积运算的结果（使用步长&lt;equation&gt;S&lt;/equation&gt;），最后在加上第&lt;equation&gt;d&lt;/equation&gt;个偏差。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对这些超参数，常见的设置是&lt;equation&gt;F=3&lt;/equation&gt;，&lt;equation&gt;S=1&lt;/equation&gt;，&lt;equation&gt;P=1&lt;/equation&gt;。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。&lt;/p&gt;&lt;p&gt;卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是&lt;equation&gt;W_1=5,H_1=5,D_1=3&lt;/equation&gt;，卷积层参数&lt;equation&gt;K=2,F=3,S=2,P=1&lt;/equation&gt;。就是说，有2个滤波器，滤波器的尺寸是&lt;equation&gt;3\cdot 3&lt;/equation&gt;，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充&lt;equation&gt;P=1&lt;/equation&gt;，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/333077b83ed421d6bd53eb7a44fd5799.jpg" data-rawwidth="734" data-rawheight="711"&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：因知乎专栏无法嵌入GIF图片，请读者前往&lt;a href="http://cs231n.github.io/convolutional-networks/" data-editable="true" data-title="斯坦福课程官网查看此演示" class=""&gt;斯坦福课程官网&lt;/a&gt;查看此演示。&lt;/i&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;用矩阵乘法实现&lt;/b&gt;：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;输入图像的局部区域被&lt;b&gt;im2col&lt;/b&gt;操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到&lt;i&gt;im2col&lt;/i&gt;操作的输出矩阵&lt;b&gt;X_col&lt;/b&gt;的尺寸是[363x3025]，其中每列是拉伸的接受区域，共有55x55=3,025个。注意因为接受区域之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。&lt;/li&gt;&lt;li&gt;卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵&lt;b&gt;W_row&lt;/b&gt;，尺寸为[96x363]。&lt;/li&gt;&lt;li&gt;现在卷积的结果和进行一个大矩阵乘&lt;b&gt;np.dot(W_row, X_col)&lt;/b&gt;是等价的了，能得到每个滤波器和每个接受区域间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。&lt;/li&gt;&lt;li&gt;结果最后必须被重新变为合理的输出尺寸[55x55x96]。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在&lt;b&gt;X_col&lt;/b&gt;中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的&lt;a href="http://www.netlib.org/blas/" data-editable="true" data-title="BLAS" class=""&gt;BLAS&lt;/a&gt; API）。还有，同样的&lt;i&gt;im2col&lt;/i&gt;思路可以用在汇聚操作中。&lt;/p&gt;&lt;p&gt;反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1x1卷积&lt;/b&gt;：一些论文中使用了1x1的卷积，这个方法最早是在论文&lt;a href="http://arxiv.org/abs/1312.4400" data-editable="true" data-title="Network in Network"&gt;Network in Network&lt;/a&gt;中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;扩张卷积&lt;/b&gt;：最近一个研究（&lt;a href="https://arxiv.org/abs/1511.07122" data-title="Fisher Yu和Vladlen Koltun的论文" class="" data-editable="true"&gt;Fisher Yu和Vladlen Koltun的论文&lt;/a&gt;）给卷积层引入了一个新的叫&lt;i&gt;扩张（dilation）&lt;/i&gt;的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器&lt;b&gt;w&lt;/b&gt;的尺寸是3，那么计算输入&lt;b&gt;x&lt;/b&gt;的方式是：&lt;b&gt;w[0]*x[0] + w[1]*x[1] + w[2]*x[2]&lt;/b&gt;，此时扩张为0。如果扩张为1，那么计算为： &lt;b&gt;w[0]*x[0] + w[1]*x[2] + w[2]*x[4]&lt;/b&gt;。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的接受区域是输入数据体中5x5的区域（可以成这些神经元的&lt;i&gt;有效接受区域&lt;/i&gt;是5x5）。如果我们对卷积进行扩张，那么这个有效接受区域就会迅速增长。&lt;/p&gt;&lt;h4&gt;汇聚层&lt;/h4&gt;&lt;p&gt;通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;输入数据体尺寸&lt;equation&gt;W_1\cdot H_1\cdot D_1&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;有两个超参数：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;空间大小&lt;equation&gt;F&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;步长&lt;equation&gt;S&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;输出数据体尺寸&lt;equation&gt;W_2\cdot H_2\cdot D_2&lt;/equation&gt;，其中&lt;/li&gt;&lt;equation&gt; W_2=(W_1-F)/S+1&lt;/equation&gt;&lt;equation&gt;H_2=(H_1-F)/S+1&lt;/equation&gt;&lt;equation&gt;D_2=D_1&lt;/equation&gt;&lt;li&gt;因为对输入进行的是固定函数计算，所以没有引入参数&lt;/li&gt;&lt;li&gt;在汇聚层中很少使用零填充&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在实践中，最大汇聚层通常只有两种形式：一种是&lt;equation&gt;F=3,S=2&lt;/equation&gt;，也叫重叠汇聚（overlapping pooling），另一个更常用的是&lt;equation&gt;F=2,S=2&lt;/equation&gt;。对更大接受区域进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;普通汇聚（General Pooling）&lt;/b&gt;：除了最大汇聚，汇聚单元还可以使用其他的函数，比如&lt;i&gt;平均&lt;/i&gt;汇聚&lt;i&gt;（average pooling）&lt;/i&gt;或&lt;i&gt;L-2范式&lt;/i&gt;汇聚&lt;i&gt;（L2-norm pooling）&lt;/i&gt;。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/641c8846abcb02d35938660cf96cef1b.jpg" data-rawwidth="1349" data-rawheight="406"&gt;汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/307530cfd15f5ca2461a2b6f633f93b8.png" data-rawwidth="1014" data-rawheight="3"&gt;&lt;p&gt;&lt;b&gt;反向传播：&lt;/b&gt;回顾一下反向传播的内容，其中&lt;equation&gt;max(x,y)&lt;/equation&gt;函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作&lt;b&gt;道岔（switches）&lt;/b&gt;），这样在反向传播的时候梯度的路由就很高效。&lt;/p&gt;&lt;p&gt;&lt;b&gt;不使用汇聚层&lt;/b&gt;：很多人不喜欢汇聚操作，认为可以不使用它。比如在&lt;a href="http://arxiv.org/abs/1412.6806" data-editable="true" data-title="Striving for Simplicity: The All Convolutional Net" class=""&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt;一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无汇聚层的结构不太可能扮演重要的角色。&lt;/p&gt;&lt;h4&gt;归一化层&lt;/h4&gt;&lt;p&gt;在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于&lt;a href="https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)" data-title="cuda-convnet library API" class="" data-editable="true"&gt;cuda-convnet library API&lt;/a&gt;的讨论。&lt;/p&gt;&lt;h4&gt;全连接层&lt;/h4&gt;&lt;p&gt;在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看&lt;i&gt;神经网络&lt;/i&gt;章节。&lt;/p&gt;&lt;h2&gt;把全连接层转化成卷积层&lt;/h2&gt;&lt;p&gt;全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。&lt;/li&gt;&lt;li&gt;相反，任何全连接层都可以被转化为卷积层。比如，一个&lt;equation&gt;K=4096&lt;/equation&gt;的全连接层，输入数据体的尺寸是&lt;equation&gt;7\times 7\times 512&lt;/equation&gt;，这个全连接层可以被等效地看做一个&lt;equation&gt;F=7,P=0,S=1,K=4096&lt;/equation&gt;的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成&lt;equation&gt;1\times 1\times 4096&lt;/equation&gt;，这个结果就和使用初始的那个全连接层一样了。&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;全连接层转化为卷积层&lt;/b&gt;：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：&lt;ul&gt;&lt;li&gt;针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为&lt;equation&gt;F=7&lt;/equation&gt;，这样输出数据体就为[1x1x4096]了。&lt;/li&gt;&lt;li&gt;针对第二个全连接层，令其滤波器尺寸为&lt;equation&gt;F=1&lt;/equation&gt;，这样输出数据体为[1x1x4096]。&lt;/li&gt;&lt;li&gt;对最后一个全连接层也做类似的，令其&lt;equation&gt;F=1&lt;/equation&gt;，最终输出为[1x1x1000]&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分&lt;/i&gt;），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。&lt;/p&gt;&lt;p&gt;举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解&lt;/i&gt;）&lt;/p&gt;&lt;blockquote&gt;面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。&lt;/blockquote&gt;&lt;p&gt;自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。&lt;/p&gt;&lt;p&gt;最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。（&lt;i&gt;&lt;b&gt;译者注&lt;/b&gt;：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解&lt;/i&gt;）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb" data-title="Net Surgery" class="" data-editable="true"&gt;Net Surgery&lt;/a&gt;上一个使用Caffe演示如何在进行变换的IPython Note教程。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;卷积神经网络的结构&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。&lt;/p&gt;&lt;h4&gt;层的排列规律&lt;/h4&gt;&lt;p&gt;卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此d图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：&lt;/p&gt;&lt;p&gt;&lt;b&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其中&lt;b&gt;*&lt;/b&gt;指的是重复次数，&lt;b&gt;POOL?&lt;/b&gt;指的是一个可选的汇聚层。其中&lt;b&gt;N &amp;gt;=0&lt;/b&gt;,通常&lt;b&gt;N&amp;lt;=3&lt;/b&gt;,&lt;b&gt;M&amp;gt;=0&lt;/b&gt;,&lt;b&gt;K&amp;gt;=0&lt;/b&gt;,通常&lt;b&gt;K&amp;lt;3&lt;/b&gt;。例如，下面是一些常见的网络结构规律：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; FC&lt;/b&gt;,实现一个线性分类器，此处&lt;b&gt;N = M = K = 0&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; FC&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; POOL]*2 -&amp;gt; FC -&amp;gt; RELU -&amp;gt; FC&lt;/b&gt;。此处在每个汇聚层之间有一个卷积层。&lt;/li&gt;&lt;li&gt;&lt;b&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; POOL]*3 -&amp;gt; [FC -&amp;gt; RELU]*2 -&amp;gt; FC&lt;/b&gt;。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;几个小滤波器卷积层的组合比一个大滤波器卷积层好&lt;/i&gt;：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的接受区域的卷积层，那么所有神经元的接受区域也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有&lt;equation&gt;C&lt;/equation&gt;个通道，那么单独的7x7卷积层将会包含&lt;equation&gt;C\times (7\times 7\times C)=49C^2&lt;/equation&gt;个参数，而3个3x3的卷积层的组合仅有&lt;equation&gt;3\times (C\times (3\times 3\times C))=27C^2&lt;/equation&gt;个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。&lt;/p&gt;&lt;p&gt;最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。&lt;/p&gt;&lt;h4&gt;层的尺寸设置规律&lt;/h4&gt;&lt;p&gt;到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：&lt;/p&gt;&lt;p&gt;&lt;b&gt;输入层&lt;/b&gt;（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。&lt;/p&gt;&lt;p&gt;&lt;b&gt;卷积层&lt;/b&gt;应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长&lt;equation&gt;S=1&lt;/equation&gt;。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当&lt;equation&gt;F=3&lt;/equation&gt;，那就使用&lt;equation&gt;P=1&lt;/equation&gt;来保持输入尺寸。当&lt;equation&gt;F=5,P=2&lt;/equation&gt;，一般对于任意&lt;equation&gt;F&lt;/equation&gt;，当&lt;equation&gt;P=(F-1)/2&lt;/equation&gt;的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。&lt;/p&gt;&lt;p&gt;&lt;b&gt;汇聚层&lt;/b&gt;负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2接受区域（即&lt;equation&gt;F=2&lt;/equation&gt;）的最大值汇聚，步长为2（&lt;equation&gt;S=2&lt;/equation&gt;）。注意这一操作将会报输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的接受区域，步长为2。最大值汇聚的接受区域尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。&lt;/p&gt;&lt;p&gt;&lt;i&gt;减少尺寸设置的问题&lt;/i&gt;：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。&lt;/p&gt;&lt;p&gt;&lt;i&gt;为什么在卷积层使用1的步长&lt;/i&gt;？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。&lt;/p&gt;&lt;p&gt;&lt;i&gt;为何使用零填充&lt;/i&gt;？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。&lt;/p&gt;&lt;p&gt;&lt;i&gt;因为内存限制所做的妥协&lt;/i&gt;：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。&lt;/p&gt;&lt;h4&gt;案例学习&lt;/h4&gt;&lt;p&gt;下面是卷积神经网络领域中比较有名的几种结构：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;LeNet&lt;/b&gt;： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" data-editable="true" data-title="LeNet" class=""&gt;LeNet&lt;/a&gt;结构。&lt;/li&gt;&lt;li&gt;&lt;b&gt;AlexNet&lt;/b&gt;：&lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" data-editable="true" data-title="AlexNet" class=""&gt;AlexNet&lt;/a&gt;卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的&lt;a href="http://www.image-net.org/challenges/LSVRC/2014/" data-editable="true" data-title="ImageNet ILSVRC challenge" class=""&gt;ImageNet ILSVRC 竞赛&lt;/a&gt;中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。&lt;/li&gt;&lt;li&gt;&lt;b&gt;ZF Net&lt;/b&gt;：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 &lt;a href="http://arxiv.org/abs/1311.2901" data-editable="true" data-title="ZFNet" class=""&gt;ZFNet&lt;/a&gt;（Zeiler &amp;amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。&lt;/li&gt;&lt;li&gt;&lt;b&gt;GoogLeNet&lt;/b&gt;：ILSVRC 2014的胜利者是谷歌的&lt;a href="http://arxiv.org/abs/1409.4842" data-editable="true" data-title="Szeged等" class=""&gt;Szeged等&lt;/a&gt;实现的卷积神经网络。它主要的贡献就是实现了一个&lt;i&gt;奠基模块&lt;/i&gt;，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是&lt;a href="http://arxiv.org/abs/1602.07261" data-editable="true" data-title="Inception-v4" class=""&gt;Inception-v4&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;VGGNet&lt;/b&gt;：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="VGGNet" class=""&gt;VGGNet&lt;/a&gt;。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="预训练模型"&gt;预训练模型&lt;/a&gt;是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。&lt;/li&gt;&lt;li&gt;&lt;b&gt;ResNet&lt;/b&gt;：&lt;a href="http://arxiv.org/abs/1512.03385" data-editable="true" data-title="残余网络" class=""&gt;残差网络&lt;/a&gt;（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的&lt;i&gt;跳跃链接&lt;/i&gt;，大量使用了&lt;a href="http://arxiv.org/abs/1502.03167" data-title="批量归一化" class="" data-editable="true"&gt;批量归一化&lt;/a&gt;（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="视频"&gt;视频&lt;/a&gt;，&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="PPT"&gt;PPT&lt;/a&gt;），以及一些使用Torch重现网络的&lt;a href="https://github.com/gcr/torch-residual-networks" data-editable="true" data-title="实验"&gt;实验&lt;/a&gt;。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文&lt;a href="https://arxiv.org/abs/1603.05027" data-editable="true" data-title="Identity Mappings in Deep Residual Networks"&gt;Identity Mappings in Deep Residual Networks&lt;/a&gt;，2016年3月发表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;VGGNet的细节：&lt;/b&gt;我们进一步对&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" data-editable="true" data-title="VGGNet"&gt;VGGNet&lt;/a&gt;的细节进行分析学习。整个VGGNet中的卷积层都是以步长为1进行3x3的卷积，使用了1的零填充，汇聚层都是以步长为2进行了2x2的最大值汇聚。可以写出处理过程中每一步数据体尺寸的变化，然后对数据尺寸和整体权重的数量进行查看：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意，大部分的内存和计算时间都被前面的卷积层占用，大部分的参数都用在后面的全连接层，这在卷积神经网络中是比较常见的。在这个例子中，全部参数有140M，但第一个全连接层就包含了100M的参数。&lt;/p&gt;&lt;h2&gt;计算上的考量&lt;/h2&gt;&lt;p&gt;在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。&lt;/li&gt;&lt;li&gt;来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。&lt;/li&gt;&lt;li&gt;卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;拓展资源&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;和实践相关的拓展资源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/soumith/convnet-benchmarks" data-editable="true" data-title="Soumith benchmarks for CONV performance" class=""&gt;Soumith benchmarks for CONV performance&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" data-editable="true" data-title="ConvNetJS CIFAR-10 demo" class=""&gt;ConvNetJS CIFAR-10 demo&lt;/a&gt; 可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。&lt;/li&gt;&lt;li&gt;&lt;a href="http://caffe.berkeleyvision.org/" data-editable="true" data-title="Caffe" class=""&gt;Caffe&lt;/a&gt;，一个流行的卷积神经网络库。&lt;/li&gt;&lt;li&gt;&lt;a href="http://torch.ch/blog/2016/02/04/resnets.html" data-editable="true" data-title="State of the art ResNets in Torch7" class=""&gt;State of the art ResNets in Torch7&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;卷积神经网络笔记&lt;/b&gt;结束。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;译者反馈&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接，否则保留维权权利&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;各位知友如果发现翻译不当的地方，欢迎通过评论和私信等方式批评指正，我们会在文中补充感谢贡献者；&lt;/li&gt;&lt;li&gt;CS231n的翻译进入尾声，&lt;b&gt;欢迎知友们建议后续的翻译方向&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;本文最初的译稿由杜客和我于五月底完成，为了保证质量，我们两人每人翻译了一个版本，然后结合两个译稿的长处来编辑最终的版本。本文的绝大部分都保留了杜客的版本，在少数段落和语句上采用了我的版本。整个翻译小组在校对工作上付出了很多努力，为译稿提出了近百条修改意见，使得译稿逐渐完善。杜客为了鼓励新手，让此文以我的ID投稿发表。这是我第一次发表文章，非常激动^_^；&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/chen-yi-91-27" class="" data-editable="true" data-title="陈一"&gt;陈一&lt;/a&gt; 对细节的指正。&lt;/li&gt;&lt;/ol&gt;</description><author>猴子</author><pubDate>Sat, 20 Aug 2016 12:24:22 GMT</pubDate></item><item><title>「无中生有」计算机视觉探奇</title><link>https://zhuanlan.zhihu.com/p/21341440</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3db4d60c513ffde3c66eb745bbf36f99_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;计算机视觉 (Computer Vision, CV) 是一门研究如何使机器“看”的科学。1963年来自MIT的Larry Roberts发表的该领域第一篇博士论文“&lt;a href="https://dspace.mit.edu/handle/1721.1/11589" class="" data-editable="true" data-title="Machine Perception of Three-Dimensional Solids"&gt;Machine Perception of Three-Dimensional Solids&lt;/a&gt;”，标志着CV作为一门新兴人工智能方向研究的开始。在发展了50多年后的今天，我们就来聊聊最近让计算机视觉拥有「无中生有」能力的几个有趣尝试：1）超分辨率重建；2）图像着色；3）看图说话；4）人像复原；5）图像自动生成。可以看出，这五个尝试层层递进，难度和趣味程度也逐步提升。（注：本文在此只谈视觉问题，不提太过具体的技术细节，若大家对某部分感兴趣，以后再来单独写文章讨论 :)&lt;/p&gt;&lt;h2&gt;超分辨率重建 (Image Super-Resolution)&lt;/h2&gt;&lt;p&gt;去年夏天，一款名为“&lt;a href="http://waifu2x.udp.jp/" class="" data-title="waifu 2x" data-editable="true"&gt;waifu 2x&lt;/a&gt;”的岛国应用在动画和计算机图形学中着实火了一把。waifu 2x借助深度「卷积神经网络」(Convolutional Neural Network, CNN) 可以将图像的分辨率提升2倍，同时还能对图像降噪。简单来说，就是让计算机「无中生有」的填充一些原图中并没有的像素，从而让漫画看起来更清晰真切。大家不妨看看下图，真想童年时候看的就是如此&lt;a href="http://www.bilibili.com/video/av2812282/" class="" data-title="高清的龙珠" data-editable="true"&gt;高清的龙珠&lt;/a&gt;啊！&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/702440cec176c72f871d009a8568b3eb.jpg" data-rawwidth="1348" data-rawheight="864"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a50629abf235a9750a72aa65ffc06b5c.jpg" data-rawwidth="1344" data-rawheight="1044"&gt;&lt;p&gt;不过需要指出的是，图像超分辨率的研究始于2009年左右，只是得力于「深度学习」的发展，waifu 2x可以做出更好的效果。在具体训练CNN时，输入图像为原分辨率，

















而对应的超分辨率图像则作为目标，以此构成训练的“图像对”
(image pair)，经过模型训练便可得到超分辨率重建模型。waifu 2x的深度网络原型基于香港中文大学汤晓欧教授团队的工作&lt;a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" class="" data-title="[1]" data-editable="true"&gt;[1]&lt;/a&gt;。有趣的是，[1]中指出可以用传统方法给予深度模型以定性的解释。如下图，低分辨率图像通过CNN的卷积
(convolution) 和池化
(pooling) 操作后可以得到抽象后的特征图 (feature map)。基于低分辨率特征图，同样可以利用卷积和池化实现从低分辨率到高分辨率特征图的非线性映射 (non-linear mapping)。最后的步骤则是利用高分辨率特征图重建高分辨率图像。实际上，所述三个步骤与传统超分辨率重建方法的三个过程是一致的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/be6e82cc9984511924e111fd94684c00.jpg" data-rawwidth="1318" data-rawheight="480"&gt;&lt;h2&gt;图像着色 (Image Colorization)&lt;/h2&gt;&lt;p&gt;顾名思义，图像着色是将原本「没有」颜色的黑白图像进行彩色填充。图像着色同样借助卷积神经网络，输入为黑白和对应彩色图像的image pair，但是仅仅通过对比黑白像素和RGB像素来确定填充的颜色，效果欠佳。因为颜色填充的结果要符合我们的认知习惯，比如，把一条汪星人的毛涂成鲜绿色就会让人觉得很怪异。于是近期，早稻田大学发表在2016年计算机图形学国际顶级会议SIGGRAPH上的一项工作&lt;a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/" class="" data-editable="true" data-title="[2]"&gt;[2]&lt;/a&gt;就在原来深度模型的基础上，加入了「分类网络」来预先确定图像中物体的类别，以此为“依据”再做以颜色填充。下图分别是模型结构图和颜色恢复demo，其恢复效果还是颇为逼真的。另外，此类工作还可用于黑白电影的颜色恢复，操作时只需简单的将视频中每一帧拿出来作colorization即可。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e4dfc9b082694131d25be97bf97b3709.jpg" data-rawwidth="1640" data-rawheight="702"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/a710c710a592ca7a6b723328207b280f.jpg" data-rawwidth="1602" data-rawheight="652"&gt;&lt;h2&gt;"看图说话" (Image Caption)&lt;/h2&gt;&lt;p&gt;常说“图文并茂”，文字是除图像外另一种描述世界的方式。

















近期，一项名为“image
caption”的研究逐渐升温起来，其主要任务是通过计算机视觉和机器学习的方法实现对一张图像自动地生成人类自然语言的描述，即“看图说话”。值得一提的是，在今年的CV国际顶会&lt;a href="http://cvpr2016.thecvf.com/" class="" data-editable="true" data-title="CVPR"&gt;CVPR&lt;/a&gt;上，image caption被列为了一个单独的session，其热度可见一斑。一般来讲在image caption中，CNN用来获取图像特征，接着将图像特征作为语言模型&lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="" data-editable="true" data-title="LSTM"&gt;LSTM&lt;/a&gt;（RNN的一种）的输入，整体作为一个「end-to-end」的结构进行联合训练，最终输出对图像的语言描述（见下图）。&lt;/p&gt;&lt;p&gt;目前image caption领域的最好结果&lt;a href="http://arxiv.org/abs/1506.01144" data-editable="true" data-title="[3]"&gt;[3]&lt;/a&gt;来自澳大利亚University of Adelaide的&lt;a href="http://cs.adelaide.edu.au/~chhshen/index.html" class="" data-editable="true" data-title="Chunhua Shen"&gt;Chunhua Shen&lt;/a&gt;教授团队。与之前image caption工作相比，他们的改进与刚才提到的颜色恢复简直有异曲同工之妙，同样是考虑利用图像中物体的类别作为较精准的“依据”来更好的生成自然语言描述，即下图中的红色框框圈起的部分。Image caption的急速发展不仅加速了CV和NLP在AI大领域内的交融，同时也为增强现实应用奠定了更加坚实的技术基础。另外，我们更乐于看到今后日趋成熟的image caption技术嵌入到穿戴式设备上，那一天盲人便可以间接的“看到光明”。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/dffe9d6d4757cece60ec8edd37d20f5a.jpg" data-rawwidth="1518" data-rawheight="1290"&gt;&lt;h2&gt;人像复原 (Sketch Inversion)&lt;/h2&gt;&lt;p&gt;就在六月初，荷兰科学家在&lt;a href="http://arxiv.org/" data-editable="true" data-title="arXiv"&gt;arXiv&lt;/a&gt;上发布了他们的最新研究成果&lt;a href="https://arxiv.org/abs/1606.03073" data-editable="true" data-title="[4]" class=""&gt;[4]&lt;/a&gt;——通过深度网络对人脸轮廓图进行「复原」。如下图所示，

















在模型训练阶段，首先对真实的人脸图像利用传统的线下边缘化方法获得对应人脸的轮廓图，并以原图和轮廓图组成的“图像对”作为深度网络的输入，进行类似超分辨率重建的模型训练。在预测阶段，输入为人脸轮廓（左二sketch），经过卷积神经网络的层层抽象和后续的“还原”可以逐步把相片般的人脸图像复原出来（右一），与最左边的人脸真实图像对比，足够以假乱真。在模型流程图下还另外展示了一些人像复原的结果，左侧一列为真实人像，中间列为画家手工描绘的人脸轮廓图，并以此作为网络输入进行人像复原，最终得到右侧一列的复原结果——目测以后刑侦警察再也不用苦练美术了    😂。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/03fde095d58a761d251f4d97529636ff.jpg" data-rawwidth="1840" data-rawheight="256"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/3f6c0451b50f638a9f89fd0823525862.jpg" data-rawwidth="1534" data-rawheight="502"&gt;&lt;h2&gt;图像自动生成&lt;/h2&gt;&lt;p&gt;回顾刚才的四个工作，其实他们的共同点是仍然需要依靠一些“素材”方可「无中生有」，例如“人像复原”还是需要一个轮廓画才可以恢复人像。接下来的这个工作则可以做到由任意一条随机向量生成一张逼近真实场景下的图像。&lt;/p&gt;&lt;p&gt;「无监督学习」可谓计算机视觉的圣杯。最近该方向的一项开创性工作是由Ian Goodfellow和 Yoshua Bengio等提出的「生成对抗网络」&lt;a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" data-title="(Generative Adversarial Nets, GAN)" class="" data-editable="true"&gt;(Generative Adversarial Nets, GAN)&lt;/a&gt;。该工作的灵感来自博弈论中的&lt;a href="https://en.wikipedia.org/wiki/Zero-sum_game" data-title="零和博弈" class="" data-editable="true"&gt;零和博弈&lt;/a&gt;。在二元零和博弈中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。而GAN中的两位博弈方分别由一个「判别式网络」（图左）和一个「生成式网络」（图右下半部分）充当。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/32913217c610e3197d79685e6a03545a.jpg" data-rawwidth="1626" data-rawheight="1110"&gt;其中，「判别式网络」&lt;equation&gt;D&lt;/equation&gt;的输入为图像&lt;equation&gt;x&lt;/equation&gt;，其作用是判断&lt;equation&gt;x&lt;/equation&gt;是一张真实图像还是一张由计算机生成的图像；「生成式网络」&lt;equation&gt;G&lt;/equation&gt;的输入为一条随机向量&lt;equation&gt;z&lt;/equation&gt;，&lt;equation&gt;z&lt;/equation&gt;可以通过网络“生成”一张合成图像。这张合成图像亦可作为「判别式网络」&lt;equation&gt;D&lt;/equation&gt;的输入，只是此时，在理想情况下&lt;equation&gt;D&lt;/equation&gt;应能判断出它是由计算机生成的。&lt;/p&gt;&lt;p&gt;接下来，GAN中的零和博弈就发生在「判别式网络」和「生成式网络」上：「生成式网络」想方设法的让自己生成的图像逼近真实图像，从而可以“骗过”「判别式网络」；而「判别式网络」也时刻提高警惕，防止「生成式网络」蒙混过关……你来我往，如此迭代下去，颇有点“左右互搏”的意味。GAN整个过程的最终目标是习得一个可以逼近真实数据分布的「生成式网络」，从而掌握整体真实数据的分布情况，因此取名「生成对抗网络」。需要强调的是，GAN不再像传统的监督式深度学习那样需要海量带有类别标记的图像，GAN不需任何图像标记即可训练，也就是进行无监督条件下的深度学习。2016年初，在GAN的基础上，Indico Research和Facebook AI实验室将GAN用深度卷积神经网络进行实现（称作，&lt;a href="https://arxiv.org/abs/1511.06434" data-editable="true" data-title="DCGAN"&gt;DCGAN&lt;/a&gt;, Deep Convolutional GAN），工作发表在国际表示学习重要会议&lt;a href="http://www.iclr.cc/doku.php" data-editable="true" data-title="ICLR 2016"&gt;ICLR 2016&lt;/a&gt;上，并在无监督深度学习模型中取得了当时最好的效果。下图展示了一些由DCGAN生成的"bedroom"图像。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d0b5f3083c1985d8c121e5e787a137d9.jpg" data-rawwidth="1368" data-rawheight="688"&gt;更为interesting的是，DCGAN还可以像&lt;a href="https://en.wikipedia.org/wiki/Word2vec" data-editable="true" data-title="word2vec" class=""&gt;word2vec&lt;/a&gt;一样支持图像“语义”层面的加减。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7babca273fd052cd7a85664cca776edf.jpg" data-rawwidth="1274" data-rawheight="670"&gt;另外，前些天“生成式计算机视觉”研究领域大牛UCLA的Song-Chun Zhu教授团队发布了他们基于生成式卷积网络的最新工作&lt;a href="http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html" class="" data-title="STGConvNet" data-editable="true"&gt;STGConvNet&lt;/a&gt;：不仅可以自动合成动态纹理，同时还可以合成声音，可以说将无监督计算机视觉又向前推进了一大步。（下图是两个demo GIF，左侧是真实动态纹理，右侧是STGConvNet的合成纹理。不动&lt;a href="http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html" data-editable="true" data-title="戳我" class=""&gt;戳我&lt;/a&gt;～）&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/272674a266f12f970a963b28d4da50f6.jpg" data-rawwidth="463" data-rawheight="224"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/080497cf13089e6b1403823527ef5524.jpg" data-rawwidth="463" data-rawheight="224"&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;如今借着「深度学习」的东风，计算机视觉中绝大多数任务的performance都被“刷”上了新高，甚至连“人像复原”，“图像生成”类似「无中生有」的“奇谈”都能以较高质量地实现，着实让人们激动不已。不过尽管如此，事实上距离所谓的颠覆人类的AI“奇点”还相当遥远，并且可以预见，现阶段甚至相当长的一段时间内，计算机视觉或人工智能还不可能做到真正意义上的「无中生有」，即“自我开创”或称为“自我意识”。&lt;/p&gt;&lt;p&gt;然而，也非常庆幸我们可以目睹并且亲身经历这次计算机视觉乃至是整个人工智能的革命浪潮，相信今后一定还会有更多「无中生有」般的奇迹发生。此刻，我们站在浪潮之巅，因此我们兴奋不已、彻夜难眠。&lt;/p&gt;&lt;p&gt;6月18日于南京&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多精彩内容，还请关注&lt;a href="https://www.zhihu.com/people/wei-xiu-shen"&gt;我&lt;/a&gt;的专栏&lt;a href="https://zhuanlan.zhihu.com/furthersight" class="" data-editable="true" data-title="「欲穷千里目」"&gt;「欲穷千里目」&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;（声明：原文版权属于CSDN《程序员》杂志原创，作者为本人，发表于2016年7月刊。欢迎订阅！）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5c3e27c7bd10db36c4db98aeec0f0715.jpg" data-rawwidth="591" data-rawheight="756"&gt;&lt;h2&gt;&lt;b&gt;References:&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;[1] Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. &lt;b&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/b&gt;, &lt;i&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/i&gt;, Preprint, 2015.&lt;/p&gt;&lt;p&gt;[2] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. &lt;b&gt;Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification&lt;/b&gt;, &lt;i&gt;In Proc. of SIGGRAPH 2016&lt;/i&gt;, to appear.&lt;/p&gt;&lt;p&gt;[3] Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel. &lt;b&gt;What value do explicit high level concepts have in vision to language problems, &lt;/b&gt;&lt;i&gt;In Proc. of CVPR 2016&lt;/i&gt;, to appear.&lt;/p&gt;&lt;p&gt;[4] Yağmur Güçlütürk, Umut Güçlü, Rob van Lier, Marcel A. J. van Gerven. &lt;b&gt;Convolutional Sketch Inversion&lt;/b&gt;, &lt;i&gt;arXiv:1606.03073&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. &lt;b&gt;Generative Adversarial Nets&lt;/b&gt;, &lt;i&gt;In Proc. of NIPS 2014.&lt;/i&gt;&lt;/p&gt;[6] Jianwen Xie, Song-Chun Zhu, Ying Nian Wu. &lt;b&gt;Synthesizing Dynamic Textures and Sounds by Spatial-Temporal Generative ConvNet&lt;/b&gt;, &lt;i&gt;arXiv:1606.00972.&lt;/i&gt;</description><author>魏秀参</author><pubDate>Mon, 04 Jul 2016 20:32:07 GMT</pubDate></item><item><title>智能单元专栏投稿说明</title><link>https://zhuanlan.zhihu.com/p/21917736</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7d2d39eb9ae721a13607713c73193172_r.jpg"&gt;&lt;/p&gt;首先感谢各位知友对于本专栏的支持！发布本说明是因为收到知友的投稿和建议，促使我和&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 重新思考专栏的定位和意义。&lt;h2&gt;来龙去脉&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原本定位&lt;/b&gt;：这个专栏&lt;b&gt;原本只是我们整理记录深度学习笔记和思考的地方&lt;/b&gt;，立足相互促进学习，自产自销，没考虑过有人投稿。由于各位知友抬爱，也收到了一些投稿及意见。&lt;/li&gt;&lt;li&gt;&lt;b&gt;主要疑虑&lt;/b&gt;：接受投稿，就要负起审稿甚至查询是否原创等相关责任，而我们时间有限，刚开始存在多一事不如少一事的心理。自己写的文章，自己把关。面对知友的投稿，我们可能方向不同，或水平有限，也难以做出权威的判断。&lt;/li&gt;&lt;li&gt;&lt;b&gt;主要动力&lt;/b&gt;：知友的支持，增进交流的初心和把专栏做好的小小成就感。最终，我们还是决定开放投稿，并讨论出一些向本专栏投稿的原则性思路。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;重要说明&lt;/b&gt;：我们只是凭借自己的知识背景进行讨论，缺乏更多法律背景上的相关知识，&lt;u&gt;&lt;b&gt;如果知友发现我们的投稿要求有不妥的地方，敬请评论或私信指正&lt;/b&gt;！&lt;/u&gt;我们也&lt;b&gt;会根据大家的反馈持续对下面的投稿要求进行修改&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;向专栏投稿&lt;/h2&gt;&lt;p&gt;&lt;b&gt;投稿原则&lt;/b&gt;：本专栏接受的投稿应满足：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原创&lt;/b&gt;：我们更推崇原创的实践与思考，以及由此而激起的理性探讨交流。不强求首发，如果您想投稿自己以前的文章，也是可以的。&lt;/li&gt;&lt;li&gt;&lt;b&gt;题材&lt;/b&gt;：深度学习相关的技术内容。比如前沿的新技术原创实践与思考，重要的资料翻译，和自己的实践与思考总结等。&lt;/li&gt;&lt;li&gt;&lt;b&gt;排版&lt;/b&gt;：请合理使用知乎的文章的编辑功能，让排版简约大方。&lt;/li&gt;&lt;li&gt;&lt;b&gt;版权&lt;/b&gt;：&lt;b&gt;版权归原作者所有&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;互动&lt;/b&gt;：&lt;b&gt;倡导&lt;/b&gt;投稿者尽可能与读者评论进行优质互动，维护专栏良好的讨论氛围。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;赞赏所得&lt;/b&gt;：本专栏已经开通赞赏功能，&lt;b&gt;赞赏所得归投稿者所有&lt;/b&gt;。具体操作请参考知乎产品专栏中的说明：&lt;a href="https://zhuanlan.zhihu.com/p/21268480?refer=zhihu-product" data-title="知乎专栏文章开始内测" class=""&gt;知乎专栏文章开始内测&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;审稿原则&lt;/b&gt;：我们会在认真阅读投稿后和你进行讨论。如果在投稿题材上不是很有把握，也可以直接私信&lt;a href="https://www.zhihu.com/people/928affb05b0b70a2c12e109d63b6bae5" data-hash="928affb05b0b70a2c12e109d63b6bae5" class="member_mention" data-title="@杜客" data-hovercard="p$b$928affb05b0b70a2c12e109d63b6bae5"&gt;@杜客&lt;/a&gt; 或&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 提前交流。 &lt;/p&gt;&lt;h2&gt;最后的话&lt;/h2&gt;&lt;p&gt;期待更多小伙伴的投稿、交流和拍砖的同时，我和&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt; 依旧会坚持原创与翻译，保证高质量的输出。&lt;/p&gt;&lt;h2&gt;修正记录&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;经知友建议，增加投稿原则中关于与读者互动的建议性原则。2016.08.22；&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Sun, 14 Aug 2016 21:30:21 GMT</pubDate></item><item><title>斯坦福CS231n课程作业# 3简介</title><link>https://zhuanlan.zhihu.com/p/21946525</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/89758df295a0927d26da59356338a2ff_r.jpg"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，由@&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译自斯坦福CS231n课程作业1介绍页面&lt;a href="http://cs231n.github.io/assignments2016/assignment2/" class="" data-editable="true" data-title="[Assignment #2]"&gt;[Assignment #2]&lt;/a&gt;。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;在本作业中，你将实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。我们还会介绍TinyImageNet数据集，然后在这个数据集使用一个预训练的模型来查看图像梯度的不同应用。本作业的目标如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;理解&lt;i&gt;循环神经网络（RNN）&lt;/i&gt;的结构，知道它们是如何随时间共享权重来对序列进行操作的。&lt;/li&gt;&lt;li&gt;理解普通循环神经网络和长短基记忆（Long-Short Term Memory）循环神经网络之间的差异。&lt;/li&gt;&lt;li&gt;理解在测试时如何从RNN生成序列。&lt;/li&gt;&lt;li&gt;理解如何将卷积神经网络和循环神经网络结合在一起来实现图像标注。&lt;/li&gt;&lt;li&gt;理解一个训练过的卷积神经网络是如何用来从输入图像中计算梯度的。&lt;/li&gt;&lt;li&gt;进行高效的交叉验证并为神经网络结构找到最好的超参数。&lt;/li&gt;&lt;li&gt;实现图像梯度的不同应用，比如显著图，搞笑图像，类别可视化，特征反演和DeepDream。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;安装&lt;/h2&gt;&lt;p&gt;有两种方法来完成作业：在本地使用自己的机器，或者使用&lt;a href="https://link.zhihu.com/?target=http%3A//Terminal.com" class="" data-editable="true" data-title="http://Terminal.com"&gt;http://Terminal.com&lt;/a&gt;的虚拟机。&lt;/p&gt;&lt;h3&gt;云端作业&lt;/h3&gt;&lt;p&gt;Terminal公司为我们的课程创建了一个单独的子域名：&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/" class="" data-editable="true" data-title="www.stanfordterminalcloud.com"&gt;www.stanfordterminalcloud.com&lt;/a&gt;。在该域名下注册。作业2的快照可以在&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/snapshot/49f5a1ea15dc424aec19155b3398784d57c55045435315ce4f8b96b62819ef65" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;找到。如果你注册到了本课程，就可以联系上助教（更多信息请上Piazza）来得到用来做作业的点数。一旦你启动了快照，所有的环境都是为你配置好的，马上就可以开始作业。我们在Terminal上写了一个简明&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/terminal-tutorial/" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;本地作业&lt;/h3&gt;&lt;p&gt;点击&lt;a href="http://cs231n.stanford.edu/winter1516_assignment3.zip" class="" data-editable="true" data-title="此处"&gt;此处&lt;/a&gt;下载代码压缩文件。初次之外还有些库间依赖的配置：&lt;/p&gt;&lt;p&gt;&lt;b&gt;[选项1]使用Anaconda&lt;/b&gt;：推荐方法是安装&lt;a href="https://link.zhihu.com/?target=https%3A//www.continuum.io/downloads" class="" data-editable="true" data-title="Anaconda"&gt;Anaconda&lt;/a&gt;，它是Python的一个发布版，包含了最流行的科研、数学、工程和数据分析Python包。一旦安装了它，下面的提示就都可略过，准备直接开始写作业吧。&lt;i&gt;译者注：推荐。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;[选项2]手动安装，虚拟环境&lt;/b&gt;：如果你不想用Anaconda，想要走一个充满风险的手动安装路径，那么可能就要为项目创建一个&lt;a href="https://link.zhihu.com/?target=http%3A//docs.python-guide.org/en/latest/dev/virtualenvs/" class="" data-editable="true" data-title="虚拟环境"&gt;虚拟环境&lt;/a&gt;了。如果你不想用虚拟环境，那么你的确保所有代码需要的依赖关系都是景在你的机器上被安装了。要建立虚拟环境，运行下面代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd assignment3
sudo pip install virtualenv      # This may already be installed
virtualenv .env                  # Create a virtual environment
source .env/bin/activate         # Activate the virtual environment
pip install -r requirements.txt  # Install dependencies
# Work on the assignment for a while ...
deactivate                       # Exit the virtual environment
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;下载数据&lt;/b&gt;：一旦得到作业初始代码，你就需要下载CIFAR-10数据集，然后在assignment1目录下运行下面代码：&lt;i&gt;译者注：也可手动下载解压后放到&lt;/i&gt;&lt;i&gt;cs231n/datasets目录&lt;/i&gt;&lt;i&gt;。&lt;/i&gt;&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd cs231n/datasets 
./get_coco_captioning.sh
./get_tiny_imagenet_a.sh
./get_pretrained_model.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;编译Cython扩展包&lt;/b&gt;：卷积神经网络需要一个高效的实现。我们使用&lt;a href="http://cython.org/" data-editable="true" data-title="Cython" class=""&gt;Cython&lt;/a&gt;实现了一些函数。在运行代码前，你需要编译Cython扩展包。在cs231n目录下，运行下面命令：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;python setup.py build_ext --inplace
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;b&gt;启用IPython&lt;/b&gt;：得到了CIFAR-10数据集之后，你应该在作业assignment1目录中启用IPython notebook的服务器，如果对IPython notebook不熟悉，可以阅读&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/ipython-tutorial" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意&lt;/b&gt;：如果你是在OSX上的虚拟环境中工作，可能会遇到一个由matplotlib导致的错误，原因在&lt;a href="https://link.zhihu.com/?target=http%3A//matplotlib.org/faq/virtualenv_faq.html" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。你可以通过在assignment2目录中运行start_ipython_osx.sh脚本来解决问题。&lt;/p&gt;&lt;h2&gt;提交作业&lt;/h2&gt;&lt;p&gt;无论你是在云终端还是在本地完成作业，一旦完成作业，就运行collectSubmission.sh脚本；这样将会产生一个assignment3.zip的文件，然后将这个文件上传到你的dropbox中这门课的&lt;a href="https://coursework.stanford.edu/portal/site/W15-CS-231N-01/" class="" data-editable="true" data-title="作业页面"&gt;作业页面&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;Q1：使用普通RNN进行图像标注（40分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;RNN_Captioning.ipynb&lt;/b&gt;将会带你使用普通RNN实现一个在微软COCO数据集上的图像标注系统。&lt;/p&gt;&lt;h3&gt;Q2：使用LSTM进行图像标注（35分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;LSTM_Captioning.ipynb&lt;/b&gt;将会带你实现LSTM，并应用于在微软COCO数据集上进行图像标注。&lt;/p&gt;&lt;h3&gt;Q3：图像梯度：显著图和高效图像（10分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;ImageGradients.ipynb&lt;/b&gt;将会介绍TinyImageNet数据集。你将使用一个训练好的模型在这个数据集上计算梯度，然后将其用于生成显著图和高效图像。&lt;/p&gt;&lt;h3&gt;Q4：图像生成：类别，反演和DeepDream（30分）&lt;/h3&gt;&lt;p&gt;在IPython Notebook文件&lt;b&gt;ImageGeneration.ipynb&lt;/b&gt;中，你将使用一个训练好的TinyImageNet模型来生成图像。具体说来，你将生成类别可视化，实现特征反演和DeepDream。&lt;/p&gt;&lt;h3&gt;Q5：做点儿其他的！（+10分）&lt;/h3&gt;&lt;p&gt;根据作业内容，做点够酷的事儿。比如作业中没有讲过的其他生成图像的方式？&lt;/p&gt;&lt;p&gt;&lt;b&gt;全文完。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;译者反馈：&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;转载须全文转载并注明原文链接，否则保留维权权利；&lt;/li&gt;&lt;li&gt;如对翻译有意见建议，请通过评论批评指正，贡献者均会补充提及；&lt;/li&gt;&lt;li&gt;后续将根据作业内容和自己的学习笔记原创教程；&lt;/li&gt;&lt;li&gt;感谢&lt;a href="https://www.zhihu.com/people/f38193bc2f7f3c10c55042d009b411a5" data-hash="f38193bc2f7f3c10c55042d009b411a5" class="member_mention" data-hovercard="p$b$f38193bc2f7f3c10c55042d009b411a5"&gt;@Frankenstein&lt;/a&gt;的纠错。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Thu, 11 Aug 2016 06:30:36 GMT</pubDate></item><item><title>斯坦福CS231n课程作业# 2简介</title><link>https://zhuanlan.zhihu.com/p/21941485</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/4b4c825806caf0c939f97a95be32944c_r.jpg"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，由@&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译自斯坦福CS231n课程作业1介绍页面&lt;a href="http://cs231n.github.io/assignments2016/assignment2/" class="" data-editable="true" data-title="[Assignment #2]"&gt;[Assignment #2]&lt;/a&gt;。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;在本作业中，你将练习编写反向传播代码，训练神经网络和卷积神经网络。本作业的目标如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;理解&lt;b&gt;神经网络&lt;/b&gt;及其分层结构。&lt;/li&gt;&lt;li&gt;理解并实现（向量化）&lt;b&gt;反向传播&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;实现多个用于神经网络最优化的&lt;b&gt;更新方法&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;实现用于训练深度网络的&lt;b&gt;批量归一化&lt;/b&gt;（ &lt;strong&gt;batch normalization&lt;/strong&gt; ）。&lt;/li&gt;&lt;li&gt;实现&lt;b&gt;随机失活&lt;/b&gt;（&lt;b&gt;dropout&lt;/b&gt;）。&lt;/li&gt;&lt;li&gt;进行高效的&lt;b&gt;交叉验证&lt;/b&gt;并为神经网络结构找到最好的超参数。&lt;/li&gt;&lt;li&gt;理解&lt;b&gt;卷积神经网络&lt;/b&gt;的结构，并积累在数据集上训练此类模型的经验。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;安装&lt;/h2&gt;&lt;p&gt;有两种方法来完成作业：在本地使用自己的机器，或者使用&lt;a href="https://link.zhihu.com/?target=http%3A//Terminal.com" class="" data-editable="true" data-title="http://Terminal.com"&gt;http://Terminal.com&lt;/a&gt;的虚拟机。&lt;/p&gt;&lt;h3&gt;云端作业&lt;/h3&gt;&lt;p&gt;Terminal公司为我们的课程创建了一个单独的子域名：&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/" class="" data-editable="true" data-title="www.stanfordterminalcloud.com"&gt;www.stanfordterminalcloud.com&lt;/a&gt;。在该域名下注册。作业2的快照可以在&lt;a href="https://link.zhihu.com/?target=https%3A//www.stanfordterminalcloud.com/snapshot/49f5a1ea15dc424aec19155b3398784d57c55045435315ce4f8b96b62819ef65" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;找到。如果你注册到了本课程，就可以联系上助教（更多信息请上Piazza）来得到用来做作业的点数。一旦你启动了快照，所有的环境都是为你配置好的，马上就可以开始作业。我们在Terminal上写了一个简明&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/terminal-tutorial/" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;本地作业&lt;/h3&gt;&lt;p&gt;点击&lt;a href="http://vision.stanford.edu/teaching/cs231n/winter1516_assignment2.zip" class="" data-editable="true" data-title="此处"&gt;此处&lt;/a&gt;下载代码压缩文件。初次之外还有些库间依赖的配置：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;[选项1]使用Anaconda&lt;/strong&gt;：推荐方法是安装&lt;a href="https://link.zhihu.com/?target=https%3A//www.continuum.io/downloads" class="" data-editable="true" data-title="Anaconda"&gt;Anaconda&lt;/a&gt;，它是Python的一个发布版，包含了最流行的科研、数学、工程和数据分析Python包。一旦安装了它，下面的提示就都可略过，准备直接开始写作业吧。&lt;i&gt;&lt;b&gt;译者注：推荐。&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;[选项2]手动安装，虚拟环境&lt;/strong&gt;：如果你不想用Anaconda，想要走一个充满风险的手动安装路径，那么可能就要为项目创建一个&lt;a href="https://link.zhihu.com/?target=http%3A//docs.python-guide.org/en/latest/dev/virtualenvs/" class="" data-editable="true" data-title="虚拟环境"&gt;虚拟环境&lt;/a&gt;了。如果你不想用虚拟环境，那么你的确保所有代码需要的依赖关系都是景在你的机器上被安装了。要建立虚拟环境，运行下面代码：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd assignment2
sudo pip install virtualenv      # This may already be installed
virtualenv .env                  # Create a virtual environment
source .env/bin/activate         # Activate the virtual environment
pip install -r requirements.txt  # Install dependencies
# Work on the assignment for a while ...
deactivate                       # Exit the virtual environment
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;下载数据&lt;/strong&gt;：一旦得到作业初始代码，你就需要下载CIFAR-10数据集，然后在&lt;b&gt;assignment1&lt;/b&gt;目录下运行下面代码：&lt;i&gt;&lt;b&gt;译者注：也可手动下载解压后放到&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;cs231n/datasets目录&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;。&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;cd cs231n/datasets 
./get_datasets.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;编译Cython扩展包：&lt;/strong&gt;卷积神经网络需要一个高效的实现。我们使用&lt;a href="http://cython.org/" data-editable="true" data-title="Cython" class=""&gt;Cython&lt;/a&gt;实现了一些函数。在运行代码前，你需要编译Cython扩展包。在&lt;b&gt;cs231n&lt;/b&gt;目录下，运行下面命令：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;python setup.py build_ext --inplace
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;启用IPython&lt;/strong&gt;：得到了CIFAR-10数据集之后，你应该在作业&lt;b&gt;assignment1&lt;/b&gt;目录中启用IPython notebook的服务器，如果对IPython notebook不熟悉，可以阅读&lt;a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/ipython-tutorial" class="" data-editable="true" data-title="教程"&gt;教程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：如果你是在OSX上的虚拟环境中工作，可能会遇到一个由&lt;strong&gt;matplotlib&lt;/strong&gt;导致的错误，原因在&lt;a href="https://link.zhihu.com/?target=http%3A//matplotlib.org/faq/virtualenv_faq.html" class="" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。你可以通过在&lt;b&gt;assignment2&lt;/b&gt;目录中运行&lt;b&gt;start_ipython_osx.sh&lt;/b&gt;脚本来解决问题。&lt;/p&gt;&lt;h2&gt;提交作业&lt;/h2&gt;&lt;p&gt;无论你是在云终端还是在本地完成作业，一旦完成作业，就运行&lt;b&gt;collectSubmission.sh&lt;/b&gt;脚本；这样将会产生一个&lt;b&gt;assignment2.zip&lt;/b&gt;的文件，然后将这个文件上传到你的dropbox中这门课的&lt;a href="https://coursework.stanford.edu/portal/site/W15-CS-231N-01/" class="" data-editable="true" data-title="作业页面"&gt;作业页面&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;Q1：全连接神经网络（30分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;FullyConnectedNets&lt;/b&gt;&lt;strong&gt;.ipynb&lt;/strong&gt;将会向你介绍我们的模块化设计，然后使用不同的层来构建任意深度的全连接网络。为了对模型进行最优化，还需要实现几个常用的更新方法。&lt;/p&gt;&lt;h3&gt;Q2：批量归一化（30分）&lt;/h3&gt;&lt;p&gt;在IPython Notebook文件&lt;b&gt;BatchNormalization.ipynb&lt;/b&gt;中，你需要实现批量归一化，然后将其应用于深度全连接网络的训练中。&lt;/p&gt;&lt;h3&gt;Q3：随机失活（Dropout）（10分）&lt;/h3&gt;&lt;p&gt;IPython Notebook文件&lt;b&gt;Dropout.ipynb&lt;/b&gt;将会帮助你需要实现随机失活，然后在模型泛化中检查它的效果。&lt;/p&gt;&lt;h3&gt;Q4：在CIFAR-10上运行卷积神经网络（30分）&lt;/h3&gt;&lt;p&gt;在IPython Notebook文件&lt;b&gt;ConvolutionalNetworks.ipynb&lt;/b&gt;中，你将实现几个卷积神经网络中常用的新的层。你将在CIFAR-10上训练一个深度较浅的卷积神经网络，然后由你决定竭尽所能地训练处一个最好网络。&lt;/p&gt;&lt;h3&gt;Q5：做点儿其他的！（+10分）&lt;/h3&gt;&lt;p&gt;在训练网络的过程中，为了得到更好的结果，你可以自由实现任何想法。你可以修改训练器（solver），实现额外的层，使用不同的正则化方法，使用模型集成，或者任何其它你想到的东西。如果你实现了作业要求以外的内容，那么将得到加分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全文完。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;译者反馈：&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;转载须全文转载并注明原文链接，否则保留维权权利；&lt;/li&gt;&lt;li&gt;如对翻译有意见建议，请通过评论批评指正，贡献者均会补充提及；&lt;/li&gt;&lt;li&gt;后续将根据作业内容和自己的学习笔记原创教程。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Wed, 10 Aug 2016 11:19:25 GMT</pubDate></item><item><title>CS231n课程笔记翻译：神经网络笔记3（下）</title><link>https://zhuanlan.zhihu.com/p/21798784</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/940c2e8d2edc3018771c752d977a6f27_r.png"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元" class=""&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href="http://cs231n.github.io/neural-networks-3/" class="" data-editable="true" data-title="Neural Nets notes 3"&gt;Neural Nets notes 3&lt;/a&gt;，课程教师&lt;a href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;授权翻译。本篇教程由&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href="https://www.zhihu.com/people/kun-kun-97-81" class="" data-editable="true" data-title="堃堃"&gt;堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="巩子嘉"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练与验证准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新 &lt;i&gt;&lt;b&gt;译者注：下篇翻译起始处&lt;/b&gt;&lt;/i&gt;&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;参数更新&lt;/h2&gt;&lt;p&gt;一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新了。进行参数更新有好几种方法，接下来都会进行讨论。&lt;/p&gt;&lt;p&gt;深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对于细节感兴趣的读者，我们提供了一些拓展阅读。&lt;/p&gt;&lt;h3&gt;随机梯度下降及各种更新方法&lt;/h3&gt;&lt;p&gt;&lt;b&gt;普通更新&lt;/b&gt;。最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）。假设有一个参数向量&lt;b&gt;x&lt;/b&gt;及其梯度&lt;b&gt;dx&lt;/b&gt;，那么最简单的更新的形式是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 普通更新
x += - learning_rate * dx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。&lt;/p&gt;&lt;p&gt;&lt;b&gt;动量（&lt;/b&gt;&lt;strong&gt;Momentum&lt;/strong&gt;&lt;b&gt;）更新&lt;/b&gt;是另一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对于最优化问题得到的启发。损失值可以理解为是山的高度（因此高度势能是&lt;equation&gt;U=mgh&lt;/equation&gt;，所以有&lt;equation&gt;U\propto h&lt;/equation&gt;）。用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。&lt;/p&gt;&lt;p&gt;因为作用于质点的力与梯度的潜在能量（&lt;equation&gt;F=-\nabla U&lt;/equation&gt;）有关，质点&lt;b&gt;所受的力&lt;/b&gt;就是损失函数的&lt;b&gt;（负）梯度&lt;/b&gt;。还有，因为&lt;equation&gt;F=ma&lt;/equation&gt;，所以在这个观点下（负）梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降（SDG）是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 动量更新
v = mu * v - learning_rate * dx # 与速度融合
x += v # 与位置融合
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在这里引入了一个初始化为0的变量&lt;b&gt;v&lt;/b&gt;和一个超参数&lt;b&gt;mu&lt;/b&gt;。说得不恰当一点，这个变量（mu）在最优化的过程中被看做&lt;i&gt;动量&lt;/i&gt;（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Nesterov动量&lt;/b&gt;与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。&lt;/p&gt;&lt;p&gt;Nesterov动量的核心思路是，当参数向量位于某个位置&lt;b&gt;x&lt;/b&gt;时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过&lt;b&gt;mu * v&lt;/b&gt;稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置&lt;b&gt;x + mu * v&lt;/b&gt;看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算&lt;b&gt;x + mu * v&lt;/b&gt;的梯度而不是“旧”位置&lt;b&gt;x&lt;/b&gt;的梯度就有意义了。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/412afb713ddcff0ba9165ab026563304.png" data-rawwidth="1580" data-rawheight="514"&gt;&lt;p&gt;Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;也就是说，添加一些注释后，实现代码如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;x_ahead = x + mu * v
# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对&lt;b&gt;x_ahead = x + mu * v&lt;/b&gt;使用变量变换进行改写是可以做到的，然后用&lt;b&gt;x_ahead&lt;/b&gt;而不是&lt;b&gt;x&lt;/b&gt;来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。&lt;b&gt;x_ahead&lt;/b&gt;的公式（将其重新命名为&lt;b&gt;x&lt;/b&gt;）就变成了：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对于NAG（Nesterov's Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Yoshua Bengio的&lt;a href="http://arxiv.org/pdf/1212.0901v2.pdf" data-editable="true" data-title="Advances in optimizing Recurrent Networks" class=""&gt;Advances in optimizing Recurrent Networks&lt;/a&gt;，Section 3.5。&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf" data-editable="true" data-title="Ilya Sutskever's thesis"&gt;Ilya Sutskever's thesis&lt;/a&gt; (pdf)在section 7.2对于这个主题有更详尽的阐述。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;学习率退火&lt;/h3&gt;&lt;p&gt;在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;随步数衰减&lt;/b&gt;：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。&lt;/li&gt;&lt;li&gt;&lt;b&gt;指数衰减&lt;/b&gt;。数学公式是&lt;equation&gt;\alpha=\alpha_0e^{-kt}&lt;/equation&gt;，其中&lt;equation&gt;\alpha_0,k&lt;/equation&gt;是超参数，&lt;equation&gt;t&lt;/equation&gt;是迭代次数（也可以使用周期作为单位）。&lt;/li&gt;&lt;li&gt;&lt;b&gt;1/t衰减&lt;/b&gt;的数学公式是&lt;equation&gt;\alpha=\alpha_0/(1+kt)&lt;/equation&gt;，其中&lt;equation&gt;\alpha_0,k&lt;/equation&gt;是超参数，t是迭代次数。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比&lt;equation&gt;k&lt;/equation&gt;更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。&lt;/p&gt;&lt;h3&gt;二阶方法&lt;/h3&gt;&lt;p&gt;在深度网络背景下，第二类常用的最优化方法是基于&lt;a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" data-editable="true" data-title="牛顿法" class=""&gt;牛顿法&lt;/a&gt;的，其迭代如下：&lt;/p&gt;&lt;equation&gt;\displaystyle x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)&lt;/equation&gt;&lt;p&gt;这里&lt;equation&gt;Hf(x)&lt;/equation&gt;是&lt;a href="https://en.wikipedia.org/wiki/Hessian_matrix" data-editable="true" data-title="Hessian矩阵" class=""&gt;Hessian矩阵&lt;/a&gt;，它是函数的二阶偏导数的平方矩阵。&lt;equation&gt;\nabla f(x)&lt;/equation&gt;是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。&lt;/p&gt;&lt;p&gt;然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的&lt;i&gt;拟&lt;/i&gt;-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是&lt;a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" data-editable="true" data-title="L-BFGS"&gt;L-BFGS&lt;/a&gt;，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。&lt;/p&gt;&lt;p&gt;然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;实践&lt;/b&gt;。在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。&lt;/p&gt;&lt;p&gt;参考资料：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="http://research.google.com/archive/large_deep_networks_nips2012.html" data-editable="true" data-title="Large Scale Distributed Deep Networks"&gt;Large Scale Distributed Deep Networks&lt;/a&gt; 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://arxiv.org/abs/1311.2115" data-editable="true" data-title="SFO"&gt;SFO&lt;/a&gt;算法想要把SGD和L-BFGS的优势结合起来。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;逐参数适应学习率方法&lt;/h2&gt;&lt;p&gt;前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Adagrad&lt;/b&gt;是一个由&lt;a href="http://jmlr.org/papers/v12/duchi11a.html" data-editable="true" data-title="Duchi等"&gt;Duchi等&lt;/a&gt;提出的适应性学习率算法&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 假设有梯度和参数向量x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意，变量&lt;b&gt;cache&lt;/b&gt;的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子&lt;b&gt;eps&lt;/b&gt;（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RMSprop&lt;/b&gt;。是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的&lt;a href="http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf" data-editable="true" data-title="第六课的第29页PPT"&gt;第六课的第29页PPT&lt;/a&gt;。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;cache =  decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中&lt;b&gt;x+=&lt;/b&gt;和Adagrad中是一样的，但是&lt;b&gt;cache&lt;/b&gt;变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Adam&lt;/b&gt;。&lt;a href="http://arxiv.org/abs/1412.6980" data-editable="true" data-title="Adam"&gt;Adam&lt;/a&gt;是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度&lt;b&gt;m&lt;/b&gt;，而不是用的原始梯度向量&lt;b&gt;dx&lt;/b&gt;。论文中推荐的参数值&lt;b&gt;eps=1e-8, beta1=0.9, beta2=0.999&lt;/b&gt;。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置&lt;em&gt;（bias）矫正&lt;/em&gt;机制，因为&lt;b&gt;m,v&lt;/b&gt;两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。&lt;/p&gt;&lt;p&gt;拓展阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://arxiv.org/abs/1312.6055" data-editable="true" data-title="Unit Tests for Stochastic Optimization" class=""&gt;Unit Tests for Stochastic Optimization&lt;/a&gt;一文展示了对于随机最优化的测试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/7fd7404711c99456237cdff7b3a3bad7.png" data-rawwidth="1598" data-rawheight="640"&gt;&lt;b&gt;&lt;i&gt;译者注：上图原文中为动图，知乎专栏不支持动图，知友可点击&lt;a href="http://cs231n.github.io/neural-networks-3/" data-editable="true" data-title="原文链接" class=""&gt;原文链接&lt;/a&gt;查看。&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面的动画可以帮助你理解学习的动态过程。&lt;b&gt;左边&lt;/b&gt;是一个损失函数的等高线图，上面跑的是不同的最优化算法。注意基于动量的方法出现了射偏了的情况，使得最优化过程看起来像是一个球滚下山的样子。&lt;b&gt;右边&lt;/b&gt;展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）。注意SGD很难突破对称性，一直卡在顶部。而RMSProp之类的方法能够看到马鞍方向有很低的梯度。因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进。图片版权：&lt;a href="https://twitter.com/alecrad" data-editable="true" data-title="Alec Radford" class=""&gt;Alec Radford&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;h2&gt;超参数调优&lt;/h2&gt;&lt;p&gt;我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;初始学习率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;学习率衰减方式（例如一个衰减常量）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;正则化强度（L2惩罚，随机失活强度）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及其时间表的设置等。在本节中将介绍一些额外的调参要点和技巧：&lt;/p&gt;&lt;p&gt;&lt;b&gt;实现&lt;/b&gt;。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用&lt;b&gt;仆程序&lt;/b&gt;持续地随机设置参数然后进行最优化。在训练过程中，&lt;b&gt;仆程序&lt;/b&gt;会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个&lt;b&gt;主程序&lt;/b&gt;，它可以启动或者结束计算集群中的&lt;b&gt;仆程序&lt;/b&gt;，有时候也可能根据条件查看&lt;b&gt;仆程序&lt;/b&gt;写下的记录点，输出它们的训练统计数据等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;比起交叉验证最好使用一个验证集&lt;/b&gt;。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。&lt;/p&gt;&lt;p&gt;&lt;b&gt;超参数范围&lt;/b&gt;。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：&lt;b&gt;learning_rate = 10 ** uniform(-6, 1)&lt;/b&gt;。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.01，那么对于学习进程会有很大影响。然而当学习率是10的时候，影响就微乎其微了。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：&lt;b&gt;dropout=uniform(0,1)&lt;/b&gt;）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机搜索优于网格搜索&lt;/b&gt;。Bergstra和Bengio在文章&lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" data-editable="true" data-title="Random Search for Hyper-Parameter Optimization"&gt;Random Search for Hyper-Parameter Optimization&lt;/a&gt;中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d25cf561835c7b96ae6d1c91868bcbff.png" data-rawwidth="1596" data-rawheight="432"&gt;在&lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" data-editable="true" data-title="Random Search for Hyper-Parameter Optimization"&gt;Random Search for Hyper-Parameter Optimization&lt;/a&gt;中的核心说明图。通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;b&gt;对于边界上的最优值要小心&lt;/b&gt;。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用&lt;b&gt;learning_rate = 10 ** uniform(-6,1)&lt;/b&gt;来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从粗到细地分阶段搜索&lt;/b&gt;。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。&lt;/p&gt;&lt;p&gt;&lt;b&gt;贝叶斯超参数最优化&lt;/b&gt;是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： &lt;a href="https://github.com/JasperSnoek/spearmint" data-editable="true" data-title="Spearmint"&gt;Spearmint&lt;/a&gt;, &lt;a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/" data-editable="true" data-title="SMAC"&gt;SMAC&lt;/a&gt;, 和&lt;a href="http://jaberg.github.io/hyperopt/" data-editable="true" data-title="Hyperopt" class=""&gt;Hyperopt&lt;/a&gt;。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。&lt;a href="http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;有更详细的讨论。&lt;/p&gt;&lt;h2&gt;评价&lt;/h2&gt;&lt;h3&gt;模型集成&lt;/h3&gt;&lt;p&gt;在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;同一个模型，不同的初始化&lt;/b&gt;。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;在交叉验证中发现最好的模型&lt;/b&gt;。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;一个模型设置多个记录点&lt;/b&gt;。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;在训练的时候跑参数的平均值&lt;/b&gt;。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“&lt;a href="https://www.youtube.com/watch?v=EK61htlw8hY" data-editable="true" data-title="Dark Knowledge"&gt;Dark Knowledge&lt;/a&gt;”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;训练一个神经网络需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;利用小批量数据对实现进行梯度检查，还要注意各种错误。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;进行模型集成来获得额外的性能提高。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;拓展阅读&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Leon Bottou的《&lt;a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf" data-title="SGD要点和技巧" class="" data-editable="true"&gt;SGD要点和技巧&lt;/a&gt;》。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yann LeCun的《&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" data-editable="true" data-title="Efficient BackProp" class=""&gt;Efficient BackProp&lt;/a&gt;》。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;Yoshua Bengio的《&lt;a href="http://arxiv.org/pdf/1206.5533v2.pdf" data-editable="true" data-title="Practical Recommendations for Gradient-Based Training of Deep Architectures" class=""&gt;Practical Recommendations for Gradient-Based Training of Deep Architectures&lt;/a&gt;》。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接&lt;/b&gt;，否则保留维权权利；&lt;/li&gt;&lt;li&gt;请知友们通过评论和私信等方式批评指正，贡献者均会补充提及。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Tue, 09 Aug 2016 15:12:17 GMT</pubDate></item><item><title>深度增强学习之Policy Gradient方法1</title><link>https://zhuanlan.zhihu.com/p/21725498</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;在之前的深度增强学习系列文章中，我们已经详细分析了DQN算法，一种基于价值Value的算法，那么在今天，我们和大家一起分析深度增强学习中的另一种算法，也就是基于策略梯度Policy Gradient的算法。这种算法和基于价值Value的算法结合而成的Actor-Critic算法是目前效果最好的深度增强学习算法。&lt;/p&gt;&lt;p&gt;那么关于Policy Gradient方法的学习，有以下一些网上的资源值得看：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Andrej Karpathy blog: &lt;a href="https://link.zhihu.com/?target=http%3A//karpathy.github.io/2016/05/31/rl/" class="" data-editable="true" data-title="Deep Reinforcement Learning: Pong from Pixels"&gt;Deep Reinforcement Learning: Pong from Pixels&lt;/a&gt;&lt;/li&gt;&lt;li&gt;David Silver ICML 2016：&lt;a href="https://link.zhihu.com/?target=http%3A//icml.cc/2016/tutorials/deep_rl_tutorial.pdf" class="" data-editable="true" data-title="深度增强学习Tutorial"&gt;深度增强学习Tutorial&lt;/a&gt;&lt;/li&gt;&lt;li&gt;John Schulman：&lt;a href="https://link.zhihu.com/?target=http%3A//learning.mpi-sws.org/mlss2016/speakers/" class="" data-editable="true" data-title="Machine Learning Summer School"&gt;Machine Learning Summer School&lt;/a&gt;&lt;/li&gt;&lt;li&gt;David Silver的增强学习课程（有视频和ppt）: &lt;a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" class="" data-editable="true" data-title="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html"&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么实际上Andrej Karpathy的blog已经很详细的分析了Policy Gradient的方法，这里我将综合以上的内容根据我自己的理解来说以下Policy Gradient。&lt;/p&gt;&lt;h2&gt;2 Why Policy Network?&lt;/h2&gt;&lt;p&gt;我们已经知道DQN是一个基于价值value的方法。换句话说就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法。那么，更直接的做法是什么？&lt;/p&gt;&lt;p&gt;&lt;b&gt;能不能直接更新策略网络Policy Network呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;什么是策略网络Policy Network？就是一个神经网络，输入是状态，输出直接就是动作（不是Q值）。&lt;/p&gt;&lt;equation&gt;a = \pi(s,\theta)&lt;/equation&gt;&lt;equation&gt;a = \pi(s,\theta)  &lt;/equation&gt;&lt;p&gt;或者输出概率：&lt;equation&gt;a = \pi(a|s,\theta)&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;这里要提一下概率输出的问题。对于DQN来说，本质上是一个接近于确定性输出的算法。至多就是采用&lt;equation&gt;\epsilon-greedy&lt;/equation&gt;进行探索。但是有很多时候，在某一个特定状态下，很多动作的选择可能都是可以的。比如说我有20块钱去买饭。那么不管我买的是蛋炒饭还是土豆肉片盖码饭，结果都是一样的填饱肚子。因此，采用输出概率会更通用一些。而DQN并不能输出动作的概率，所以采用Policy Network是一个更好的办法。&lt;/p&gt;&lt;h2&gt;3 Policy Gradient&lt;/h2&gt;&lt;p&gt;要更新策略网络，或者说要使用梯度下降的方法来更新网络，我们需要有一个目标函数。对于策略网络，目标函数其实是比较容易给定的，就是很直接的，最后的结果！也就是&lt;/p&gt;&lt;p&gt;&lt;equation&gt;L(\theta) = \mathbb E(r_1+\gamma r_2 + \gamma^2 r_3 + ...|\pi(,\theta))&lt;/equation&gt; 所有带衰减reward的累加期望&lt;/p&gt;&lt;p&gt;那么问题就在于如何利用这个目标来更新参数&lt;equation&gt;\theta&lt;/equation&gt;呢？咋一看这个损失函数和策略网络简直没有什么直接联系，reward是环境给出的，如何才能更新参数？换个说法就是如何能够计算出损失函数关于参数的梯度（也就是策略梯度）：&lt;/p&gt;&lt;equation&gt;\nabla_{\theta} L(\theta)&lt;/equation&gt;&lt;p&gt;咋一看根本就没有什么思路是不是，所以先换一个思路来考虑问题。&lt;/p&gt;&lt;h2&gt;4 就给我一个Policy Network，也没有loss，怎么更新？&lt;/h2&gt;&lt;blockquote&gt;改变动作的出现概率！&lt;/blockquote&gt;&lt;p&gt;现在我们不考虑别的，就仅仅从概率的角度来思考问题。我们有一个策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到reward，或者result。那么这个时候，我们有个非常简单的想法：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;如果某一个动作得到reward多，那么我们就使其出现的概率增大，如果某一个动作得到的reward少，那么我们就使其出现的概率减小。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;当然，也显然的，用reward来评判动作的好坏是不准确的，甚至用result来评判也是不准确的。毕竟任何一个reward，result都依赖于大量的动作才导致的。但是这并不妨碍我们做这样的思考：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;假设这个评价指标是&lt;equation&gt;f(s,a)&lt;/equation&gt;,那么我们的Policy Network输出的是概率。一般情况下，更常使用log likelihood &lt;equation&gt;log \pi(a|s,\theta)&lt;/equation&gt;。原因的话看这里&lt;a href="http://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution" class="" data-title="Why we consider log likelihood instead of Likelihood in Gaussian Distribution"&gt;Why we consider log likelihood instead of Likelihood in Gaussian Distribution&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;因此，我们就可以构造一个损失函数如下：&lt;/p&gt;&lt;equation&gt;L(\theta) = \sum log\pi(a|s,\theta)f(s,a)&lt;/equation&gt;&lt;p&gt;怎么理解呢？举个简单的AlphaGo的例子吧。对于AlphaGo而言，f(s,a)就是最后的结果。也就是一盘棋中，如果这盘棋赢了，那么这盘棋下的每一步都是认为是好的，如果输了，那么都认为是不好的。好的f(s,a)就是1，不好的就-1。所以在这里，如果a被认为是好的，那么目标就是最大化这个好的动作的概率，反之亦然。&lt;/p&gt;&lt;p&gt;这就是Policy Gradient最基本的思想。&lt;/p&gt;&lt;h2&gt;5 另一个角度：直接算&lt;/h2&gt;&lt;p&gt;f(s,a)不仅仅可以作为动作的评价指标，还可以作为目标函数。就如同AlphaGo，评价指标就是赢或者输，而目标就是结果赢。这和之前分析的目标完全没有冲突。因此，我们可以利用评价指标f(s,a)来优化Policy，同时也是在优化的同时优化了f(s,a).那么问题就变成对f(s,a)求关于参数的梯度。下面的公式直接摘自Andrej Karpathy的blog，f(x)即是f(s,a)&lt;/p&gt;&lt;equation&gt;\begin{align}
\nabla_{\theta} E_x[f(x)] &amp;amp;= \nabla_{\theta} \sum_x p(x) f(x) &amp;amp; \text{definition of expectation} \\
&amp;amp; = \sum_x \nabla_{\theta} p(x) f(x) &amp;amp; \text{swap sum and gradient} \\
&amp;amp; = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) &amp;amp; \text{both multiply and divide by } p(x) \\
&amp;amp; = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) &amp;amp; \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
&amp;amp; = E_x[f(x) \nabla_{\theta} \log p(x) ] &amp;amp; \text{definition of expectation}
\end{align}&lt;/equation&gt;&lt;p&gt;从公式得到的结论可以看到正好和上一小结分析得到的目标函数一致。&lt;/p&gt;&lt;p&gt;因此，Policy Gradient方法就这么确定了。&lt;/p&gt;&lt;h2&gt;6 小结&lt;/h2&gt;&lt;p&gt;本篇blog作为一个引子，介绍下Policy Gradient的基本思想。那么大家会发现，如何确定这个评价指标才是实现Policy Gradient方法的关键所在。所以，在下一篇文章中。我们将来分析一下这个评价指标的问题。&lt;/p&gt;</description><author>Flood Sung</author><pubDate>Wed, 03 Aug 2016 11:04:25 GMT</pubDate></item><item><title>CS231n课程笔记翻译：神经网络笔记3（上）</title><link>https://zhuanlan.zhihu.com/p/21741716</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b629f243297cf32d2507bdaa1bc38e12_r.jpg"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元" class=""&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href="http://cs231n.github.io/neural-networks-3/" class="" data-editable="true" data-title="Neural Nets notes 3"&gt;Neural Nets notes 3&lt;/a&gt;，课程教师&lt;a href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;授权翻译。本篇教程由&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href="https://www.zhihu.com/people/kun-kun-97-81" class="" data-editable="true" data-title="堃堃"&gt;堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="巩子嘉"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练集与验证集准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;b&gt;&lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;学习过程&lt;/h1&gt;&lt;p&gt;在前面章节中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。&lt;/p&gt;&lt;h2&gt;梯度检查&lt;/h2&gt;&lt;p&gt;理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用中心化公式。&lt;/strong&gt;在使用有限差值近似来计算数值梯度的时候，常见的公式是：&lt;/p&gt;&lt;equation&gt;\displaystyle \frac{df(x)}{dx}=\frac{f(x+h)-f(x)}{h}(bad,\ do\ not\ use)&lt;/equation&gt;&lt;p&gt;其中&lt;equation&gt;h&lt;/equation&gt;是一个很小的数字，在实践中近似为1e-5。在实践中证明，使用&lt;i&gt;中心化&lt;/i&gt;公式效果更好：&lt;/p&gt;&lt;equation&gt;\displaystyle \frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}(use\ instead)&lt;/equation&gt;&lt;p&gt;该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对&lt;equation&gt;f(x+h)&lt;/equation&gt;和&lt;equation&gt;f(x-h)&lt;/equation&gt;使用泰勒展开，可以看到第一个公式的误差近似&lt;equation&gt;O(h)&lt;/equation&gt;，第二个公式的误差近似&lt;equation&gt;O(h^2)&lt;/equation&gt;（是个二阶近似）。&lt;i&gt;&lt;b&gt;（译者注：泰勒展开相关内容可阅读《高等数学》第十二章第四节：函数展开成幂级数。）&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;使用相对误差来比较&lt;/b&gt;。比较数值梯度&lt;equation&gt;f'_n&lt;/equation&gt;和解析梯度&lt;equation&gt;f'_a&lt;/equation&gt;的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值&lt;equation&gt;|f'_a-f'_n|&lt;/equation&gt;或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用&lt;i&gt;相对误差&lt;/i&gt;总是更合适一些：&lt;/p&gt;&lt;equation&gt;\displaystyle \frac{|f'_a-f'_n|}{max(|f'_a|,|f'_n|)}&lt;/equation&gt;&lt;p&gt;上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;相对误差&amp;gt;1e-2：通常就意味着梯度可能出错。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-2&amp;gt;相对误差&amp;gt;1e-4：要对这个值感到不舒服才行。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-4&amp;gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-7或者更小：好结果，可以高兴一把了。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用双精度。&lt;/strong&gt;一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;保持在浮点数的有效范围。&lt;/strong&gt;建议通读《&lt;a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" data-editable="true" data-title="What Every Computer Scientist Should Konw About Floating-Point Artthmetic" class=""&gt;What Every Computer Scientist Should Konw About Floating-Point Artthmetic&lt;/a&gt;》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。&lt;/p&gt;&lt;p&gt;&lt;b&gt;目标函数的不可导点（kinks）&lt;/b&gt;。在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（&lt;equation&gt;max(0,x)&lt;/equation&gt;）等函数，或SVM损失，Maxout神经元等引入。考虑当&lt;equation&gt;x=-1e6&lt;/equation&gt;的时，对ReLU函数进行梯度检查。因为&lt;equation&gt;x&amp;lt;0&lt;/equation&gt;，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为&lt;equation&gt;f(x+h)&lt;/equation&gt;可能越过了不可导点(例如：如果&lt;equation&gt;h&amp;gt;1e-6&lt;/equation&gt;)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个&lt;equation&gt;max(0,x)&lt;/equation&gt;式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。&lt;/p&gt;&lt;p&gt;注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有&lt;equation&gt;max(x,y)&lt;/equation&gt;形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算&lt;equation&gt;f(x+h)&lt;/equation&gt;和&lt;equation&gt;f(x-h)&lt;/equation&gt;的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用少量数据点。&lt;/strong&gt;解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;谨慎设置步长h。&lt;/strong&gt;在实践中h并不是越小越好，因为当&lt;equation&gt;h&lt;/equation&gt;特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将&lt;equation&gt;h&lt;/equation&gt;调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。这篇&lt;a href="https://en.wikipedia.org/wiki/Numerical_differentiation" data-editable="true" data-title="维基百科文章" class=""&gt;维基百科文章&lt;/a&gt;中有一个图表，其x轴为&lt;equation&gt;h&lt;/equation&gt;值，y轴为数值梯度误差。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在操作的特性模式中梯度检查。&lt;/strong&gt;有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不要让正则化吞没数据。&lt;/strong&gt;通常损失函数是数据损失和正则化损失的和（例如L2对权重的惩罚）。需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;记得关闭随机失活（dropout）和数据扩张（augmentation）&lt;/b&gt;。在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算&lt;equation&gt;f(x+h)&lt;/equation&gt;和&lt;equation&gt;f(x-h)&lt;/equation&gt;前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;检查少量的维度。&lt;/strong&gt;在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。&lt;strong&gt;注意&lt;/strong&gt;&lt;b&gt;：&lt;/b&gt;确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机地从向量中取维度，一定要把这种情况考虑到，确保所有参数都收到了正确的梯度。&lt;/p&gt;&lt;h2&gt;学习之前：合理性检查的提示与技巧&lt;/h2&gt;&lt;p&gt;在进行费时费力的最优化之前，最好进行一些合理性检查：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;寻找特定情况的正确损失值。&lt;/strong&gt;在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第二个合理性检查：提高正则化强度时导致损失值变大。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;对小数据子集过拟合。&lt;/strong&gt;最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;检查整个学习过程&lt;/h2&gt;&lt;p&gt;在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。&lt;/p&gt;&lt;p&gt;在下面的图表中，x轴通常都是表示&lt;b&gt;&lt;u&gt;周期（epochs）&lt;/u&gt;&lt;/b&gt;单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。&lt;/p&gt;&lt;h2&gt;损失函数&lt;/h2&gt;&lt;p&gt;训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化，尤其是曲线形状会给出关于学习率设置的情况：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/753f398b46cc28c1916d6703cf2080f5.png" data-rawwidth="1578" data-rawheight="706"&gt;&lt;b&gt;左图&lt;/b&gt;展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。&lt;b&gt;右图&lt;/b&gt;显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。&lt;/p&gt;&lt;p&gt;有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。&lt;/p&gt;&lt;p&gt;有时候损失函数看起来很有意思：&lt;a href="http://lossfunctions.tumblr.com" data-editable="true" data-title="lossfunctions.tumblr.com" class=""&gt;lossfunctions.tumblr.com&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;训练集和验证集准确率&lt;/h3&gt;&lt;p&gt;在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/05a6960a01c0204ced8d875ac3d91fba.jpg" data-rawwidth="660" data-rawheight="200"&gt;在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;h3&gt;权重更新比例&lt;/h3&gt;&lt;p&gt;最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是&lt;em&gt;更新的&lt;/em&gt;，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 假设参数向量为W，其梯度向量为dW
param_scale = np.linalg.norm(W.ravel())
update = -learning_rate*dW # 简单SGD更新
update_scale = np.linalg.norm(update.ravel())
W += update # 实际更新
print update_scale / param_scale # 要得到1e-3左右
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。&lt;/p&gt;&lt;h3&gt;每层的激活数据及梯度分布&lt;/h3&gt;&lt;p&gt;一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。&lt;/p&gt;&lt;h2&gt;第一层可视化&lt;/h2&gt;&lt;p&gt;最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/96573094f9d7f4b3b188069726840a2e.png" data-rawwidth="1602" data-rawheight="552"&gt;将神经网络第一层的权重可视化的例子。&lt;b&gt;左图&lt;/b&gt;中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。&lt;b&gt;右图&lt;/b&gt;的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;b&gt;神经网络笔记3 （上）结束。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接&lt;/b&gt;，否则保留维权权利；&lt;/li&gt;&lt;li&gt;请知友们通过评论和私信等方式批评指正，贡献者均会补充提及；&lt;/li&gt;&lt;li&gt;CS231n的翻译即将进入尾声，&lt;b&gt;欢迎知友们建议后续的翻译方向；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;知友@&lt;a href="https://www.zhihu.com/people/ksma" class="" data-editable="true" data-title="猪皮"&gt;猪皮&lt;/a&gt;建议下一步的翻译方向是领域内的一些经典论文；&lt;/li&gt;&lt;li&gt;知友@&lt;a href="https://www.zhihu.com/people/nan-tian-qi-6" class="" data-editable="true" data-title="一蓑烟灰"&gt;一蓑烟灰&lt;/a&gt;在评论中详细解释了自己学习CS231n及本科毕设相关情况，建议下一步的翻译方向是课程作业解析。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Tue, 02 Aug 2016 14:26:25 GMT</pubDate></item><item><title>CS231n课程笔记翻译：神经网络笔记 2</title><link>https://zhuanlan.zhihu.com/p/21560667</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/9843b69865ebb95506dfe9b4df48e31c_r.jpg"&gt;&lt;/p&gt;译者注：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" data-editable="true" data-title="智能单元" class=""&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href="http://cs231n.github.io/neural-networks-2/" class="" data-editable="true" data-title="Neural Nets notes 2"&gt;Neural Nets notes 2&lt;/a&gt;，课程教师&lt;a href="https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" class="" data-editable="true" data-title="Andrej Karpathy"&gt;Andrej Karpathy&lt;/a&gt;授权翻译。本篇教程由&lt;a href="https://www.zhihu.com/people/du-ke" class="" data-editable="true" data-title="杜客"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href="https://www.zhihu.com/people/kun-kun-97-81" class="" data-editable="true" data-title="堃堃"&gt;堃堃&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;设置数据和模型&lt;ul&gt;&lt;li&gt;数据预处理&lt;/li&gt;&lt;li&gt;权重初始化&lt;/li&gt;&lt;li&gt;批量归一化（Batch Normalization）&lt;/li&gt;&lt;li&gt;正则化（L2/L1/Maxnorm/Dropout）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;设置数据和模型&lt;/h1&gt;&lt;p&gt;在上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了&lt;b&gt;评分&lt;/b&gt;&lt;strong&gt;函数（score function）&lt;/strong&gt;的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。&lt;/p&gt;&lt;h2&gt;数据预处理&lt;/h2&gt;&lt;p&gt;关于数据预处理我们有3个常用的符号，数据矩阵&lt;b&gt;X&lt;/b&gt;，假设其尺寸是&lt;b&gt;[N x D]&lt;/b&gt;（&lt;b&gt;N&lt;/b&gt;是数据样本的数量，&lt;b&gt;D&lt;/b&gt;是数据的维度）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;均值减法（&lt;/strong&gt;&lt;strong&gt;Mean subtraction&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;是预处理最常用的形式。它对数据中每个独立&lt;em&gt;特征&lt;/em&gt;减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码&lt;b&gt;X -= np.mean(X, axis=0)&lt;/b&gt;实现。而对于图像，更常用的是对所有像素都减去一个值，可以用&lt;b&gt;X -= np.mean(X)&lt;/b&gt;实现，也可以在3个颜色通道上分别操作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;归一化（&lt;/strong&gt;&lt;strong&gt;Normalization&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为&lt;b&gt;X /= np.std(X, axis=0)&lt;/b&gt;。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e743b6777775b1671c3b5503d7afbbc4.png" data-rawwidth="1592" data-rawheight="560"&gt;一般数据预处理流程：&lt;b&gt;左边：&lt;/b&gt;原始的2维输入数据。&lt;b&gt;中间：&lt;/b&gt;在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。&lt;b&gt;右边：&lt;/b&gt;每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PCA和白化（&lt;/strong&gt;&lt;strong&gt;Whitening&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 假设输入数据矩阵X的尺寸为[N x D]
X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)
cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的&lt;i&gt;协方差&lt;/i&gt;。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和&lt;a href="https://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices" data-editable="true" data-title="半正定" class=""&gt;半正定&lt;/a&gt;的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;U,S,V = np.linalg.svd(cov)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;Xrot = np.dot(X,U) # 对数据去相关性
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算&lt;b&gt;Xrot&lt;/b&gt;的协方差矩阵，将会看到它是对角对称的。&lt;b&gt;np.linalg.svd&lt;/b&gt;的一个良好性质是在它的返回值&lt;b&gt;U&lt;/b&gt;中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有&lt;u&gt;&lt;b&gt;方差&lt;/b&gt;&lt;/u&gt;的维度。 这个操作也被称为主成分分析（ &lt;a href="http://en.wikipedia.org/wiki/Principal_component_analysis" data-editable="true" data-title="Principal Component Analysis" class=""&gt;Principal Component Analysis&lt;/a&gt; 简称PCA）降维：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大&lt;u&gt;&lt;b&gt;方差&lt;/b&gt;&lt;/u&gt;的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。&lt;/p&gt;&lt;p&gt;最后一个在实践中会看见的变换是&lt;strong&gt;白化（&lt;/strong&gt;&lt;strong&gt;whitening&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt;。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# 对数据进行白化操作:
# 除以特征值 
Xwhite = Xrot / np.sqrt(S + 1e-5)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;警告：夸大的噪声&lt;/em&gt;。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/aae11de6e6a29f50d46b9ea106fbb02a.png" data-rawwidth="1602" data-rawheight="542"&gt;PCA/白化。&lt;b&gt;左边&lt;/b&gt;是二维的原始数据。&lt;b&gt;中间&lt;/b&gt;：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。&lt;b&gt;右边&lt;/b&gt;：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;我们可以使用CIFAR-10数据将这些变化可视化出来。CIFAR-10训练集的大小是50000x3072，其中每张图片都可以拉伸为3072维的行向量。我们可以计算[3072 x 3072]的协方差矩阵然后进行奇异值分解（比较耗费计算性能），那么经过计算的特征向量看起来是什么样子呢？&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/8608c06086fc196228f4dda78499a2d9.png" data-rawwidth="1604" data-rawheight="450"&gt;&lt;b&gt;最左&lt;/b&gt;：一个用于演示的集合，含49张图片。&lt;b&gt;左二&lt;/b&gt;：3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。&lt;b&gt;第三张&lt;/b&gt;是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以U.transpose()[:144,:]来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，这正好说明前面的特征向量获取了较低的频率。然而，大多数信息还是保留了下来。&lt;b&gt;最右&lt;/b&gt;：将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以U.transpose()[:144,:]转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实践操作。&lt;/strong&gt;在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;常见错误。&lt;/strong&gt;进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。&lt;strong&gt;应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;译者注：此处确为初学者常见错误，请务必注意！&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;权重初始化&lt;/h2&gt;&lt;p&gt;我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;错误：全零初始化。&lt;/strong&gt;让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;小随机数初始化。&lt;/strong&gt;因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来&lt;i&gt;打破对称性&lt;/i&gt;。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：&lt;b&gt;W = 0.01 * np.random.randn(D,H)。&lt;/b&gt;其中&lt;b&gt;randn&lt;/b&gt;函数是基于零均值和标准差的一个高斯分布（&lt;b&gt;&lt;i&gt;译者注：国内教程一般习惯称均值参数为期望&lt;equation&gt;\mu&lt;/equation&gt;&lt;/i&gt;&lt;/b&gt;）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;b&gt;警告&lt;/b&gt;。&lt;/em&gt;并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用1/sqrt(n)校准方差&lt;/strong&gt;&lt;strong&gt;。&lt;/strong&gt;上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：&lt;b&gt;w = np.random.randn(n) / sqrt(n)。&lt;/b&gt;其中&lt;b&gt;n&lt;/b&gt;是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。&lt;/p&gt;&lt;p&gt;上述结论的推导过程如下：假设权重&lt;equation&gt;w&lt;/equation&gt;和输入&lt;equation&gt;x&lt;/equation&gt;之间的内积为&lt;equation&gt;s=\sum^n_iw_ix_i&lt;/equation&gt;，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查&lt;equation&gt;s&lt;/equation&gt;的方差：&lt;/p&gt;&lt;equation&gt;\displaystyle Var(s)=Var(\sum^n_iw_ix_i)&lt;/equation&gt;&lt;equation&gt;\displaystyle =\sum^n_iVar(w_ix_i)&lt;/equation&gt;&lt;equation&gt;\displaystyle =\sum^n_i[E(w_i)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)&lt;/equation&gt;&lt;equation&gt;\displaystyle =\sum^n_iVar(x_i)Var(w_i)&lt;/equation&gt;&lt;equation&gt;\displaystyle =(nVar(w))Var(x)&lt;/equation&gt;&lt;p&gt;在前两步，使用了&lt;a href="http://en.wikipedia.org/wiki/Variance" data-editable="true" data-title="方差的性质"&gt;方差的性质&lt;/a&gt;。在第三步，因为假设输入和权重的平均值都是0，所以&lt;equation&gt;E[x_i]=E[w_i]=0&lt;/equation&gt;。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的&lt;equation&gt;w_i,x_i&lt;/equation&gt;都服从同样的分布。从这个推导过程我们可以看见，如果想要&lt;equation&gt;s&lt;/equation&gt;有和输入&lt;equation&gt;x&lt;/equation&gt;一样的方差，那么在初始化的时候必须保证每个权重&lt;equation&gt;w&lt;/equation&gt;的方差是&lt;equation&gt;1/n&lt;/equation&gt;。又因为对于一个随机变量&lt;equation&gt;X&lt;/equation&gt;和标量&lt;equation&gt;a&lt;/equation&gt;，有&lt;equation&gt;Var(aX)=a^2Var(X)&lt;/equation&gt;，这就说明可以基于一个标准高斯分布，然后除以&lt;equation&gt;a=\sqrt{1/n}&lt;/equation&gt;，使其方差为&lt;equation&gt;1/n&lt;/equation&gt;，于是得出：&lt;b&gt;w = np.random.randn(n) / sqrt(n)&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;Glorot等在论文&lt;a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" data-editable="true" data-title="Understanding the difficulty of training deep feedforward neural networks" class=""&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;中作出了类似的分析。在论文中，作者推荐初始化公式为&lt;equation&gt; \( \text{Var}(w) = 2/(n_{in} + n_{out}) \) &lt;/equation&gt;，其中&lt;equation&gt;\(n_{in}, n_{out}\)&lt;/equation&gt;是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：&lt;a href="http://arxiv-web3.library.cornell.edu/abs/1502.01852" data-editable="true" data-title="Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" class=""&gt;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification&lt;/a&gt;，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是&lt;equation&gt;2.0/n&lt;/equation&gt;。代码为&lt;strong&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/strong&gt;。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;稀疏初始化（&lt;/strong&gt;&lt;strong&gt;Sparse initialization&lt;/strong&gt;&lt;strong&gt;）。&lt;/strong&gt;另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;偏置（biases）的初始化。&lt;/strong&gt;通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实践。&lt;/strong&gt;当前的推荐是使用ReLU激活函数，并且使用&lt;strong&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/strong&gt;来进行权重初始化，关于这一点，&lt;a href="http://arxiv-web3.library.cornell.edu/abs/1502.01852" data-editable="true" data-title="这里" class=""&gt;这篇文章&lt;/a&gt;有讨论。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;批量归一化（&lt;/strong&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;&lt;strong&gt;）。&lt;/strong&gt;&lt;a href="http://arxiv.org/abs/1502.03167" data-editable="true" data-title="批量归一化"&gt;批量归一化&lt;/a&gt;是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：），其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！&lt;/p&gt;&lt;h2&gt;正则化 Regularization&lt;/h2&gt;&lt;p&gt;有不少方法是通过控制神经网络的容量来防止其过拟合的：&lt;/p&gt;&lt;p&gt;&lt;b&gt;L2正则化&lt;/b&gt;可能是最常用的正则化方法了。可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重&lt;equation&gt;w&lt;/equation&gt;，向目标函数中增加一个&lt;equation&gt;\frac{1}{2}\lambda w^2&lt;/equation&gt;，其中&lt;equation&gt;\lambda&lt;/equation&gt;是正则化强度。前面这个&lt;equation&gt;\frac{1}{2}&lt;/equation&gt;很常见，是因为加上&lt;equation&gt;\frac{1}{2}&lt;/equation&gt;后，该式子关于&lt;equation&gt;w&lt;/equation&gt;梯度就是&lt;equation&gt;\lambda w&lt;/equation&gt;而不是&lt;equation&gt;2\lambda w&lt;/equation&gt;了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以&lt;b&gt;w += -lambda * W&lt;/b&gt;向着0线性下降。&lt;/p&gt;&lt;p&gt;&lt;b&gt;L1正则化&lt;/b&gt;是另一个相对常用的正则化方法。对于每个&lt;equation&gt;w&lt;/equation&gt;我们都向目标函数增加一个&lt;equation&gt;\lambda|w|&lt;/equation&gt;。L1和L2正则化也可以进行组合：&lt;equation&gt;\lambda_1|w|+\lambda_2w^2&lt;/equation&gt;，这也被称作&lt;a href="http://web.stanford.edu/%7Ehastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&amp;amp;%20Hastie.pdf" data-editable="true" data-title="Elastic net regularizaton" class=""&gt;Elastic net regularizaton&lt;/a&gt;。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最大范式约束（Max norm constraints）。&lt;/b&gt;另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量&lt;equation&gt;\overrightarrow{w}&lt;/equation&gt;必须满足&lt;equation&gt;||\overrightarrow{w}||_2&amp;lt;c&lt;/equation&gt;这一条件，一般&lt;equation&gt;c&lt;/equation&gt;值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;随机失活（Dropout）&lt;/strong&gt;是一个简单又极其有效的正则化方法。该方法由Srivastava在论文&lt;a href="http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" data-editable="true" data-title="Dropout: A Simple Way to Prevent Neural Networks from Overfitting" class=""&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数&lt;equation&gt;p&lt;/equation&gt;的概率被激活或者被设置为0。&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/63fcf4cc655cb04f21a37e86aca333cf.png" data-rawwidth="1590" data-rawheight="608"&gt;图片来源自&lt;a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" data-editable="true" data-title="论文"&gt;论文&lt;/a&gt;，展示其核心思路。在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;一个3层神经网络的普通版随机失活可以用下面代码实现：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;""" 普通版随机失活: 不推荐实现 (看下面笔记) """

p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱

def train_step(X):
  """ X中是输入数据 """
  
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) &amp;lt; p # 第一个随机失活遮罩
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) &amp;lt; p # 第二个随机失活遮罩
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # 反向传播:计算梯度... (略)
  # 进行参数更新... (略)
  
def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p
  out = np.dot(W3, H2) + b3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在上面的代码中，&lt;b&gt;train_step&lt;/b&gt;函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据&lt;b&gt;X创建&lt;/b&gt;一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩&lt;b&gt;U1&lt;/b&gt;和&lt;b&gt;U2&lt;/b&gt;加入进去。&lt;/p&gt;&lt;p&gt;注意：在&lt;b&gt;predict&lt;/b&gt;函数中不进行随机失活，但是对于两个隐层的输出都要乘以&lt;equation&gt;p&lt;/equation&gt;，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以&lt;equation&gt;p=0.5&lt;/equation&gt;为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元&lt;equation&gt;x&lt;/equation&gt;的输出，那么进行随机失活的时候，该神经元的输出就是&lt;equation&gt;px+(1-p)0&lt;/equation&gt;，这是有&lt;equation&gt;1-p&lt;/equation&gt;的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整&lt;equation&gt;x\to px&lt;/equation&gt;来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。&lt;/p&gt;&lt;p&gt;上述操作不好的性质是必须在测试时对激活数据要按照&lt;equation&gt;p&lt;/equation&gt;进行数值范围调整。既然测试性能如此关键，实际更倾向使用&lt;b&gt;反向随机失活（inverted dropout）&lt;/b&gt;，它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;""" 
反向随机失活: 推荐实现方式.
在训练的时候drop和调整数值范围，测试时不做任何事.
"""

p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱

def train_step(X):
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p # 第一个随机失活遮罩. 注意/p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) &amp;lt; p) / p # 第二个随机失活遮罩. 注意/p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

  # 反向传播:计算梯度... (略)
  # 进行参数更新... (略)

def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" data-editable="true" data-title="Dropout paper"&gt;Dropout paper&lt;/a&gt; by Srivastava et al. 2014.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf" data-editable="true" data-title="Dropout Training as Adaptive Regularization" class=""&gt;Dropout Training as Adaptive Regularization&lt;/a&gt;：“我们认为：在使用费希尔信息矩阵（&lt;a href="https://en.wikipedia.org/wiki/Fisher_information_metric" data-editable="true" data-title="fisher information matrix"&gt;fisher information matrix&lt;/a&gt;）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;前向传播中的噪音。&lt;/strong&gt;在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过&lt;i&gt;分析法&lt;/i&gt;（在使用随机失活的本例中就是乘以&lt;equation&gt;p&lt;/equation&gt;）或&lt;i&gt;数值法&lt;/i&gt;（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是&lt;a href="http://cs.nyu.edu/%7Ewanli/dropc/" data-editable="true" data-title="DropConnect"&gt;DropConnect&lt;/a&gt;，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;偏置正则化。&lt;/strong&gt;在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;每层正则化。&lt;/strong&gt;对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实践&lt;/strong&gt;：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。&lt;equation&gt;p&lt;/equation&gt;值一般默认设为0.5，也可能在验证集上调参。&lt;/p&gt;&lt;h2&gt;损失函数&lt;/h2&gt;&lt;p&gt;我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分是&lt;em&gt;数据损失&lt;/em&gt;，它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，&lt;equation&gt;L=\frac{1}{N}\sum_iL_i&lt;/equation&gt;中，&lt;equation&gt;N&lt;/equation&gt;是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为&lt;equation&gt;f=f(x_i;W)&lt;/equation&gt;，在实际中你可能需要解决以下几类问题：&lt;/p&gt;&lt;p&gt;&lt;b&gt;分类问题&lt;/b&gt;是我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：&lt;/p&gt;&lt;equation&gt;\displaystyle L_i=\sum_{j\not=y_i}max(0,f_j-f_{y_i}+1)&lt;/equation&gt;&lt;p&gt;之前简要提起过，有些学者的论文中指出平方折叶损失（即使用&lt;equation&gt;max(0,f_j-f_{y_i}+1)^2&lt;/equation&gt;）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：&lt;/p&gt;&lt;equation&gt;\displaystyle L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})&lt;/equation&gt;&lt;p&gt;&lt;strong&gt;问题：类别数目巨大。&lt;/strong&gt;当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用&lt;em&gt;分层Softmax（&lt;/em&gt;&lt;i&gt;Hierarchical Softmax&lt;/i&gt;&lt;em&gt;）&lt;/em&gt;了（&lt;a href="http://arxiv.org/pdf/1310.4546.pdf" data-editable="true" data-title="参考文献" class=""&gt;参考文献&lt;/a&gt;）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;属性（Attribute）分类。&lt;/b&gt;上面两个损失公式的前提，都是假设每个样本只有一个正确的标签&lt;equation&gt;y_i&lt;/equation&gt;。但是如果&lt;equation&gt;y_i&lt;/equation&gt;是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：&lt;/p&gt;&lt;equation&gt;\displaystyle L_i=\sum_jmax(0,1-y_{ij}f_j)&lt;/equation&gt;&lt;p&gt;上式中，求和是对所有分类&lt;equation&gt;j&lt;/equation&gt;，&lt;equation&gt;y_{ij}&lt;/equation&gt;的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量&lt;equation&gt;f_j&lt;/equation&gt;为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。&lt;/p&gt;&lt;p&gt;另一种方法是对每种属性训练一个独立的逻辑回归分类器。二分类的逻辑回归分类器只有两个分类（0，1），其中对于分类1的概率计算为：&lt;/p&gt;&lt;equation&gt;\displaystyle P(y=1|x;w,b)=\frac{1}{1+e^{-(w^Tx+b)}}=\sigma(w^Tx+b)&lt;/equation&gt;&lt;p&gt;因为类别0和类别1的概率和为1，所以类别0的概率为：&lt;equation&gt;\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)&lt;/equation&gt;。这样，如果&lt;equation&gt;\sigma(w^Tx+b)&amp;gt;0.5&lt;/equation&gt;或者&lt;equation&gt;w^Tx+b&amp;gt;0&lt;/equation&gt;，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：&lt;/p&gt;&lt;equation&gt;\displaystyle L_i=\sum_jy_{ij}log(\sigma(f_j))+(1-y_{ij})log(1-\sigma(f_j))&lt;/equation&gt;&lt;p&gt;上式中，假设标签&lt;equation&gt;y_{ij}&lt;/equation&gt;非0即1，&lt;equation&gt;\sigma(.)&lt;/equation&gt;就是sigmoid函数。上面的公式看起来吓人，但是&lt;equation&gt;f&lt;/equation&gt;的梯度实际上非常简单：&lt;equation&gt;\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)&lt;/equation&gt;（你可以自己求导来验证）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;回归问题&lt;/b&gt;是预测实数的值的问题，比如预测房价，预测图片中某个东西的长度等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。对于某个样本，L2范式计算如下：&lt;/p&gt;&lt;equation&gt;L_i=||f-y_i||^2_2&lt;/equation&gt;&lt;p&gt;之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：&lt;/p&gt;&lt;equation&gt;L_i=||f-y_i||_1=\sum_j|f_j-(y_i)_j|&lt;/equation&gt;&lt;p&gt;在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即&lt;equation&gt;\sum_j&lt;/equation&gt;。观察第i个样本的第j维，用&lt;equation&gt;\delta_{ij}&lt;/equation&gt;表示预测值与真实值之间的差异。关于该维度的梯度（也就是&lt;equation&gt;\partial L_i/\partial f_j&lt;/equation&gt;）能够轻松地通过被求导为L2范式的&lt;equation&gt;\delta_{ij}&lt;/equation&gt;或&lt;equation&gt;sign(\delta_{ij})&lt;/equation&gt;。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。&lt;/p&gt;&lt;p&gt;&lt;em&gt;注意&lt;/em&gt;：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;结构化预测（s&lt;/strong&gt;&lt;strong&gt;tructured prediction&lt;/strong&gt;&lt;strong&gt;）。&lt;/strong&gt;结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构&lt;equation&gt;y_i&lt;/equation&gt;和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;小结如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。&lt;/li&gt;&lt;li&gt;使用标准差为&lt;equation&gt;\sqrt{2/n}&lt;/equation&gt;的高斯分布来初始化权重，其中&lt;equation&gt;n&lt;/equation&gt;是输入的神经元数。例如用numpy可以写作：&lt;b&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;使用L2正则化和随机失活的倒置版本。&lt;/li&gt;&lt;li&gt;使用批量归一化。&lt;/li&gt;&lt;li&gt;讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在，我们预处理了数据，初始化了模型。在下一节中，我们将讨论算法的学习过程及其运作特性。&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接&lt;/b&gt;，否则保留维权权利；&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/chen-di-cd" class="" data-editable="true" data-title="陈狄"&gt;陈狄&lt;/a&gt;和@&lt;a href="https://www.zhihu.com/people/han-jie-qun" class="" data-editable="true" data-title="韩劼群"&gt;韩劼群&lt;/a&gt;的建议，本系列对于&lt;b&gt;model ensemble&lt;/b&gt;将固定翻译为&lt;b&gt;模型集成&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/clarkzdyhit" class="" data-editable="true" data-title="天堂之拳"&gt;天堂之拳&lt;/a&gt;的细致指正和讨论，本系列对于&lt;b&gt;pooling&lt;/b&gt;将固定翻译为&lt;b&gt;汇合；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;感谢知友@&lt;a href="https://www.zhihu.com/people/chen-di-cd" class="" data-editable="true" data-title="陈狄"&gt;陈狄&lt;/a&gt;和@&lt;a href="https://www.zhihu.com/people/han-jie-qun" class="" data-editable="true" data-title="韩劼群"&gt;韩劼群&lt;/a&gt;的建议，本系列对于&lt;b&gt;dropout&lt;/b&gt;将固定翻译为&lt;b&gt;随机失活&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;针对部分知友建议保留英文不做翻译的建议，请参考下方我的回复中关于Emil Cioran名句的引用，以及我的个人态度；&lt;/li&gt;&lt;li&gt;请知友们通过评论和私信等方式批评指正，贡献者均会补充提及。&lt;/li&gt;&lt;/ol&gt;</description><author>杜客</author><pubDate>Tue, 26 Jul 2016 17:19:34 GMT</pubDate></item></channel></rss>