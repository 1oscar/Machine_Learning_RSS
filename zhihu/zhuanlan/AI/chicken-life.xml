<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>菜鸡的啄米日常 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/chicken-life</link><description>本菜鸡关于机器学习、深度学习的啄米日常</description><lastBuildDate>Sun, 02 Oct 2016 01:17:51 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>【啄米日常】9：keras 1.1.0（抱歉来迟）</title><link>https://zhuanlan.zhihu.com/p/22697113</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-52ff7f13a0f56665505a83ca61d25ddc_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;抱歉了各位看官……最近在忙于找工作，焦头烂额，以至于没有及时更新&lt;/p&gt;&lt;p&gt;现在虽然工作还没定，但估计还能有口饭吃，我就赶紧滚过来更文了= =其实有时候很想写点别的，但是不知道大家感兴趣啥，暂时就先这样吧 &lt;/p&gt;&lt;p&gt;所幸，Keras1.1.0的更新没有啥了不得的东西，简单列出如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;默认后端从theano切换为TensorFlow，这个消息宝宝早就猜到了，以TensorFlow现在的热度绝对是主流，之前就说过theano药丸&lt;/li&gt;&lt;li&gt;增加了一套SpatialDropout，包含2D和3D两个版本，这个东西是Dropout的升级版。Dropout是断开一定比例神经元到神经元的连接，这个SpatialDropout是断开一定比例的featuremap连接，排场大了很多。&lt;/li&gt;&lt;li&gt;增加了AtrousConvolution1D，这是带孔洞的卷积的1D版本，2D版已经加了好久了这次加1D版本也很正常&lt;/li&gt;&lt;li&gt;增加了一套Cropping层，包含1D，2D和3D三个版本，这玩意就是裁剪featuremap，用于把featuremap的周围一圈去掉。&lt;/li&gt;&lt;li&gt;比较重要的是这条，现在model.load_weights()可以通过byname=True参数来将权重匹配到与原来模型结构不同的模型了。啥意思呢？就是说你训练了一个模型，比方说VGG-16，把它的权重保存好了，然后你在原模型里插入一些层，去掉一些层，改变一些连接方式后，还可以将刚才保存的权重再载入回去。Keras会按照层名字对权重进行匹配，并把匹配到的权重载入对应的层。这个选项用于做transfer learning十分方便&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;大致是这样，我还要继续找工作所以就……先这样&lt;/p&gt;&lt;p&gt;然后我觉得中文文档可以更好的利用起来，别人Keras官方讨论的很热烈，我们也可以搞一个小论坛来玩嘛。建站什么的动作太大，你萌可以把问题，经验神马的开issue到&lt;a href="https://github.com/MoyanZitto/keras-cn/issues" data-title="keras-cn issues" class=""&gt;keras-cn issues&lt;/a&gt;这里来，我会（尽量）每天上去看看有没有能帮忙的地方，也会号召群里的大神们多上去帮忙~&lt;/p&gt;&lt;p&gt;high起来啊大家！打比赛，合作论文，问问题，求debug，吹水，找工作，求内推，想跳槽，做好玩的，都可以来keras-cn issues玩耍~&lt;/p&gt;&lt;p&gt;让我萌把github玩起来吧~！ &lt;/p&gt;&lt;p&gt;好像没人好奇为啥没有1.0.9......&lt;/p&gt;&lt;p&gt;反正我挺好奇的……为啥没1.0.9咧？还是曾经有过然后出大bug就又下了？ &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22697113&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 29 Sep 2016 16:45:34 GMT</pubDate></item><item><title>【啄米日常】8：Keras1.0.8，都有些啥新闻？</title><link>https://zhuanlan.zhihu.com/p/22258509</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/83270094b2c1b18d5b301d121aa5eb9c_r.jpg"&gt;&lt;/p&gt;3天前，Keras放出了1.x时代的第8个release版本1.0.8~还没有更新的你赶快更新哟！现在，本鸡及本鸡的老婆污码撸就一同为大家介绍一下Keras 1.0.8都有啥了不起的大新闻吧！有观众朋友问了，那之前怎么不见你们俩出来介绍版本变化啊？是不是之前的版本更新都不重要啊？其实不是的，之前没有介绍，主要原因是因为没想起来，本文最后会简单的提一下我能记住的一些“大概老用户还不太清楚”的变化。1.0.8的更新如下：&lt;h2&gt;【新增】Application&lt;/h2&gt;之前跟你们讲keras要做keras zoo了，1.0.8新增的模块Application就是一个Keras zoo，除了VGG系列网络，ResNet50以外，这次一同上线的网络还有Inception V3。也就是说，现在你可以直接用import载入这些网络和对应的权重了。值得注意的是，对每个网络，Keras都提供了4种权重，分别是：&lt;ul&gt;&lt;li&gt;tf后端，dim_ordering=tf&lt;/li&gt;&lt;li&gt;tf后端，dim_ordering=th&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;th后端，dim_ordering=tf&lt;/li&gt;&lt;li&gt;th后端，dim_ordering=th&lt;/li&gt;&lt;/ul&gt;后端和dim_ordering搭配起来版本效率较高，而不匹配的版本提供了无缝兼容用户自身代码的可能性，就问你爽不爽？&lt;h2&gt;【新增】LocallyConnected&lt;/h2&gt;1.0.8Keras还新增了局部连接层模块LocallyConnected，包含LocallyConnected1D和LocallyConnected2D两种层。简单的说，局部连接层类似于卷积层，但不进行权值共享，即滤波器只对固定区域起作用而不进行“滑动”&lt;h2&gt;【新增】SeparableConvolution2D，DeConvolution2D&lt;/h2&gt;1.0.8新增了两种卷积，可分离卷积SeparableConvolution2D和反卷积DeConvolution2D，可分离卷积由一个沿深度方向的卷积跟一个深度方向的逐点卷积构成，内部实现使用的是TensorFlow的&lt;a data-editable="true" class="" data-title="tf.nn.separable_conv2d" href="http://wiki.jikexueyuan.com/project/tensorflow-zh/api_docs/python/nn.html#separable_conv2d"&gt;tf.nn.separable_conv2d&lt;/a&gt;，因此目前这个层只能在TensorFlow作为后端时使用。DeConvolution2D是反卷积，当然卷积本身是不可逆的一个运算，反卷积也是通过学习一堆参数来反的。&lt;h2&gt;【新增】GlobalPooling系列&lt;/h2&gt;1.0.8新增加了全局池化GlobalPooling，包括1D和2D的全局最大值池化、平均值池化，全局的池化就是在整个featuremap的范围做池化，4D的输入tensor，它的输出tensor的shape就只是（nb_samples, channels）而已哟&lt;h2&gt;【新增】Bidirectional包装器 &lt;/h2&gt;继（唯一的）TimeDistributed包装器之后，现在Keras提供了一种新的包装器Bidirectional，如名所示，该包装期可以将一个RNN层，如LSTM，包装为双向的RNN，因此BLSTM可借由此包装器实现，关于BLSTM的例子可以参考Keras的一个示例代码imdb_bidirectional_lstm&lt;h2&gt;【新增】model.predict_generator&lt;/h2&gt; 这个预料之中，fit_generator,evaluate_generator都有了，就等老兄predict_generator构成生成器系列三剑客了，没啥可说的&lt;h2&gt; 【新增/修改】一堆example&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;imdb_bidirectional_lstm：双向LSTM，之前提了&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;image_ocr ：这个例子实现了字符的OCR识别，用了一个CTC loss做字符识别——好吧我也不知道这是啥玩意，有意思的是作者十分耿直：“I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Keras”。这位老兄实在是太可爱&lt;/p&gt;&lt;/li&gt;&lt;li&gt; imdb_fasttext：利用fasttext方法进行文本分类&lt;/li&gt;&lt;li&gt;mnist_hierarchical_rnn：用Hirerachical RNN进行mnist分类&lt;/li&gt;&lt;li&gt;mnist_net2net：用net2net的方法进行mnist分类&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下面说一下我觉得某些朋友还不太清楚的新特性，不一定是1.0.8版本的，蜻蜓点水的说，产个卵就算完&lt;/p&gt;&lt;ul&gt;&lt;li&gt;保存模型：以前是模型和权重分开保存的，十分不方便，现在调用model.save可以将模型和权重一下子保存成一个文件，重建时用load_model重建回来，重建动作包含模型搭建，权重载入和编译等过程。当然原来的分开保存的API也继续可用&lt;/li&gt;&lt;li&gt;Pooling被割开了：之前的Pooling层位于convolutional里面，现在pooling单独成为了一个模块&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;model.pop：在transfer learning时经常需要丢掉一些层，以前大概都用model.layers.pop()，现在model新增了方法.pop()，每次调用都会抛掉模型的最后一层。&lt;/li&gt;&lt;li&gt;AtrousConvolution2D，新加了一种带洞洞的卷积，是我一个朋友提的PR&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Adamax和Nadam：新增了两种优化器&lt;/li&gt;&lt;li&gt;kullback_leibler_divergence：新增了一种目标函数，也就是KL距离了&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;flow_from_directory：图像预处理模块的ImageDataGenerator新增了这种方法，按照提示调整好文件夹结构，调用此方法能够自动的从你的文件夹中生成图片数据和标签数据，一键搞定哟&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;好像暂时没什么了，我在1.0.7和1.0.8的对比中还看到几个奇怪的layer，但是文档中没看到，暂时先不写，查证了再补。&lt;/p&gt;&lt;p&gt;今天先这样，欢迎关注&lt;a data-title="Keras中文文档" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/"&gt;Keras中文文档&lt;/a&gt;哟 &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22258509&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Wed, 31 Aug 2016 22:08:09 GMT</pubDate></item><item><title>【啄米日常】7：一个不负责任的Keras介绍（下）</title><link>https://zhuanlan.zhihu.com/p/22135796</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/af958a62aa26b0a502c4501571042e2e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本来上篇文末说，（下）可能会推迟一会儿……&lt;/p&gt;&lt;p&gt;但是我控制不住我即己啊！一开始写就想一下子全部写完啊！！&lt;/p&gt;&lt;p&gt;所以我又开始笔耕不缀了，这篇专栏写完，我基本是棵废菇了……&lt;/p&gt;&lt;p&gt;（下）主要说一下Keras的有用特性，以及一些常见问题，如果还有精力的话，补一些使用Keras的陷阱，没精力这部分就留到番外篇了。理解这些特性对深入了解Keras有比较重要的帮助。&lt;/p&gt;&lt;h2&gt;callable，全部Layer都要callable!&lt;/h2&gt;&lt;p&gt;Keras的一大性质是&lt;b&gt;所有的layer对象都是callable的&lt;/b&gt;。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。比方说你想算算一个向量x的sigmoid值是多少，如果用keras的话，你可以这样写：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import keras.backend as K
from keras.layers import Activation
import numpy as np

x = K.placeholder(shape=(3,))
y = Activation('sigmoid')(x)
f = K.function([x],[y])
out = f([np.array([1,2,3])])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;很显然，我绝对不会写这种代码来求sigmoid，这个例子只是说明，可以这么干，而可以这么干的话很多工作就会很方便。比方说有一次我想给目标函数加一项全变差的正则项，而全变差可以用特定的卷积来实现， 那么我的目标函数的全变差正则项完全就可以用一个Convolution2D层来实现。把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。&lt;/p&gt;&lt;p&gt;顺便我们也复习一下上一篇文章说的符号式计算方法。正文第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置
“的符号，翻译成中文就是”此处应有一个长为3的向量“。注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面
又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation('sigmoid')’是被当做一个函数来使用的。上篇文章说层就是
张量到张量的运算，那么其输出y自然也是一个张量。&lt;/p&gt;&lt;p&gt;第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。&lt;/p&gt;&lt;p&gt;之前说了，&lt;b&gt;模型也是张量到张量的映射，所以Layer是Model的父类&lt;/b&gt;，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。&lt;/p&gt;&lt;h2&gt;Node：Keras的网络层复用 &lt;/h2&gt;&lt;p&gt;Keras的网络层复用是一个很常用的需求，例如当某一层与多个层相连时，实际上这层是将同一种计算方式复用多次。再比如你用一个网络来抽取两条微博的特征，然后在后面用网络来判断二者是否是同一个主题，那么抽取两次微博的特征这一工作就可以复用同一个网络。&lt;/p&gt;&lt;p&gt;Keras的网络复用由一个叫“Node”，或称“计算节点”的东西来实现。笼统地说，每当在某个输入上调用层时，就会为网络层添加一个节点。这个节点将输入张量映射为输出的张量，当你多次调用该层，就会产生多个结点，结点的下标是0,1,2,3...&lt;/p&gt;&lt;p&gt;如果仅仅是这样，这部分的东西你依然不需要了解，问题在于，当一个层有多个计算节点时，它的input，output，input_shape，output_shape等属性可能是ill-defined的，因为不清楚你想要的output或input是哪一个。&lt;/p&gt;&lt;p&gt;此时，需要使用get_output_at()，get_input_at()，get_output_shape_at()等以at为后缀结尾的函数，at的对象就是层的节点编号。例如get_output_shape_at(2)就会返回第3个输出张量的shape。 &lt;/p&gt;&lt;h2&gt;Shape与Shape自动推断&lt;/h2&gt;&lt;p&gt;使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。&lt;/p&gt;&lt;p&gt;Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现，如果大家还记得上一篇文章的话，在提到如何编写自己的Keras层时，我们提到如果你的网络层改变了输入张量的shape，就应该复写get_output_shape_for这个函数，以使后面的层能知道本层输出的shape。&lt;/p&gt;&lt;p&gt;在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。&lt;/p&gt;&lt;p&gt;然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。&lt;/p&gt;&lt;p&gt;一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个224*224*3的彩色图片，在内部运行时数据的shape是(None，224，224，3)，这点在你自己编写层是需要注意。&lt;/p&gt;&lt;h2&gt;TH与TF的相爱相杀&lt;/h2&gt;&lt;p&gt;相爱没有，全是相杀&lt;/p&gt;&lt;p&gt;Keras提供了两套后端，Theano和Tensorflow，这是一件幸福的事，手中拿着馒头，想蘸红糖蘸红糖，想蘸白糖蘸白糖th和tf的大部分功能都被backend统一包装起来了，但二者还是存在不小的冲突，有时候你需要特别注意Keras是运行在哪种后端之上，它们的主要冲突有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;dim_ordering，也就是维度顺序。比方说一张224*224的彩色图片，theano的维度顺序是(3，224，224)，即通道维在前。而tf的维度顺序是(224，224，3)，即通道维在后。dim_ordering&lt;b&gt;不是&lt;/b&gt;一个必须和后端搭配的指标，它只规定了输入图片的维度顺序，只要输入图片按此维度顺序设置即可正确运行。然而，如果dim_ordering与后端搭配的话——我指的是所有层的dim_ordering都与后端搭配，会提高程序的运行效率。否则，数据的shape会在计算过程中不断转来转去，效率会低一些。&lt;/li&gt;&lt;li&gt;卷积层权重的shape：从无到有训练一个网络，不会有任何问题。但是如果你想把一个th训练出来的卷积层权重载入风格为tf的卷积层……说多了都是泪。我一直觉得这个是个bug，数据的dim_ordering有问题就罢了，为啥卷积层权重的shape还需要变换咧？我迟早要提个PR把这个bug修掉！&lt;/li&gt;&lt;li&gt;然后是卷积层kernel的翻转不翻转问题，这个我们说过很多次了，就不再多提。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总而言之，相爱没有，全部都是相杀。尽管Keras已经在统一theano和tensorflow上走了很多很多步，但还需要走更多的一些步。&lt;/p&gt;&lt;h2&gt;FAQ与学习资料&lt;/h2&gt;&lt;p&gt;FAQ模块请参考这里：&lt;a data-title="FAQ - Keras中文文档" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/getting_started/FAQ/"&gt;FAQ - Keras中文文档&lt;/a&gt;&lt;/p&gt;&lt;p&gt;另外，在这里有一些使用示范，包括与TensorFlow的联动，分布式训练等：&lt;/p&gt;&lt;p&gt;&lt;a data-title="CNN眼中的世界" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/blog/cnn_see_world/"&gt;CNN眼中的世界&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="花式自动编码器" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/autoencoder/"&gt;花式自动编码器&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="面向小数据集构建图像分类模型" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/image_classification_using_very_little_data/"&gt;面向小数据集构建图像分类模型&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="在Keras模型中使用预训练的词向量" data-editable="true" href="http://keras-cn.readthedocs.io/en/latest/blog/word_embedding/"&gt;在Keras模型中使用预训练的词向量&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a data-title="将Keras作为tensorflow的精简接口" data-editable="true" class="" href="http://keras-cn.readthedocs.io/en/latest/blog/keras_and_tensorflow/"&gt;将Keras作为tensorflow的精简接口&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class="" data-title="Keras/Python深度学习中的网格搜索超参数调优（附源码）" data-editable="true" href="http://geek.csdn.net/news/detail/95494"&gt;Keras/Python深度学习中的网格搜索超参数调优（附源码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Keras已经开始组建自己的Keras Zoo，也就是预训练的模型库，这个github在：&lt;/p&gt;&lt;p&gt;&lt;a data-title="GitHub - Keras Zoo" data-editable="true" class="" href="https://github.com/fchollet/deep-learning-models"&gt;GitHub - fchollet/fchollet/deep-learning-models&lt;/a&gt;&lt;/p&gt;&lt;p&gt; 关于我们老提的TH和TF的卷积核转换，这里是使用示范：&lt;/p&gt;&lt;p&gt;&lt;a data-title="TF kernel - TH kernel" data-editable="true" class="" href="https://github.com/fchollet/keras/wiki/Converting-convolution-kernels-from-Theano-to-TensorFlow-and-vice-versa"&gt;TF kernel - TH kernel&lt;/a&gt;&lt;/p&gt;&lt;p&gt;学习Keras最快的方法莫过于直接阅读示例代码了，这里是example：&lt;/p&gt;&lt;p&gt;&lt;a data-title="keras/examples at master · fchollet/keras · GitHub" data-editable="true" class="" href="https://github.com/fchollet/keras/tree/master/examples"&gt;keras examples&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个哥们儿fork的Keras自己搞了一个caffe到keras的模型转换模块，好像不是很完美，不过说不定能凑合用，先放这儿了：&lt;/p&gt;&lt;p&gt;&lt;a data-title="GitHub - MarcBS/keras: Keras' fork with several new functionalities. Caffe2Keras converter, multimodal layers, etc." data-editable="true" class="" href="https://github.com/MarcBS/keras"&gt;GitHub - MarcBS/keras&lt;/a&gt;&lt;/p&gt;&lt;p&gt;如果有什么疑问请留言提问~能回答的我会回答然后贴到这里……回答不来的……我就装没看见了&lt;/p&gt;&lt;p&gt;最近会出一个Keras使用的陷阱集锦，或称防坑指南，如果能做好的话可以作为番外篇~ &lt;/p&gt;&lt;p&gt;好，先这样吧~鞠躬，下台！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22135796&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Tue, 23 Aug 2016 21:57:52 GMT</pubDate></item><item><title>【啄米日常】6：一个不负责任的Keras介绍（中）</title><link>https://zhuanlan.zhihu.com/p/22129301</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b0f672d59ea6c07a2285375fffcd4bfa_r.jpg"&gt;&lt;/p&gt;上回书我们简单的把Keras的框架理了一下，下面我们深入(也不怎么深)具体的模块理一下Keras，主要聊一聊每个模块的具体功能和核心函数&lt;h2&gt;backend：百货商店&lt;/h2&gt;&lt;p&gt;backend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数——当然是对我来说比较有用，毕竟这是一份不怎么负责任的介绍，如果你想找对你有用的函数，就去backend淘一淘吧~：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。&lt;/li&gt;&lt;li&gt;image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。&lt;/li&gt;&lt;li&gt;learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。&lt;/li&gt;&lt;li&gt;int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。&lt;/li&gt;&lt;li&gt;gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 &lt;/p&gt;&lt;h2&gt;models/layers：Keras的核心主题&lt;/h2&gt;&lt;p&gt;使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为泛型模型。&lt;/p&gt;&lt;p&gt;模型不但是张量的计算方式，还是层对象的容器，模型用来将它所含有的层整合起来，大家手拉手一起走&lt;/p&gt;&lt;p&gt;模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。&lt;/li&gt;&lt;li&gt;fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他的函数请自己学习。 &lt;/p&gt;&lt;p&gt;另外，模型还有几个常用的属性和函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;layers：该属性是模型全部层对象的列表，是的就是一个普通的python list&lt;/li&gt;&lt;li&gt;get_layer()：这个函数通过名字来返回模型中某个层对象&lt;/li&gt;&lt;li&gt;pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有，这些有用的属性稍后介绍。&lt;/p&gt;&lt;p&gt;Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，Keras还有一些不是那么烂大街的东西：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下&lt;/li&gt;&lt;li&gt;Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。&lt;/li&gt;&lt;li&gt;Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。&lt;/li&gt;&lt;li&gt;Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的&lt;/li&gt;&lt;li&gt;Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） &lt;/li&gt;&lt;li&gt;Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。&lt;/li&gt;&lt;li&gt;stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Keras的层对象还有一些有用的属性和方法，比较有用的是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。&lt;/li&gt;&lt;li&gt;trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。&lt;/li&gt;&lt;li&gt;input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用&lt;/li&gt;&lt;li&gt;get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;既然是核心主题，我们就多唠两句，在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。前两天群里有朋友想编写一个卷积核大小不一样的卷积层（虽然不知道为啥他这么想不开……活着不好吗？），这个显然就要自己编写层了。 &lt;/p&gt;&lt;p&gt;要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。&lt;/li&gt;&lt;li&gt;call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。&lt;/li&gt;&lt;li&gt;get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实也不难~是吧，不要忘记Keras是基于Python的框架，你可以随时随地查看那些已经写好的层的代码，模仿着看看你自己的层要怎么写~&lt;/p&gt;&lt;h2&gt;优化器，目标函数，初始化策略，等等... &lt;/h2&gt;&lt;p&gt;和model，layers这种核心功能相比，这些模块的重要性就没有那么大，我们简单介绍一下，里面的具体技术，（下）篇可能会说，也可能不会……我还没想好，但是基本上不说也没什么影响&lt;/p&gt;&lt;p&gt;objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成（记得上篇我就提过了）&lt;/p&gt;&lt;p&gt;optimizers是优化器，没什么可说了，如何选用合适的优化器不在本文讨论范畴。注意模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中。 另外，最新版本的Keras为所有优化器额外设置了两个参数clipnorm和clipvalue，用来对梯度进行裁剪。&lt;/p&gt;&lt;p&gt;activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数，如果想使用高级激活函数，请翻到高级激活函数层。&lt;/p&gt;&lt;p&gt;callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。回调函数LearningRateScheduler支持按照用户的策略调整学习率，做模型精调或研究优化器的同学可能对这个感兴趣。&lt;/p&gt;&lt;p&gt;值得注意的是，&lt;b&gt;History是模型训练函数fit的返回值&lt;/b&gt;，也就是说即使你没有使用任何回调函数，找一个变量接住model.fit()，还是能得到不少训练过程中的有用信息。&lt;/p&gt;&lt;p&gt;另外，回调函数还支持将信息发送到远程服务器，以及与Tensorflow的tensorboard联动，在网页上动态展示出训练和测试的情况（需要使用tensorflow为后端）&lt;/p&gt;&lt;p&gt;回调函数支持用户自定义，定义方法也非常简单，请参考文档说明编写即可&lt;/p&gt;&lt;p&gt;初始化方法，正则项，约束项，可视化没有什么特别值得注意的，就略过了。Keras中所有的模块都是可以用户自己定义的，这就是开源和Python的魅力，讲真你让我拿C++写这么个东西……我光把结构摸清楚就要吐血了！&lt;/p&gt;&lt;p&gt;另一个文档中没有但实际上有的东西是metrices，这里面定义了一系列用于评价模型的指标，例如“accuracy”。在训练模型时，可以选择一个或多个指标来衡量模型性能。&lt;/p&gt;&lt;h2&gt;数据预处理和utils&lt;/h2&gt;&lt;p&gt;数据预处理是Keras提供的用于预处理图像、文本和序列数据的一套工具，这个地方就属于各回各家各找各妈了，你处理什么问题就从这里面找什么工具。&lt;/p&gt;&lt;p&gt;特别指出的是，数据预处理的图像预处理部分，提供了一套用于实时图像数据提升的工具，这个东西支持用各种各样的方法对输入图像进行数据提升，然后以生成器的形式返回。另外，该工具还支持从文件夹中自动生成数据和标签，简直方便的不要不要的。&lt;/p&gt;&lt;p&gt;utils文档中没有，里面包含的函数也不必怎么了解，除了两个。一个是说了很多遍的to_catgoraical，另一个是convert_kernel。后者的主要作用是把卷积滤波器的卷积核在th和tf之间互相转换。theano和tensorflow相爱想杀，到处搞对抗。其中之一就是卷积核，卷积这个东西，按照信号与系统（哼，才不会告诉你们我是信号系统助教咧）的定义，是翻转-&amp;gt;平移-&amp;gt;相乘-&amp;gt;相加。但反正卷积网络的卷积核都是训练出来的，翻转不翻转有什么关系？&lt;/p&gt;&lt;p&gt;所以有些人没翻转，有些人翻转了。是的，说的就是你俩，theano和tensorflow。于是如果一个网络预训练权重是由其中一种后端训练出来的，又要在另一种后端上跑，那么你就需要用kernel_convert这个函数搞一搞了。&lt;/p&gt;&lt;p&gt;估计这事儿太不地道，作者也看不下去了。现在utils出了一个新的layer_utils，里面有一个convert_all_kernels_in_model函数，用来把一个模型的所有卷积核全部进行转换，以后就用这个吧~&lt;/p&gt;&lt;h2&gt;与scikit-learn联动 &lt;/h2&gt;&lt;p&gt;上一篇有人留言说希望多讲点这块的内容，很抱歉……我……我也不怎么会，原谅我毕竟是一只菜鸡&lt;/p&gt;&lt;p&gt;虽然不怎么会，但是不妨碍我知道这应该是一个非常重要和有潜力的功能。Keras与scikit-learn的协作通过keras.wrapper实现，在这个脚本里定义了两个类，分别是KerasClassifier和KerasRegressor，搭建好Sequential模型（只能是Sequential）将被它们包装为sklearn的分类器和迭代器加入的sklearn的工作流中。&lt;/p&gt;&lt;p&gt;这里有一个使用sklearn对Keras进行超参数调整的例子，大家可以参考这篇文章学习Keras和sklearn的联动：&lt;a class="" data-title="Keras/Python深度学习中的网格搜索超参数调优（附源码）" data-editable="true" href="http://geek.csdn.net/news/detail/95494"&gt;Keras/Python深度学习中的网格搜索超参数调优（附源码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;中篇就到这里，下篇来介绍Keras的实现原理/原则，常见问题与解答，以及Keras中比较隐蔽和诡异的坑。可能会过一段时间才发哟~最近还是略忙&lt;/p&gt;&lt;p&gt;另外你们觉得我这种菜鸡能找到啥工作啊有没有内推什么的求往脸上砸！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22129301&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Tue, 23 Aug 2016 14:07:32 GMT</pubDate></item><item><title>【啄米日常】5：一个不负责任的Keras介绍（上）</title><link>https://zhuanlan.zhihu.com/p/22129946</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/b9e25c524119f16f2376979d3cbf5483_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;如果很长时间不更专栏的话，会不会给大家一种本菜鸡平常不怎么学习的错(zhen)觉(xiang)？&lt;/p&gt;&lt;p&gt;还是稍微更一更吧，听说知乎最近引进了打赏功能……是吧&lt;/p&gt;&lt;p&gt;准备分上下两篇，也可能是上中下三篇，来做一个（不）负责任的Keras介绍~上篇主要从宏观的角度来讲，属于面向新手的。中/下篇细节一点，偏重原理/特色。&lt;/p&gt;&lt;p&gt;另外，最近准备做一个Keras使用过程中常见坑的踩法，如果大家在Keras使用过程中遇到过什么坑，不妨私信告我哦。好的，开始了&lt;/p&gt;&lt;h2&gt;Keras：宏观特性 &lt;/h2&gt;&lt;p&gt;Keras是最近蒸蒸日上的深度学习框架， 非常的蒸蒸日上，5月的时候有这么个图：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="462" data-rawwidth="690" src="c15027968033d49109d03f2bd82c3069.jpg"&gt;Caffe是老牌选手，Tensorflow有个神爹，不跟这俩比Keras的表现还是十分亮眼的，我想现在如果有排名也会一样出色（注意mxnet是万年老5虽然我一直觉得mxnet其实非常出色……）&lt;/p&gt;&lt;p&gt;那么，Keras有啥特点呢，我想下面这些可能是属于Keras的关键词：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;符号主义&lt;/li&gt;&lt;li&gt;Python &lt;/li&gt;&lt;li&gt;快速原型&lt;/li&gt;&lt;li&gt;轻量级，高度模块化 &lt;/li&gt;&lt;li&gt;易扩展&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Keras事实上是一个基于Theano和Tensorflow上的一个包装，所谓站在巨人的肩膀上也不外如是了。 因为Theano和Tensorflow都是符号主义的东西（下面讲再说这是啥），因此Keras自然也是符号主义的。&lt;/p&gt;&lt;p&gt;Keras由纯Python编写，这意味着它的源代码简单易懂，你可以随时进去看看它都做了什么，怎么做的。并且，当你需要修改源代码的时候，大胆修改就可以了，它会立刻生效。尽管Python的运行效率要低于C++，但Keras只是Tensorflow和Theano的包装而已，这层包装的运行代价是很小的。&lt;/p&gt;&lt;p&gt;Keras是一款高度模块化的框架，使用它搭建网络和训练网络将会非常容易，如果你需要深入模型中控制细节，通常使用Keras提供的一些函数就可以了，很少需要深入到Tensorflow或Theano那一层。因此，Keras适合于快速原型生成，如果你经常需要很快的实现一下自己的idea，Keras会是一个不错的选择。&lt;/p&gt;&lt;p&gt;另外，Keras的预训练模型库也在逐步建设，目前有VGG-16，VGG-19，resnet50，Inceptionv3四种预训练好的模型供大家使用。 &lt;/p&gt;&lt;h2&gt;计算图，符号主义和张量&lt;/h2&gt;&lt;p&gt;符号主义，Google一下会发现是一个机器学习的名词，但我们这说的符号主义不是那个东西，这里说的符号主义，指的是使用&lt;b&gt;符号式编程&lt;/b&gt;的一种方法。 另一种相对的方法是&lt;b&gt;命令式编程&lt;/b&gt;。或者是&lt;/p&gt;&lt;p&gt;要说Theano/Tensorflow/Keras，就不能不提它的符号主义特性&lt;/p&gt;&lt;p&gt;事实上，Theano也好，Tensorflow也好，其实是一款符号主义的计算框架，未必是专为深度学习设计的。假如你有一个与深度学习完全无关的计算任务想运行在GPU上，你完全可以通过Theano/Tensorflow编写和运行。&lt;/p&gt;&lt;p&gt;假如我们要求两个数a和b的和，通常只要把值赋值给a和b，然后计算a+b就可以了，正常人类都是这么写的：&lt;/p&gt;&lt;p&gt;a=3b=5z = a + b &lt;/p&gt;&lt;p&gt;运行到第一行，a真的是3.运行到第2行，b真的是5，然后运行第三行，电脑真的把a和b的值加起来赋给z了。&lt;/p&gt;&lt;p&gt;一点儿都不神奇。 &lt;/p&gt;&lt;p&gt;但总有不正常的，不正常的会这么想问题：a+b这个计算任务，可以分为三步。（1）声明两个变量a，b。建立输出变量z（2）确立a，b和z的计算关系，z=a+b（3）将两个数值a和b赋值到变量中，计算结果z&lt;/p&gt;&lt;p&gt;后面那种“先确定符号以及符号之间的计算关系，然后才放数据进去计算”的办法，就是符号式编程。当你声明a和b时，它们里面是空的。当你确立z=a+b的计算关系时，a，b和z仍然是空的，只有当你真的把数据放入a和b了，程序才开始做计算。&lt;/p&gt;&lt;p&gt;符号之间的运算关系，就称为运算图。 &lt;/p&gt;&lt;p&gt;这样做当然不是闲的无聊，符号式计算的一大优点是，当确立了输入和输出的计算关系后，在进行运算前我们可以对这种运算关系进行自动化简，从而减少计算量，提高计算速度。另一个优势是，运算图一旦确定，整个计算过程就都清楚了，可以用内存复用的方式减少程序占用的内存。&lt;/p&gt;&lt;p&gt;在Keras，theano和Tensorflow中，参与符号运算的那些变量统一称作张量。张量是矩阵的进一步推广。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;规模最小的张量是0阶张量，即标量，也就是一个数。&lt;/p&gt;&lt;p&gt;当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量&lt;/p&gt;&lt;p&gt;如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵&lt;/p&gt;&lt;p&gt;把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体&lt;/p&gt;&lt;p&gt;把矩阵摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;一言以蔽之，Keras的计算过程，就是建立一个从&lt;b&gt;张量到张量的映射函数&lt;/b&gt;，然后再放入真实数据进行计算。对深度学习而言，这个“映射函数”就是一个神经网络，而神经网络中的每个层自然也都是从张量到张量的映射。&lt;/p&gt;&lt;h2&gt;Keras框架结构&lt;/h2&gt;&lt;p&gt;我想画一个图，可是想了半天画不明白……我就罗列就好了，Keras的结构大致是这样的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;backend：后端，对Tensorflow和Theano进行封装，完成低层的张量运算、计算图编译等&lt;/li&gt;&lt;li&gt;models：模型，模型是层的有序组合，也是层的“容器”，是“神经网络”的整体表示&lt;/li&gt;&lt;li&gt;layers：层，神经网络的层本质上规定了一种从输入张量到输出张量的计算规则，显然，整个神经网络的模型也是这样一种张量到张量的计算规则，因此keras的model是layer的子类&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的三个模块是Keras最为要紧和核心的三块内容，搭建一个神经网络，就只用上面的内容即可。注意的是，backend虽然很重要，但其内容多而杂，大部分内容都是被其他keras模块调用，而不是被用户直接使用。所以它不是新手马上就应该学的，初学Keras不妨先将backend放一旁，从model和layers学起。&lt;/p&gt;&lt;p&gt;为了训练神经网络，必须定义一个神经网络优化的目标和一套参数更新的方式，这部分就是目标函数和优化器：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;objectives：目标函数，规定了神经网络的优化方向&lt;/li&gt;&lt;li&gt;optimizers：优化器，规定了神经网络的参数如何更新&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的两个模块的内容，是在训练一个网络时必须提供的。此外，Keras提供了一组模块用来对神经网络进行配置：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;initialization：初始化策略，规定了网络参数的初始化方法&lt;/li&gt;&lt;li&gt;regularizers：正则项，提供了一些用于参数正则的方法，以对抗过拟合&lt;/li&gt;&lt;li&gt;constraints：约束项，提供了对网络参数进行约束的方法&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了方便调试、分析和使用网络，处理数据，Keras提供了下面的模块：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;callbacks：回调函数，在网络训练的过程中返回一些预定义/自定义的信息&lt;/li&gt;&lt;li&gt;visualization：可视化，用于将网络结构绘制出来，以直观观察&lt;/li&gt;&lt;li&gt;preprocessing：提供了一组用于对文本、图像、序列信号进行预处理的函数 &lt;/li&gt;&lt;li&gt;utils：常用函数库，比较重要的是utils.np_utils中的to_categorical，用于将1D标签转为one-hot的2D标签和convert_kernel函数，用于将卷积核在theano模式和Tensorflow模式之间转换。最新的代码显示utils的utils.layer_utils里提供了将模型中全部卷积核进行模式转换的函数。大部分其他utils的函数你或许很难用到，但有空不妨一读，或有进益。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，为了能让用户一上手就能跑一些模型，Keras提供了一个常用数据库的模块，用来载入常用的数据库：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;datasets：提供了一些常用数据库的接口，用户将通过这些接口下载和载入数据集&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;额外的一点是，如果用户希望将Keras与scikit-learn联动，Keras也提供了这种联动机制，这个模块是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; wrappers.scikit-learn &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;嗯，这个篇幅可以了，这篇先这样。中篇我们把各个模块的东西详细点儿聊一聊，下篇……写着再看，但肯定有下篇的&lt;/p&gt;&lt;p&gt;谢谢大家~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22129946&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Mon, 22 Aug 2016 22:49:57 GMT</pubDate></item><item><title>【啄米日常】4：快来围观Keras的“keras zoo”</title><link>https://zhuanlan.zhihu.com/p/21868244</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/5b3ccc91dbf5fa01097674283bdd805e_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;Caffe 之所以这么流行，我看一大半的功劳要归功于“caffe zoo”。自己编写或训练一个网络总是困难的，而在别人网络的基础上finetune就爽太多了。&lt;/p&gt;&lt;p&gt;Caffe最先出来-&amp;gt;很多研究成果用Caffe实现-&amp;gt;壮大了Caffe Zoo-&amp;gt;follower从caffe zoo下载模型做新的研究-&amp;gt;进一步壮大caffe zoo&lt;/p&gt;&lt;p&gt;良性循环~&lt;/p&gt;&lt;p&gt;Keras终于觉醒啦！&lt;/p&gt;&lt;p&gt;就在&lt;b&gt;十几个小时&lt;/b&gt;之前，fchollet新开了一个github仓库，“deep learning models”，据介绍，这个仓库将收录很多流行网络的&lt;b&gt;Keras实现&lt;/b&gt;和&lt;b&gt;预训练权重&lt;/b&gt;，说白了就是个Keras Zoo了&lt;/p&gt;&lt;p&gt;目前收录的网络有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;VGG-16&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;VGG-19&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Resnet50 &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;咦，等等……好像混进去什么了不得的东西！&lt;/p&gt;&lt;p&gt;没错！本菜鸡贡献的50层残差网络在十几个小时之前被merge到Keras中了，而且自恋的想一想，可能正是这个50层残差网络使fchollet神起了建立一个“keras zoo”的念头，然后创建了这个仓库。&lt;/p&gt;&lt;p&gt;啊……好激动，比PR被merge还激动，菜鸡原来也有能改写历史（并没有）的一天吗？&lt;/p&gt;&lt;p&gt;今晚吃顿好的奖励一下自己~&lt;/p&gt;&lt;p&gt;对了，fchollet神的这个版本基于我的代码进行了补充，添加了各种说明，运用了各种花式代码技巧——这些都不是最重要的，最重要的是fchollet神解决了困扰我好久的问题，就是对theano后端的支持。&lt;/p&gt;&lt;p&gt;&lt;b&gt;现在，resnet支持tensorflow和theano两种后端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Keras Zoo的下一步工作是添加Inception V3网络，欢迎大家持续围观，下载使用！&lt;/p&gt;&lt;p&gt;对了，github地址在这里：&lt;a class="" href="https://github.com/fchollet/deep-learning-models"&gt;https://github.com/fchollet/deep-learning-models&lt;/a&gt;&lt;/p&gt;&lt;p&gt;鞠躬！&lt;/p&gt;&lt;p&gt;（你们说要不要把那个152层的大家伙也弄进去呢……还有一个1000层的咧~）&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21868244&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 04 Aug 2016 16:21:58 GMT</pubDate></item><item><title>【啄米日常】3：Keras残差网络的修正和测试</title><link>https://zhuanlan.zhihu.com/p/21672785</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d06f2ddf136eff0a70e35a6daa2e652d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;好像是&lt;a data-title="@王峰" data-editable="true" class="member_mention" href="https://www.zhihu.com/people/085dfee88ffb3c4224c6ff7a60649d8e" data-hash="085dfee88ffb3c4224c6ff7a60649d8e" data-tip="p$b$085dfee88ffb3c4224c6ff7a60649d8e"&gt;@王峰&lt;/a&gt; 大大给我点了个赞……然后本小透明的专栏就呼啦呼啦涨了好多粉！&lt;/p&gt;&lt;p&gt;开心，并且忐忑，因为我只是一只菜鸡啊！&lt;/p&gt;&lt;p&gt;以及，上一篇用Keras搭建深度残差网络的工作，其实我自己都还没测试过！这不是坑人吗！&lt;/p&gt;&lt;p&gt;然后前几天教研室断网，GPU电脑崩溃，急着补测试都没机会。今天搞了一下午搞通了，亡羊补牢为时未晚，希望被我坑的人还不算多……&lt;/p&gt;&lt;p&gt;根据&lt;a data-title="@徐拉达" data-editable="true" class="member_mention" href="https://www.zhihu.com/people/42b4b091fb2e7afd7e5585b264868c62" data-hash="42b4b091fb2e7afd7e5585b264868c62" data-tip="p$b$42b4b091fb2e7afd7e5585b264868c62"&gt;@徐拉达&lt;/a&gt; 提出的意见，我发现了自己代码里的一些漏洞，主要是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;太专注于处理res开头的layer，把第一个stage的卷积层忘记载入了，这个卷基层的名字叫‘conv1’，刚好不是‘res’开头……汗&lt;/li&gt;&lt;li&gt;同样的原因，‘fc1000’这层的权重也忘记载入了，因此下面的这个错误没有发现&lt;/li&gt;&lt;li&gt;因为上面全连接层忘记载入权重的原因，没有发现这个网络从stage3开始，conv_block的主路和辅路的卷积层都有一个（2，2）的subsample，也就是通常说的strides。如果载入‘fc1000’权重的话显然因为忽略这个strides的原因，会出现维度不匹配的问题&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;主要是这三个问题，全部fix掉以后，写一个读取图片和预处理的函数，然后从某个github仓库拿了一份有分类信息的文本，就可以进行测试了。测试图片是一只猫咪&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="536" data-rawwidth="536" src="ef43e88e97b8c289247e7c76f13039ab.jpg"&gt;还有一架客机：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="496" data-rawwidth="682" src="b20efbdc98f49a89f218450051edc331.jpg"&gt;图像预处理的步骤大概是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将图片按照较短边进行resize，使得其短边是256，长边按比例缩放&lt;/li&gt;&lt;li&gt;将resize后的图片从中间crop出一个（224，224）的区域&lt;/li&gt;&lt;li&gt;图像各通道减去ImageNet的均值&lt;/li&gt;&lt;li&gt;RGB通道交换R和B，变为BGR（Caffe的图像格式，好像不变问题也不大）&lt;/li&gt;&lt;li&gt;转换（224，224，3）为（3，224，224）&lt;/li&gt;&lt;li&gt;测试的话，要扩充一个维度，使得最后的维度变为（1，3，224，224）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然后在predict后，对得到的vector进行argmax，求出最大点下标，然后查分类表输出该下标数字对应的文字类别&lt;/p&gt;&lt;p&gt;最后的结果是：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;result for test 1 is&lt;/p&gt;&lt;p&gt;n02123045 tabby, tabby cat&lt;/p&gt;&lt;p&gt;result for test 2 is &lt;/p&gt;&lt;p&gt;n02690373 airliner&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; 以上结果为本人看着截图手打，敲错一两个字母就不要在意了&lt;/p&gt;&lt;p&gt;另外，根据Keras作者fchollet大神的说法，tensorflow backend和theano backend的卷积核的形式是不一样的，特此注明，这个预训练权重适用于tensorflow的backend。在把这个example提交给Keras时fchollet神有让我转一个theano backend的权重，我转了，没成功……测试的结果不对&lt;/p&gt;&lt;p&gt;所以先这样吧，反正theano我看也是快要死了的东西，大家就欢快的用tensorflow吧！&lt;/p&gt;&lt;p&gt;请在这里下载最新的脚本，预训练权重的下载地址在脚本上面的注释里。 &lt;/p&gt;&lt;p&gt;&lt;a class="" href="https://github.com/MoyanZitto/keras-scripts"&gt;https://github.com/MoyanZitto/keras-scripts&lt;/a&gt;&lt;/p&gt;&lt;p&gt;再次鸣谢&lt;a class="member_mention" href="https://www.zhihu.com/people/085dfee88ffb3c4224c6ff7a60649d8e" data-hash="085dfee88ffb3c4224c6ff7a60649d8e" data-tip="p$b$085dfee88ffb3c4224c6ff7a60649d8e"&gt;@王峰&lt;/a&gt;大大给涨粉，以及鸣谢&lt;a class="member_mention" href="https://www.zhihu.com/people/42b4b091fb2e7afd7e5585b264868c62" data-hash="42b4b091fb2e7afd7e5585b264868c62" data-tip="p$b$42b4b091fb2e7afd7e5585b264868c62"&gt;@徐拉达&lt;/a&gt;的关键性指导意见&lt;/p&gt;&lt;p&gt;就这样~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21672785&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 21 Jul 2016 17:59:48 GMT</pubDate></item><item><title>【啄米日常】2：使用keras搭建残差网络</title><link>https://zhuanlan.zhihu.com/p/21586417</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/0acfc6d78ecbfe888ddd0ed5440dfce1_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;开了专栏我就不写文章了……&lt;/p&gt;&lt;p&gt;因为发现这个专栏真的很多余……知乎上这么多大神，怎么轮得到我这只菜鸡讲技术呢……但是空着又不好，我硬着头皮写吧&lt;/p&gt;&lt;p&gt;今天的文章是使用keras搭建残差网络，没搞错的话，应该是全球首发……文章发在我的CSDN博客上（所以说这个专栏开的多余），但是要审核好久好久，而且也算一个比较有意义的工作，我简单的把文章内容总结到这里，全文请移步CSDN博客。【实话说吧就是为了充数哈哈哈哈！】&lt;/p&gt;&lt;p&gt;MSRA的Kaiming 
He的残差网络绝对要算的上是2015年深度学习的大新闻了，深度残差网络将深度网络的层深拓展到一个不可思议的程度（152层），并在ImageNet
竞赛中以绝对优势取得多个项目的冠军。后来，Kaiming 
He大神后来又把这一深度加到1000层的深度，能训练这么深的网络，残差网络的优势可见一斑。Kaiming He也因为这项工作获得了2016 
CVPR的best paper award，这也是他获得的第二个CVPR best paper，顺便，据知乎爆料，Kaiming 
He还是广东省某年的高考状元哟！ 果然神的成长路径我们只能仰望……&lt;/p&gt;&lt;p&gt;这篇文章的主要目的是提供一个Keras的深度残差网络的实现，先大致回顾一下深度残差网络，基本上就是Highway的一个特殊情况，通过跨层连接使网络的学习目标变为最小化残差，一图胜千言：&lt;img rel="noreferrer" data-rawheight="269" data-rawwidth="453" src="358f29d5d8fab3ed6bea88ca7075f3a9.png"&gt;要使用Keras实现这个，毫无疑问肯定是用泛型模型Model了，首先先看看kaiming he的github提供的模型结构图&lt;a data-title="res50" data-editable="true" href="ethereon.github.io/netscope/#/gist/db945b393d40bfa26006"&gt;res50&lt;/a&gt;，这个模型有两种结构模块，一种是主路辅路都有卷积的，这种模块称为conv_block吧，另一种是主路带卷积辅路不带卷积的，这种称为identity_block好了，两种结构我们继续上图，图上的多了文章不知不觉的就长了！&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="588" data-rawwidth="321" src="ab8ddd6f4efd4635222211443c72de6d.png"&gt;&lt;img rel="noreferrer" data-rawheight="621" data-rawwidth="268" src="7b95380d6bec5e74a3253ce5ff0fd724.png"&gt;整个网络就是这两个模块的来回重复，从结构上看，res50分为5个stage，每个stage由一个conv_block领起，后面跟若干个identity_block，差不多就是这样：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" data-rawheight="600" data-rawwidth="800" src="2433d434fd9ca69e30ea734fb3acb1f5.png"&gt;所以只要写两个实现两种block的函数，反复调用即可搭建网络，本文只是摘要，就不贴代码了，说一下搭建网络要注意的几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;注意每个stage卷积核的数目不同，因此函数应该接收一个参数来指定主path的各个卷积层有多少个核&lt;/li&gt;&lt;li&gt;注意conv_block的辅路的卷积核个数始终是与主路的第三个卷积层是一样的，否则两个tensor加不起来（shape不同）&lt;/li&gt;&lt;li&gt;注意主路的第二个卷积层要设定border_mode='same'以保持shape不变，或者在该层之前插入一个ZeroPadding2D进行padding。&lt;/li&gt;&lt;li&gt;注意网络结尾处的池化是AveragePooling，在卷积网络与全连接网络相接时记得插入Flatten把featuremap压扁。&lt;/li&gt;&lt;li&gt;注意在函数中合并的操作要使用keras.layers的merge函数，而不是keras.layers.core的Merge层。&lt;/li&gt;&lt;li&gt;注意从第3个stage开始，conv_block的主路第一个卷积与shortcut的卷积都有一个（2，2）的subsample&lt;/li&gt;&lt;li&gt;注意全连接层载入是权重要转置一下 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt; 好，大概就这么多，顺利完成网络的搭建后，下一步要载入预训练的权重，原文的权重是caffe给出的，所以这部分的工作分为三个步骤 &lt;/p&gt;&lt;ul&gt;&lt;li&gt;将caffemodel文件转为h5文件&lt;/li&gt;&lt;li&gt;将caffe中各层的名字与keras模型的名字匹配&lt;/li&gt;&lt;li&gt;处理keras和caffe的BN层实现不同导致的问题 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;将caffemodel转为h5比较简单，载入caffe后搭建网络，依次读取网络的params.data数据，保存在h5文件中即可。这里注意一点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;原网络中所有的分支上的卷积层都没有偏置项，所以要考虑params.data[1]不存在的情况，会出现超出指标范围错误，建议当params.data[1]不存在时生成全0偏置，虽然会稍微浪费一点存储空间，但对后面的步骤比较有利。 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;转换完成后要进行两个模型的名字匹配，这样才能把权重正确载入，resnet50的命名方法是：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;[type][stage][block]_branch[branch][layer]&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;type：层类型，'res'为卷积层，'bn'为bn层的前半截，'scale'为bn层的后半截&lt;/li&gt;&lt;li&gt;stage：stage号，stage的含义看上面的图&lt;/li&gt;&lt;li&gt;block：一个stage中的block号，依次为'a','b','c'等&lt;/li&gt;&lt;li&gt;branch：shortcut为0，主路为1&lt;/li&gt;&lt;li&gt;layer：具体的layer号，依次为'a','b','c'等 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;按照这个命名习惯可以为网络的大部分层命名，因此在上面实现的conv_block和identity_block中还要传入stage和block的参数，以生成正确的名字&lt;/p&gt;&lt;p&gt;然后处理BN的问题，caffe的BN层将规范化和放缩分为BN和scale两个层，而keras只是单一的BN层，需要根据bn层的名字找到对应的scale层，将两者的权重组合为一个list，再传入BN层中，这里需要注意的点有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Keras的BN层通过set_weights方法将传入的权重设置好，set_weights的实现代码中，weights是trainable_weights+non_trainable_weights，对于BN而言non_trainable-weights是均值和标准差，这部分对应于scale层，trainable_weights是Gamma和Beta，对应于caffe的BN层，因此组合list要把scale的权重放在前面，BN的权重放在后面。这是很难察觉到的一个陷阱，而且放错不会有任何警告和错误。&lt;/li&gt;&lt;li&gt;Keras的BN层通过axis指定要做规范化的轴，原模型的bn层和scale层权重的shape表明，resnet50规范化的轴是channel轴，即第二个轴，所以BN层要指定axis=1，否则会出现维度不匹配无法载入的问题。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上就是整个工作的摘要，如果你发现其中有任何瑕疵或漏洞，欢迎留言告知。&lt;/p&gt;&lt;p&gt;详细过程参考我的CSDN： &lt;a class="" href="http://blog.csdn.net/yichenmoyan"&gt;http://blog.csdn.net/yichenmoyan&lt;/a&gt;&lt;/p&gt;&lt;p&gt;全部代码请参考我的github：&lt;a href="https://github.com/MoyanZitto/keras-scripts"&gt;https://github.com/MoyanZitto/keras-scripts&lt;/a&gt;&lt;/p&gt;&lt;p&gt;转换完毕的h5文件地址和提取码在CSDN文章和github脚本的注释中都有，欢迎下载使用。记得给github加星哦～&lt;/p&gt;&lt;p&gt;CSDN文末有我的面对面收钱二维码，如果你觉得这项工作真的太有用了，请一定用力的给我投食～就不在这贴了&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21586417&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Thu, 14 Jul 2016 01:21:37 GMT</pubDate></item><item><title>【啄米日常】1：keras中文文档发布</title><link>https://zhuanlan.zhihu.com/p/20868721</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ef04deb891576e12c7011d1322003471_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;        作为基于python的最流行的深度学习框架，keras以其快速上手，支持theano/tensorflow无缝切换，文档丰富等若干优点广受好评……额好吧总体来说它还是比较小众就是了。虽然也有很多缺点，但做快速原型真是不错。&lt;/p&gt;        经过一周左右艰苦的【并没有】的工作，本鸡完成了目前keras中文文档翻译，目前的一个初稿放在这里：&lt;a href="http://keras-cn.readthedocs.io/" data-title="keras中文文档" class="" data-editable="true"&gt;keras中文文档&lt;/a&gt;&lt;p&gt;        本鸡毕竟还是一只菜鸡，水平有限的很，许多部分，尤其是递归神经网络和涉及到自然语言处理的部分，可能有一些错漏之处。但好歹架子是搭起来了，如果各位觉得哪里做的不好，或者有哪里可以补充，欢迎知乎私信我。所有的contribution都会被记录在网站的“致谢”一栏里。文档中我也挖了一些“此处应有XXX”的坑，如果你乐意填坑，我也是欢迎之至。&lt;/p&gt;&lt;p&gt;        同时，也很真诚的欢迎大家提供更多的keras使用实例、代码解释以及使用的tips。目前的文档在我看来还比较粗糙，安装本鸡的计划，这个文档的下一步工作要添加更友好的使用建议，澄清相关的深度学习概念。这件事只靠本菜鸡是完不成的，还需要各位鼎立相助~让我们建设一个更NB的keras文档吧！&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;        好了，闲事说完了，说正事。&lt;/p&gt;&lt;p&gt;        一时兴起申了个专栏……其实也没有完全想好要写什么进去&lt;/p&gt;&lt;p&gt;        唉好烦啊，实在没有什么可写的话，写成技术博客好了。本鸡虽然水平有限，但讲东西还是有一套的。同学有云，“我们是学了10分的东西，能讲出7分就不错了。你是学了5分的东西，却能讲出10分”。&lt;/p&gt;&lt;p&gt;       (●´▽｀●)&lt;/p&gt;&lt;p&gt;       同学你憋走，大家一样的鸡为毛你学的是10分老子就只学到5分啊&lt;/p&gt;&lt;p&gt;       唉，总而言之呢，这个技术博客的内容不会很深，很多大概也是老生常谈的东西吧，毕竟太高深的东西我也不会讲，我讲了你也不懂，你懂……别人也不懂。反正放上来的东西我保证你都能看明白就是了。&lt;/p&gt;&lt;p&gt;      看不明白也憋说出来，悄悄私信我，我再改。&lt;/p&gt;&lt;p&gt;      就酱，玩的愉快。&lt;/p&gt;&lt;p&gt;      污码撸~~&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20868721&amp;pixel&amp;useReferer"/&gt;</description><author>BigMoyan</author><pubDate>Mon, 09 May 2016 14:28:09 GMT</pubDate></item></channel></rss>